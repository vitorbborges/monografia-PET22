Publication Type,Authors,Book Authors,Book Editors,Book Group Authors,Author Full Names,Book Author Full Names,Group Authors,Article Title,Source Title,Book Series Title,Book Series Subtitle,Language,Document Type,Conference Title,Conference Date,Conference Location,Conference Sponsor,Conference Host,Author Keywords,Keywords Plus,Abstract,Addresses,Affiliations,Reprint Addresses,Email Addresses,Researcher Ids,ORCIDs,Funding Orgs,Funding Name Preferred,Funding Text,Cited References,Cited Reference Count,"Times Cited, WoS Core","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,Publisher,Publisher City,Publisher Address,ISSN,eISSN,ISBN,Journal Abbreviation,Journal ISO Abbreviation,Publication Date,Publication Year,Volume,Issue,Part Number,Supplement,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,DOI Link,Book DOI,Early Access Date,Number of Pages,WoS Categories,Web of Science Index,Research Areas,IDS Number,Pubmed Id,Open Access Designations,Highly Cited Status,Hot Paper Status,Date of Export,UT (Unique WOS ID),Web of Science Record
C,"Quteineh, H; Samothrakis, S; Sutcliffe, R",,,Assoc Computat Linguist,"Quteineh, Husam; Samothrakis, Spyridon; Sutcliffe, Richard",,,Textual Data Augmentation for Efficient Active Learning on Tiny Datasets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria. We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function. Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset. Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7400,7410,,,,,,,,,,,,,,,,WOS:000855160707047,0
C,"Ranasinghe, T; Zampieri, M",,,Assoc Computat Linguist,"Ranasinghe, Tharindu; Zampieri, Marcos",,,Multilingual Offensive Language Identification with Cross-lingual Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of these studies deal with English partially because most annotated datasets available contain English data. In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources. We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish. Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5838,5844,,,,,,,,,,,,,,,,WOS:000855160706003,0
C,"Shwartz, V; West, P; Le Bras, R; Bhagavatula, C; Choi, Y",,,Assoc Computat Linguist,"Shwartz, Vered; West, Peter; Le Bras, Ronan; Bhagavatula, Chandra; Choi, Yejin",,,Unsupervised Commonsense Question Answering with Self-Talk,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice common-sense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as what is the definition of... to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4615,4629,,,,,,,,,,,,,,,,WOS:000855160704061,0
C,"Smit, A; Jain, S; Rajpurkar, P; Pareek, A; Ng, AY; Lungren, MP",,,Assoc Computat Linguist,"Smit, Akshay; Jain, Saahil; Rajpurkar, Pranav; Pareek, Anuj; Ng, Andrew Y.; Lungren, Matthew P.",,,CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then fine-tuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rule-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.",,,,,,"Rajpurkar, Pranav/0000-0002-8030-3727",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1500,1519,,,,,,,,,,,,,,,,WOS:000855160701057,0
C,"Tian, Z; Zhang, Y; Liu, K; Zhao, J; Jia, Y; Sheng, Z",,,Assoc Computat Linguist,"Tian, Zhixing; Zhang, Yuanzhe; Liu, Kang; Zhao, Jun; Jia, Yantao; Sheng, Zhicheng",,,Scene Restoring for Narrative Machine Reading Comprehension,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3063,3073,,,,,,,,,,,,,,,,WOS:000855160703022,0
C,"Trong, HMD; Le, DT; Ben Veyseh, AP; Nguyen, T; Nguyen, TH",,,Assoc Computat Linguist,"Trong, Hieu Man Duc; Le, Duc Trong; Veyseh, Amir Pouran Ben; Nguyen, Thuat; Nguyen, Thien Huu",,,Introducing a New Dataset for Event Detection in Cybersecurity Texts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5381,5390,,,,,,,,,,,,,,,,WOS:000855160705043,0
C,"Tsakalidis, A; Liakata, M",,,Assoc Computat Linguist,"Tsakalidis, Adam; Liakata, Maria",,,Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time. In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time. Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection. Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8485,8497,,,,,,,,,,,,,,,,WOS:000855160708057,0
C,"Wang, XZ; Wang, ZQ; Han, X; Jiang, WY; Han, R; Liu, ZY; Li, JZ; Li, P; Lin, YK; Zhou, J",,,Assoc Computat Linguist,"Wang, Xiaozhi; Wang, Ziqi; Han, Xu; Jiang, Wangyi; Han, Rong; Liu, Zhiyuan; Li, Juanzi; Li, Peng; Lin, Yankai; Zhou, Jie",,,MAVEN: A Massive General Domain Event Detection Dataset,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4, 480 Wikipedia documents, 118, 732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1652,1671,,,,,,,,,,,,,,,,WOS:000855160701069,0
C,"Wang, YJ; Sun, CZ; Wu, YB; Yan, JC; Gao, P; Xie, GT",,,Assoc Computat Linguist,"Wang, Yijun; Sun, Changzhi; Wu, Yuanbin; Yan, Junchi; Gao, Peng; Xie, Guotong",,,Pre-training Entity Relation Encoder with Intra-span and Inter-span Information,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model. To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs. In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss. Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1692,1705,,,,,,,,,,,,,,,,WOS:000855160701072,0
C,"Wang, YR; Zhai, CX; Awadalla, HH",,,Assoc Computat Linguist,"Wang, Yiren; Zhai, ChengXiang; Awadalla, Hany Hassan",,,Multi-task Learning for Multilingual Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1022,1034,,,,,,,,,,,,,,,,WOS:000855160701015,0
C,"Wang, Y; Li, Y; Tong, H; Zhu, Z",,,Assoc Computat Linguist,"Wang, Yu; Li, Yun; Tong, Hanghang; Zhu, Ziye",,,HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity. To address this issue, we present a novel nested NER model named HIT. Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary. Specifically, we design (1) HeadTail Detector based on the multi-head selfattention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary. Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-ofthe-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6027,6036,,,,,,,,,,,,,,,,WOS:000855160706019,0
C,"Weller, O; Lourie, N; Gardner, M; Peters, ME",,,Assoc Computat Linguist,"Weller, Orion; Lourie, Nicholas; Gardner, Matt; Peters, Matthew E.",,,Learning from Task Descriptions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1361,1375,,,,,,,,,,,,,,,,WOS:000855160701045,0
C,"Wu, HL; Ma, TF; Wu, LF; Manyumwa, T; Ji, SL",,,Assoc Computat Linguist,"Wu, Hanlu; Ma, Tengfei; Wu, Lingfei; Manyumwa, Tariro; Ji, Shouling",,,Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3612,3621,,,,,,,,,,,,,,,,WOS:000855160703069,0
C,"Yordanov, Y; Camburu, OM; Kocijan, V; Lukasiewicz, T",,,Assoc Computat Linguist,"Yordanov, Yordan; Camburu, Oana-Maria; Kocijan, Vid; Lukasiewicz, Thomas",,,Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4963,4969,,,,,,,,,,,,,,,,WOS:000855160705012,0
C,"Bhattamishra, S; Ahuja, K; Goyal, N",,,Assoc Computat Linguist,"Bhattamishra, Satwik; Ahuja, Kabir; Goyal, Navin",,,On the Ability and Limitations of Transformers to Recognize Formal Languages,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7096,7116,,,,,,,,,,,,,,,,WOS:000855160707025,0
C,"Bjerva, J; Bhutani, N; Golshan, B; Tan, WC; Augenstein, I",,,Assoc Computat Linguist,"Bjerva, Johannes; Bhutani, Nikita; Golshan, Behzad; Tan, Wang-Chiew; Augenstein, Isabelle",,,SUBJQA: A Dataset for Subjectivity and Review Comprehension,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and wordsense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We develop a new dataset which allows us to investigate this relationship. We find that subjectivity is an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance than found in previous work on sentiment analysis. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset ( SUBJQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5480,5494,,,,,,,,,,,,,,,,WOS:000855160705052,0
C,"Blloshmi, R; Tripodi, R; Navigli, R",,,Assoc Computat Linguist,"Blloshmi, Rexhina; Tripodi, Rocco; Navigli, Roberto",,,XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting. In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a crosslingual AMR parser, XL-AMR. This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing. The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish. Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages. We release XL-AMR at github.com/SapienzaNLP/xlamr.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2487,2500,,,,,,,,,,,,,,,,WOS:000855160702052,0
C,"Calvillo, J; Fang, L; Cole, J; Reitter, D",,,Assoc Computat Linguist,"Calvillo, JestIs; Fang, Le; Cole, Jeremy; Reitter, David",,,Surprisal Predicts Code-Switching in Chinese-English Bilingual Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese. The model includes known control variables together with word surprisal and word entropy. We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors. We also found sentence length to be a significant predictor, which has been related to sentence complexity. We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language. We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4029,4039,,,,,,,,,,,,,,,,WOS:000855160704018,0
C,"Chen, WH; Su, Y; Yan, XF; Wang, WY",,,Assoc Computat Linguist,"Chen, Wenhu; Su, Yu; Yan, Xifeng; Wang, William Yang",,,KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In this paper, we propose to leverage pre-training and transfer learning to address this issue. We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text. 2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web. The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text. We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness. Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines. Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8635,8648,,,,,,,,,,,,,,,,WOS:000855160708072,0
C,"Chen, WQ; Tian, JD; Xiao, LQ; He, H; Jin, YH",,,Assoc Computat Linguist,"Chen, Wenqing; Tian, Jidong; Xiao, Liqiang; He, Hao; Jin, Yaohui",,,Exploring Logically Dependent Multi-task Learning with Causal Inference,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks. Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks. In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models. We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors. In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL. We conduct experiments on two English datasets and one Chinese dataset. Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions' consistency.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2213,2225,,,,,,,,,,,,,,,,WOS:000855160702030,0
C,"Chiang, CH; Huang, SF; Lee, HY",,,Assoc Computat Linguist,"Chiang, Cheng-Han; Huang, Sung-Feng; Lee, Hung-Yi",,,Pretrained Language Model Embryology: The Birth of ALBERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent(1) language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6813,6828,,,,,,,,,,,,,,,,WOS:000855160707002,0
C,"Cui, BY; Li, YM; Zhang, ZF",,,Assoc Computat Linguist,"Cui, Baiyun; Li, Yingming; Zhang, Zhongfei",,,BERT-enhanced Relational Sentence Ordering Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BERSON) by leveraging BERT for capturing a better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. This enables us to strengthen both local and global dependencies among sentences. Extensive evaluations are conducted on six public datasets. The experimental results demonstrate the effectiveness and promise of BERSON, showing a significant improvement over the state-of-the-art by a wide margin.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6310,6320,,,,,,,,,,,,,,,,WOS:000855160706044,0
C,"Fisher, J; Mittal, A; Palfrey, D; Christodoulopoulos, C",,,Assoc Computat Linguist,"Fisher, Joseph; Mittal, Arpit; Palfrey, Dave; Christodoulopoulos, Christos",,,Debiasing knowledge graph embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7332,7345,,,,,,,,,,,,,,,,WOS:000855160707042,0
C,"Fu, ZH; Shi, B; Lam, W; Bing, LD; Liu, ZY",,,Assoc Computat Linguist,"Fu, Zihao; Shi, Bei; Lam, Wai; Bing, Lidong; Liu, Zhiyuan",,,Partially-Aligned Data-to-Text Generation with Distant Supervision,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data's supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (1) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9183,9193,,,,,,,,,,,,,,,,WOS:000855160709033,0
C,"Groeneveld, D; Khot, T; Mausam; Sabharwal, A",,,Assoc Computat Linguist,"Groeneveld, Dirk; Khot, Tushar; Mausam; Sabharwal, Ashish",,,A Simple Yet Strong Pipeline for HotpotQA,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular multi-hop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named QUARK, performs surprisingly well. Specifically, on HotpotQA, QUARK outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of QUARK resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8839,8845,,,,,,,,,,,,,,,,WOS:000855160709006,0
C,"Hessel, J; Zhu, ZH; Pang, B; Soricut, R",,,Assoc Computat Linguist,"Hessel, Jack; Zhu, Zhenhai; Pang, Bo; Soricut, Radu",,,Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining work has been limited to only instructional videos; a priori, we expect this domain to be relatively easy: speakers in instructional videos will often reference the literal objects/actions being depicted. We ask: can similar models be trained on more diverse video corpora? And, if so, what types of videos are grounded and what types are not? We fit a representative pretraining model to the diverse YouTube8M dataset, and study its success and failure cases. We find that visual-textual grounding is indeed possible across previously unexplored video categories, and that pretraining on a more diverse set results in representations that generalize to both non-instructional and instructional domains.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8812,8822,,,,,,,,,,,,,,,,WOS:000855160709004,0
C,"Jayanthi, SM; Pruthi, D; Neubig, G",,,Assoc Computat Linguist,"Jayanthi, Sai Muralidhar; Pruthi, Danish; Neubig, Graham",,,NeuSpell: A Neural Spelling Correction Toolkit,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use contextual representations. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at neuspell.github.io.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,158,164,,,,,,,,,,,,,,,,WOS:000855177700021,0
C,"Larson, S; Zheng, A; Mahendran, A; Tekriwal, R; Cheung, A; Guldan, E; Leach, K; Kummerfeld, JK",,,Assoc Computat Linguist,"Larson, Stefan; Zheng, Anthony; Mahendran, Anish; Tekriwal, Rishi; Cheung, Adrian; Guldan, Eric; Leach, Kevin; Kummerfeld, Jonathan K.",,,Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows are special cases of our approach, and present a way to apply the approach to dialog tasks such as intent classification and slot-filling. Using our method, we create more challenging versions of test sets from prior dialog datasets and find dramatic performance drops for standard models. Finally, we show that our approach is complementary to recent work on improving data diversity, and training on data collected with our approach leads to more robust models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8097,8106,,,,,,,,,,,,,,,,WOS:000855160708025,0
C,"Le, H; Sahoot, DY; Liu, CH; Chen, NF; Hoi, SCH",,,Assoc Computat Linguist,"Le, Hung; Sahoot, Doyen; Liu, Chenghao; Chen, Nancy F.; Hoi, Steven C. H.",,,UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose UniConv - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the Multi-WOZ2.1 benchmark, achieving superior performance over competitive baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1860,1877,,,,,,,,,,,,,,,,WOS:000855160702003,0
C,"Lei, WQ; Wang, WX; Ma, ZX; Gan, T; Lu, W; Kan, MY; Chua, TS",,,Assoc Computat Linguist,"Lei, Wenqiang; Wang, Weixin; Ma, Zhixin; Gan, Tian; Lu, Wei; Kan, Min-Yen; Chua, Tat-Seng",,,Re-examining the Role of Schema Linking in Text-to-SQL,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking. We also build a simple BERT-based baseline, called Schema-Linking SQL (SLSQL) to perform a data-driven study. We find when schema linking is done well, SLSQL demonstrates good performance on Spider despite its structural simplicity. Many remaining errors are attributable to corpus noise. This suggests schema linking is the crux for the current text-to-SQL task. Our analytic studies provide insights on the characteristics of schema linking for future developments of text-to-SQL tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6943,6954,,,,,,,,,,,,,,,,WOS:000855160707013,0
C,"Lin, A; Wohlwend, J; Chen, H; Lei, T",,,Assoc Computat Linguist,"Lin, Alexander; Wohlwend, Jeremy; Chen, Howard; Lei, Tao",,,Autoregressive Knowledge Distillation through Imitation Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6121,6133,,,,,,,,,,,,,,,,WOS:000855160706027,0
C,"Liu, LY; Liu, XD; Gao, JF; Chen, WZ; Han, JW",,,Assoc Computat Linguist,"Liu, Liyuan; Liu, Xiaodong; Gao, Jianfeng; Chen, Weizhu; Han, Jiawei",,,Understanding the Difficulty of Training Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand what complicates Transformer training from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially-for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5747,5763,,,,,,,,,,,,,,,,WOS:000855160705073,0
C,"Meister, C; Cotterell, R; Vieira, T",,,Assoc Computat Linguist,"Meister, Clara; Cotterell, Ryan; Vieira, Tim",,,"If Beam Search is the Answer, What was the Question?",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results (Stahlberg and Byrne, 2019). Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU. Our code is publicly available at https://github.com/rycolab/uid-decoding.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2173,2185,,,,,,,,,,,,,,,,WOS:000855160702027,0
C,"Omura, K; Kawahara, D; Kurohashi, S",,,Assoc Computat Linguist,"Omura, Kazumasa; Kawahara, Daisuke; Kurohashi, Sadao",,,A Method for Building a Commonsense Inference Dataset based on Basic Events,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9%), the accuracy of a highperformance transfer learning model is reasonably low (76.0%). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2450,2460,,,,,,,,,,,,,,,,WOS:000855160702049,0
C,"Parikh, AP; Wang, XZ; Gehrmann, S; Faruqui, M; Dhingra, B; Yang, DY; Das, D",,,Assoc Computat Linguist,"Parikh, Ankur P.; Wang, Xuezhi; Gehrmann, Sebastian; Faruqui, Manaal; Dhingra, Bhuwan; Yang, Diyi; Das, Dipanjan",,,ToTTo: A Controlled Table-To-Text Generation Dataset,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present TOTTO, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given aWikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also faithful to the source table, we introduce a dataset construction process where annotators directly revise existing candidate sentences from Wikipedia. We present systematic analyses of our dataset and annotation process as well as results achieved by several state-of-the-art baselines. While usually fluent, existing methods often hallucinate phrases that are not supported by the table, suggesting that this dataset can serve as a useful research benchmark for high-precision conditional text generation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1173,1186,,,,,,,,,,,,,,,,WOS:000855160701029,0
C,"Reich, S; Mueller, D; Andrews, N",,,Assoc Computat Linguist,"Reich, Steven; Mueller, David; Andrews, Nicholas",,,"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast-Choose Three",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs. However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available. In this paper, we study ensemble distillation as a general framework for producing wellcalibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles. We validate this framework on two tasks: named-entity recognition and machine translation. We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5583,5595,,,,,,,,,,,,,,,,WOS:000855160705060,0
C,"Sen, I; Flock, F; Wagner, C",,,Assoc Computat Linguist,"Sen, Indira; Flock, Fabian; Wagner, Claudia",,,On the Reliability and Validity of Detecting Approval of Political Actors in Tweets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors' approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users' political opinions based on their content on social media. In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors' approval by benchmarking them across several datasets. We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data. We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust. Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians' approval from tweets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1413,1426,,,,,,,,,,,,,,,,WOS:000855160701050,0
C,"Sun, S; Duh, K",,,Assoc Computat Linguist,"Sun, Shuo; Duh, Kevin",,,CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIR-Matrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages. In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date. This collection is intended to support research in end-to-end neural information retrieval and is publicly available at https: //github.com/ssun32/CLIRMatrix. We provide baseline neural model results on BI139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4160,4170,,,,,,,,,,,,,,,,WOS:000855160704028,0
C,"Wang, DF; Hu, W; Cao, EM; Sun, WJ",,,Assoc Computat Linguist,"Wang, Difeng; Wei Hu; Cao, Ermei; Sun, Weijian",,,Global-to-Local Neural Networks for Document-Level Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3711,3721,,,,,,,,,,,,,,,,WOS:000855160703078,0
C,"Wei, XP; Yu, H; Hu, Y; Weng, RX; Xing, LX; Luo, WH",,,Assoc Computat Linguist,"Wei, Xiangpeng; Yu, Heng; Hu, Yue; Weng, Rongxiang; Xing, Luxi; Luo, Weihua",,,Uncertainty-Aware Semantic Augmentation for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference. This leads to a discrepancy of the data distribution between the training and the inference phases. To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations. Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2724,2735,,,,,,,,,,,,,,,,WOS:000855160702073,0
C,"Yu, WH; Wu, LF; Deng, Y; Mahindru, R; Zeng, QK; Guven, S; Jiang, M",,,Assoc Computat Linguist,"Yu, Wenhao; Wu, Lingfei; Deng, Yu; Mahindru, Ruchi; Zeng, Qingkai; Guven, Sinem; Jiang, Meng",,,A Technical Question Answering System with Transfer Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving proper answers based on correctly answered similar questions in the past. TransTQA is built upon a siamese ALBERT network, which enables it to respond quickly and accurately. Furthermore, TransTQA adopts a standard deep transfer learning strategy to improve its capability of supporting multiple technical domains.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,92,99,,,,,,,,,,,,,,,,WOS:000855177700013,0
C,"Zhao, M; Lin, T; Mi, F; Jaggi, M; Schutze, H",,,Assoc Computat Linguist,"Zhao, Mengjie; Lin, Tao; Mi, Fei; Jaggi, Martin; Schutze, Hinrich",,,Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2226,2241,,,,,,,,,,,,,,,,WOS:000855160702031,0
C,"Bhat, IA; Anthonio, TR; Roth, M",,,Assoc Computat Linguist,"Bhat, Irshad Ahmad; Anthonio, Talita Rani; Roth, Michael",,,Towards Modeling Revision Requirements in wikiHow Instructions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"wikiHow is a resource of how-to guides that describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8407,8414,,,,,,,,,,,,,,,,WOS:000855160708050,0
C,"Chiu, JT; Rush, AM",,,Assoc Computat Linguist,"Chiu, Justin T.; Rush, Alexander M.",,,Scaling Hidden Markov Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are more accurate than previous HMM and n-gram-based methods, making progress towards the performance of state-of-the-art neural models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1341,1349,,,,,,,,,,,,,,,,WOS:000855160701043,0
C,"Clark, K; Luong, MT; Le, QV; Manning, CD",,,Assoc Computat Linguist,"Clark, Kevin; Luong, Minh-Thang; Le, Quoc V.; Manning, Christopher D.",,,Pre-Training Transformers as Energy-Based Cloze Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,285,294,,,,,,,,,,,,,,,,WOS:000855160700020,0
C,"Desai, S; Xu, JC; Durrett, G",,,Assoc Computat Linguist,"Desai, Shrey; Xu, Jiacheng; Durrett, Greg",,,Compressive Summarization with Plausibility and Salience Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Compressive summarization systems typically rely on a crafted set of syntactic rules to determine what spans of possible summary sentences can be deleted, then learn a model of what to actually delete by optimizing for content selection (ROUGE). In this work, we propose to relax the rigid syntactic constraints on candidate spans and instead leave compression decisions to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and spans are salient if they contain important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark summarization datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain: our system fine-tuned on only 500 samples from a new domain can match or exceed an in-domain extractive model trained on much more data.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6259,6274,,,,,,,,,,,,,,,,WOS:000855160706040,0
C,"Fisch, A; Lee, K; Chang, MW; Clark, JH; Barzilay, R",,,Assoc Computat Linguist,"Fisch, Adam; Lee, Kenton; Chang, Ming-Wei; Clark, Jonathan H.; Barzilay, Regina",,,CAPWAP Captioning with a Purpose,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CAPWAP). Our goal is to develop systems that can be tailored to be useful for the information needs of an intended population, rather than merely provide generic information about an image. In this task, we use question-answer (QA) pairs-a natural expression of information need-from users, instead of reference captions, for both training and post-inference evaluation. We show that it is possible to use reinforcement learning to directly optimize for the intended information need, by rewarding outputs that allow a question answering model to provide correct answers to sampled user questions. We convert several visual question answering datasets into CAPWAP datasets, and demonstrate that under a variety of scenarios our purposeful captioning system learns to anticipate and fulfill specific information needs better than its generic counterparts, as measured by QA performance on user questions from unseen images, when using the caption alone as context.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8755,8768,,,,,,,,,,,,,,,,WOS:000855160708080,0
C,"Gao, LY; Dai, ZY; Callan, J",,,Assoc Computat Linguist,"Gao, Luyu; Dai, Zhuyun; Callan, Jamie",,,Modularized Transfomer-based Ranking Framework,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4180,4190,,,,,,,,,,,,,,,,WOS:000855160704030,0
C,"Huang, JJ; Cai, XY; Church, K",,,Assoc Computat Linguist,"Huang, Jiaji; Cai, Xingyu; Church, Kenneth",,,Improving Bilingual Lexicon Induction for Low Frequency Words,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1310,1314,,,,,,,,,,,,,,,,WOS:000855160701040,0
C,"Li, JQ; Liu, XK; Zhao, HH; Xu, RF; Yang, M; Jin, YH",,,Assoc Computat Linguist,"Li, Jianquan; Liu, Xiaokang; Zhao, Honghong; Xu, Ruifeng; Yang, Min; Jin, Yaohong",,,BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers. In this way, our model can learn from different teacher layers adaptively for various NLP tasks. In addition, we leverage Earth Mover's Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network. EMD enables the effective matching for many-to-many layer mapping. Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model's performance and accelerate convergence time. Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression. For reproducibility, we release the code and data at https://github.com/lxk00/BERT-EMD.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3009,3018,,,,,,,,,,,,,,,,WOS:000855160703017,0
C,"Limkonchotiwat, P; Phatthiyaphaibu, W; Sarwar, R; Chuangsuwanicht, E; Nutanong, S",,,Assoc Computat Linguist,"Limkonchotiwat, Peerat; Phatthiyaphaibu, Wannaphong; Sarwar, Raheem; Chuangsuwanicht, Ekapol; Nutanong, Sarana",,,Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as black boxes. We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation. We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning. Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3841,3847,,,,,,,,,,,,,,,,WOS:000855160704003,0
C,"Lu, Y; Dong, Y; Charlin, L",,,Assoc Computat Linguist,"Lu, Yao; Dong, Yue; Charlin, Laurent",,,Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multidocument summarization task: writing the related-work section of a paper based on its abstract and the articles it references. Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches. Descriptive statistics and empirical results-using several state-of-the-art models trained on the Multi-XScience dataset-reveal that Multi-XScience is well suited for abstractive models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8068,8074,,,,,,,,,,,,,,,,WOS:000855160708023,0
C,"Lukasik, M; Dadachev, B; Papineni, K; Simoes, G",,,Assoc Computat Linguist,"Lukasik, Michal; Dadachev, Boris; Papineni, Kishore; Simoes, Goncalo",,,Text Segmentation by Cross Segment Attention,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4707,4716,,,,,,,,,,,,,,,,WOS:000855160704068,0
C,"Meng, Y; Zhang, YY; Huang, JX; Xiong, CY; Ji, H; Zhang, C; Han, JW",,,Assoc Computat Linguist,"Meng, Yu; Zhang, Yunyi; Huang, Jiaxin; Xiong, Chenyan; Ji, Heng; Zhang, Chao; Han, Jiawei",,,Text Classification Using Label Names Only: A Language Model Self-Training Approach,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9006,9017,,,,,,,,,,,,,,,,WOS:000855160709019,0
C,"Oncevay, A; Haddow, B; Birch, A",,,Assoc Computat Linguist,"Oncevay, Arturo; Haddow, Barry; Birch, Alexandra",,,Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, which is also released as a tool, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2391,2406,,,,,,,,,,,,,,,,WOS:000855160702044,0
C,"Saha, S; Nie, YX; Bansal, M",,,Assoc Computat Linguist,"Saha, Swarnadeep; Nie, Yixin; Bansal, Mohit",,,CONJNLI: Natural Language Inference Over Conjunctive Sentences,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce CONJNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions (and, or, but, nor) with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, CONJI\ILI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8240,8252,,,,,,,,,,,,,,,,WOS:000855160708036,0
C,"Sun, Z; Chen, MH; Hu, W; Wang, CM; Dai, J; Zhang, W",,,Assoc Computat Linguist,"Sun, Zequn; Chen, Muhao; Hu, Wei; Wang, Chengming; Dai, Jian; Zhang, Wei",,,Knowledge Association with Hyperbolic Knowledge Graph Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5704,5716,,,,,,,,,,,,,,,,WOS:000855160705070,0
C,"Swaminathan, A; Zhang, H; Mahata, D; Gosangi, R; Shah, RR; Stent, A",,,Assoc Computat Linguist,"Swaminathan, Avinash; Zhang, Haimin; Mahata, Debanjan; Gosangi, Rakesh; Shah, Rajiv Ratn; Stent, Amanda",,,A Preliminary Exploration of GANs for Keyphrase Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs). For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard benchmark datasets. We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques. Although we achieve promising results using GANs, they are not significantly better than the stateof-the-art generative models. To our knowledge, this is one of the first works that use GANs for keyphrase generation. We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8021,8030,,,,,,,,,,,,,,,,WOS:000855160708020,0
C,"Varkel, Y; Globerson, A",,,Assoc Computat Linguist,"Varkel, Yuval; Globerson, Amir",,,Pre-training Mention Representations in Coreference Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8534,8540,,,,,,,,,,,,,,,,WOS:000855160708062,0
C,"Vo, N; Lee, K",,,Assoc Computat Linguist,"Vo, Nguyen; Lee, Kyumin",,,Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users' consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters' followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7717,7731,,,,,,,,,,,,,,,,WOS:000855160707068,0
C,"Vulie, I; Ponti, EM; Litschko, R; Glava, G; Korhonen, A",,,Assoc Computat Linguist,"Vulie, Ivan; Ponti, Edoardo M.; Litschko, Robert; Glava, Goran; Korhonen, Anna",,,Probing Pretrained Language Models for Lexical Semantics,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7222,7240,,,,,,,,,,,,,,,,WOS:000855160707033,0
C,"Weber, N; Rudinger, R; Van Durme, B",,,Assoc Computat Linguist,"Weber, Noah; Rudinger, Rachel; Van Durme, Benjamin",,,Causal Inference of Script Knowledge,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7583,7596,,,,,,,,,,,,,,,,WOS:000855160707059,0
C,"Wei, HR; Zhang, ZR; Chen, BX; Luo, WH",,,Assoc Computat Linguist,"Wei, Hao-Ran; Zhang, Zhirui; Chen, Boxing; Luo, Weihua",,,Iterative Domain-Repaired Back-Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation. To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data. To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly. Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5884,5893,,,,,,,,,,,,,,,,WOS:000855160706007,0
C,"Wu, XF; Feng, Y; Shao, CZ",,,Assoc Computat Linguist,"Wu, Xuanfu; Feng, Yang; Shao, Chenze",,,Generating Diverse Translation from Model Distribution with Dropout,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1088,1097,,,,,,,,,,,,,,,,WOS:000855160701022,0
C,"Xing, X; Jin, Z; Jin, D; Wang, BN; Qi, Z; Huang, XJ",,,Assoc Computat Linguist,"Xing, Xiaoyu; Jin, Zhijing; Di Jin; Wang, Bingning; Qi Zhang; Huang, Xuanjing",,,"Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect's sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models' performance on ARTS by up to 32.85%.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3594,3605,,,,,,,,,,,,,,,,WOS:000855160703067,0
C,"Yu, JF; Jiang, J; Khoo, LMS; Chieu, HL; Xia, R",,,Assoc Computat Linguist,"Yu, Jianfei; Jing Jiang; Khoo, Ling Min Serena; Chieu, Hai Leong; Xia, Rui",,,Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer(1), which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1392,1401,,,,,,,,,,,,,,,,WOS:000855160701048,0
C,"Zanzotto, FM; Onorati, D; Santilli, A; Tommasino, P; Ranaldi, L; Fallucchi, F",,,Assoc Computat Linguist,"Zanzotto, Fabio Massimo; Onorati, Dario; Santilli, Andrea; Tommasino, Pierfrancesco; Ranaldi, Leonardo; Fallucchi, Francesca",,,KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference. We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,256,267,,,,,,,,,,,,,,,,WOS:000855160700018,0
C,"Baziotis, C; Haddow, B; Birch, A",,,Assoc Computat Linguist,"Baziotis, Christos; Haddow, Barry; Birch, Alexandra",,,Language Model Prior for Low-Resource Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM disagrees with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis on the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7622,7634,,,,,,,,,,,,,,,,WOS:000855160707062,0
C,"Bowman, SR; Palomaki, J; Soares, LB; Pitler, E",,,Assoc Computat Linguist,"Bowman, Samuel R.; Palomaki, Jennimaria; Soares, Livio Baldini; Pitler, Emily",,,New Protocols and Negative Results for Textual Entailment Data Collection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these purposes, so it is likely far from ideal. We propose four alternative protocols, each aimed at improving either the ease with which annotators can produce sound training examples or the quality and diversity of those examples. Using these alternatives and a fifth baseline protocol, we collect and compare five new 8.5k-example training sets. In evaluations focused on transfer learning applications, our results are solidly negative, with models trained on our baseline dataset yielding good transfer performance to downstream tasks, but none of our four new methods (nor the recent ANLI) showing any improvements over that baseline. In a small silver lining, we observe that all four new protocols, especially those where annotators edit pre filled text boxes, reduce previously observed issues with annotation artifacts.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8203,8214,,,,,,,,,,,,,,,,WOS:000855160708033,0
C,"Chen, CH; Teng, ZY; Zhang, Y",,,Assoc Computat Linguist,"Chen, Chenhua; Teng, Zhiyang; Zhang, Yue",,,Inducing Target-Specific Latent Structures for Aspect Sentiment Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory. To tackle this problem, we associate linguistic dependency trees with automatically induced aspect-specific graphs. We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks. Our model can complement supervised syntactic features with latent semantic dependencies. Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5596,5607,,,,,,,,,,,,,,,,WOS:000855160705061,0
C,"Chen, JA; Yang, DY",,,Assoc Computat Linguist,"Chen, Jiaao; Yang, Diyi",,,Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations-an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers-remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4106,4118,,,,,,,,,,,,,,,,WOS:000855160704024,0
C,"Corro, C",,,Assoc Computat Linguist,"Corro, Caio",,,Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n(6)) down to O(n(3)),PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(n(6)) down to O(n(3)). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bertbased neural networks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2753,2764,,,,,,,,,,,,,,,,WOS:000855160702076,0
C,"Ding, B; Liu, L; Bing, L; Kruengkrai, C; Nguyen, TH; Joty, S; Si, L; Miao, C",,,Assoc Computat Linguist,"Ding, Bosheng; Liu, Linlin; Bing, Lidong; Kruengkrai, Canasai; Nguyen, Thien Hai; Joty, Shafiq; Si, Luo; Miao, Chunyan",,,DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Data augmentation techniques have been widely used to improve machine learning performance as they enhance the generalization capability of models. In this work, to generate high quality synthetic data for low-resource tagging tasks, we propose a novel augmentation method with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6045,6057,,,,,,,,,,,,,,,,WOS:000855160706021,0
C,"Ding, ZX; Xia, R; Yu, JF",,,Assoc Computat Linguist,"Ding, Zixiang; Rui Xia; Yu, Jianfei",,,End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering. However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step. To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL). The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move. Finally, CMLL and EMLL are integrated to obtain the final result. We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3574,3583,,,,,,,,,,,,,,,,WOS:000855160703065,0
C,"Du, X; Cardie, C",,,Assoc Computat Linguist,"Du, Xinya; Cardie, Claire",,,Event Extraction by Answering (Almost) Natural Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,671,683,,,,,,,,,,,,,,,,WOS:000855160700049,0
C,"Gaddy, D; Klein, D",,,Assoc Computat Linguist,"Gaddy, David; Klein, Dan",,,Digital Voicing of Silent Speech,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5521,5530,,,,,,,,,,,,,,,,WOS:000855160705055,0
C,"Galkin, M; Trivedi, P; Maheshwari, G; Usbeck, R; Lehmann, J",,,Assoc Computat Linguist,"Galkin, Mikhail; Trivedi, Priyansh; Maheshwari, Gaurav; Usbeck, Ricardo; Lehmann, Jens",,,Message Passing for Hyper-Relational Knowledge Graphs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - STARE capable of modeling such hyper-relational KGs. Unlike existing approaches, STARE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a newWikidata-based dataset - WD50K. Our experiments demonstrate that STARE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7346,7359,,,,,,,,,,,,,,,,WOS:000855160707043,0
C,"Huang, L; Ji, H",,,Assoc Computat Linguist,"Huang, Lifu; Ji, Heng",,,Semi-supervised New Event Type Induction and Event Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types. We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations. A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution. Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,718,724,,,,,,,,,,,,,,,,WOS:000855160700053,0
C,"Jaques, N; Shen, JH; Ghandehariouni, A; Ferguson, C; Lapedriza, A; Jones, N; Gu, SS; Picard, R",,,Assoc Computat Linguist,"Jaques, Natasha; Shen, Judy Hanwen; Ghandehariouni, Asma; Ferguson, Craig; Lapedriza, Agata; Jones, Noah; Gu, Shixiang Shane; Picard, Rosalind",,,Human-centric dialog training via offline reinforcement learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pretrained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3985,4003,,,,,,,,,,,,,,,,WOS:000855160704015,0
C,"Krishna, A; Gupta, A; Garasangi, D; Satuluri, P; Goyal, P",,,Assoc Computat Linguist,"Krishna, Amrith; Gupta, Ashim; Garasangi, Deepak; Satuluri, Pavankumar; Goyal, Pawan",,,Keep It Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework's default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4791,4797,,,,,,,,,,,,,,,,WOS:000855160704076,0
C,"Li, M; Zeng, Q; Lin, Y; Cho, K; Ji, H; May, J; Chambers, N; Voss, C",,,Assoc Computat Linguist,"Li, Manling; Zeng, Qi; Lin, Ying; Cho, Kyunghyun; Ji, Heng; May, Jonathan; Chambers, Nathanael; Voss, Clare",,,Connecting the Dots: Event Graph Schema Induction with Path Language Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.(1)",,,,,"Zeng, Qi/GLT-7682-2022; Lin, Ying/HGE-7388-2022","Zeng, Qi/0000-0002-3696-1538; ",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,684,695,,,,,,,,,,,,,,,,WOS:000855160700050,0
C,"Ma, QL; Lin, ZX; Yan, JY; Chen, ZP; Yu, LH",,,Assoc Computat Linguist,"Ma, Qianli; Lin, Zhenxi; Yan, Jiangyue; Chen, Zipeng; Yu, Liuhong",,,MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6705,6715,,,,,,,,,,,,,,,,WOS:000855160706077,0
C,"Majumder, N; Hong, PF; Peng, SS; Lu, JK; Ghosal, D; Gelbukh, A; Mihalcea, R; Poria, S",,,Assoc Computat Linguist,"Majumder, Navonil; Hong, Pengfei; Peng, Shanshan; Lu, Jiankun; Ghosal, Deepanway; Gelbukh, Alexander; Mihalcea, Rada; Poria, Soujanya",,,MIME: MiMicking Emotions for Empathetic Response Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at https : //github.com/declare-lab/MIME.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8968,8979,,,,,,,,,,,,,,,,WOS:000855160709016,0
C,"Malmi, E; Severyn, A; Rothe, S",,,Assoc Computat Linguist,"Malmi, Eric; Severyn, Aliaksei; Rothe, Sascha",,,Unsupervised Text Style Transfer with Padded Masked Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose MASKER, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source-target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms of likelihood. This allows us to identify the source tokens to delete to transform the source text to match the style of the target domain. The deleted tokens are replaced with the target MLM, and by using a padded MLM variant, we avoid having to predetermine the number of inserted tokens. Our experiments on sentence fusion and sentiment transfer demonstrate that MASKER performs competitively in a fully unsupervised setting. Moreover, in low-resource settings, it improves supervised methods' accuracy by over 10 percentage points when pre-training them on silver training data generated by MASKER.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8671,8680,,,,,,,,,,,,,,,,WOS:000855160708074,0
C,"Qi, FC; Zhang, L; Yang, YH; Liu, ZY; Sun, MS",,,Assoc Computat Linguist,"Qi, Fanchao; Zhang, Lei; Yang, Yanhui; Liu, Zhiyuan; Sun, Maosong",,,WantWords: An Open-source Online Reverse Dictionary System,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary systems, but they support English reverse dictionary queries only and their performance is far from perfect. In this paper, we present a new open-source online reverse dictionary system named WantWords (https://wantwords.thunlp.org/). It not only significantly outperforms other reverse dictionary systems on English reverse dictionary performance, but also supports Chinese and English-Chinese as well as Chinese-English cross-lingual reverse dictionary queries for the first time. Moreover, it has user-friendly front-end design which can help users find the words they need quickly and easily.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,175,181,,,,,,,,,,,,,,,,WOS:000855177700023,0
C,"Sha, L",,,Assoc Computat Linguist,"Sha, Lei",,,Gradient-guided Unsupervised Lexically Constrained Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Lexically-constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply beamsearch-based methods or stochastic searching methods to lexically-constrained generation. However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution. At the same time, stochastic search methods always cost too many steps to find the correct optimization direction. In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem. We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word). The word updating process of the inserted/replaced word also benefits from the guidance of gradient. Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model. We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation. The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8692,8703,,,,,,,,,,,,,,,,WOS:000855160708076,0
C,"Ustun, A; Bisazza, A; Bouma, G; Van Noord, G",,,Assoc Computat Linguist,"Ustun, Ahmet; Bisazza, Arianna; Bouma, Gosse; Van Noord, Gertjan",,,UDapter: Language Adaptation for Truly Universal Dependency Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2302,2315,,,,,,,,,,,,,,,,WOS:000855160702037,0
C,"Wu, SJ; Dredze, M",,,Assoc Computat Linguist,"Wu, Shijie; Dredze, Mark",,,Do Explicit Alignments Robustly Improve Multilingual Encoders?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multilingual BERT (Devlin et al., 2019, mBERT), XLM-RoBERTa (Conneau et al., 2019, XLMR) and other unsupervised multilingual encoders can effectively learn crosslingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4471,4482,,,,,,,,,,,,,,,,WOS:000855160704050,0
C,"Yang, Z; Hu, BJ; Han, A; Huang, S; Ju, Q",,,Assoc Computat Linguist,"Yang, Zhen; Hu, Bojie; Han, Ambyera; Huang, Shen; Ju, Qi",,,CSP: Code-Switching Pre-training for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoderdecoder framework: its encoder takes the codemixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrainfinetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2624,2636,,,,,,,,,,,,,,,,WOS:000855160702065,0
C,"Ye, DM; Lin, YK; Du, JJ; Liu, ZG; Li, P; Sun, MS; Liu, ZY",,,Assoc Computat Linguist,"Ye, Deming; Lin, Yankai; Du, Jiaju; Liu, Zhenghao; Li, Peng; Sun, Maosong; Liu, Zhiyuan",,,Coreferential Reasoning Learning for Language Representation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7170,7186,,,,,,,,,,,,,,,,WOS:000855160707029,0
C,"Emelin, D; Titov, I; Sennrich, R",,,Assoc Computat Linguist,"Emelin, Denis; Titov, Ivan; Sennrich, Rico",,,Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7635,7653,,,,,,,,,,,,,,,,WOS:000855160707063,0
C,"Fan, A; Piktus, A; Petroni, F; Wenzek, G; Saeidi, M; Vlachos, A; Bordes, A; Riedel, S",,,Assoc Computat Linguist,"Fan, Angela; Piktus, Aleksandra; Petroni, Fabio; Wenzek, Guillaume; Saeidi, Marzieh; Vlachos, Andreas; Bordes, Antoine; Riedel, Sebastian",,,Generating Fact Checking Briefs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fact checking at scale is difficult-while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim detection. We investigate how to increase the accuracy and efficiency of fact checking by providing information about the claim before performing the check, in the form of natural language briefs. We investigate passage-based briefs, containing a relevant passage from Wikipedia, entity-centric ones consisting of Wikipedia pages of mentioned entities, and Question-Answering Briefs, with questions decomposing the claim, and their answers. To produce QABriefs, we develop QABRIEFER, a model that generates a set of questions conditioned on the claim, searches the web for evidence, and generates answers. To train its components, we introduce QABRIEFDATASET which we collected via crowdsourcing. We show that fact checking with briefs- in particular QABriefs- increases the accuracy of crowdworkers by 10% while slightly decreasing the time taken. For volunteer (unpaid) fact checkers, QABriefs slightly increase accuracy and reduce the time required by around 20%.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7147,7161,,,,,,,,,,,,,,,,WOS:000855160707027,0
C,"Hu, X; Lijie, WY; Xu, Y; Chenwei, ZY; Yu, PS",,,Assoc Computat Linguist,"Hu, Xuming; Lijie Wen; Xu, Yusong; Zhang, Chenwei; Yu, Philip S.",,,SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines. Source code is available(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3673,3682,,,,,,,,,,,,,,,,WOS:000855160703074,0
C,"Ji, H; Ke, P; Huang, S; Wei, F; Zhu, X; Huang, M",,,Assoc Computat Linguist,"Ji, Haozhe; Ke, Pei; Huang, Shaohan; Wei, Furu; Zhu, Xiaoyan; Huang, Minlie",,,Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,725,736,,,,,,,,,,,,,,,,WOS:000855160700054,0
C,"Kolluru, K; Adlakha, V; Aggarwal, S; Mausam; Chakrabarti, S",,,Assoc Computat Linguist,"Kolluru, Keshav; Adlakha, Vaibhav; Aggarwal, Samarth; Mausam; Chakrabarti, Soumen",,,OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost. On the other hand, sequence labeling approaches for OpenIE are much faster, but worse in extraction quality. In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster. This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task. We improve its performance further by applying coverage (soft) constraints on the grid at training time. Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture. This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers. Our OpenIE system, OpenIE6(1), beats the previous systems by as much as 4 pts in F1, while being much faster.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3748,3761,,,,,,,,,,,,,,,,WOS:000855160703081,0
C,"Lebanoff, L; Dernoncourt, F; Kim, DS; Wang, LD; Chang, W; Liu, F",,,Assoc Computat Linguist,"Lebanoff, Logan; Dernoncourt, Franck; Kim, Doo Soon; Wang, Lidan; Chang, Walter; Liu, Fei",,,Learning to Fuse Sentences with Transformers for Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning. In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences. Through extensive experiments, we investigate the effects of different design choices on Transformer's performance. Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4136,4142,,,,,,,,,,,,,,,,WOS:000855160704026,0
C,"Li, CY; Gao, X; Li, Y; Peng, BL; Li, XJ; Zhang, YZ; Gao, JF",,,Assoc Computat Linguist,"Li, Chunyuan; Gao, Xiang; Li, Yuan; Peng, Baolin; Li, Xiujun; Zhang, Yizhe; Gao, Jianfeng",,,OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"When trained effectively, the Variational Autoencoder (VAE) (Kingma and Welling, 2013; Bowman et al., 2016) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model OPTIMUS1. A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, OPTIMUS enables guided language generation from an abstract level using the latent vectors. Compared with BERT, OPTIMUS can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of OPTIMUS. It achieves new state-of-the-art on VAE language modeling benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4678,4699,,,,,,,,,,,,,,,,WOS:000855160704066,0
C,"Li, X; Chen, GY; Lin, CH; Li, RZ",,,Assoc Computat Linguist,"Li, Xiao; Chen, Guanyi; Lin, Chenghua; Li, Ruizhe",,,DGST: a Dual-Generator Network for Text Style Transfer,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.",,,,,,"Li, Ruizhe/0000-0003-2512-845X",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7131,7136,,,,,,,,,,,,,,,,WOS:000855160707073,0
C,"Li, Y; Yin, C; Zhong, SH; Pan, X",,,Assoc Computat Linguist,"Li, Yuncong; Yin, Cunxiang; Zhong, Sheng-Hua; Pan, Xu",,,Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN (1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3550,3560,,,,,,,,,,,,,,,,WOS:000855160703062,0
C,"McCarthy, AD; Williams, A; Liu, SJ; Yarowsky, D; Cotterell, R",,,Assoc Computat Linguist,"McCarthy, Arya D.; Williams, Adina; Liu, Shijia; Yarowsky, David; Cotterell, Ryan",,,Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages' gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information-theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. Finally, we ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5664,5675,,,,,,,,,,,,,,,,WOS:000855160705066,0
C,"Pei, JX; Jurgens, D",,,Assoc Computat Linguist,"Pei, Jiaxin; Jurgens, David",,,Quantifying Intimacy in Language,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson's r =0.87). Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings. Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology. Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5307,5326,,,,,,,,,,,,,,,,WOS:000855160705038,0
C,"Philip, J; Berard, A; Galle, M; Besacier, L",,,Assoc Computat Linguist,"Philip, Jerin; Berard, Alexandre; Galle, Matthias; Besacier, Laurent",,,Monolingual Adapters for Zero-Shot Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4465,4470,,,,,,,,,,,,,,,,WOS:000855160704049,0
C,"Rennie, SJ; Marcheret, E; Mallinar, N; Nahamoo, D; Goel, V",,,Assoc Computat Linguist,"Rennie, Steven J.; Marcheret, Etienne; Mallinar, Neil; Nahamoo, David; Goel, Vaibhava",,,Unsupervised Adaptation of Question Answering Systems via Generative Self-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1148,1157,,,,,,,,,,,,,,,,WOS:000855160701027,0
C,"Sarkar, R; Mahinder, S; Sarkar, H; KhudaBukhsh, AR",,,Assoc Computat Linguist,"Sarkar, Rupak; Mahinder, Sayantan; Sarkar, Hirak; KhudaBukhsh, Ashiqur R.",,,Social Media Attributions in the Context of Water Crisis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media data to complement traditional surveys through automatically extracting attribution factors. We present a novel prediction task of attribution tie detection of identifying the factors (e.g., poor city planning, exploding population etc.) held responsible for the crisis in a social media document. We focus on the 2019 Chennai water crisis that rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 videos relevant to the crisis), we present a neural baseline to identify attribution ties that achieves a reasonable performance (accuracy: 87.34% on attribution detection and 81.37% on attribution resolution). We release the first annotated data set of 2,500 comments in this important domain(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1402,1412,,,,,,,,,,,,,,,,WOS:000855160701049,0
C,"Sawhney, R; Agarwal, S; Wadhwa, A; Shah, RR",,,Assoc Computat Linguist,"Sawhney, Ramit; Agarwal, Shivam; Wadhwa, Arnav; Shah, Rajiv Ratn",,,Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks. The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting. We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion. Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8415,8426,,,,,,,,,,,,,,,,WOS:000855160708051,0
C,"Schmitt, M; Sharifzadeh, S; Tresp, V; Schutze, H",,,Assoc Computat Linguist,"Schmitt, Martin; Sharifzadeh, Sahand; Tresp, Volker; Schuetze, Hinrich",,,An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well in different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text <-> graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7117,7130,,,,,,,,,,,,,,,,WOS:000855160707026,0
C,"Shi, T; Malioutov, I; Irsoy, O",,,Assoc Computat Linguist,"Shi, Tianze; Malioutov, Igor; Irsoy, Ozan",,,Semantic Role Labeling as Syntactic Dependency Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format. This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art. Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7551,7571,,,,,,,,,,,,,,,,WOS:000855160707057,0
C,"Thompson, B; Koehn, P",,,Assoc Computat Linguist,"Thompson, Brian; Koehn, Philipp",,,Exploiting Sentence Order in Document Alignment,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our method improves downstream MT performance on web-scraped Sinhala-English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release. It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5997,6007,,,,,,,,,,,,,,,,WOS:000855160706016,0
C,"Yan, JH; Wang, YN; Xiang, L; Zhou, Y; Zong, CQ",,,Assoc Computat Linguist,"Yan, Jinghui; Wang, Yining; Lu Xiang; Yu Zhou; Zong, Chengqing",,,A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and combined procedures present challenges in our problem. The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions. We propose a sequence generative framework to directly generate all the corresponding medical procedure entities. we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1490,1499,,,,,,,,,,,,,,,,WOS:000855160701056,0
C,"Yan, Y; He, K; Xu, H; Liu, S; Meng, F; Hu, M; Xu, W",,,Assoc Computat Linguist,"Yan, Yuanmeng; He, Keqing; Xu, Hong; Liu, Sihong; Meng, Fanyu; Hu, Min; Xu, Weiran",,,Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context. We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence. Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6070,6075,,,,,,,,,,,,,,,,WOS:000855160706023,0
C,"Yao, SY; Rao, RH; Hausknecht, M; Narasimhan, K",,,Assoc Computat Linguist,"Yao, Shunyu; Rao, Rohan; Hausknecht, Matthew; Narasimhan, Karthik",,,Keep CALM and Explore: Language Models for Action Generation in Text-based Games,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark (Hausknecht et al., 2019a), on games unseen by CALM during training Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8736,8754,,,,,,,,,,,,,,,,WOS:000855160708079,0
C,"Zeng, XJ; Li, YL; Zhai, YC; Zhang, Y",,,Assoc Computat Linguist,"Zeng, Xiangji; Li, Yunliang; Zhai, Yuchen; Zhang, Yin",,,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective. Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset. Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples. Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels. We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented. Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation. As a result, our method eliminates part of the spurious correlations between context representation and output labels. The code is available at https://github.com/xijiz/cfgen.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7270,7280,,,,,,,,,,,,,,,,WOS:000855160707037,0
C,"Zheng, C; Kordjamshidi, P",,,Assoc Computat Linguist,"Zheng, Chen; Kordjamshidi, Parisa",,,SRLGRN: Semantic Role Labeling Graph Reasoning Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8881,8891,,,,,,,,,,,,,,,,WOS:000855160709009,0
C,"Artetxe, M; Labaka, G; Agirre, E",,,Assoc Computat Linguist,"Artetxe, Mikel; Labaka, Gorka; Agirre, Eneko",,,Translation Artifacts in Cross-lingual Transfer Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7674,7684,,,,,,,,,,,,,,,,WOS:000855160707065,0
C,"Bai, X; Song, LF; Zhang, Y",,,Assoc Computat Linguist,"Bai, Xuefeng; Song, Linfeng; Zhang, Yue",,,Online Back-Parsing for AMR-to-Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1206,1219,,,,,,,,,,,,,,,,WOS:000855160701032,0
C,"Ding, KZ; Wang, JL; Li, JD; Li, DC; Liu, H",,,Assoc Computat Linguist,"Ding, Kaize; Wang, Jianling; Li, Jundong; Li, Dingcheng; Liu, Huan",,,Be More with Less: Hypergraph Attention Networks for Inductive Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model - hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4927,4936,,,,,,,,,,,,,,,,WOS:000855160705010,0
C,"Gao, YF; Wu, CS; Lit, JJ; Joty, S; Hoi, SCH; Xiong, CM; King, I; Lyu, MR",,,Assoc Computat Linguist,"Gao, Yifan; Wu, Chien-Sheng; Li, Jingjing; Joty, Shafiq; Hoi, Steven C. H.; Xiong, Caiming; King, Irwin; Lyu, Michael R.",,,DISCERN: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose DISCERN, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision yes/no/irrelevant of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that DISCERN achieves state-ofthe-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/ Yifan- Gao/Discern.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2439,2449,,,,,,,,,,,,,,,,WOS:000855160702048,0
C,"Hasan, T; Bhattacharjee, A; Samin, K; Hasan, M; Basak, M; Rahman, MS; Shahriyar, R",,,Assoc Computat Linguist,"Hasan, Tahmid; Bhattacharjee, Abhik; Samin, Kazi; Hasan, Masum; Basak, Madhusudan; Rahman, M. Sohel; Shahriyar, Rifat",,,"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality BengaliEnglish parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https: //github.com/csebuetnlp/banglanmt.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2612,2623,,,,,,,,,,,,,,,,WOS:000855160702064,0
C,"Huang, LS; Ye, Z; Qin, JH; Lin, L; Liang, XD",,,Assoc Computat Linguist,"Huang, Lishan; Ye, Zheng; Qin, Jinghui; Lin, Liang; Liang, Xiaodan",,,GRADE Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgements. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9230,9240,,,,,,,,,,,,,,,,WOS:000855160709037,0
C,"Jia, Q; Liu, YZ; Rena, SY; Zhu, KQ",,,Assoc Computat Linguist,"Jia, Qi; Liu, Yizhu; Rena, Siyu; Zhu, Kenny Q.",,,Multi-turn Response Selection using Dialogue Dependency Relations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1911,1920,,,,,,,,,,,,,,,,WOS:000855160702007,0
C,"Jo, Y; Visser, J; Reed, C; Hovy, E",,,Assoc Computat Linguist,"Jo, Yohan; Visser, Jacky; Reed, Chris; Hovy, Eduard",,,Extracting Implicitly Asserted Propositions in Argumentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.(1)",,,,,,"Visser, Jacky/0000-0003-2778-0847",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,24,38,,,,,,,,,,,,,,,,WOS:000855160700002,0
C,"Keung, P; Lu, Y; Salazar, J; Bhardwaj, V",,,Assoc Computat Linguist,"Keung, Phillip; Lu, Yichao; Salazar, Julian; Bhardwaj, Vikas",,,Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,549,554,,,,,,,,,,,,,,,,WOS:000855160700040,0
C,"Kim, N; Linzen, T",,,Assoc Computat Linguist,"Kim, Najoung; Linzen, Tal",,,COGS: A Compositional Generalization Challenge Based on Semantic Interpretation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96-99%), but generalization accuracy was substantially lower (16-35%) and showed high sensitivity to random seed (+/- 6-8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9087,9105,,,,,,,,,,,,,,,,WOS:000855160709026,0
C,"Kryscinski, W; McCann, B; Xiong, CM; Socher, R",,,Assoc Computat Linguist,"Kryscinski, Wojciech; McCann, Bryan; Xiong, Caiming; Socher, Richard",,,Evaluating the Factual Consistency of Abstractive Text Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9332,9346,,,,,,,,,,,,,,,,WOS:000855160709045,0
C,"Le, H; Sahoo, D; Che, NF; Hoit, SCH",,,Assoc Computat Linguist,"Le, Hung; Sahoo, Doyen; Che, Nancy F.; Hoit, Steven C. H.",,,BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1846,1859,,,,,,,,,,,,,,,,WOS:000855160702002,0
C,"Mihaylova, T; Niculae, V; Martins, AFT",,,Assoc Computat Linguist,"Mihaylova, Tsvetomila; Niculae, Vlad; Martins, Andre F. T.",,,Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT-a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2186,2202,,,,,,,,,,,,,,,,WOS:000855160702028,0
C,"Nooralahzadeh, F; Bekoulis, G; Bjerva, J; Augenstein, I",,,Assoc Computat Linguist,"Nooralahzadeh, Farhad; Bekoulis, Giannis; Bjerva, Johannes; Augenstein, Isabelle",,,Zero-Shot Cross-Lingual Transfer with Meta Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.",,,,,,"Bekoulis, Giannis/0000-0003-3377-2675",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4547,4562,,,,,,,,,,,,,,,,WOS:000855160704056,0
C,"Prasanna, S; Rogers, A; Rumshisky, A",,,Assoc Computat Linguist,"Prasanna, Sai; Rogers, Anna; Rumshisky, Anna",,,"When BERT Plays the Lottery, All Tickets Are Winning",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful. We also study the good subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3208,3229,,,,,,,,,,,,,,,,WOS:000855160703034,0
C,"Su, H; Shen, XY; Xiao, Z; Zhang, Z; Chang, EN; Zhang, C; Niu, C; Zhou, J",,,Assoc Computat Linguist,"Su, Hui; Shen, Xiaoyu; Xiao, Zhou; Zhang, Zheng; Chang, Ernie; Zhang, Cheng; Niu, Cheng; Zhou, Jie",,,MovieChats: Chat like Humans in a Closed Domain,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can ever be claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of moviedomain chatbots. We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval. The model is first pretrained on the huge general-domain data, then finetuned on our corpus. We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work (1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6605,6619,,,,,,,,,,,,,,,,WOS:000855160706068,0
C,"Yan, JH; Meng, FD; Zhou, J",,,Assoc Computat Linguist,"Yan, Jianhao; Meng, Fandong; Zhou, Jie",,,Multi-Unit Transformers for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformer models (Vaswani et al., 2017) achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multi-head Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1047,1059,,,,,,,,,,,,,,,,WOS:000855160701017,0
C,"Yauney, G; Hessel, J; Mimno, D",,,Assoc Computat Linguist,"Yauney, Gregory; Hessel, Jack; Mimno, David",,,Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are abundant. Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap? Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as kitchen and bedroom, and introduce metrics to assess this document similarity. We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset. The proposed method is particularly effective for local contextual meanings of a word, for example associating granite with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2039,2045,,,,,,,,,,,,,,,,WOS:000855160702017,0
C,"Zhengbao, J; Anastasopoulos, A; Jun, A; Haibo, D; Neubig, G",,,Assoc Computat Linguist,"Zhengbao, Jiang; Anastasopoulos, Antonios; Jun, Araki; Haibo, Ding; Neubig, Graham",,,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-theblank questions such as Punta Cana is located in _. However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for 23 typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-theart LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https: //x-factr.github.io.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5943,5959,,,,,,,,,,,,,,,,WOS:000855160706012,0
C,"Zhong, PX; Zhang, C; Wang, H; Liu, Y; Miao, CY",,,Assoc Computat Linguist,"Zhong, Peixiang; Zhang, Chen; Wang, Hao; Liu, Yong; Miao, Chunyan",,,Towards Persona-Based Empathetic Conversational Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.",,,,,,"Liu, Yong/0000-0001-9031-9696",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6556,6566,,,,,,,,,,,,,,,,WOS:000855160706064,0
C,"Agarwal, R; Kann, K",,,Assoc Computat Linguist,"Agarwal, Rajat; Kann, Katharina",,,Acrostic Poem Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1230,1240,,,,,,,,,,,,,,,,WOS:000855160701034,0
C,"Bosc, T; Vincent, P",,,Assoc Computat Linguist,"Bosc, Tom; Vincent, Pascal",,,Do sequence-to-sequence VAEs learn global features of sentences?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman et al. (2016) adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner. We question this claim. We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness. To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining. These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels. Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4296,4318,,,,,,,,,,,,,,,,WOS:000855160704038,0
C,"Dubossarsky, H; Vuliel, I; Reichart, R; Korhonen, A",,,Assoc Computat Linguist,"Dubossarsky, Haim; Vulic, Ivan; Reichart, Roi; Korhonen, Anna",,,The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces. In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT. We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned. We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra. We empirically show that 1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different crosslingual tasks, and 2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret. Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2377,2390,,,,,,,,,,,,,,,,WOS:000855160702043,0
C,"Feng, S; Wan, H; Gunasekara, C; Patel, SS; Joshi, S; Lastras, LA",,,Assoc Computat Linguist,"Feng, Song; Wan, Hui; Gunasekara, Chulaka; Patel, Siva Sankalp; Joshi, Sachindra; Lastras, Luis A.",,,doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text sections as well as lower-level relations between discourse units within a section. Then we present these dialogue flows to crowd contributors to create conversational utterances. The dataset includes over 4500 annotated conversations with an average of 14 turns that are grounded in over 450 documents from four domains. Compared to the prior document-grounded dialogue datasets, this dataset covers a variety of dialogue scenes in information-seeking conversations. For evaluating the versatility of the dataset, we introduce multiple dialogue modeling tasks and present baseline approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8118,8128,,,,,,,,,,,,,,,,WOS:000855160708027,0
C,"Goldfarb-Tarrant, S; Chakrabarty, T; Weischedel, R; Peng, NY",,,Assoc Computat Linguist,"Goldfarb-Tarrant, Seraphina; Chakrabarty, Tuhin; Weischedel, Ralph; Peng, Nanyun",,,Content Planning for Neural Story Generation with Aristotelian Rescoring,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4319,4338,,,,,,,,,,,,,,,,WOS:000855160704039,0
C,"Hohenecker, P; Mtumbuka, F; Kocijan, V; Lukasiewicz, T",,,Assoc Computat Linguist,"Hohenecker, Patrick; Mtumbuka, Frank; Kocijan, Vid; Lukasiewicz, Thomas",,,Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form < subject, predicate, object >. For example, given the sentence Beethoven composed the Ode to Joy., we are expected to extract the triple < Beethoven, composed, Ode to Joy >. In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F-1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases). Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8554,8565,,,,,,,,,,,,,,,,WOS:000855160708065,0
C,"Huang, JX; Meng, Y; Guo, F; Ji, H; Han, JW",,,Assoc Computat Linguist,"Huang, Jiaxin; Meng, Yu; Guo, Fang; Ji, Heng; Han, Jiawei",,,Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, neglecting the benefit of coupling both, or are based on topic models that may contain overlapping concepts. We propose to first learn < sentiment, aspect > joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6989,6999,,,,,,,,,,,,,,,,WOS:000855160707017,0
C,"Iranzo-Sanchez, J; Pastor, AG; Silvestre-Cerda, JA; Baquero-Arnal, P; Civera, J; Juan, A",,,Assoc Computat Linguist,"Iranzo-Sanchez, Javier; Pastor, Adria Gimenez; Silvestre-Cerda, Joan Albert; Baquero-Arnal, Pau; Civera, Jorge; Juan, Alfons",,,Direct Segmentation Models for Streaming Speech Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into, hopefully, semantically self-contained chunks to be fed into the MT system. This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account. This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk. An extensive and thorough experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario. Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2599,2611,,,,,,,,,,,,,,,,WOS:000855160702063,0
C,"Jia, RP; Cao, YN; Tang, HZ; Fang, F; Cao, C; Wang, S",,,Assoc Computat Linguist,"Jia, Ruipeng; Cao, Yanan; Tang, Hengzhu; Fang Fang; Cong Cao; Shi Wang",,,Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3622,3631,,,,,,,,,,,,,,,,WOS:000855160703070,0
C,"Sun, K; Zhang, RC; Mensah, S; Mao, YY; Liu, XD",,,Assoc Computat Linguist,"Kai Sun; Zhang, Richong; Mensah, Samuel; Mao, Yongyi; Liu, Xudong",,,Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3722,3732,,,,,,,,,,,,,,,,WOS:000855160703079,0
C,"Khan, AR; Xu, J; Sun, WW",,,Assoc Computat Linguist,"Khan, Abdul Rafae; Xu, Jia; Sun, Weiwei",,,Coding Textual Inputs Boosts the Accuracy of Neural Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As alternatives to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding. Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords. We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs. Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging. The source code is available at https://github.com/abdulrafae/coding_nmt.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1350,1360,,,,,,,,,,,,,,,,WOS:000855160701044,0
C,"Krishna, K; Wieting, J; Iyyer, M",,,Assoc Computat Linguist,"Krishna, Kalpesh; Wieting, John; Iyyer, Mohit",,,Reformulating Unsupervised Style Transfer as Paraphrase Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,737,762,,,,,,,,,,,,,,,,WOS:000855160700055,0
C,"Lee, J; Shu, R; Cho, K",,,Assoc Computat Linguist,"Lee, Jason; Shu, Raphael; Cho, Kyunghyun",,,Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 En -> De, WMT'16 Ro -> En and IWSLT'16 De -> En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 En -> De, for instance, our approach is able to decode 6:2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1006,1015,,,,,,,,,,,,,,,,WOS:000855160701013,0
C,"Liang, ZJ; Jiang, WT; Hu, HF; Zhu, JY",,,Assoc Computat Linguist,"Liang, Zujie; Jiang, Weitao; Hu, Haifeng; Zhu, Jiaying",,,Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized. Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples. With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly. We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model's robustness.",,,,,,"Hu, Haifeng/0000-0002-4884-323X",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3285,3292,,,,,,,,,,,,,,,,WOS:000855160703040,0
C,"Liu, NY; Sun, X; Yul, HF; Zhangi, WK; Xui, GL",,,Assoc Computat Linguist,"Liu, Nayu; Sun, Xian; Yul, Hongfeng; Zhangi, Wenkai; Xui, Guangluan",,,Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack finegrained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the multisource modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1834,1845,,,,,,,,,,,,,,,,WOS:000855160702001,0
C,"Zhang, M; Qian, TY",,,Assoc Computat Linguist,"Mi Zhang; Qian, Tieyun",,,Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like nothing special. Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation food-was is treated equally as an adjectival complement relation was-okay in food was okay. To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs. Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information. Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs. Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs. Extensive experiments on five benchmark datasets show that our method achieves the state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3540,3549,,,,,,,,,,,,,,,,WOS:000855160703061,0
C,"Pimentel, T; Saphra, N; Williams, A; Cotterell, R",,,Assoc Computat Linguist,"Pimentel, Tiago; Saphra, Naomi; Williams, Adina; Cotterell, Ryan",,,Pareto Probing: Trading Off Accuracy for Complexity,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations-e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations. Our code can be found at https://github.com/rycolab/pareto-probing.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3138,3153,,,,,,,,,,,,,,,,WOS:000855160703029,0
C,"Segal, E; Efrat, A; Shoham, M; Globerson, A; Berant, J",,,Assoc Computat Linguist,"Segal, Elad; Efrat, Avia; Shoham, Mor; Globerson, Amir; Berant, Jonathan",,,A Simple and Effective Model for Answering Multi-span Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and QUOREF by 9.9 and 5.5 EM points respectively.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3074,3080,,,,,,,,,,,,,,,,WOS:000855160703023,0
C,"Shi, HY; Livescu, K; Gimpel, K",,,Assoc Computat Linguist,"Shi, Haoyue; Livescu, Karen; Gimpel, Kevin",,,On the Role of Supervision in Unsupervised Constituency Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F-1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Fewshot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyper-parameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7611,7621,,,,,,,,,,,,,,,,WOS:000855160707061,0
C,"Spokoyny, D; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Spokoyny, Daniel; Berg-Kirkpatrick, Taylor",,,An Empirical Investigation of Contextualized Number Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We conduct a large scale empirical investigation of contextualized number prediction in running text. Specifically, we consider two tasks: (1) masked number prediction - predicting a missing numerical value within a sentence, and (2) numerical anomaly detection detecting an errorful numeric value within a sentence. We experiment with novel combinations of contextual encoders and output distributions over the real number line. Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recurrent and transformer-based encoder architectures. We evaluate these models on two numeric datasets in the financial and scientific domain. Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical prediction and anomaly detection. We also show that our models effectively utilize textual context and benefit from general-purpose unsupervised pretraining.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4754,4764,,,,,,,,,,,,,,,,WOS:000855160704073,0
C,"Wang, BX; Pei, HZ; Pan, BY; Chen, Q; Wang, SH; Li, B",,,Assoc Computat Linguist,"Wang, Boxin; Pei, Hengzhi; Pan, Boyuan; Chen, Qian; Wang, Shuohang; Li, Bo",,,T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence ( T3( SENT)) or word (T3( WORD)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models. Our code is publicly available at https://github.com/AI- secure/T3/.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6134,6150,,,,,,,,,,,,,,,,WOS:000855160706028,0
C,"Wu, D; Ding, L; Lu, F; Xie, J",,,Assoc Computat Linguist,"Wu, Di; Ding, Liang; Lu, Fan; Xie, Jian",,,SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77). In-depth analyses show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1932,1937,,,,,,,,,,,,,,,,WOS:000855160702009,0
C,"Wu, HY; Liu, Y; Shi, SY",,,Assoc Computat Linguist,"Wu, Haiyan; Liu, Ying; Shi, Shaoyun",,,Modularized Syntactic Neural Networks for Sentence Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees. In MSNN, each node of a syntax tree is modeled by a label-related syntax module. Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation. We design a tree-parallel mini-batch strategy for efficient training and predicting. Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2786,2792,,,,,,,,,,,,,,,,WOS:000855160702079,0
C,"Zhang, S; Frey, B; Bansal, M",,,Assoc Computat Linguist,"Zhang, Shiyue; Frey, Benjamin; Bansal, Mohit",,,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world, and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/En-Chr translations, respectively, and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,577,595,,,,,,,,,,,,,,,,WOS:000855160700043,0
C,"Zhang, Y; Guo, Z; Teng, Z; Lu, W; Cohen, SB; Liu, Z; Bing, L",,,Assoc Computat Linguist,"Zhang, Yan; Guo, Zhijiang; Teng, Zhiyang; Lu, Wei; Cohen, Shay B.; Liu, Zuozhu; Bing, Lidong",,,"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2162,2172,,,,,,,,,,,,,,,,WOS:000855160702026,0
C,"Atanasova, P; Wright, D; Augenstein, I",,,Assoc Computat Linguist,"Atanasova, Pepa; Wright, Dustin; Augenstein, Isabelle",,,Generating Label Cohesive and Well-Formed Adversarial Claims,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimizing the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3168,3177,,,,,,,,,,,,,,,,WOS:000855160703031,0
C,"Caglayan, O; Ive, J; Haralampieva, V; Madhyastha, P; Barrault, L; Specia, L",,,Assoc Computat Linguist,"Caglayan, Ozan; Ive, Julia; Haralampieva, Veneta; Madhyastha, Pranava; Barrault, Loic; Specia, Lucia",,,Simultaneous Machine Translation with Visual Context,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2350,2361,,,,,,,,,,,,,,,,WOS:000855160702041,0
C,"Carton, S; Rathore, A; Tan, CH",,,Assoc Computat Linguist,"Carton, Samuel; Rathore, Anirudh; Tan, Chenhao",,,Evaluating and Characterizing Human Rationales,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using fidelity curves to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9294,9307,,,,,,,,,,,,,,,,WOS:000855160709042,0
C,"Cavalin, P; Ribeiro, VHA; Appel, AP; Pinhanez, C",,,Assoc Computat Linguist,"Cavalin, Paulo; Ribeiro, Victor Henrique Alves; Appel, Ana Paula; Pinhanez, Claudio",,,Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes. The classification is carried out by mapping text embeddings to the word graph embeddings of the classes. Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved. In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3952,3961,,,,,,,,,,,,,,,,WOS:000855160704012,0
C,"Chan, W; Stern, M",,,Assoc Computat Linguist,"Chan, William; Stern, Mitchell",,,An Empirical Study of Generation Order for Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft orderreward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, locationbased orders, frequency-based orders, contentbased orders, and model-based orders. Curiously, we find that for the WMT'14 English ! German and WMT'18 English ! Chinese translation tasks, order does not have a substantial impact on output quality. Moreover, for English ! German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5764,5773,,,,,,,,,,,,,,,,WOS:000855160705074,0
C,"Guo, JQ; Liu, Q; Lou, JG; Li, Z; Liu, X; Xie, T; Liu, T",,,Assoc Computat Linguist,"Guo, Jiaqi; Qian Liu; Lou, Jian-Guang; Li, Zhenwen; Liu, Xueqing; Tao Xie; Ting Liu",,,Benchmarking Meaning Representations in Neural Semantic Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work's performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose UNIMER, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets x four meaning representations. A thorough experimental study on UNIMER reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github.com/JasperGuo/Unimer.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1520,1540,,,,,,,,,,,,,,,,WOS:000855160701058,0
C,"He, Y; Zhu, ZW; Zhang, Y; Chen, Q; Caverlee, J",,,Assoc Computat Linguist,"He, Yun; Zhu, Ziwei; Zhang, Yin; Chen, Qin; Caverlee, James",,,"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, Clinical-BERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4604,4614,,,,,,,,,,,,,,,,WOS:000855160704060,0
C,"Kobayashi, G; Kuribayashi, T; Yokoi, S; Inui, K",,,Assoc Computat Linguist,"Kobayashi, Goro; Kuribayashi, Tatsuki; Yokoi, Sho; Inui, Kentaro",,,Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7057,7075,,,,,,,,,,,,,,,,WOS:000855160707023,0
C,"Kuznetsov, I; Gurevych, I",,,Assoc Computat Linguist,"Kuznetsov, Ilia; Gurevych, Iryna",,,A Matter of Framing: The Impact of Linguistic Formalism on Probing Results,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep pre-trained contextualized encoders like BERT (Devlin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies and should be investigated along with the commonly used cross-task and cross-lingual experimental settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,171,182,,,,,,,,,,,,,,,,WOS:000855160700013,0
C,"Lan, WW; Chen, Y; Xu, W; Ritter, A",,,Assoc Computat Linguist,"Lan, Wuwei; Chen, Yang; Xu, Wei; Ritter, Alan",,,An Empirical Study of Pre-trained Transformers for Arabic Information Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable the effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT's effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at https://github.com/lanwuwei/GigaBERT.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4727,4734,,,,,,,,,,,,,,,,WOS:000855160704070,0
C,"Li, Z; Kumar, M; Headden, W; Yin, B; Wei, Y; Zhang, Y; Yang, Q",,,Assoc Computat Linguist,"Li, Zheng; Kumar, Mukul; Headden, William; Yin, Bing; Wei, Ying; Zhang, Yu; Yang, Qiang",,,Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The recent emergence of multilingual pretraining language model (mPLM) has enabled breakthroughs on various downstream crosslingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks. To address the issues, we propose a meta graph learning (MGL) method. Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages. Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to guide cross-lingual transfer explicitly. Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2290,2301,,,,,,,,,,,,,,,,WOS:000855160702036,0
C,"Liu, DH; Gong, YY; Fu, J; Yan, Y; Chen, JS; Lv, JC; Duan, N; Zhou, M",,,Assoc Computat Linguist,"Liu, Dayiheng; Gong, Yeyun; Fu, Jie; Yan, Yu; Chen, Jiusheng; Lv, Jiancheng; Duan, Nan; Zhou, Ming",,,Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradientbased optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5798,5810,,,,,,,,,,,,,,,,WOS:000855160705077,0
C,"Liu, J; Chen, YB; Liu, K; Bi, W; Liu, XJ",,,Assoc Computat Linguist,"Liu, Jian; Chen, Yubo; Liu, Kang; Bi, Wei; Liu, Xiaojiang",,,Event Extraction as Machine Reading Comprehension,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are data-hungry and suffer from the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1641,1651,,,,,,,,,,,,,,,,WOS:000855160701068,0
C,"Liu, ZY; Cao, YX; Pan, LM; Li, JZ; Liu, ZY; Chua, TS",,,Assoc Computat Linguist,"Liu, Zhiyuan; Cao, Yixin; Pan, Liangming; Li, Juanzi; Liu, Zhiyuan; Chua, Tat-Seng",,,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6355,6364,,,,,,,,,,,,,,,,WOS:000855160706048,0
C,"Liu, ZH; Winata, GI; Xu, P; Lin, ZJ; Fung, P",,,Assoc Computat Linguist,"Liu, Zihan; Winata, Genta Indra; Xu, Peng; Lin, Zhaojiang; Fung, Pascale",,,Cross-lingual Spoken Language Understanding with Regularized Representation Alignment,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019a) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7241,7251,,,,,,,,,,,,,,,,WOS:000855160707034,0
C,"Moon, S; Okazaki, N",,,Assoc Computat Linguist,"Moon, Sangwhan; Okazaki, Naoaki",,,"PatchBERT: Just-in-Time, Out-of-Vocabulary Patching",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7846,7852,,,,,,,,,,,,,,,,WOS:000855160708006,0
C,"Tan, B; Qin, LH; Xing, EP; Hu, ZT",,,Assoc Computat Linguist,"Tan, Bowen; Qin, Lianhui; Xing, Eric P.; Hu, Zhiting",,,Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice. Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia. Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6301,6309,,,,,,,,,,,,,,,,WOS:000855160706043,0
C,"Tan, H; Bansal, M",,,Assoc Computat Linguist,"Tan, Hao; Bansal, Mohit",,,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora. Therefore, we develop a technique named vokenization that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call vokens). The vokenizer is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora. Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2066,2080,,,,,,,,,,,,,,,,WOS:000855160702019,0
C,"Tandon, N; Sakaguchi, K; Mishra, BD; Rajagopal, D; Clark, P; Guerquin, M; Richardson, K; Hovy, E",,,Assoc Computat Linguist,"Tandon, Niket; Sakaguchi, Keisuke; Mishra, Bhavana Dalvi; Rajagopal, Dheeraj; Clark, Peter; Guerquin, Michal; Richardson, Kyle; Hovy, Eduard",,,A Dataset for Tracking Entities in Open Domain Procedural Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous formulations of this task provide the text and entities involved, and ask how those entities change for just a small, pre-defined set of attributes (e.g., location), limiting their fidelity. Our solution is a new task formulation where given just a procedural text as input, the task is to generate a set of state change tuples (entity, attribute, before-state, after-state) for each step, where the entity, attribute, and state values must be predicted from an open vocabulary. Using crowdsourcing, we create OPENPI1, a high-quality (91.5% coverage as judged by humans and completely vetted), and largescale dataset comprising 29,928 state changes over 4,050 sentences from 810 procedural realworld paragraphs from WikiHow.com. A current state-of-the-art generation model on this task achieves 16.1% F1 based on BLEU metric, leaving enough room for novel model architectures.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6408,6417,,,,,,,,,,,,,,,,WOS:000855160706053,0
C,"Terenin, A; Magnusson, M; Jonsson, L",,,Assoc Computat Linguist,"Terenin, Alexander; Magnusson, Mans; Jonsson, Leif",,,Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language-an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2925,2934,,,,,,,,,,,,,,,,WOS:000855160703009,0
C,"Wang, SH; Fang, YW; Sun, SQ; Gan, Z; Cheng, Y; Liu, JJ; Jiang, J",,,Assoc Computat Linguist,"Wang, Shuohang; Fang, Yuwei; Sun, Siqi; Gan, Zhe; Cheng, Yu; Liu, Jingjing; Jiang, Jing",,,Cross-Thought for Sentence Encoder Pre-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,412,421,,,,,,,,,,,,,,,,WOS:000855160700030,0
C,"Wang, X; Jiang, Y; Bach, N; Wang, T; Huang, Z; Huang, F; Tu, K",,,Assoc Computat Linguist,"Wang, Xinyu; Jiang, Yong; Bach, Nguyen; Wang, Tao; Huang, Zhongqiang; Huang, Fei; Tu, Kewei",,,AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6019,6026,,,,,,,,,,,,,,,,WOS:000855160706018,0
C,"Yao, WL; Dai, ZY; Ramaswamy, M; Min, BA; Huang, RH",,,Assoc Computat Linguist,"Yao, Wenlin; Dai, Zeyu; Ramaswamy, Maitreyi; Min, Bonan; Huang, Ruihong",,,Weakly Supervised Subevent Knowledge Acquisition,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base. We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents. Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs. The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types. The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5345,5356,,,,,,,,,,,,,,,,WOS:000855160705040,0
C,"Zhang, L; Zhu, HY; Brahma, S; Li, YY",,,Assoc Computat Linguist,"Zhang, Li; Zhu, Huaiyu; Brahma, Siddhartha; Li, Yunyao",,,Small but Mighty: New Benchmarks for Split and Rephrase,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable syntactic cues caused by its automatic generation process. Taking advantage of such cues, we show that even a simple rule-based model can perform on par with the state-of-the-art model. To remedy such limitations, we collect and release two crowdsourced benchmark datasets. We not only make sure that they contain significantly more diverse syntax, but also carefully control for their quality according to a well-defined set of criteria. While no satisfactory automatic metric exists, we apply fine-grained manual evaluation based on these criteria using crowdsourcing, showing that our datasets better represent the task and are significantly more challenging for the models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1198,1205,,,,,,,,,,,,,,,,WOS:000855160701031,0
C,"Zhu, TG; Wang, Y; Li, HR; Wu, YZ; He, XD; Zhou, BW",,,Assoc Computat Linguist,"Zhu, Tiangang; Wang, Yue; Li, Haoran; Wu, Youzheng; He, Xiaodong; Zhou, Bowen",,JD AI Research,Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github.com/jd-aig/JAVE.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2129,2139,,,,,,,,,,,,,,,,WOS:000855160702023,0
C,"Behnke, M; Heafield, K",,,Assoc Computat Linguist,"Behnke, Maximiliana; Heafield, Kenneth",,,Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned after training. However, removing them before training a model results in lower quality. In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training, instead of doing so on a fully converged model. Our experiments on machine translation show that it is possible to remove up to three-quarters of all attention heads from a transformer-big model with an average -0.1 change in BLEU for Turkish -> English. The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training. The method is complementary to other approaches, such as teacher-student, with our English!German student losing 0.2 BLEU at 75% encoder attention sparsity.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2664,2674,,,,,,,,,,,,,,,,WOS:000855160702068,0
C,"Chen, RC; Lee, CJ",,,Assoc Computat Linguist,"Chen, Ruey-Cheng; Lee, Chia-Jung",,,Incorporating Behavioral Hypotheses for Query Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3105,3110,,,,,,,,,,,,,,,,WOS:000855160703026,0
C,"Clement, CB; Drain, D; Timcheck, J; Svyatkovskiy, A; Sundaresan, N",,,Assoc Computat Linguist,"Clement, Colin B.; Drain, Dawn; Timcheck, Jonathan; Svyatkovskiy, Alexey; Sundaresan, Neel",,,PYMT5: multi-mode translation of natural language and PYTHON code with transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PYMT5, the PYTHON method text-to-text transfer transformer, which is trained to translate between all pairs of PYTHON method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million PYTHON methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PYMT5 outperforms similarlysized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CODE-SEARCHNET test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9052,9065,,,,,,,,,,,,,,,,WOS:000855160709023,0
C,"Dinan, E; Fan, A; Williams, A; Urbanek, J; Kiela, D; Weston, J",,,Assoc Computat Linguist,"Dinan, Emily; Fan, Angela; Williams, Adina; Urbanek, Jack; Kiela, Douwe; Weston, Jason",,,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT (Urbanek et al., 2019), as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods-including the quantity of gendered words, a dialogue safety classifier, and human assessments-all of which show that our models generate less gendered, but equally engaging chit-chat responses.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8173,8188,,,,,,,,,,,,,,,,WOS:000855160708031,0
C,"Dutt, R; Joshi, R; Rose, CP",,,Assoc Computat Linguist,"Dutt, Ritam; Joshi, Rishabh; Rose, Carolyn Penstein",,,Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7473,7485,,,,,,,,,,,,,,,,WOS:000855160707052,0
C,"Fu, L; Fussell, SR; Danescu-Niculescu-Mizil, C",,,Assoc Computat Linguist,"Fu, Liye; Fussell, Susan R.; Danescu-Niculescu-Mizil, Cristian",,,Facilitating the Communication of Politeness through Fine-Grained Paraphrasing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance. As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance. We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker's intentions and the listener's perceptions in both cases.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5127,5140,,,,,,,,,,,,,,,,WOS:000855160705026,0
C,"Gu, YX; Zhang, ZY; Wang, XZ; Liu, ZY; Sun, MS",,,Assoc Computat Linguist,"Gu, Yuxian; Zhang, Zhengyan; Wang, Xiaozhi; Liu, Zhiyuan; Sun, Maosong",,,Train No Evil: Selective Masking for Task-Guided Pre-Training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns. In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning. In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns. Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens. Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient. The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6966,6974,,,,,,,,,,,,,,,,WOS:000855160707015,0
C,"Han, RJ; Zhou, YC; Peng, NY",,,Assoc Computat Linguist,"Han, Rujun; Zhou, Yichao; Peng, Nanyun",,,Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5717,5729,,,,,,,,,,,,,,,,WOS:000855160705071,0
C,"Hegel, A; Rao, S; Celikyilmaz, A; Dolan, B",,,Assoc Computat Linguist,"Hegel, Allison; Rao, Sudha; Celikyilmaz, Asli; Dolan, Bill",,,Substance over Style: Document-Level Targeted Content Transfer,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pretrained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs.(1) Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model's rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6485,6504,,,,,,,,,,,,,,,,WOS:000855160706059,0
C,"Hessel, J; Lee, L",,,Assoc Computat Linguist,"Hessel, Jack; Lee, Lillian",,,Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,861,877,,,,,,,,,,,,,,,,WOS:000855160701002,0
C,"Kong, LK; Jiang, HM; Zhuang, YC; Lyu, J; Zhao, T; Zhang, C",,,Assoc Computat Linguist,"Kong, Lingkai; Jiang, Haoming; Zhuang, Yuchen; Lyu, Jie; Zhao, Tuo; Zhang, Chao",,,Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1326,1340,,,,,,,,,,,,,,,,WOS:000855160701042,0
C,"Kotonya, N; Toni, F",,,Assoc Computat Linguist,"Kotonya, Neema; Toni, Francesca",,,Explainable Automated Fact-Checking for Public Health Claims,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims(1). We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7740,7754,,,,,,,,,,,,,,,,WOS:000855160707070,0
C,"Lin, BY; Lee, S; Khanna, R; Ren, X",,,Assoc Computat Linguist,"Lin, Bill Yuchen; Lee, Seyeon; Khanna, Rahul; Ren, Xiang",,,Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as neural knowledge bases via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NUMERSENSE1, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6862,6868,,,,,,,,,,,,,,,,WOS:000855160707006,0
C,"Louis, A; Roth, D; Radlinski, F",,,Assoc Computat Linguist,"Louis, Annie; Roth, Dan; Radlinski, Filip",,,I'd rather just go to bed: Understanding Indirect Answers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret 'I'm starving.' in response to 'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today's systems are only as sensitive to these pragmatic moves as their language model allows. We create and release(1) the first large-scale English language corpus 'Circa' with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7411,7425,,,,,,,,,,,,,,,,WOS:000855160707048,0
C,"Lu, J; Ng, V",,,Assoc Computat Linguist,"Lu, Jing; Ng, Vincent",,,Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6620,6631,,,,,,,,,,,,,,,,WOS:000855160706069,0
C,"Luo, Y; Zhao, H; Zhan, JL",,,Assoc Computat Linguist,"Luo, Ying; Zhao, Hai; Zhan, Junlang",,,Named Entity Recognition Only from Word Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features. However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources. In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings. We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks. Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8995,9005,,,,,,,,,,,,,,,,WOS:000855160709018,0
C,"Madureira, B; Schlangen, D",,,Assoc Computat Linguist,"Madureira, Brielen; Schlangen, David",,,Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The omni-directional BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,357,374,,,,,,,,,,,,,,,,WOS:000855160700026,0
C,"Nangia, N; Vania, C; Bhalerao, R; Bowman, SR",,,Assoc Computat Linguist,"Nangia, Nikita; Vania, Clara; Bhalerao, Rasika; Bowman, Samuel R.",,,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Warning: This paper contains explicit statements of offensive stereotypes and may be upsetting. Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1953,1967,,,,,,,,,,,,,,,,WOS:000855160702011,0
C,"Nielsen, E; Steedman, M; Goldwater, S",,,Assoc Computat Linguist,"Nielsen, Elizabeth; Steedman, Mark; Goldwater, Sharon",,,The role of context in neural pitch accent detection in English,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5 percent to 88.7 percent accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2 percent accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7994,8000,,,,,,,,,,,,,,,,WOS:000855160708017,0
C,"Ousidhoum, N; Song, YQ; Yeung, DY",,,Assoc Computat Linguist,"Ousidhoum, Nedjma; Song, Yangqiu; Yeung, Dit-Yan",,,Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora. We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2532,2542,,,,,,,,,,,,,,,,WOS:000855160702056,0
C,"Pethe, C; Kim, A; Skiena, S",,,Assoc Computat Linguist,"Pethe, Charuta; Kim, Allen; Skiena, Steven",,,Chapter Captor: Text Segmentation in Novels,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving an F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8373,8383,,,,,,,,,,,,,,,,WOS:000855160708047,0
C,"Saha, S; Ghosh, S; Srivastava, S; Bansal, M",,,Assoc Computat Linguist,"Saha, Swarnadeep; Ghosh, Sayan; Srivastava, Shashank; Bansal, Mohit",,,PROVER: Proof Generation for Interpretable Reasoning over Rules,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent work by Clark et al. (2020) shows that transformers can act as soft theorem provers by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PROVER generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PROVER obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for depth 5, indicating significant scope for future work.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,122,136,,,,,,,,,,,,,,,,WOS:000855160700009,0
C,"Singh, LG; Mitra, A; Singh, SR",,,Assoc Computat Linguist,"Singh, Loitongbam Gyanendro; Mitra, Anasua; Singh, Sanasam Ranbir",,,Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8932,8946,,,,,,,,,,,,,,,,WOS:000855160709013,0
C,"Song, HY; Yan, W; Zhang, WN; Zhao, ZY; Liu, T; Liu, XJ",,,Assoc Computat Linguist,"Song, Haoyu; Yan Wang; Zhang, Wei Nan; Zhao, Zhengyu; Ting Liu; Xiaojiang Liu",,,Profile Consistency Identification for Open-domain Dialogue Agents,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6651,6662,,,,,,,,,,,,,,,,WOS:000855160706072,0
C,"Tan, R; Plummer, BA; Saenko, K",,,Assoc Computat Linguist,"Tan, Reuben; Plummer, Bryan A.; Saenko, Kate",,,Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation. Our code and dataset can be downloaded from here.",,,,,,"Saenko, Kate/0000-0002-7564-7218",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2081,2106,,,,,,,,,,,,,,,,WOS:000855160702020,0
C,"Thompson, B; Post, M",,,Assoc Computat Linguist,"Thompson, Brian; Post, Matt",,,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech). This results in the paraphraser's output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and does not require human judgements for training. Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data). We also explore using our model for the task of quality estimation as a metric-conditioning on the source instead of the reference-and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,90,121,,,,,,,,,,,,,,,,WOS:000855160700008,0
C,"Wang, RZ; Tang, DY; Duan, N; Zhong, WJ; Wei, ZY; Huang, XJ; Jiang, DX; Zhou, M",,,Assoc Computat Linguist,"Wang, Ruize; Tang, Duyu; Duan, Nan; Zhong, Wanjun; Wei, Zhongyu; Huang, Xuanjing; Jiang, Daxin; Zhou, Ming",,,Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3895,3903,,,,,,,,,,,,,,,,WOS:000855160704008,0
C,"Weng, RX; Yu, H; Wei, XP; Luo, WH",,,Assoc Computat Linguist,"Weng, Rongxiang; Yu, Heng; Wei, Xiangpeng; Luo, Weihua",,,Towards Enhancing Faithfulness for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FENMT). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FENMT could improve translation quality by effectively reducing unfaithful translations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2675,2684,,,,,,,,,,,,,,,,WOS:000855160702069,0
C,"Xia, P; Wu, SJ; Van Durme, B",,,Assoc Computat Linguist,"Xia, Patrick; Wu, Shijie; Van Durme, Benjamin",,,Which *BERT? A Survey Organizing Contextualized Encoders,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find that enough has now been discovered, in different directions, that we can begin to organize advances according to common themes. Through this organization, we highlight important considerations when interpreting recent contributions and choosing which model to use.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7516,7533,,,,,,,,,,,,,,,,WOS:000855160707055,0
C,"Xu, CW; Zhou, WCS; Ge, T; Wei, FR; Zhou, M",,,Assoc Computat Linguist,"Xu, Canwen; Zhou, Wangchunshu; Ge, Tao; Wei, Furu; Zhou, Ming",,,BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7859,7869,,,,,,,,,,,,,,,,WOS:000855160708008,0
C,"Xu, L; Choi, JD",,,Assoc Computat Linguist,"Xu, Liyan; Choi, Jinho D.",,,Revealing the Myth of Higher-Order Inference in Coreference Resolution,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an endto-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1. of 80.2 on the CoNLL 2012 shared task dataset in English.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8527,8533,,,,,,,,,,,,,,,,WOS:000855160708061,0
C,"Xu, Y; Lapata, M",,,Assoc Computat Linguist,"Xu, Yumo; Lapata, Mirella",,,Coarse-to-Fine Query Focused Multi-Document Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework(1) is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3632,3645,,,,,,,,,,,,,,,,WOS:000855160703071,0
C,"Zhang, NY; Deng, SM; Bi, Z; Yu, HY; Yang, JC; Chen, MS; Huang, F; Zhang, W; Chen, HJ",,,Assoc Computat Linguist,"Zhang, Ningyu; Deng, Shumin; Bi, Zhen; Yu, Haiyang; Yang, Jiacheng; Chen, Mosha; Huang, Fei; Zhang, Wei; Chen, Huajun",,,OpenUE: An Open Toolkit of Universal Extraction from Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language processing covers a wide variety of tasks with token-level or sentence-level understandings. In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and extensible toolkit called OpenUE for various extraction tasks. OpenUE allows developers to train custom models to extract information from the text and supports quick model validation for researchers. Besides, OpenUE provides various functional modules to maintain sufficient modularity and extensibility. Except for the toolkit, we also deploy an online demo(1) with restful APIs to support real-time extraction without training and deploying. Additionally, the online system can extract information in various tasks, including relational triple extraction, slot & intent detection, event extraction, and so on.",,,,,"Deng, Shumin/AAP-7003-2021",,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,1,8,,,,,,,,,,,,,,,,WOS:000855177700001,0
C,"Zou, YY; Zhang, XX; Lu, W; Wei, F; Zhou, M",,,Assoc Computat Linguist,"Zou, Yanyan; Zhang, Xingxing; Wei Lu; Furu Wei; Ming Zhou",,,Pre-training for Abstractive Document Summarization by Reinstating Source Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (>= 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness. Code and models are public available at https://github.com/zoezou2015/abs_pretraining.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3646,3660,,,,,,,,,,,,,,,,WOS:000855160703072,0
C,"Akoury, N; Wang, SF; Whiting, J; Hood, S; Peng, NY; Iyyer, M",,,Assoc Computat Linguist,"Akoury, Nader; Wang, Shufan; Whiting, Josh; Hood, Stephen; Peng, Nanyun; Iyyer, Mohit",,,STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6470,6484,,,,,,,,,,,,,,,,WOS:000855160706058,0
C,"Chen, X; Ghoshal, A; Mehdad, Y; Zettlemoyer, L; Gupta, S",,,Assoc Computat Linguist,"Chen, Xilun; Ghoshal, Asish; Mehdad, Yashar; Zettlemoyer, Luke; Gupta, Sonal",,,Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user's intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al., 2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting taskoriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2020) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to lowresource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain taskoriented semantic parsing dataset (TOPv21).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5090,5100,,,,,,,,,,,,,,,,WOS:000855160705023,0
C,"Cheng, LY; Wu, DK; Bing, LD; Zhang, Y; Jie, ZM; Lu, W; Si, L",,,Assoc Computat Linguist,"Cheng, Liying; Wu, Dekun; Bing, Lidong; Zhang, Yan; Jie, Zhanming; Lu, Wei; Si, Luo",,,ENT-DESC: Entity Description Generation by Exploring Knowledge Graph,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in KG-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1187,1197,,,,,,,,,,,,,,,,WOS:000855160701030,0
C,"De Cao, N; Schlichtkrull, M; Aziz, W; Titov, I",,,Assoc Computat Linguist,"De Cao, Nicola; Schlichtkrull, Michael; Aziz, Wilker; Titov, Ivan",,,How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model 'knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DIFFMASK learns to maskout subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network 'knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DIFFMASK to study BERT models on sentiment classification and question answering.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3243,3255,,,,,,,,,,,,,,,,WOS:000855160703037,0
C,"Ferritto, A; Pan, L; Chakravarti, R; Roukos, S; Florian, R; Murdock, JW; Sil, A",,,Assoc Computat Linguist,"Ferritto, Anthony; Pan, Lin; Chakravarti, Rishav; Roukos, Salim; Florian, Radu; Murdock, J. William; Sil, Avirup",,,ARES: A Reading Comprehension Ensembling Service,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce ARES (A Reading Comprehension Ensembling Service): a novel Machine Reading Comprehension (MRC) demonstration system which utilizes an ensemble of models to increase F1 by 2.3 points. While many of the top leaderboard submissions in popular MRC benchmarks such as the Stanford Question Answering Dataset (SQuAD) and Natural Questions (NQ) use model ensembles, the accompanying papers do not publish their ensembling strategies. In this work, we detail and evaluate various ensembling strategies using the NQ dataset. ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,31,37,,,,,,,,,,,,,,,,WOS:000855177700005,0
C,"Gu, SH; Zhang, JC; Meng, FD; Feng, Y; Xie, WY; Zhou, J; Yu, D",,,Assoc Computat Linguist,"Gu, Shuhao; Zhang, Jinchao; Meng, Fandong; Feng, Yang; Xie, Wanying; Zhou, Jie; Yu, Dong",,,Token-level Adaptive Training for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, ENRO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1035,1046,,,,,,,,,,,,,,,,WOS:000855160701016,0
C,"Guan, J; Huang, ML",,,Assoc Computat Linguist,"Guan, Jian; Huang, Minlie",,,UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious oneto-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable UN referenced metric for evaluating Opene-Nded story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-theart metrics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9157,9166,,,,,,,,,,,,,,,,WOS:000855160709031,0
C,"Hosseini, K; Nanni, F; Ardanuy, MC",,,Assoc Computat Linguist,"Hosseini, Kasra; Nanni, Federico; Ardanuy, Mariona Coll",,,DeezyMatch: A Flexible Deep Learning Approach to Fuzzy String Matching,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present DeezyMatch, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for transfer learning in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned DeezyMatch models can be used to generate rich vector representations from string inputs. The candidate ranker component in DeezyMatch uses these vector representations to find, for a given query, the best matching candidates in a knowledge base. It uses an adaptive searching algorithm applicable to large knowledge bases and query sets. We describe DeezyMatch's functionality, design and implementation, accompanied by a use case in toponym matching and candidate ranking in realistic noisy datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,62,69,,,,,,,,,,,,,,,,WOS:000855177700009,0
C,"Lin, ZB; Cai, D; Wang, Y; Liu, XJ; Zheng, HT; Shi, SM",,,Assoc Computat Linguist,"Lin, Zibo; Cai, Deng; Wang, Yan; Liu, Xiaojiang; Zheng, Hai-Tao; Shi, Shuming",,,The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the traintest discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9220,9229,,,,,,,,,,,,,,,,WOS:000855160709036,0
C,"Xu, L; Bing, LD; Lu, W; Huang, F",,,Assoc Computat Linguist,"Lu Xu; Bing, Lidong; Wei Lu; Fei Huang",,,Aspect Sentiment Classification with Aspect-Specific Opinion Spans,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect sentiment classification, predicting the sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works are either not able to capture opinion spans as a whole or capture variable-length opinion spans. In this paper, we present a neat and effective multiple CRFs based structured attention model that is capable of extracting aspect-specific opinion spans. The sentiment polarity of the target is then classified based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3561,3567,,,,,,,,,,,,,,,,WOS:000855160703063,0
C,"Sen, P; Danilevsky, M; Li, YY; Brahma, S; Boehm, M; Chiticariu, L; Krishnamurthy, R",,,Assoc Computat Linguist,"Sen, Prithviraj; Danilevsky, Marina; Li, Yunyao; Brahma, Siddhartha; Boehm, Matthias; Chiticariu, Laura; Krishnamurthy, Rajasekar",,,Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4211,4221,,,,,,,,,,,,,,,,WOS:000855160704033,0
C,"Song, CZ; Rush, AM; Shmatikov, V",,,Assoc Computat Linguist,"Song, Congzheng; Rush, Alexander M.; Shmatikov, Vitaly",,,Adversarial Semantic Collisions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-including paraphrase identification, document retrieval, response suggestion, and extractive summarization-are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4198,4210,,,,,,,,,,,,,,,,WOS:000855160704032,0
C,"Talimaz, E; Giulianelli, M; Pezzelle, S; Sinclair, A; Fernandez, R",,,Assoc Computat Linguist,"Talimaz, Ece; Giulianelli, Mario; Pezzelle, Sandro; Sinclair, Arabella; Fernandez, Raquel",,,"Refer, Reuse, Reduce Generating Subsequent References in Visual and Conversational Contexts",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4350,4368,,,,,,,,,,,,,,,,WOS:000855160704041,0
C,"Tan, F; Hu, YF; Hu, CW; Li, KQ; Yen, K",,,Assoc Computat Linguist,"Tan, Fei; Hu, Yifan; Hu, Changwei; Li, Keqian; Yen, Kevin",,,TNT: Text Normalization based Pre-training of Transformers for Content Moderation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion. Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery. As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task. Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4735,4741,,,,,,,,,,,,,,,,WOS:000855160704071,0
C,"Warstadt, A; Zhang, Y; Li, HS; Liu, HK; Bowman, SR",,,Assoc Computat Linguist,"Warstadt, Alex; Zhang, Yian; Li, Haau-Sing; Liu, Haokun; Bowman, Samuel R.",,,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa(BASE). We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTaBASE does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,217,235,,,,,,,,,,,,,,,,WOS:000855160700016,0
C,"Xu, P; Patwary, M; Shoeybi, M; Puri, R; Fung, P; Anandkumar, A; Catanzaro, B",,,Assoc Computat Linguist,"Xu, Peng; Patwary, Mostofa; Shoeybi, Mohammad; Puri, Raul; Fung, Pascale; Anandkumar, Anima; Catanzaro, Bryan",,,MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2831,2845,,,,,,,,,,,,,,,,WOS:000855160703001,0
C,"Zhang, Y; He, RD; Liu, ZH; Lim, KH; Bing, L",,,Assoc Computat Linguist,"Yan Zhang; He, Ruidan; Liu, Zuozhu; Lim, Kwan Hui; Bing, Lidong",,,An Unsupervised Sentence Embedding Method by Mutual Information Maximization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1601,1610,,,,,,,,,,,,,,,,WOS:000855160701064,0
C,"Yu, CL; Han, JL; Wang, PF; Song, YQ; Zhang, H; Ng, W; Shi, SM",,,Assoc Computat Linguist,"Yu, Changlong; Han, Jialong; Wang, Peifeng; Song, Yangqiu; Zhang, Hongming; Ng, Wilfred; Shi, Shuming",,,When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We address hypernymy detection, i.e., whether an is-a relationship exists between words (x; y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x; y) pairs relieved. However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern. For the first time, this paper quantifies the non-negligible existence of those specific cases. We also demonstrate that distributional methods are ideal to make up for patternbased ones in such cases. We devise a complementary framework, under which a patternbased and a distributional model collaborate seamlessly in cases which they each prefer. On several benchmark datasets, our framework achieves competitive improvements and the case study shows its better interpretability.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6208,6217,,,,,,,,,,,,,,,,WOS:000855160706035,0
C,"Zhong, V; Lewis, M; Wang, SI; Zettlemoyer, L",,,Assoc Computat Linguist,"Zhong, Victor; Lewis, Mike; Wang, Sida I.; Zettlemoyer, Luke",,,Grounded Adaptation for Zero-shot Executable Semantic Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6869,6882,,,,,,,,,,,,,,,,WOS:000855160707007,0
C,"Zhu, QF; Zhang, WN; Liu, T; Wang, WY",,,Assoc Computat Linguist,"Zhu, Qingfu; Zhang, Weinan; Liu, Ting; Wang, William Yang",,,Counterfactual Off-Policy Training for Neural Dialogue Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3438,3448,,,,,,,,,,,,,,,,WOS:000855160703051,0
C,"August, T; Kim, L; Reinecke, K; Smith, NA",,,Assoc Computat Linguist,"August, Tal; Kim, Lauren; Reinecke, Katharina; Smith, Noah A.",,,Writing Strategies for Science Communication: Data and Computational Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly support science communication efforts by enabling tools to detect and suggest strategies for writers. We compile a set of writing strategies drawn from a wide range of prescriptive sources and develop an annotation scheme allowing humans to recognize them. We collect a corpus of 128K science writing documents in English and annotate a subset of this corpus.1 We use the annotations to train transformer-based classifiers and measure the strategies' use in the larger corpus. We find that the use of strategies, such as storytelling and emphasizing the most important findings, varies significantly across publications with different reader audiences.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5327,5344,,,,,,,,,,,,,,,,WOS:000855160705039,0
C,"Chaudhury, S; Kimura, D; Talamadupula, K; Tatsubori, M; Munawar, A; Tachibana, R",,,Assoc Computat Linguist,"Chaudhury, Subhajit; Kimura, Daiki; Talamadupula, Kartik; Tatsubori, Michiaki; Munawar, Asim; Tachibana, Ryuki",,,Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second boot-strapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3002,3008,,,,,,,,,,,,,,,,WOS:000855160703016,0
C,"Nguyen, DQ; Vu, T; Nguyen, AT",,,Assoc Computat Linguist,Dat Quoc Nguyen; Thanh Vu; Anh Tuan Nguyen,,,BERTweet: A pre-trained language model for English Tweets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERTbase (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa(base) and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,9,14,,,,,,,,,,,,,,,,WOS:000855177700002,0
C,"Freitag, M; Grangier, D; Caswell, I",,,Assoc Computat Linguist,"Freitag, Markus; Grangier, David; Caswell, Isaac",,,BLEU might be Guilty but References are not Innocent,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English -> German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,61,71,,,,,,,,,,,,,,,,WOS:000855160700005,0
C,"Gao, S; Gormley, MR",,,Assoc Computat Linguist,"Gao, Sida; Gormley, Matthew R.",,,Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are rarely used due to their prohibitive computational cost. In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling. Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging. We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical. The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03. We obtain new state-of-the-art results on Dutch.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4999,5011,,,,,,,,,,,,,,,,WOS:000855160705016,0
C,"Han, XC; Tsvetkov, Y",,,Assoc Computat Linguist,"Han, Xiaochuang; Tsvetkov, Yulia",,,Fortifying Toxic Speech Detectors Against Veiled Toxicity,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector's training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.(1) Warning: this paper contains examples that may be offensive or upsetting.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7732,7739,,,,,,,,,,,,,,,,WOS:000855160707069,0
C,"He, JF; Zhang, XC; Lei, S; Chen, ZQ; Chen, FG; Alhamadani, A; Xiao, B; Lu, CT",,,Assoc Computat Linguist,"He, Jianfeng; Zhang, Xuchao; Lei, Shuo; Chen, Zhiqian; Chen, Fanglan; Alhamadani, Abdulaziz; Xiao, Bei; Lu, Chang-Tien",,,Towards More Accurate Uncertainty Estimation In Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed for rectification. However, few uncertainty models focus on improving the performance of text classification where human resources are involved. To achieve this, we aim at generating accurate uncertainty score by improving the confidence of winning scores. Thus, a model called MSD, which includes three independent components as mix-up, self-ensembling, distinctiveness score, is proposed to improve the accuracy of uncertainty score by reducing the effect of overconfidence of winning score and considering the impact of different categories of uncertainty simultaneously. MSD can be applied with different Deep Neural Networks. Extensive experiments with ablation setting are conducted on four real-world datasets, on which, competitive results are obtained.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8362,8372,,,,,,,,,,,,,,,,WOS:000855160708046,0
C,"Jiang, CY; Zhao, YG; Chu, SB; Shen, LB; Tu, KW",,,Assoc Computat Linguist,"Jiang, Chengyue; Zhao, Yinggong; Chu, Shanbo; Shen, Libin; Tu, Kewei",,,Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios. In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules. An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios. It can also utilize labeled data for training to achieve improved prediction accuracy. After training, an FA-RNN often remains interpretable and can be converted back into regular expressions. We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3193,3207,,,,,,,,,,,,,,,,WOS:000855160703033,0
C,"Kato, Y; Matsubara, S",,,Assoc Computat Linguist,"Kato, Yoshihide; Matsubara, Shigeki",,,Parsing Gapping Constructions Based on Grammatical and Semantic Roles,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.,,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2747,2752,,,,,,,,,,,,,,,,WOS:000855160702075,0
C,"Li, B; Thomas, G; Xu, Y; Rudzicz, F",,,Assoc Computat Linguist,"Li, Bai; Thomas, Guillaume; Xu, Yang; Rudzicz, Frank",,,Word class flexibility: A deep contextualized approach,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages(1). We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,983,994,,,,,,,,,,,,,,,,WOS:000855160701011,0
C,"Lianhui, Q; Vered, S; Peter, W; Chandra, B; Jena, DH; Ronan, LB; Antoine, B; Choi, Y",,,Assoc Computat Linguist,"Lianhui, Qin; Vered, Shwartz; Peter, West; Chandra, Bhagavatula; Jena, D. Hwang; Ronan, Le Bras; Antoine, Bosselut; Choi, Yejin",,,Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DELOREAN, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DELOREAN can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DELOREAN outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,794,805,,,,,,,,,,,,,,,,WOS:000855160700058,0
C,"Lindemann, M; Groschwitz, J; Koller, A",,,Assoc Computat Linguist,"Lindemann, Matthias; Groschwitz, Jonas; Koller, Alexander",,,Fast semantic parsing with well-typedness guarantees,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3929,3951,,,,,,,,,,,,,,,,WOS:000855160704011,0
C,"Patra, B; Bhattacharya, P; Fufa, C; Lee, C",,,Assoc Computat Linguist,"Patra, Barun; Bhattacharya, Pamela; Fufa, Chala; Lee, Charles",,,To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"State of the art research for date-time(1) entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don't fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task. Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time. We showcase a novel model for extracting task-specific date-time entities along with their negation constraints. We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant. Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8445,8455,,,,,,,,,,,,,,,,WOS:000855160708053,0
C,"Pfeiffer, J; Ruckle, A; Poth, C; Kamath, A; Vulic, I; Ruder, S; Cho, K; Gurevych, I",,,Assoc Computat Linguist,"Pfeiffer, Jonas; Ruckle, Andreas; Poth, Clifton; Kamath, Aishwarya; Vulic, Ivan; Ruder, Sebastian; Cho, Kyunghyun; Gurevych, Iryna",,,AdapterHub: A Framework for Adapting Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of hundreds of millions, or even billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters-small learnt bottleneck layers inserted within each layer of a pre-trained model-ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic stichingin of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,46,54,,,,,,,,,,,,,,,,WOS:000855177700007,0
C,"Trivedi, H; Balasubramanian, N; Khot, T; Sabharwal, A",,,Assoc Computat Linguist,"Trivedi, Harsh; Balasubramanian, Niranjan; Khot, Tushar; Sabharwal, Ashish",,,Is Multihop QA in DIRE Condition? Measuring and Reducing Disconnected Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments(1) suggest that there hasn't been much progress in multifact QA in the reading comprehension setting. For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning (19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8846,8863,,,,,,,,,,,,,,,,WOS:000855160709007,0
C,"Voita, E; Titov, I",,,Assoc Computat Linguist,"Voita, Elena; Titov, Ivan",,,Information-Theoretic Probing with Minimum Description Length,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates 'the amount of effort' needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,183,196,,,,,,,,,,,,,,,,WOS:000855160700014,0
C,"Wang, CY; Qiu, MH; Huang, J; He, XF",,,Assoc Computat Linguist,"Wang, Chengyu; Qiu, Minghui; Huang, Jun; He, Xiaofeng",,,Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initialization and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3094,3104,,,,,,,,,,,,,,,,WOS:000855160703025,0
C,"Wang, Y; Wang, LY; Li, VOK; Tu, ZP",,,Assoc Computat Linguist,"Wang, Yong; Wang, Longyue; Li, Victor O. K.; Tu, Zhaopeng",,,On the Sparsity of Neural Machine Translation Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1060,1066,,,,,,,,,,,,,,,,WOS:000855160701018,0
C,"Welleck, S; Kulikov, I; Kim, J; Pang, RY; Cho, K",,,Assoc Computat Linguist,"Welleck, Sean; Kulikov, Ilia; Kim, Jaedeok; Pang, Richard Yuanzhe; Cho, Kyunghyun",,,Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a selfterminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5553,5568,,,,,,,,,,,,,,,,WOS:000855160705058,0
C,"Wu, QZ; Zhang, Q; Fu, JL; Huang, XJ",,,Assoc Computat Linguist,"Wu, Qinzhuo; Zhang, Qi; Fu, Jinlan; Huang, Xuanjing",,,A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on local features while neglecting global information. To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph. Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations. Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information. Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7137,7146,,,,,,,,,,,,,,,,WOS:000855160707074,0
C,"Xia, R; Xuan, KZ; Yu, JF",,,Assoc Computat Linguist,"Xia, Rui; Xuan, Kaizhou; Yu, Jianfei",,,A State-independent and Time-evolving Network for Early Rumor Detection in Social Media,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event's life cycle. Such coarse-grained methods failed to capture the event's unique features in different states. To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation. Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events. For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction. This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection. Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems. We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9042,9051,,,,,,,,,,,,,,,,WOS:000855160709022,0
C,"Yao, ZY; Tang, YQ; Yih, WT; Sun, HA; Su, Y",,,Assoc Computat Linguist,"Yao, Ziyu; Tang, Yiqi; Yih, Wen-Tau; Sun, Huan; Su, Yu",,,An Imitation Game for Learning Semantic Parsers from User Interaction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the widely successful applications, building a semantic parser is still a tedious process in practice with challenges from costly data annotation and privacy risks. We suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6883,6902,,,,,,,,,,,,,,,,WOS:000855160707008,0
C,"Zhao, JY; Chang, KW",,,Assoc Computat Linguist,"Zhao, Jieyu; Chang, Kai -Wei",,,LOGAN: Local Group Bias Detection by Clustering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1968,1977,,,,,,,,,,,,,,,,WOS:000855160702012,0
C,"Torres, R; Pardo, TAS",,,Assoc Computat Linguist,"Anchieta, Rafael Torres; Pardo, Thiago Alexandre Salgueiro",,,Semantically Inspired AMR Alignment for the Portuguese Language,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages. Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair. We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1595,1600,,,,,,,,,,,,,,,,WOS:000855160701063,0
C,"Berend, G",,,Assoc Computat Linguist,"Berend, Gabor",,,Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations. We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets. We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different tree-banks. Our results indicate a significant improvement over the application of the dense word representations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8498,8508,,,,,,,,,,,,,,,,WOS:000855160708058,0
C,"Cho, J; Lu, JS; Schwenk, D; Hajishirzi, H; Kembhavi, A",,,Assoc Computat Linguist,"Cho, Jaemin; Lu, Jiasen; Schwenk, Dustin; Hajishirzi, Hannaneh; Kembhavi, Aniruddha",,,"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8785,8805,,,,,,,,,,,,,,,,WOS:000855160709002,0
C,"Cui, W; Zheng, G; Wang, W",,,Assoc Computat Linguist,"Cui, Wanyun; Zheng, Guangyu; Wang, Wei",,,Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5511,5520,,,,,,,,,,,,,,,,WOS:000855160705054,0
C,"Fadnis, KP; Mills, N; Ganhotra, J; Roitman, H; Pandey, G; Cohen, D; Mass, Y; Erera, S; Gunasekara, C; Contractor, D; Patel, SS; Liao, QV; Joshi, S; Lastras, LA; Konopnicki, D",,,Assoc Computat Linguist,"Fadnis, Kshitij P.; Mills, Nathaniel; Ganhotra, Jatin; Roitman, Haggai; Pandey, Gaurav; Cohen, Doron; Mass, Yosi; Erera, Shai; Gunasekara, Chulaka; Contractor, Danish; Patel, Siva Sankalp; Liao, Q. Vera; Joshi, Sachindra; Lastras, Luis A.; Konopnicki, David",,,Agent Assist through Conversation Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Customer support agents play a crucial role as an interface between an organization and its end-users. We propose CAIRAA: Conversational Approach to Information Retrieval for Agent Assistance, to reduce the cognitive workload of support agents who engage with users through conversation systems. CAIRAA monitors an evolving conversation and recommends both responses and URLs of documents the agent can use in replies to their client. We combine traditional information retrieval (IR) approaches with more recent Deep Learning (DL) models to ensure high accuracy and efficient run-time performance in the deployed system. Here, we describe the CAIRAA system and demonstrate its effectiveness in a pilot study via a short video(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,151,157,,,,,,,,,,,,,,,,WOS:000855177700020,0
C,"Fan, A; Gardent, C",,,Assoc Computat Linguist,"Fan, Angela; Gardent, Claire",,,Multilingual AMR-to-Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. For eighteen languages, based on automatic metrics, our multilingual models surpass baselines that generate into a single language. We analyse the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2889,2901,,,,,,,,,,,,,,,,WOS:000855160703006,0
C,"Fei, H; Ren, YF; Ji, D",,,Assoc Computat Linguist,"Fei, Hao; Ren, Yafeng; Ji, Donghong",,,Retrofitting Structure-aware Transformer Language Model for End Tasks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2151,2161,,,,,,,,,,,,,,,,WOS:000855160702025,0
C,"Gomez-Perez, JM; Ortega, R",,,Assoc Computat Linguist,"Gomez-Perez, Jose Manuel; Ortega, Raul",,,ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails. Rather than training a language-visual transformer from scratch we rely on pretrained transformers, fine-tuning and ensembling. We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options. Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions. ISAAQ also demonstrates its broad applicability, obtaining state-of-theart results in other demanding datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5469,5479,,,,,,,,,,,,,,,,WOS:000855160705051,0
C,"Hossain, MM; Kovatchev, V; Dutta, P; Kao, T; Wei, E; Blanco, E",,,Assoc Computat Linguist,"Hossain, Md Mosharaf; Kovatchev, Venelin; Dutta, Pranoy; Kao, Tiffany; Wei, Elizabeth; Blanco, Eduardo",,,An Analysis of Natural Language Inference Benchmarks through the Lens of Negation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays an important role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9106,9118,,,,,,,,,,,,,,,,WOS:000855160709027,0
C,"Lai, VD; Nguyen, TN; Nguyen, TH",,,Assoc Computat Linguist,"Lai, Viet Dac; Nguyen, Tuan Ngo; Nguyen, Thien Huu",,,Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5405,5411,,,,,,,,,,,,,,,,WOS:000855160705045,0
C,"Liang, YB; Duan, N; Gong, YY; Wu, N; Guo, FF; Qi, WZ; Gong, M; Shou, LJ; Jiang, DX; Cao, GH; Fan, XD; Zhang, RF; Agrawal, R; Cui, E; Wei, S; Bharti, T; Qiao, Y; Chen, JH; Wu, W; Liu, S; Yang, F; Campos, D; Majumder, R; Zhou, M",,,Assoc Computat Linguist,"Liang, Yaobo; Duan, Nan; Gong, Yeyun; Wu, Ning; Guo, Fenfei; Qi, Weizhen; Gong, Ming; Shou, Linjun; Jiang, Daxin; Cao, Guihong; Fan, Xiaodong; Zhang, Ruofei; Agrawal, Rahul; Cui, Edward; Wei, Sining; Bharti, Taroon; Qiao, Ying; Chen, Jiun-Hung; Wu, Winnie; Liu, Shuguang; Yang, Fan; Campos, Daniel; Majumder, Rangan; Zhou, Ming",,,"XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6008,6018,,,,,,,,,,,,,,,,WOS:000855160706017,0
C,"Liu, DH; Gong, YY; Yan, Y; Fu, J; Shao, B; Jiang, DX; Lv, JC; Duan, N",,,Assoc Computat Linguist,"Liu, Dayiheng; Gong, Yeyun; Yan, Yu; Fu, Jie; Shao, Bo; Jiang, Daxin; Lv, Jiancheng; Duan, Nan",,,"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, highquality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphraseaware news headline corpus, which contains over 180K aligned triples of hnews article, headline, keyphrasei. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-theart results in terms of quality and diversity(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6241,6250,,,,,,,,,,,,,,,,WOS:000855160706038,0
C,"Liu, H; Huang, W; Mungra, DA; Bowman, SR",,,Assoc Computat Linguist,"Liu, Haokun; Huang, William; Mungra, Dhara A.; Bowman, Samuel R.",,,Precise Task Formalization Matters in Winograd Schema Evaluations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the Super-GLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We hypothesize that much of this improvement comes from recent changes in task formalization- the combination of input specification, loss function, and reuse of pretrained parameters- by users of the dataset, rather than improvements in the pretrained model's reasoning ability. We perform an ablation on two Winograd Schema datasets that interpolates between the formalizations used before and after this surge, and find (i) framing the task as multiple choice improves performance by 2-6 points and (ii) several additional techniques, including the reuse of a pretrained language modeling head, can mitigate the model's extreme sensitivity to hyperparameters. We urge future benchmark creators to impose additional structure to minimize the impact of formalization decisions on reported results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8275,8280,,,,,,,,,,,,,,,,WOS:000855160708039,0
C,"Mostafazadeh, N; Kalyanpur, A; Moon, L; Buchanant, D; Berkowitz, L; Biran, O; Chu-Carroll, J",,,Assoc Computat Linguist,"Mostafazadeh, Nasrin; Kalyanpur, Aditya; Moon, Lori; Buchanant, David; Berkowitz, Lauren; Biran, Or; Chu-Carroll, Jennifer",,,GLUCOSE: GeneraLized and COntextualized Story Explanations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowd-sourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of similar to 670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4569,4586,,,,,,,,,,,,,,,,WOS:000855160704058,0
C,"Nagata, M; Chousa, K; Nishino, M",,,Assoc Computat Linguist,"Nagata, Masaaki; Chousa, Katsuki; Nishino, Masaaki",,,A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multi-lingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token's context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,555,565,,,,,,,,,,,,,,,,WOS:000855160700041,0
C,"Rashkin, H; Celikyilmaz, A; Choi, Y; Gao, JF",,,Assoc Computat Linguist,"Rashkin, Hannah; Celikyilmaz, Asli; Choi, Yejin; Gao, Jianfeng",,,PLOTMACHINES: Outline-Conditioned Generation with Dynamic Plot State Tracking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PLOTMACHINES, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PLOTMACHINES with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and GROVER, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4274,4295,,,,,,,,,,,,,,,,WOS:000855160704037,0
C,"Ruckle, A; Pfeiffer, J; Gurevych, I",,,Assoc Computat Linguist,"Rueckle, Andreas; Pfeiffer, Jonas; Gurevych, Iryna",,,MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2471,2486,,,,,,,,,,,,,,,,WOS:000855160702051,0
C,"Van der Wal, O; de Boer, S; Bruni, E; Hupkes, D",,,Assoc Computat Linguist,"Van der Wal, Oskar; de Boer, Silvan; Bruni, Elia; Hupkes, Dieuwke",,,The Grammar of Emergent Languages,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3339,3359,,,,,,,,,,,,,,,,WOS:000855160703045,0
C,"Van Noord, R; Toral, A; Bos, J",,,Assoc Computat Linguist,"Van Noord, Rik; Toral, Antonio; Bos, Johan",,,Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",,,,,,"Toral, Antonio/0000-0003-2357-2960; Bos, Johannes/0000-0002-9079-5438; van Noord, Rik/0000-0002-2666-7466",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4587,4603,,,,,,,,,,,,,,,,WOS:000855160704059,0
C,"Wang, YA; Chen, YN",,,Assoc Computat Linguist,"Wang, Yu-An; Chen, Yun-Nung",,,What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6840,6849,,,,,,,,,,,,,,,,WOS:000855160707004,0
C,"Wang, ZR; Mehta, SV; Poczos, B; Carbonell, J",,,Assoc Computat Linguist,"Wang, Zirui; Mehta, Sanket Vaibhav; Poczos, Barnabas; Carbonell, Jaime",,,Efficient Meta Lifelong-Learning with Limited Memory,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,535,548,,,,,,,,,,,,,,,,WOS:000855160700039,0
C,"Xu, DQ; Li, JH; Zhu, MH; Zhang, M; Zhou, GD",,,Assoc Computat Linguist,"Xu, Dongqin; Li, Junhui; Zhu, Muhua; Min Zhang; Zhou, Guodong",,,Improving AMR Parsing with Sequence-to-Sequence Pre-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https:// github.com/xdqkid/S2S- AMR- Parser.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2501,2511,,,,,,,,,,,,,,,,WOS:000855160702053,0
C,"Yamada, I; Asai, A; Shindo, H; Takeda, H; Matsumoto, Y",,,Assoc Computat Linguist,"Yamada, Ikuya; Asai, Akari; Shindo, Hiroyuki; Takeda, Hideaki; Matsumoto, Yuji",,,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer (Vaswani et al., 2017). The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT (Devlin et al., 2019). The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https: //github.com/studio- ousia/luke.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6442,6454,,,,,,,,,,,,,,,,WOS:000855160706056,0
C,"Yang, Y; Katiyar, A",,,Assoc Computat Linguist,"Yang, Yi; Katiyar, Arzoo",,,Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches. We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training. We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6365,6375,,,,,,,,,,,,,,,,WOS:000855160706049,0
C,"Zhao, YP; Titov, I",,,Assoc Computat Linguist,"Zhao, Yanpeng; Titov, Ivan",,,Visually Grounded Compound PCFGs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via REINFORCE and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more 'abstract' categories (e.g., +55.1% recall on VPs).(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4369,4379,,,,,,,,,,,,,,,,WOS:000855160704042,0
C,"Zhu, ZN; Rudzicz, F",,,Assoc Computat Linguist,"Zhu, Zining; Rudzicz, Frank",,,An information theoretic view on selecting linguistic probes,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier - or probe - to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either the representation being rich in knowledge, or the probe learning the task, which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the methods to construct and select good probes proposed by the two papers, control task (Hewitt and Liang, 2019) and control function (Pimentel et al., 2020), are equivalent - the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9251,9262,,,,,,,,,,,,,,,,WOS:000855160709039,0
C,"Zuo, CY; Acharya, N; Banerjee, R",,,Assoc Computat Linguist,"Zuo, Chaoyuan; Acharya, Narayan; Banerjee, Ritwik",,,Querying Across Genres for Medical Claims in News,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a query-based biomedical information retrieval task across two vastly different genres - newswire and research literature where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1783,1789,,,,,,,,,,,,,,,,WOS:000855160701079,0
C,"Aroca-Ouellette, S; Rudzicz, F",,,Assoc Computat Linguist,"Aroca-Ouellette, Stephane; Rudzicz, Frank",,,On Losses for Modern Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks - sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a Fast-Sent variant, and a Quick Thoughts variant - that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERTBase on the GLUE benchmark using fewer than a quarter of the training tokens.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4970,4981,,,,,,,,,,,,,,,,WOS:000855160705013,0
C,"Choi, S; Park, H; Yeo, J; Hwang, SW",,,Assoc Computat Linguist,"Choi, Seungtaek; Park, Haeju; Yeo, Jinyoung; Hwang, Seung-Won",,,Less is More: Attention Supervision with Counterfactuals for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual reasoning, over associative reasoning typically used in attention supervision. Our empirical results show that this machine-augmented human attention supervision is more effective than existing methods requiring a higher annotation cost, in text classification tasks, including sentiment analysis and news categorization.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6695,6704,,,,,,,,,,,,,,,,WOS:000855160706076,0
C,"Conforti, C; Hirmer, S; Morgan, D; Basaldella, M; Ben Or, Y",,,Assoc Computat Linguist,"Conforti, Costanza; Hirmer, Stephanie; Morgan, David; Basaldella, Marco; Ben Or, Yau",,,Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recent years, there has been an increasing interest in the application of Artificial Intelligence - and especially Machine Learning - to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high potential of NLP to enhance project sustainability. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. Here, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new extreme multi-class multi-label Automatic User-Perceived Value classification task. We release Stories2Insights (S2I), an expert-annotated dataset of interviews carried out in Uganda, we provide a detailed corpus analysis, and we implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leaves considerable room for future research at the intersection of NLP and SD.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8427,8444,,,,,,,,,,,,,,,,WOS:000855160708052,0
C,"Eskander, R; Muresan, S; Collins, M",,,Assoc Computat Linguist,"Eskander, Ramy; Muresan, Smaranda; Collins, Michael",,,Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages. Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4820,4831,,,,,,,,,,,,,,,,WOS:000855160705002,0
C,"Garg, S; Ramakrishnan, G",,,Assoc Computat Linguist,"Garg, Siddhant; Ramakrishnan, Goutham",,,BAE: BERT-based Adversarial Examples for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to outof-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6174,6181,,,,,,,,,,,,,,,,WOS:000855160706031,0
C,"Hofmannt, V; Pierrehumbertt, JB; Schiitzet, H",,,Assoc Computat Linguist,"Hofmannt, Valentin; Pierrehumbertt, Janet B.; Schiitzet, Hinrich",,,DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT's derivational capabilities in different settings, ranging from using the unmodified pretrained model to full fine tuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT's derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3848,3861,,,,,,,,,,,,,,,,WOS:000855160704004,0
C,"Ishiwatari, T; Yasuda, Y; Miyazaki, T; Goto, J",,,Assoc Computat Linguist,"Ishiwatari, Taichi; Yasuda, Yuki; Miyazaki, Taro; Goto, Jun",,,Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account. In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT). However, graph-based neural networks do not take sequential information into account. In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure. Accordingly, our RGAT model can capture both the speaker dependency and the sequential information. Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations. In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.",,,,,"Miyazaki, Taro/HIK-3257-2022",,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7360,7370,,,,,,,,,,,,,,,,WOS:000855160707044,0
C,"Ku, A; Anderson, P; Patel, R; Le, E; Baldridge, J",,,Assoc Computat Linguist,"Ku, Alexander; Anderson, Peter; Patel, Roma; Le, Eugene; Baldridge, Jason",,,Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018b). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in simulated, photo-realistic environments.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4392,4412,,,,,,,,,,,,,,,,WOS:000855160704044,0
C,"Li, YT; Chen, B; Liu, Q; Gao, Y; Lou, JG; Zhang, Y; Zhang, DM",,,Assoc Computat Linguist,"Li, Yuntao; Chen, Bei; Liu, Qian; Gao, Yan; Lou, Jian-Guang; Zhang, Yan; Zhang, Dongmei",,,What Do You Mean by That? A Parser-Independent Interactive Approach for Enhancing Text-to-SQL,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason stems from the difficulty of fully understanding the users' natural language questions. In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers. Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers. These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6913,6922,,,,,,,,,,,,,,,,WOS:000855160707010,0
C,"Liu, JM; Gardner, M; Cohen, SB; Lapata, M",,,Assoc Computat Linguist,"Liu, Jiangming; Gardner, Matt; Cohen, Shay B.; Lapata, Mirella",,,Multi-Step Inference for Reasoning Over Paragraphs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3040,3050,,,,,,,,,,,,,,,,WOS:000855160703020,0
C,"Mekala, D; Zhang, XY; Shang, JB",,,Assoc Computat Linguist,"Mekala, Dheeraj; Zhang, Xinyang; Shang, Jingbo",,,META: Metadata-Empowered Weak Supervision for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely available across various domains. Strong label indicators exist in the metadata and it has been long overlooked mainly due to the following challenges: (1) metadata is multi-typed, requiring systematic modeling of different types and their combinations, (2) metadata is noisy, some metadata entities (e.g., authors, venues) are more compelling label indicators than others. In this paper, we propose a novel framework, META, which goes beyond the existing paradigm and leverages metadata as an additional source of weak supervision. Specifically, we organize the text data and metadata together into a text-rich network and adopt network motifs to capture appropriate combinations of metadata. Based on seed words, we rank and filter motif instances to distill highly label-indicative ones as seed motifs, which provide additional weak supervision. Following a boot-strapping manner, we train the classifier and expand the seed words and seed motifs iteratively. Extensive experiments and case studies on real-world datasets demonstrate superior performance and significant advantages of leveraging metadata as weak supervision.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8351,8361,,,,,,,,,,,,,,,,WOS:000855160708045,0
C,"Ruiter, D; Van Genabith, J; Espana-Bonet, C",,,Assoc Computat Linguist,"Ruiter, Dana; Van Genabith, Josef; Espana-Bonet, Cristina",,,Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2560,2571,,,,,,,,,,,,,,,,WOS:000855160702059,0
C,"Shen, JM; Ji, H; Han, JW",,,Assoc Computat Linguist,"Shen, Jiaming; Ji, Heng; Han, Jiawei",,,Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,303,313,,,,,,,,,,,,,,,,WOS:000855160700022,0
C,"Sun, SQ; Gan, Z; Cheng, Y; Fang, YW; Wang, SH; Liu, JJ",,,Assoc Computat Linguist,"Sun, Siqi; Gan, Zhe; Cheng, Yu; Fang, Yuwei; Wang, Shuohang; Liu, Jingjing",,,Contrastive Distillation on Intermediate Representations for Language Model Compression,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing language model compression methods mostly use a simple L-2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CODIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,498,508,,,,,,,,,,,,,,,,WOS:000855160700036,0
C,"Tang, JZ; Feng, YS; Zhao, DY",,,Assoc Computat Linguist,"Tang, Jizhi; Feng, Yansong; Zhao, Dongyan",,,Understanding Procedural Text using Interactive Entity Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned. In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking. In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions. Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes. We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes. Our code is available at: https://github.com/esddse/IEN.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7281,7290,,,,,,,,,,,,,,,,WOS:000855160707038,0
C,"Tian, Y; Yan, S; Xia, F",,,Assoc Computat Linguist,"Tian, Yuanhe; Yan, Song; Xia, Fei",,,Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6037,6044,,,,,,,,,,,,,,,,WOS:000855160706020,0
C,"Uchendu, A; Le, T; Shu, K; Lee, D",,,Assoc Computat Linguist,"Uchendu, Adaku; Le, Thai; Shu, Kai; Lee, Dongwon",,,Authorship Attribution for Neural Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from human-written texts with the naked eye. Despite many benefits and utilities of such neural methods, in some applications, being able to tell the author of a text in question becomes critically important. In this work, in the context of this Turing Test, we investigate the so-called authorship attribution problem in three versions: (1) given two texts T-1 and T-2, are both generated by the same method or not? (2) is the given text T written by a human or machine? (3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? Against one human-written and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XL-NET, PPLM, FAIR), we empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve. However, the qualities of texts generated by GPT2, GROVER, and FAIR are better, often confusing machine classifiers in solving three problems. All codes and datasets of our experiments are available at: https://bit.ly/302zWdz",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8384,8395,,,,,,,,,,,,,,,,WOS:000855160708048,0
C,"Yu, ZW; Zang, HY; Wan, XJ",,,Assoc Computat Linguist,"Yu, Zhiwei; Zang, Hongyu; Wan, Xiaojun",,,Routing Enforced Generative Model for Recipe Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a routing method to dive into the content selection under the internal restrictions. The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences. Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3797,3806,,,,,,,,,,,,,,,,WOS:000855160703086,0
C,"Zhu, WR; Wang, XE; Narayana, P; Sone, K; Basu, S; Wang, WY",,,Assoc Computat Linguist,"Zhu, Wanrong; Wang, Xin Eric; Narayana, Pradyumna; Sone, Kazoo; Basu, Sugato; Wang, William Yang",,,Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that human-generated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8806,8811,,,,,,,,,,,,,,,,WOS:000855160709003,0
C,"Allaway, E; McKeown, K",,,Assoc Computat Linguist,"Allaway, Emily; McKeown, Kathleen",,,Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8913,8931,,,,,,,,,,,,,,,,WOS:000855160709012,0
C,"Broscheit, S; Ruffinelli, D; Kochsiek, A; Betz, P; Gemulla, R",,,Assoc Computat Linguist,"Broscheit, Samuel; Ruffinelli, Daniel; Kochsiek, Adrian; Betz, Patrick; Gemulla, Rainer",,,LibKGE A knowledge graph embedding library for reproducible research,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"LIBKGE(1) is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LIBKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LIBKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LIBKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates in-depth analysis. LIBKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LIBKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,165,174,,,,,,,,,,,,,,,,WOS:000855177700022,0
C,"Cao, M; Dong, Y; Wu, JP; Cheung, JCK",,,Assoc Computat Linguist,"Cao, Meng; Dong, Yue; Wu, Jiapeng; Cheung, Jackie Chi Kit",,,Factual Error Correction for Abstractive Summarization Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by an error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6251,6258,,,,,,,,,,,,,,,,WOS:000855160706039,0
C,"Card, D; Henderson, P; Khandelwal, U; Jia, R; Mahowald, K; Jurafsky, D",,,Assoc Computat Linguist,"Card, Dallas; Henderson, Peter; Khandelwal, Urvashi; Jia, Robin; Mahowald, Kyle; Jurafsky, Dan",,,With Little Power Comes Great Responsibility,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of I BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9263,9274,,,,,,,,,,,,,,,,WOS:000855160709040,0
C,"Chen, MY; Ge, T; Zhang, XX; Wei, FR; Zhou, M",,,Assoc Computat Linguist,"Chen, Mengyun; Ge, Tao; Zhang, Xingxing; Wei, Furu; Zhou, Ming",,,Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7162,7169,,,,,,,,,,,,,,,,WOS:000855160707028,0
C,"Chronopoulou, A; Stojanovski, D; Fraser, A",,,Assoc Computat Linguist,"Chronopoulou, Alexandra; Stojanovski, Dario; Fraser, Alexander",,,Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields stateof-the-art results. When limited data is available for one language, however, this method leads to poor translations. We present an effective approach that reuses an LM that is pretrained only on a high-resource language. The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model. To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language. We therefore propose a novel vocabulary extension method. Our approach, RE- LM, outperforms a competitive cross-lingual pretraining model ( XLM) in English-Macedonian (En-Mk) and EnglishAlbanian (En-Sq), yielding more than +8:3 BLEU points for all four translation directions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2703,2711,,,,,,,,,,,,,,,,WOS:000855160702071,0
C,"Deriul, J; Tuggenerl, D; Von Danikenl, P; Campos, JA; Rodrigo, A; Belkacem, T; Soroa, A; Agirre, E; Cieliebakl, M",,,Assoc Computat Linguist,"Deriul, Jan; Tuggenerl, Don; Von Danikenl, Pius; Campos, Jon Ander; Rodrigo, Alvaro; Belkacem, Thiziri; Soroa, Aitor; Agirre, Eneko; Cieliebakl, Mark",,,Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chatbots regarding their ability to mimic the conversational behavior of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chatbot can uphold human-like behavior the longest, i.e., Survival Analysis. This metric has the ability to correlate a bot's performance to certain of its characteristics (e.g., fluency or sensibleness), yielding interpretable results. The comparably low cost of our framework allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3971,3984,,,,,,,,,,,,,,,,WOS:000855160704014,0
C,"El-Kishky, A; Chaudhary, V; Guzman, F; Koehn, P",,,Assoc Computat Linguist,"El-Kishky, Ahmed; Chaudhary, Vishrav; Guzman, Francisco; Koehn, Philipp",,,CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different language pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of over 392 million URL pairs from Common Crawl covering documents in 8144 language pairs of which 137 pairs include English. In addition to curating this massive dataset, we introduce baseline methods that leverage crosslingual representations to identify aligned documents based on their textual content. Finally, we demonstrate the value of this parallel documents dataset through a downstream task of mining parallel sentences and measuring the quality of machine translations from models trained on this mined data. Our objective in releasing this dataset is to foster new research in cross-lingual NLP across a variety of low, medium, and high-resource languages.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5960,5969,,,,,,,,,,,,,,,,WOS:000855160706013,0
C,"Elder, H; O'Connor, A",,,Assoc Computat Linguist,"Elder, Henry; O'Connor, Alexander",,,How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task. The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2877,2888,,,,,,,,,,,,,,,,WOS:000855160703005,0
C,"Graham, Y; Haddow, B; Koehn, P",,,Assoc Computat Linguist,"Graham, Yvette; Haddow, Barry; Koehn, Philipp",,,Statistical Power and Translationese in Machine Translation Evaluation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT. One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation. Since the very aim of past evaluations was the investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test. We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,72,81,,,,,,,,,,,,,,,,WOS:000855160700006,0
C,"Gui, T; Ye, JC; Zhang, Q; Li, ZY; Fei, ZC; Gong, YY; Huang, XJ",,,Assoc Computat Linguist,"Gui, Tao; Ye, Jiacheng; Zhang, Qi; Li, Zhengyan; Fei, Zichu; Gong, Yeyun; Huang, Xuanjing",,,Uncertainty-Aware Label Refinement for Sequence Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient. A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction. In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation. The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2316,2326,,,,,,,,,,,,,,,,WOS:000855160702038,0
C,"Jung, J; Son, B; Lyu, S",,,Assoc Computat Linguist,"Jung, Jaehun; Son, Bokyung; Lyu, Sungwon",,,AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance. To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows. Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context. Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset. Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3484,3497,,,,,,,,,,,,,,,,WOS:000855160703055,0
C,"Khosla, S; Vashishth, S; Lehman, JF; Rose, C",,,Assoc Computat Linguist,"Khosla, Sopan; Vashishth, Shikhar; Lehman, Jill Fain; Rose, Carolyn",,,MEDFILTER: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles. Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue. In this paper, we propose the novel modeling approach MEDFILTER, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task. We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MEDFILTER is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve). Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7781,7797,,,,,,,,,,,,,,,,WOS:000855160708001,0
C,"Li, L; Ma, R; Guo, Q; Xue, X; Qiu, X",,,Assoc Computat Linguist,"Li, Linyang; Ma, Ruotian; Guo, Qipeng; Xue, Xiangyang; Qiu, Xipeng",,,BERT-ATTACK: Adversarial Attack Against BERT Using BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-theart attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT- Attack.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6193,6202,,,,,,,,,,,,,,,,WOS:000855160706033,0
C,"Liu, RB; Xu, GX; Jia, CY; Ma, WC; Wang, LL; Vosoughi, S",,,Assoc Computat Linguist,"Liu, Ruibo; Xu, Guangxuan; Jia, Chenyan; Ma, Weicheng; Wang, Lili; Vosoughi, Soroush",,,Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7% on average when given only 10% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",,,,,,"Jia, Chenyan/0000-0002-8407-9224",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9031,9041,,,,,,,,,,,,,,,,WOS:000855160709021,0
C,"Lu, Y; Singhal, S; Strub, F; Pietquin, O; Courville, A",,,Assoc Computat Linguist,"Lu, Yuchen; Singhal, Soumye; Strub, Florian; Pietquin, Olivier; Courville, Aaron",,,Supervised Seeded Iterated Learning for Interactive Language Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL). While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring. In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus. Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses. We then show the effectiveness of SSIL in the language-drift translation game.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3962,3970,,,,,,,,,,,,,,,,WOS:000855160704013,0
C,"Lv, X; Han, X; Hou, L; Li, JZ; Liu, ZY; Zhang, W; Zhang, YC; Kong, H; Wu, SH",,,Assoc Computat Linguist,"Lv, Xin; Han, Xu; Hou, Lei; Li, Juanzi; Liu, Zhiyuan; Zhang, Wei; Zhang, Yichi; Kong, Hao; Wu, Suhui",,,Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model named DacKGR over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5694,5703,,,,,,,,,,,,,,,,WOS:000855160705069,0
C,"Ma, W; Liu, R; Wang, L; Vosoughi, S",,,Assoc Computat Linguist,"Ma, Weicheng; Liu, Ruibo; Wang, Lili; Vosoughi, Soroush",,,YY Multi-resolution Annotations for Emoji Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspectlevel emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multiclass annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pretrained BERT model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6684,6694,,,,,,,,,,,,,,,,WOS:000855160706075,0
C,"Perez, E; Lewis, P; Yih, WT; Cho, K; Kiela, D",,,Assoc Computat Linguist,"Perez, Ethan; Lewis, Patrick; Yih, Wen-tau; Cho, Kyunghyun; Kiela, Douwe",,,Unsupervised Question Decomposition for Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HoTPoTQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using subquestions is promising for shedding light on why a QA system makes a prediction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8864,8880,,,,,,,,,,,,,,,,WOS:000855160709008,0
C,"Ramponi, A; Van der Goot, R; Lombardo, R; Plank, B",,,Assoc Computat Linguist,"Ramponi, Alan; Van der Goot, Rob; Lombardo, Rosario; Plank, Barbara",,,Biomedical Event Extraction as Sequence Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce Biomedical Event Extraction as Sequence Labeling (BEESL), a joint endto-end neural information extraction model. BEESL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning. BEESL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools. BEESL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task. Importantly, we also provide first results on biomedical event extraction without gold entity information. Empirical results show that BEESL's speed and accuracy makes it a viable approach for large-scale real-world scenarios.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5357,5367,,,,,,,,,,,,,,,,WOS:000855160705041,0
C,"Roberts, A; Raffel, C; Shazeer, N",,,Assoc Computat Linguist,"Roberts, Adam; Raffel, Colin; Shazeer, Noam",,,How Much Knowledge Can You Pack Into the Parameters of a Language Model?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5418,5426,,,,,,,,,,,,,,,,WOS:000855160705047,0
C,"Ross, H; Cai, J; Min, BN",,,Assoc Computat Linguist,"Ross, Hayley; Cai, Jonathon; Min, Bonan",,,Exploring Contextualized Neural Language Models for Temporal Dependency Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https: / /github . com/bnmin/tdp_ranking.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8548,8553,,,,,,,,,,,,,,,,WOS:000855160708064,0
C,"Shavrina, T; Fenogenova, A; Emelyanov, A; Shevelev, D; Artemova, E; Malykh, V; Mikhailov, V; Tikhonova, M; Chertok, A; Evlampiev, A",,,Assoc Computat Linguist,"Shavrina, Tatiana; Fenogenova, Alena; Emelyanov, Anton; Shevelev, Denis; Artemova, Ekaterina; Malykh, Valentin; Mikhailov, Vladislav; Tikhonova, Maria; Chertok, Andrey; Evlampiev, Andrey",,,RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we introduce an advanced Russian general language understanding evaluation benchmark - RussianGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology (Wang et al., 2019), was developed from scratch for the Russian language. We provide baselines, human level evaluation, an open-source framework for evaluating models and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the adapted diagnostic test set and offer the first steps to further expanding or assessing state-of-the-art models independently of language.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4717,4726,,,,,,,,,,,,,,,,WOS:000855160704069,0
C,"Spell, GP; Guay, B; Hillygus, DS; Carin, L",,,Assoc Computat Linguist,"Spell, Gregory P.; Guay, Brian; Hillygus, D. Sunshine; Carin, Lawrence",,,An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators' political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators' attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,627,641,,,,,,,,,,,,,,,,WOS:000855160700046,0
C,"Tigunova, A; Yates, A; Mirza, P; Weikum, G",,,Assoc Computat Linguist,"Tigunova, Anna; Yates, Andrew; Mirza, Paramita; Weikum, Gerhard",,,CHARM: Inferring Personal Attributes from Conversations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Personal knowledge about users' professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5391,5404,,,,,,,,,,,,,,,,WOS:000855160705044,0
C,"Welch, C; Mihalcea, R; Kummerfeld, JK",,,Assoc Computat Linguist,"Welch, Charles; Mihalcea, Rada; Kummerfeld, Jonathan K.",,,Improving Low Compute Language Modeling with In-Domain Embedding Initialisation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with bytepair encoding. We show that for our target setting in English, initialising and freezing input embeddings using in-domain data can improve language model performance by providing a useful representation of rare words, and this pattern holds across several different domains. In the process, we show that the standard convention of tying input and output embeddings does not improve perplexity when initializing with embeddings trained on in-domain data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8625,8634,,,,,,,,,,,,,,,,WOS:000855160708071,0
C,"Wu, YM; Passban, P; Rezagholizadeh, M; Liu, Q",,,Assoc Computat Linguist,"Wu, Yimeng; Passban, Peyman; Rezagholizadeh, Mehdi; Liu, Qun",,,Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S). Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative. In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese -> English, Turkish -> English, and English -> German directions. Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1016,1021,,,,,,,,,,,,,,,,WOS:000855160701014,0
C,"Yang, D; Zhang, WX; Wai, L",,,Assoc Computat Linguist,"Yang Deng; Zhang, Wenxuan; Wai Lam",,,Multi-hop Inference for Question-driven Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for nonfactoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms stateof-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6734,6744,,,,,,,,,,,,,,,,WOS:000855160706080,0
C,"Yin, WP; Rajani, NF; Radev, D; Socher, R; Xiong, CM",,,Assoc Computat Linguist,"Yin, Wenpeng; Rajani, Nazneen Fatema; Radev, Dragomir; Socher, Richard; Xiong, Caiming",,,Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pre-trained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP1 can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-ENTAIL). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited. Code: https: //github.com/salesforce/UniversalFewShotNLP",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8229,8239,,,,,,,,,,,,,,,,WOS:000855160708035,0
C,"Yuan, CF; Fan, C; Bao, JZ; Xu, RF",,,Assoc Computat Linguist,"Yuan, Chaofa; Chuang Fan; Bao, Jianzhu; Xu, Ruifeng",,,Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we regard the task as a sequence labeling problem and propose a novel tagging scheme with coding the distance between linked components into the tags, so that emotions and the corresponding causes can be extracted simultaneously. Accordingly, an end-to-end model is presented to process the input texts from left to right, always with linear time complexity, leading to a speed up. Experimental results show that our proposed model achieves the best performance, outperforming the state-of-the-art method by 2.26% (p < 0.001) in F1 measure.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3568,3573,,,,,,,,,,,,,,,,WOS:000855160703064,0
C,"Zhuang, Y; Jiang, TY; Riloff, E",,,Assoc Computat Linguist,"Zhuang, Yuan; Jiang, Tianyu; Riloff, Ellen",,,Affective Event Classification with Discourse-enhanced Self-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective polarity to event phrases. First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base. Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data. The key idea is to exploit event phrases that occur with a coreferent sentiment expression. The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier's predictions and the polarities of the event's coreferent sentiment expressions. Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5608,5617,,,,,,,,,,,,,,,,WOS:000855160705062,0
C,"Ahrabian, K; Feizi, A; Salehi, Y; Hamilton, WL; Bose, AJ",,,Assoc Computat Linguist,"Ahrabian, Kian; Feizi, Aarash; Salehi, Yasmin; Hamilton, William L.; Bose, Avishek Joey",,,Structure Aware Negative Sampling in Knowledge Graphs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data. While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives. In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node's k-hop neighborhood. Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6093,6101,,,,,,,,,,,,,,,,WOS:000855160706025,0
C,"Baly, R; Da San Martino, G; Glass, J; Nakov, P",,,Assoc Computat Linguist,"Baly, Ramy; Da San Martino, Giovanni; Glass, James; Nakov, Preslav",,,We Can Detect Your Bias: Predicting the Political Ideology of News Articles,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology-left, center, or right-, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4982,4991,,,,,,,,,,,,,,,,WOS:000855160705014,0
C,"Ben Veyseh, AP; Nouri, N; Dernoncourt, F; Dou, DJ; Nguyen, TH",,,Assoc Computat Linguist,"Ben Veyseh, Amir Pouran; Nouri, Nasim; Dernoncourt, Franck; Dou, Dejing; Thien Huu Nguyen",,,Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8947,8956,,,,,,,,,,,,,,,,WOS:000855160709014,0
C,"Budur, E; Ozcelik, R; Gungor, T; Potts, C",,,Assoc Computat Linguist,"Budur, Emrah; Ozcelik, Riza; Gungor, Tunga; Potts, Christopher",,,Data and Representation for Turkish Natural Language Inference,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8253,8267,,,,,,,,,,,,,,,,WOS:000855160708037,0
C,"Chen, XY; Meng, FD; Li, P; Chen, FL; Xu, S; Xu, B; Zhou, J",,,Assoc Computat Linguist,"Chen, Xiuyi; Meng, Fandong; Li, Peng; Chen, Feilong; Xu, Shuang; Xu, Bo; Zhou, Jie",,,Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3426,3437,,,,,,,,,,,,,,,,WOS:000855160703050,0
C,"Chollampatt, S; Susanto, RH; Tan, L; Szymanska, E",,,Assoc Computat Linguist,"Chollampatt, Shamil; Susanto, Raymond Hendy; Tan, Liling; Szymanska, Ewa",,,Can Automatic Post-Editing Improve NMT?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https:// github.com/shamilcm/pedra.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2736,2746,,,,,,,,,,,,,,,,WOS:000855160702074,0
C,"Chuang, YS; Su, SY; Chen, YN",,,Assoc Computat Linguist,"Chuang, Yung-Sung; Su, Shang-Yu; Chen, Yun-Nung",,,Lifelong Language Knowledge Distillation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2914,2924,,,,,,,,,,,,,,,,WOS:000855160703008,0
C,"Govindarajan, VS; Chen, BT; Warholic, R; Erk, K; Li, JJ",,,Assoc Computat Linguist,"Govindarajan, Venkata S.; Chen, Benjamin T.; Warholic, Rebecca; Erk, Katrin; Li, Junyi Jessy",,,Help! Need Advice on Identifying Advice,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of adviceseeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rulebased systems, advice identification is challenging, and we identify directions for future research.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5295,5306,,,,,,,,,,,,,,,,WOS:000855160705037,0
C,"Huang, J; Wang, ZL; Chang, KCC; Hwu, WM; Xiong, JJ",,,Assoc Computat Linguist,"Huang, Jie; Wang, Zilong; Chang, Kevin Chen-Chuan; Hwu, Wen-Mei; Xiong, Jinjun",,,Exploring Semantic Capacity of Terms,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8509,8518,,,,,,,,,,,,,,,,WOS:000855160708059,0
C,"Jin, XS; Du, JY; Sadhu, A; Nevatia, R; Ren, X",,,Assoc Computat Linguist,"Jin, Xisen; Du, Junyi; Sadhu, Arka; Nevatia, Ram; Ren, Xiang",,,Visually Grounded Continual Learning of Compositional Phrases,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work (1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2018,2029,,,,,,,,,,,,,,,,WOS:000855160702015,0
C,"Kar, S; Aguilar, G; Lapata, M; Solorio, T",,,Assoc Computat Linguist,"Kar, Sudipta; Aguilar, Gustavo; Lapata, Mirella; Solorio, Thamar",,,Multi-view Story Characterization from Movie Plot Synopses and Reviews,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events). Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses. Finally, we demonstrate how can we take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision. We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5629,5646,,,,,,,,,,,,,,,,WOS:000855160705064,0
C,"Keung, P; Lu, YC; Szarvas, G; Smith, NA",,,Assoc Computat Linguist,"Keung, Phillip; Lu, Yichao; Szarvas, Gyorgy; Smith, Noah A.",,,The Multilingual Amazon Reviews Corpus,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., 'books', 'appliances', etc.) The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language. For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively. We report baseline results for supervised text classification and zero-shot crosslingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4563,4568,,,,,,,,,,,,,,,,WOS:000855160704057,0
C,"Liu, ZZ; Ding, GX; Bukkittu, A; Gupta, M; Gao, PZ; Ahmed, A; Zhang, SK; Gao, X; Singhavi, S; Li, LW; Wei, W; Hu, ZC; Shi, HR; Liang, XD; Mitamura, T; Xing, EP; Hu, ZT",,,Assoc Computat Linguist,"Liu, Zhengzhong; Ding, Guanxiong; Bukkittu, Avinash; Gupta, Mansi; Gao, Pengzhi; Ahmed, Atif; Zhang, Shikun; Gao, Xin; Singhavi, Swapnil; Li, Linwei; Wei, Wei; Hu, Zecong; Shi, Haoran; Liang, Xiaodan; Mitamura, Teruko; Xing, Eric P.; Hu, Zhiting",,,A Data-Centric Framework for Composable NLP Workflows,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation, and visualization. We establish a unified open-source framework to support fast development of such sophisticated NLP workflows in a composable manner. The framework introduces a uniform data representation to encode heterogeneous results by a wide range of NLP tasks. It offers a large repository of processors for NLP tasks, visualization, and annotation, which can be easily assembled with full interoperability under the unified representation. The highly extensible framework allows plugging in custom processors from external off-the-shelf NLP and deep learning libraries. The whole framework is delivered through two modularized yet integratable open-source projects, namely Forte(1) (for workflow infrastructure and NLP function processors) and Stave(2) (for user interaction, visualization, and annotation).",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,197,204,,,,,,,,,,,,,,,,WOS:000855177700026,0
C,"Mager, M; Cetinoglu, O; Kann, K",,,Assoc Computat Linguist,"Mager, Manuel; Cetinoglu, Ozlem; Kann, Katharina",,,Tackling the Low-resource Challenge for Canonical Segmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches outperform existing ones on all languages by up to 11:4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly lowresource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5237,5250,,,,,,,,,,,,,,,,WOS:000855160705033,0
C,"Pasca, M",,,Assoc Computat Linguist,"Pasca, Marius",,,Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century) within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories. In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semanticallyanchored properties, rather than as mere strings in a previous method. It does so at a better trade-off between precision and recall.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6218,6228,,,,,,,,,,,,,,,,WOS:000855160706036,0
C,"Rosenman, S; Jacovi, A; Goldberg, Y",,,Assoc Computat Linguist,"Rosenman, Shachar; Jacovi, Alon; Goldberg, Yoav",,,Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3702,3710,,,,,,,,,,,,,,,,WOS:000855160703077,0
C,"Sosea, T; Caragea, C",,,Assoc Computat Linguist,"Sosea, Tiberiu; Caragea, Cornelia",,,CANCEREMO : A Dataset for Fine-Grained Emotion Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce CANCEREMO, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best BERT model achieves an average F1 of 71%, which we improve further using domain-specific pre-training.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8892,8904,,,,,,,,,,,,,,,,WOS:000855160709010,0
C,"Takmaz, E; Pezzelle, S; Beinborn, L; Fernandez, R",,,Assoc Computat Linguist,"Takmaz, Ece; Pezzelle, Sandro; Beinborn, Lisa; Fernandez, Raquel",,,Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural-particularly when gaze is encoded with a dedicated recurrent component.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4664,4677,,,,,,,,,,,,,,,,WOS:000855160704065,0
C,"Song, W; Kai, Z; Fu, RJ; Liu, LH; Liu, T; Cheng, MM",,,Assoc Computat Linguist,"Wei Song; Kai Zhang; Fu, Ruiji; Liu, Lizhen; Liu, Ting; Cheng, Miaomiao",,,Multi-Stage Pre-training for Automated Chinese Essay Scoring,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised crossprompt fine-tuning and supervised targetprompt fine-tuning. An essay scorer is first pretrained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision. The pre-trained essay scorer would be further fine-tuned on previously rated essays from existing prompts, which have the same score range with the target prompt and provide extra supervision. At last, the scorer is fine-tuned on the target-prompt training data. The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6723,6733,,,,,,,,,,,,,,,,WOS:000855160706079,0
C,"Wu, JP; Cao, M; Cheung, JCK; Hamilton, WL",,,Assoc Computat Linguist,"Wu, Jiapeng; Cao, Meng; Cheung, Jackie Chi Kit; Hamilton, William L.",,,TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments1 on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5730,5746,,,,,,,,,,,,,,,,WOS:000855160705072,0
C,"Xia, P; Sedoc, J; Van Durme, B",,,Assoc Computat Linguist,"Xia, Patrick; Sedoc, Joao; Van Durme, Benjamin",,,Incremental Neural Coreference Resolution in Constant Memory,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity representations created from the earlier document context (if any). These spans are then used to update the entity's representations before being forgotten; we only retain a fixed set of salient entities throughout the document. In this work, we successfully convert a high-performing model (Joshi et al., 2020), asymptotically reducing its memory usage to constant space with only a 0.3% relative loss in F1 on OntoNotes 5.0.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8617,8624,,,,,,,,,,,,,,,,WOS:000855160708070,0
C,"Xiao, LQ; Wang, L; He, H; Jin, YH",,,Assoc Computat Linguist,"Xiao, Liqiang; Lu Wang; Hao He; Jin, Yaohui",,,Modeling Content Importance for Summarization with Pre-trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units (n-grams or sentences). Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3606,3611,,,,,,,,,,,,,,,,WOS:000855160703068,0
C,"Yokoi, S; Takahashi, R; Akama, R; Suzuki, J; Inui, K",,,Assoc Computat Linguist,"Yokoi, Sho; Takahashi, Ryo; Akama, Reina; Suzuki, Jun; Inui, Kentaro",,,Word Rotator's Distance,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A key principle in assessing textual similarity is measuring the degree of semantic overlap between two texts by considering the word alignment. Such alignment-based approaches are intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. To address this issue, we focus on and demonstrate the fact that the norm of word vectors is a good proxy for word importance, and their angle is a good proxy for word similarity. Alignment-based approaches do not distinguish them, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose a method that first decouples word vectors into their norm and direction, and then computes alignment-based similarity using earth mover's distance (i.e., optimal transport cost), which we refer to as word rotator's distance. Besides, we find how to grow the norm and direction of word vectors (vector converter), which is a new systematic approach derived from sentence-vector estimation methods. On several textual similarity datasets, the combination of these simple proposed methods outperformed not only alignment-based approaches but also strong baselines. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2944,2960,,,,,,,,,,,,,,,,WOS:000855160703011,0
C,"Yu, L; Ettinger, A",,,Assoc Computat Linguist,"Yu, Lang; Ettinger, Allyson",,,Assessing Phrasal Representation and Composition in Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4896,4907,,,,,,,,,,,,,,,,WOS:000855160705008,0
C,"Yu, T; Joty, S",,,Assoc Computat Linguist,"Yu, Tao; Joty, Shafiq",,,Online Conversation Disentanglement with Pointer Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6321,6330,,,,,,,,,,,,,,,,WOS:000855160706045,0
C,"Zhang, P; Chen, BX; Ge, NY; Fan, K",,,Assoc Computat Linguist,"Zhang, Pei; Chen, Boxing; Ge, Niyu; Fan, Kai",,,Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation. Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors. We examine our approach on the two publicly available document-level datasets. We can achieve a strong result in BLEU and capture discourse phenomena.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1081,1087,,,,,,,,,,,,,,,,WOS:000855160701021,0
C,"Zhang, WY; Li, XL; Li, Y; Wang, SG; Li, DY; Jian, L; Zheng, JX",,,Assoc Computat Linguist,"Zhang, Wenyue; Li, Xiaoli; Li, Yang; Wang, Suge; Li, Deyu; Jian, Liao; Zheng, Jianxing",,,Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data. Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.",,,,,,"Li, Xiaoli/0000-0002-0762-6562",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3762,3767,,,,,,,,,,,,,,,,WOS:000855160703082,0
C,"Zhou, DY; Hu, XM; Wang, R",,,Assoc Computat Linguist,"Zhou, Deyu; Hu, Xuemeng; Wang, Rui",,,Neural Topic Modeling by Incorporating Document Relationship Graph,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph. Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences. By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution. Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3790,3796,,,,,,,,,,,,,,,,WOS:000855160703085,0
C,"Aghajanyan, A; Maillard, J; Shrivastava, A; Diedrick, K; Haeger, M; Li, H; Mehdad, Y; Stoyanov, V; Kumar, A; Lewis, M; Gupta, S",,,Assoc Computat Linguist,"Aghajanyan, Armen; Maillard, Jean; Shrivastava, Akshat; Diedrick, Keith; Haeger, Mike; Li, Haoran; Mehdad, Yashar; Stoyanov, Ves; Kumar, Anuj; Lewis, Mike; Gupta, Sonal",,,Conversational Semantic Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5026,5035,,,,,,,,,,,,,,,,WOS:000855160705018,0
C,"Arase, Y; Tsujii, J",,,Assoc Computat Linguist,"Arase, Yuki; Tsujii, Junichi",,,Compositional Phrase Alignment and Beyond,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice. We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations. Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments. Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1611,1623,,,,,,,,,,,,,,,,WOS:000855160701065,0
C,"Chaudhary, A; Anastasopoulos, A; Pratapa, A; Mortensen, DR; Sheikh, Z; Tsvetkov, Y; Neubig, G",,,Assoc Computat Linguist,"Chaudhary, Aditi; Anastasopoulos, Antonios; Pratapa, Adithya; Mortensen, David R.; Sheikh, Zaid; Tsvetkov, Yulia; Neubig, Graham",,,Automatic Extraction of Rules Governing Morphological Agreement,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world's languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https: //neulab.github.io/lase/. The code is publicly available here.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5212,5236,,,,,,,,,,,,,,,,WOS:000855160705032,0
C,"Chen, Z; Qian, TY",,,Assoc Computat Linguist,"Chen, Zhuang; Qian, Tieyun",,,Enhancing Aspect Term Extraction with Soft Prototypes,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure. In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes. These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms. Our proposed model is a general framework and can be combined with almost all sequence taggers. Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2107,2117,,,,,,,,,,,,,,,,WOS:000855160702021,0
C,"Feng, SX; Ren, XC; Chen, HS; Sun, B; Li, K; Sun, X",,,Assoc Computat Linguist,"Feng, Shaoxiong; Ren, Xuancheng; Chen, Hongshen; Sun, Bin; Li, Kan; Sun, Xu",,,Regularizing Dialogue Generation by Imitating Implicit Scenarios,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-theart baselines on diversity and relevance, and expresses scenario-specific knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6592,6604,,,,,,,,,,,,,,,,WOS:000855160706067,0
C,"Gao, X; Zhang, YZ; Galley, M; Brockett, C; Dolan, B",,,Assoc Computat Linguist,"Gao, Xiang; Zhang, Yizhe; Galley, Michel; Brockett, Chris; Dolan, Bill",,,Dialogue Response Ranking Training with Large-Scale Human Feedback Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DIALOGRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,386,395,,,,,,,,,,,,,,,,WOS:000855160700028,0
C,"Hu, XM; Wang, R; Zhou, DY; Xiong, YX",,,Assoc Computat Linguist,"Hu, Xuemeng; Wang, Rui; Zhou, Deyu; Xiong, Yuxuan",,,Neural Topic Modeling with Cycle-Consistent Adversarial Training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToM-CAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9018,9030,,,,,,,,,,,,,,,,WOS:000855160709020,0
C,"Kou, XY; Lin, YK; Liu, SB; Li, P; Zhou, J; Zhang, Y",,,Assoc Computat Linguist,"Kou, Xiaoyu; Lin, Yankai; Liu, Shaobo; Li, Peng; Zhou, Jie; Zhang, Yan",,,Disentangle-based Continual Graph Representation Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a graph embedding model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiC-GRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2961,2972,,,,,,,,,,,,,,,,WOS:000855160703012,0
C,"Lauscher, A; Ravishankar, V; Vulic, I; Glavas, G",,,Assoc Computat Linguist,"Lauscher, Anne; Ravishankar, Vinit; Vulic, Ivan; Glavas, Goran",,,From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4483,4499,,,,,,,,,,,,,,,,WOS:000855160704051,0
C,"Lertvittayakumjorn, P; Specia, L; Toni, F",,,Assoc Computat Linguist,"Lertvittayakumjorn, Piyawat; Specia, Lucia; Toni, Francesca",,,FIND: Human-in-the-Loop Debugging Deep Text Classifiers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar traintest distributions).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,332,348,,,,,,,,,,,,,,,,WOS:000855160700024,0
C,"Leung, JY; Emerson, G; Cotterell, R",,,Assoc Computat Linguist,"Leung, Jun Yen; Emerson, Guy; Cotterell, Ryan",,,Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Across languages, multiple consecutive adjectives modifying a noun (e.g. the big red dog) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4016,4028,,,,,,,,,,,,,,,,WOS:000855160704017,0
C,"Li, BZ; Min, S; Iyer, S; Mehdad, Y; Yih, WT",,,Assoc Computat Linguist,"Li, Belinda Z.; Min, Sewon; Iyer, Srinivasan; Mehdad, Yashar; Yih, Wen-Tau",,,Efficient One-Pass End-to-End Entity Linking for Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever (Min et al., 2019).(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6433,6441,,,,,,,,,,,,,,,,WOS:000855160706055,0
C,"Li, CM; Fisher, E; Thomas, R; Pittard, S; Hertzberg, V; Choi, JD",,,Assoc Computat Linguist,"Li, Changmao; Fisher, Elaine; Thomas, Rebecca; Pittard, Steve; Hertzberg, Vicki; Choi, Jinho D.",,,Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC). Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines. As a result, a high Kappa score of 61% is achieved for interannotator agreement. Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2). Our best models using section encoding and multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2. Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8456,8466,,,,,,,,,,,,,,,,WOS:000855160708054,0
C,"Lombardo, P; Boiardi, A; Colombo, L; Schiavone, A; Tamagnone, N",,,Assoc Computat Linguist,"Lombardo, Pierangelo; Boiardi, Alessio; Colombo, Luca; Schiavone, Angelo; Tamagnone, Nicolo",,,Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks. In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks. Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3081,3093,,,,,,,,,,,,,,,,WOS:000855160703024,0
C,"Papadimitriou, I; Jurafsky, D",,,Assoc Computat Linguist,"Papadimitriou, Isabel; Jurafsky, Dan",,,Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6829,6839,,,,,,,,,,,,,,,,WOS:000855160707003,0
C,"Pilault, J; Li, R; Subramanian, S; Pal, C",,,Assoc Computat Linguist,"Pilault, Jonathan; Li, Raymond; Subramanian, Sandeep; Pal, Christopher",,,On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-grain copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9308,9319,,,,,,,,,,,,,,,,WOS:000855160709043,0
C,"Sia, S; Dalmia, A; Mielke, SJ",,,Assoc Computat Linguist,"Sia, Suzanna; Dalmia, Ayush; Mielke, Sabrina J.",,,Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1728,1736,,,,,,,,,,,,,,,,WOS:000855160701075,0
C,"Tenney, I; Wexler, J; Bastings, J; Bolukbasi, T; Coenen, A; Gehrmann, S; Jiang, E; Pushkarna, M; Radebaugh, C; Reif, E; Yuan, A",,,Assoc Computat Linguist,"Tenney, Ian; Wexler, James; Bastings, Jasmijn; Bolukbasi, Tolga; Coenen, Andy; Gehrmann, Sebastian; Jiang, Ellen; Pushkarna, Mahima; Radebaugh, Carey; Reif, Emily; Yuan, Ann",,,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models-including classification, seq2seq, and structured prediction-and is highly extensible through a declarative, framework-agnostic API.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,107,118,,,,,,,,,,,,,,,,WOS:000855177700015,0
C,"Tian, RZ; Mao, YY; Zhang, R",,,Assoc Computat Linguist,"Tian, Runzhi; Mao, Yongyi; Zhang, Richong",,,Learning VAE-LDA Models with Rounded Reparameterization Trick,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1315,1325,,,,,,,,,,,,,,,,WOS:000855160701041,0
C,"Tu, LF; Liu, TY; Gimpel, K",,,Assoc Computat Linguist,"Tu, Lifu; Liu, Tianyu; Gimpel, Kevin",,,An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and selfattention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5569,5582,,,,,,,,,,,,,,,,WOS:000855160705059,0
C,"Vargas, F; Cotterell, R",,,Assoc Computat Linguist,"Vargas, Francisco; Cotterell, Ryan",,,Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a nonlinear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2902,2913,,,,,,,,,,,,,,,,WOS:000855160703007,0
C,"Wang, WS; Hoi, SCH; Joty, S",,,Assoc Computat Linguist,"Wang, Weishi; Hoi, Steven C. H.; Joty, Shafiq",,,Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6581,6591,,,,,,,,,,,,,,,,WOS:000855160706066,0
C,"Wang, YX; Guo, Y; Zhu, SQ",,,Assoc Computat Linguist,"Wang, Yexiang; Guo, Yi; Zhu, Siqi",,,Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3019,3028,,,,,,,,,,,,,,,,WOS:000855160703018,0
C,"Wu, CS; Hoi, S; Socher, R; Xiong, C",,,Assoc Computat Linguist,"Wu, Chien-Sheng; Hoi, Steven; Socher, Richard; Xiong, Caiming",,,TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,917,929,,,,,,,,,,,,,,,,WOS:000855160701006,0
C,"Zhang, L; Lyu, Q; Callison-Burch, C",,,Assoc Computat Linguist,"Zhang, Li; Lyu, Qing; Callison-Burch, Chris",,,"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a suite of reasoning tasks on two types of relations between procedural events: GOAL-STEP relations (learn poses is a step in the larger goal of doing yoga) and STEP-STEP TEMPORAL relations (buy a yoga mat typically precedes learn poses). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for commonsense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero-and few-shot settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4630,4639,,,,,,,,,,,,,,,,WOS:000855160704062,0
C,"Zhangt, WX; Dengt, Y; Mat, J; Lamt, W",,,Assoc Computat Linguist,"Zhang, Wenxuan; Yang Deng; Jing Ma; Wai Lam",,,AnswerFact: Fact Checking in Product Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business. To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums. Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings. We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem. Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2407,2417,,,,,,,,,,,,,,,,WOS:000855160702045,0
C,"Zmigrod, R; Vieira, T; Cotterell, R",,,Assoc Computat Linguist,"Zmigrod, Ran; Vieira, Tim; Cotterell, Ryan",,,Please Mind the Root: Decoding Arborescences for Dependency Parsing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly degrades as the size of the training set decreases. In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4809,4819,,,,,,,,,,,,,,,,WOS:000855160705001,0
C,"Bansal, T; Jha, R; Munkhdalai, T; McCallum, A",,,Assoc Computat Linguist,"Bansal, Trapit; Jha, Rishikesh; Munkhdalai, Tsendsuren; McCallum, Andrew",,,Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient - when there are few labeled examples, accuracy can be low. Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem. However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult. This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text. This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms. This yields as many unique meta-training tasks as the number of subsets of vocabulary terms. We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework. On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning. Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,522,534,,,,,,,,,,,,,,,,WOS:000855160700038,0
C,"Basaldella, M; Liu, F; Shareghi, E; Collier, N",,,Assoc Computat Linguist,"Basaldella, Marco; Liu, Fangyu; Shareghi, Ehsan; Collier, Nigel",,,COMETA: A Corpus for Medical Entity Linking in the Social Media,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3122,3137,,,,,,,,,,,,,,,,WOS:000855160703028,0
C,"Cheng, JP; Agrawal, D; Alonso, HM; Bhargava, S; Driesen, J; Flego, F; Kaplan, D; Kartsaklis, D; Li, L; Piraviperumal, D; Williams, JD; Yu, H; Seaghdha, DO; Johannsen, A",,,Assoc Computat Linguist,"Cheng, Jianpeng; Agrawal, Devang; Alonso, Hector Martinez; Bhargava, Shruti; Driesen, Joris; Flego, Federico; Kaplan, Dain; Kartsaklis, Dimitri; Li, Lin; Piraviperumal, Dhivya; Williams, Jason D.; Yu, Hong; Seaghdha, Diarmuid O.; Johannsen, Anders",,,Conversational Semantic Parsing for Dialog State Tracking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We consider a new perspective on dialog state tracking (DST), the task of estimating a user's goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts.1 We describe an encoder-decoder framework for DST with hierarchical representations, which leads to 20% improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8107,8117,,,,,,,,,,,,,,,,WOS:000855160708026,0
C,"Cho, S; Song, KQ; Li, C; Yu, D; Foroosh, H; Liu, F",,,Assoc Computat Linguist,"Cho, Sangwoo; Song, Kaiqiang; Li, Chen; Yu, Dong; Foroosh, Hassan; Liu, Fei",,,Better Highlighting: Creating Sub-Sentence Summary Highlights,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short. In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion. Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights. To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets. Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6282,6300,,,,,,,,,,,,,,,,WOS:000855160706042,0
C,"Dalvi, F; Sajjad, H; Durrani, N; Belinkov, Y",,,Assoc Computat Linguist,"Dalvi, Fahim; Sajjad, Hassan; Durrani, Nadir; Belinkov, Yonatan",,,Analyzing Redundancy in Pretrained Transformer Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as: i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4908,4926,,,,,,,,,,,,,,,,WOS:000855160705009,0
C,"Ding, KY; Li, J; Zhang, YJ",,,Assoc Computat Linguist,"Ding, Keyang; Jing Li; Zhang, Yuji",,,"Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers' responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chinese microblog Sina Weibo with over 13 thousand trending topics, emotion votes in 24 fine-grained types from massive participants, and user comments to allow context understanding.(1) In experiments, we examine baseline performance to predict a topic's possible social emotions in a multi-label classification setting. The results show that a seq2seq model with user comment modeling performs the best, even surpassing human prediction. More analyses shed light on the effects of emotion types, topic description lengths, contexts from user comments, and the limited capacity of the existing models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1376,1382,,,,,,,,,,,,,,,,WOS:000855160701046,0
C,"Dufter, P; Schutze, H",,,Assoc Computat Linguist,"Dufter, Philipp; Schuetze, Hinrich",,,Identifying Elements Essential for BERT's Multilinguality,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4423,4437,,,,,,,,,,,,,,,,WOS:000855160704046,0
C,"Fu, XY; Shi, WJ; Yu, XD; Zhao, Z; Roth, D",,,Assoc Computat Linguist,"Fu, Xingyu; Shi, Weijia; Yu, Xiaodong; Zhao, Zian; Roth, Dan",,,Design Challenges in Low-resource Cross-lingual Entity Linking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia's interlanguage links and thus suffer when the foreign language's Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL shows an average increase of 25% in gold candidate recall and of 13% in end-to-end linking accuracy over state-of-the-art baselines.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6418,6432,,,,,,,,,,,,,,,,WOS:000855160706054,0
C,"Ganhotra, J; Roitman, H; Cohen, D; Mills, N; Gunasekara, C; Mass, Y; Joshi, S; Lastras, L; Konopnicki, D",,,Assoc Computat Linguist,"Ganhotra, Jatin; Roitman, Haggai; Cohen, Doron; Mills, Nathaniel; Gunasekara, Chulaka; Mass, Yosi; Joshi, Sachindra; Lastras, Luis; Konopnicki, David",,,Conversational Document Prediction to Assist Customer Care Agents,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users' needs. We study the task of predicting the documents that customer care agents can use to facilitate users' needs. We also introduce a new public dataset(1) which supports the aforementioned problem. Using this dataset and two others, we investigate state-of-the-art deep learning (DL) and information retrieval (IR) models for the task. We also analyze the practicality of such systems in terms of inference time complexity. Our results show that an hybrid IR+DL approach provides the best of both worlds.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,349,356,,,,,,,,,,,,,,,,WOS:000855160700025,0
C,"Guo, D; Kim, Y; Rush, AM",,,Assoc Computat Linguist,"Guo, Demi; Kim, Yoon; Rush, Alexander M.",,,Sequence-Level Mixed Sample Data Augmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set. We connect this approach to existing techniques such as SwitchOut (Wang et al., 2018) and word dropout (Sennrich et al., 2016), and show that these techniques are all approximating variants of a single objective. SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines. On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5547,5552,,,,,,,,,,,,,,,,WOS:000855160705057,0
C,"Han, WJ; Zhang, LW; Jiang, Y; Tu, KW",,,Assoc Computat Linguist,"Han, Wenjuan; Zhang, Liwen; Jiang, Yong; Tu, Kewei",,,Adversarial Attack and Defense of Structured Prediction Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2327,2338,,,,,,,,,,,,,,,,WOS:000855160702039,0
C,"Hoyle, A; Goel, P; Resnik, P",,,Assoc Computat Linguist,"Hoyle, Alexander; Goel, Pranav; Resnik, Philip",,,Improving Neural Topic Models using Knowledge Distillation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1752,1771,,,,,,,,,,,,,,,,WOS:000855160701077,0
C,"Jeon, S; Strube, M",,,Assoc Computat Linguist,"Jeon, Sungho; Strube, Michael",,,Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7458,7472,,,,,,,,,,,,,,,,WOS:000855160707051,0
C,"Kale, M; Rastogi, A",,,Assoc Computat Linguist,"Kale, Mihir; Rastogi, Abhinav",,,Template Guided Text Generation for Task-Oriented Dialogue,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs. First, we propose a schemaguided approach which conditions the generation on a schema describing the API in natural language. Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API. To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance. A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text. Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6505,6520,,,,,,,,,,,,,,,,WOS:000855160706060,0
C,"Li, DF; Hu, BT; Chen, QC; Peng, WH; Wang, AQ",,,Assoc Computat Linguist,"Li, Dongfang; Hu, Baotian; Chen, Qingcai; Peng, Weihua; Wang, Anqi",,,Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2% in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1427,1438,,,,,,,,,,,,,,,,WOS:000855160701051,0
C,"Li, MZ; Chen, XY; Gao, S; Chan, ZM; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Li, Mingzhe; Chen, Xiuying; Gao, Shen; Chan, Zhangming; Zhao, Dongyan; Yan, Rui",,,VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset(1) show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9360,9369,,,,,,,,,,,,,,,,WOS:000855160709047,0
C,"Mao, YN; Qu, YR; Xie, YQ; Ren, X; Han, JW",,,Assoc Computat Linguist,"Mao, Yuning; Qu, Yanru; Xie, Yiqing; Ren, Xiang; Han, Jiawei",,,Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1737,1751,,,,,,,,,,,,,,,,WOS:000855160701076,0
C,"Ning, Q; Wu, H; Dasigi, P; Dua, D; Gardner, M; Logan, RL; Marasovic, A; Nie, Z",,,Assoc Computat Linguist,"Ning, Qiang; Wu, Hao; Dasigi, Pradeep; Dua, Dheeru; Gardner, Matt; Logan, Robert L.; Marasovic, Ana; Nie, Zhen",,,"Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"High-quality and large-scale data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators efficiently; and (3) reproducibility. To address these problems, we introduce CROWDAQ,(1) an open-source platform that standardizes the data collection pipeline with customizable user-interface components, automated annotator qualification, and saved pipelines in a re-usable format. We show that CROWDAQ simplifies data annotation significantly on a diverse set of data collection use cases and we hope it will be a convenient tool for the community.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,127,134,,,,,,,,,,,,,,,,WOS:000855177700017,0
C,"Peng, H; Gao, T; Han, X; Lin, Y; Li, P; Zhiyuan, LY; Sun, M; Zhou, J",,,Assoc Computat Linguist,"Peng, Hao; Gao, Tianyu; Han, Xu; Lin, Yankai; Li, Peng; Zhiyuan, Liu; Sun, Maosong; Zhou, Jie",,,Learning from Context or Names? An Empirical Study on Neural Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3661,3672,,,,,,,,,,,,,,,,WOS:000855160703073,0
C,"Ponti, EM; Glaves, G; Majewska, O; Liu, Q; Vulic, I; Korhonen, A",,,Assoc Computat Linguist,"Ponti, Edoardo M.; Glaves, Goran; Majewska, Olga; Liu, Qianchu; Vulic, Ivan; Korhonen, Anna",,,XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur ' imac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2362,2376,,,,,,,,,,,,,,,,WOS:000855160702042,0
C,"Shen, T; Mao, Y; He, PC; Long, GD; Trischler, A; Chen, WZ",,,Assoc Computat Linguist,"Shen, Tao; Mao, Yi; He, Pengcheng; Long, Guodong; Trischler, Adam; Chen, Weizhu",,,Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8980,8994,,,,,,,,,,,,,,,,WOS:000855160709017,0
C,"Tran, T; Hu, YF; Hu, CW; Yen, K; Tan, F; Lee, KM; Park, S",,,Assoc Computat Linguist,"Tran, Thanh; Hu, Yifan; Hu, Changwei; Yen, Kevin; Tan, Fei; Lee, Kyumin; Park, Serim",,,HABERTOR: An Efficient and Effective Deep Hatespeech Detector,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT's architecture, but is different in four aspects: (i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; (ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; (iii) it uses our proposed multisource ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and (iv) it uses a regularized adversarial training with our proposed finegrained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our HABERTOR is 4 similar to 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pretrain it by using less than 1% of the number of words. Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7486,7502,,,,,,,,,,,,,,,,WOS:000855160707053,0
C,"Vulic, I; Ruder, S; Sogaard, A",,,Assoc Computat Linguist,"Vulic, Ivan; Ruder, Sebastian; Sogaard, Anders",,,Are All Good Word Vector Spaces Isomorphic?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. under-training).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3178,3192,,,,,,,,,,,,,,,,WOS:000855160703032,0
C,"Wang, ZR; Lipton, ZC; Tsvetkov, Y",,,Assoc Computat Linguist,"Wang, Zirui; Lipton, Zachary C.; Tsvetkov, Yulia",,,On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers' generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4438,4450,,,,,,,,,,,,,,,,WOS:000855160704047,0
C,"Wilcox, E; Qian, P; Futrell, R; Kohita, R; Levy, R; Ballesteros, M",,,Assoc Computat Linguist,"Wilcox, Ethan; Qian, Peng; Futrell, Richard; Kohita, Ryosuke; Levy, Roger; Ballesteros, Miguel",,,Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes. First, we assess few-shot learning capabilities by developing controlled experiments that probe models' syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training. Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence). We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision (Dyer et al., 2016; Charniak et al., 2016). We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model. All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4640,4652,,,,,,,,,,,,,,,,WOS:000855160704063,0
C,"Xiao, C; Yao, Y; Xie, RB; Han, X; Liu, ZY; Sun, MS; Lin, F; Lin, L",,,Assoc Computat Linguist,"Xiao, Chaojun; Yuan Yao; Xie, Ruobing; Han, Xu; Liu, Zhiyuan; Sun, Maosong; Fen Lin; Lin, Leyu",,,Denoising Relation Extraction from Document-level Distant Supervision,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Distant supervision (DS) has been widely used to generate auto-labeled data for sentence-level relation extraction (RE), which improves RE performance. However, the existing success of DS cannot be directly transferred to the more challenging document-level relation extraction (DocRE), since the inherent noise in DS may be even multiplied in document level and significantly harm the performance of RE. To address this challenge, we propose a novel pre-trained model for DocRE, which denoises the document-level DS data via multiple pre-training tasks. Experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy DS data and achieve promising results. The source code of this paper can be found in https://github.com/thunlp/DSDocRE.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3683,3688,,,,,,,,,,,,,,,,WOS:000855160703075,0
C,"Zhang, JY; Van Genabith, J",,,Assoc Computat Linguist,"Zhang, Jingyi; Van Genabith, Josef",,,Translation Quality Estimation by Jointly Learning to Score and Rank,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits crosslingual sentence embeddings from pretrained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2592,2598,,,,,,,,,,,,,,,,WOS:000855160702062,0
C,"Briakou, E; Carpuat, M",,,Assoc Computat Linguist,"Briakou, Eleftheria; Carpuat, Marine",,,Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1563,1580,,,,,,,,,,,,,,,,WOS:000855160701061,0
C,"Durrani, N; Sajjad, H; Dalvi, F; Belinkov, Y",,,Assoc Computat Linguist,"Durrani, Nadir; Sajjad, Hassan; Dalvi, Fahim; Belinkov, Yonatan",,,Analyzing Individual Neurons in Pre-trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry out a neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pretrained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4865,4880,,,,,,,,,,,,,,,,WOS:000855160705006,0
C,"Fang, YW; Sun, SQ; Gan, Z; Pillai, R; Wang, SH; Liu, JJ",,,Assoc Computat Linguist,"Fang, Yuwei; Sun, Siqi; Gan, Zhe; Pillai, Rohit; Wang, Shuohang; Liu, Jingjing",,,Hierarchical Graph Network for Multi-hop Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8823,8838,,,,,,,,,,,,,,,,WOS:000855160709005,0
C,"Han, Z; Chen, P; Ma, YP; Tresp, V",,,Assoc Computat Linguist,"Han, Zhen; Chen, Peng; Ma, Yunpu; Tresp, Volker",,,DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7301,7316,,,,,,,,,,,,,,,,WOS:000855160707040,0
C,"He, Y; Wang, Z; Zhang, Y; Huang, RH; Caverlee, J",,,Assoc Computat Linguist,"He, Yun; Wang, Zhuoer; Zhang, Yin; Huang, Ruihong; Caverlee, James",,,PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7572,7582,,,,,,,,,,,,,,,,WOS:000855160707058,0
C,"Jain, P; Rathi, S; Mausam; Chakrabarti, S",,,Assoc Computat Linguist,"Jain, Prachi; Rathi, Sushant; Mausam; Chakrabarti, Soumen",,,Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Research on temporal knowledge bases, which associate a relational fact (s, r, o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space. TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3733,3747,,,,,,,,,,,,,,,,WOS:000855160703080,0
C,"Kim, A; Pethe, C; Skiena, S",,,Assoc Computat Linguist,"Kim, Allen; Pethe, Charuta; Skiena, Steven",,,What time is it? Temporal Analysis of Novels,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recognizing the flow of time in a story is a crucial aspect of understanding it. Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases. To do so, we construct a data set of hourly time phrases from 52,183 fictional books. We then construct a time-of-day classification model that achieves an average error of 2.27 hours. Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day. This approach improves upon baselines by over two hour. Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past. Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9076,9086,,,,,,,,,,,,,,,,WOS:000855160709025,0
C,"Liu, Q; Chen, B; Lou, JG; Zhou, B; Zhang, DM",,,Assoc Computat Linguist,"Liu, Qian; Chen, Bei; Lou, Jian-Guang; Zhou, Bin; Zhang, Dongmei",,,Incomplete Utterance Rewriting as Semantic Segmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2846,2857,,,,,,,,,,,,,,,,WOS:000855160703002,0
C,"Luan, Y; Hauer, B; Mou, L; Kondrak, G",,,Assoc Computat Linguist,"Luan, Yixing; Hauer, Bradley; Mou, Lili; Kondrak, Grzegorz",,,Improving Word Sense Disambiguation with Translations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation. Since our approach is language independent, we perform WSD experiments on several languages. The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-of-the-art results in both English and multilingual WSD. To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4055,4065,,,,,,,,,,,,,,,,WOS:000855160704020,0
C,"Ma, X; Shen, Y; Fang, G; Chen, C; Jia, C; Lu, W",,,Assoc Computat Linguist,"Ma, Xinyin; Shen, Yongliang; Fang, Gongfan; Chen, Chen; Jia, Chenghao; Lu, Weiming",,,Adversarial Self-Supervised Data-Free Distillation for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel twostage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6182,6192,,,,,,,,,,,,,,,,WOS:000855160706032,0
C,"Maxwell, F; Jena, DH; Vered, S; Maarten, S; Choi, Y",,,Assoc Computat Linguist,"Maxwell, Forbes; Jena, D. Hwang; Vered, Shwartz; Maarten, Sap; Choi, Yejin",,,SOCIAL CHEMISTRY 101: Learning to Reason about Social and Moral Norms,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social norms-the unspoken commonsense rules about acceptable social behavior-are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as wanting to call cops on my neighbor are social norms that inform our conduct, such as It is expected that you report crimes. We present SOCIAL CHEMISTRY, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as It is rude to run a blender at 5am as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, NEURAL NORM TRANSFORMER, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,653,670,,,,,,,,,,,,,,,,WOS:000855160700048,0
C,"Moradshahi, M; Campagna, G; Semnani, SJ; Xu, S; Lam, MS",,,Assoc Computat Linguist,"Moradshahi, Mehrad; Campagna, Giovanni; Semnani, Sina J.; Xu, Silei; Lam, Monica S.",,,Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a fewshot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators. We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains. Our models achieve an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set. We show our approach outperforms the previous state-of-theart methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested. Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours. Our code is released open-source.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5970,5983,,,,,,,,,,,,,,,,WOS:000855160706014,0
C,"Qiu, L; Zhao, YZ; Shi, WY; Liang, Y; Shi, F; Yuan, T; Yu, Z; Zhu, SC",,,Assoc Computat Linguist,"Qiu, Liang; Zhao, Yizhou; Shi, Weiyan; Liang, Yuan; Shi, Feng; Yuan, Tao; Yu, Zhou; Zhu, Song-Chun",,,Structured Attention for Unsupervised Dialogue Structure Induction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1889,1899,,,,,,,,,,,,,,,,WOS:000855160702005,0
C,"Safavi, T; Koutra, D",,,Assoc Computat Linguist,"Safavi, Tara; Koutra, Danai",,,CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present CoDEx, a set of knowledge graph Completion Datasets Extracted from Wiki-data and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8328,8350,,,,,,,,,,,,,,,,WOS:000855160708044,0
C,"Schuff, H; Adel, H; Vu, NT",,,Assoc Computat Linguist,"Schuff, Hendrik; Adel, Heike; Vu, Ngoc Thang",,,F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F-1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7076,7095,,,,,,,,,,,,,,,,WOS:000855160707024,0
C,"Sundararaman, D; Si, SJ; Subramanian, V; Wang, GY; Hazarika, D; Carin, L",,,Assoc Computat Linguist,"Sundararaman, Dhanasekar; Si, Shijing; Subramanian, Vivek; Wang, Guoyin; Hazarika, Devamanyu; Carin, Lawrence",,,Methods for Numeracy-Preserving Word Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in a text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream applications by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4742,4753,,,,,,,,,,,,,,,,WOS:000855160704072,0
C,"Upadhye, S; Bergen, L; Kehler, A",,,Assoc Computat Linguist,"Upadhye, Shiva; Bergen, Leon; Kehler, Andrew",,,Predicting Reference: What do Language Models Learn about Discourse Models?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,977,982,,,,,,,,,,,,,,,,WOS:000855160701010,0
C,"Utama, PA; Moosavi, NS; Gurevych, I",,,Assoc Computat Linguist,"Utama, Prasetya Ajie; Moosavi, Nafise Sadat; Gurevych, Iryna",,,Towards Debiasing NLU Models from Unknown Biases,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7597,7610,,,,,,,,,,,,,,,,WOS:000855160707060,0
C,"Wang, LJ; Zhang, A; Wu, K; Sun, K; Li, ZH; Wu, H; Zhang, M; Wang, HF",,,Assoc Computat Linguist,"Wang, Lijie; Zhang, Ao; Wu, Kun; Sun, Ke; Li, Zhenghua; Wu, Hua; Zhang, Min; Wang, Haifeng",,,DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6923,6935,,,,,,,,,,,,,,,,WOS:000855160707011,0
C,"Wang, SH; Zhou, KJ; Lai, KF; Shen, JP",,,Assoc Computat Linguist,"Wang, Sihan; Zhou, Kaijie; Lai, Kunfeng; Shen, Jianping",,,Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3461,3471,,,,,,,,,,,,,,,,WOS:000855160703053,0
C,"Wu, MX; Wang, WY; Pan, SJ",,,Assoc Computat Linguist,"Wu, Meixi; Wang, Wenya; Pan, Sinno Jialin",,,Deep Weighted MaxSAT for Aspect-based Opinion Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5618,5628,,,,,,,,,,,,,,,,WOS:000855160705063,0
C,"Xu, W; Haider, B; Mansour, S",,,Assoc Computat Linguist,"Xu, Weijia; Haider, Batool; Mansour, Saab",,,End-to-End Slot Alignment and Recognition for Cross-Lingual NLU,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5052,5063,,,,,,,,,,,,,,,,WOS:000855160705020,0
C,"Yang, XY; Nie, F; Feng, YF; Liu, Q; Chen, ZG; Zhu, XD",,,Assoc Computat Linguist,"Yang, Xiaoyu; Nie, Feng; Feng, Yufei; Liu, Quan; Chen, Zhigang; Zhu, Xiaodan",,,Program Enhanced Fact Verification with Verbalization and Graph Attention Network,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph ATtention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT. Our code is available at https://github.com/arielsho/Program-Enhanced-Table-Fact-Checking.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7810,7825,,,,,,,,,,,,,,,,WOS:000855160708003,0
C,"Ye, H; Tan, QY; He, RD; Li, JT; Ng, HT; Bing, LD",,,Assoc Computat Linguist,"Ye, Hai; Tan, Qingyu; He, Ruidan; Li, Juntao; Ng, Hwee Tou; Bing, Lidong",,,Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7386,7399,,,,,,,,,,,,,,,,WOS:000855160707046,0
C,"Zeng, GT; Yang, WMA; Ju, ZQ; Yang, Y; Wang, SC; Zhang, RS; Zhou, M; Zeng, JQ; Dong, XY; Zhang, RY; Fang, HC; Zhu, PH; Chen, S; Xie, PT",,,Assoc Computat Linguist,"Zeng, Guangtao; Yang, Wenmian; Ju, Zeqian; Yang, Yue; Wang, Sicheng; Zhang, Ruisi; Zhou, Meng; Zeng, Jiaqi; Dong, Xiangyu; Zhang, Ruoyu; Fang, Hongchao; Zhu, Penghui; Chen, Shu; Xie, Pengtao",,,MedDialog: Large-scale Medical Dialogue Datasets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets - MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9241,9250,,,,,,,,,,,,,,,,WOS:000855160709038,0
C,"Zhou, X; Pappas, N; Smith, NA",,,Assoc Computat Linguist,"Zhou, Xuhui; Pappas, Nikolaos; Smith, Noah A.",,,Multilevel Text Alignment with Cross-Document Attention,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5012,5025,,,,,,,,,,,,,,,,WOS:000855160705017,0
C,"Abdul-Mageed, M; Zhang, CY; Elmadany, A; Ungar, L",,,Assoc Computat Linguist,"Abdul-Mageed, Muhammad; Zhang, Chiyu; Elmadany, AbdelRahim; Ungar, Lyle",,,Toward Micro-Dialect Identification in Diaglossic and Code-Switched Environments,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message. For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models. To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT predicts micro-dialects with 9.9% F-1, similar to 76x better than a majority class baseline. Our new language model also establishes new state-of-the-art on several external tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5855,5876,,,,,,,,,,,,,,,,WOS:000855160706005,0
C,"Cai, YT; Wan, XJ",,,Assoc Computat Linguist,"Cai, Yitao; Wan, Xiaojun",,,IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historical user inputs. In this work, in addition to using encoders to capture historical information of user inputs, we propose a database schema interaction graph encoder to utilize historicalal information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6903,6912,,,,,,,,,,,,,,,,WOS:000855160707009,0
C,"Chandu, KR; Dong, RP; Black, AW",,,Assoc Computat Linguist,"Chandu, Khyathi Raghavi; Dong, Ruo-Ping; Black, Alan W.",,,Reading Between the Lines: Exploring Infilling in Visual Narratives,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1220,1229,,,,,,,,,,,,,,,,WOS:000855160701033,0
C,"Chen, J; Wang, ZH; Tian, R; Yang, ZC; Yang, DY",,,Assoc Computat Linguist,"Chen, Jiaao; Wang, Zhenghui; Tian, Ran; Yang, Zichao; Yang, Diyi",,,Local Additivity Based Data Augmentation for Semi-supervised NER,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1241,1251,,,,,,,,,,,,,,,,WOS:000855160701035,0
C,"Chen, XH; Li, Q; Wang, JP",,,Assoc Computat Linguist,"Chen, Xinhong; Li, Qing; Wang, Jianping",,,Conditional Causal Relationships between Emotions and Causes in Texts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related. To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset. Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses. Experiments demonstrate the effectiveness and generality of our aggregation module.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3111,3121,,,,,,,,,,,,,,,,WOS:000855160703027,0
C,"Dou, ZY; Anastasopoulos, A; Neubig, G",,,Assoc Computat Linguist,"Dou, Zi-Yi; Anastasopoulos, Antonios; Neubig, Graham",,,Dynamic Data Selection and Weighting for Iterative Back-Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5894,5904,,,,,,,,,,,,,,,,WOS:000855160706008,0
C,"Ferguson, J; Gardner, M; Hajishirzi, H; Khot, T; Dasigi, P",,,Assoc Computat Linguist,"Ferguson, James; Gardner, Matt; Hajishirzi, Hannaneh; Khot, Tushar; Dasigi, Pradeep",,,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system's performance at identifying a potential lack of sufficient information and locating sources for that information. To fill this gap, we present a dataset, IIRC, with more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. This process also gave many questions without answers, and those that require discrete reasoning, increasing the difficulty of the task. We follow recent modeling work on various reading comprehension datasets to construct a baseline model for this dataset, finding that it achieves 31.1% F1 on this task, while estimated human performance is 88.4%. The dataset, code for the baseline system, and a leaderboard can be found at https://allennlp.org/iirc.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1137,1147,,,,,,,,,,,,,,,,WOS:000855160701026,0
C,"Khashabi, D; Khot, T; Sabharwal, A",,,Assoc Computat Linguist,"Khashabi, Daniel; Khot, Tushar; Sabharwal, Ashish",,,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well. Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples. Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input. To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch. We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,163,170,,,,,,,,,,,,,,,,WOS:000855160700012,0
C,"Ko, M; Lee, J; Kim, H; Kim, G; Kang, J",,,Assoc Computat Linguist,"Ko, Miyoung; Lee, Jinhyuk; Kim, Hyunjae; Kim, Gangwoo; Kang, Jaewoo",,,Look at the First Sentence: Position Bias in Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1109,1121,,,,,,,,,,,,,,,,WOS:000855160701024,0
C,"Lu, JQ; Du, L; Liu, M; Dipnall, J",,,Assoc Computat Linguist,"Lu, Jueqing; Du, Lan; Liu, Ming; Dipnall, Joanna",,,Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class. In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification. The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations. Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.",,,,,,"Du, Lan/0000-0002-9925-0223",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2935,2943,,,,,,,,,,,,,,,,WOS:000855160703010,0
C,"Lukasik, M; Jain, H; Menon, AK; Kim, S; Bhojanapalli, S; Yu, F; Kumar, S",,,Assoc Computat Linguist,"Lukasik, Michal; Jain, Himanshu; Menon, Aditya Krishna; Kim, Seungyeon; Bhojanapalli, Srinadh; Yu, Felix; Kumar, Sanjiv",,,Semantic Label Smoothing for Sequence to Sequence Problems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label denoising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs. Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence. Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar. Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4992,4998,,,,,,,,,,,,,,,,WOS:000855160705015,0
C,"Mohiuddin, T; Bari, MS; Joty, S",,,Assoc Computat Linguist,"Mohiuddin, Tasnim; Bari, M. Saiful; Joty, Shafiq",,,LNMAP: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2712,2723,,,,,,,,,,,,,,,,WOS:000855160702072,0
C,"Nie, YY; Tian, YH; Wan, X; Song, Y; Dai, B",,,Assoc Computat Linguist,"Nie, Yuyang; Tian, Yuanhe; Wan, Xiang; Yan Song; Bo Dai",,,Named Entity Recognition for Social Media Texts with Semantic Augmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation. In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account. In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively. Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1383,1391,,,,,,,,,,,,,,,,WOS:000855160701047,0
C,"Pyatkin, V; Klein, A; Tsarfaty, R; Dagan, I",,,Assoc Computat Linguist,"Pyatkin, Valentina; Klein, Ayal; Tsarfaty, Reut; Dagan, Ido",,,"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source widecoverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2804,2819,,,,,,,,,,,,,,,,WOS:000855160702081,0
C,"Jin, Q; Tan, CQ; Chen, M; Liu, XZ; Huang, SF",,,Assoc Computat Linguist,"Qiao Jin; Tan, Chuanqi; Mosha Chen; Liu, Xiaozhong; Huang, Songfang",,,Predicting Clinical Trial Results by Implicit Evidence Integration,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.",,,,,,"Jin, Qiao/0000-0002-1268-7239",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1461,1477,,,,,,,,,,,,,,,,WOS:000855160701054,0
C,"Rodriguez, P; Crook, P",,,Assoc Computat Linguist,"Rodriguez, Pedro; Crook, Paul",,,Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user's pre-existing knowledge. Assuming a correlation between engagement and user responses such as liking messages or asking followup questions, we design a Wizard-of-Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts related to what they know. Through crowd-sourcing of this experiment, we collect and release 14K dialogs (181K utterances) where users and assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages. Responses using a user's prior knowledge increase engagement. We incorporate this knowledge into a multi-task model that reproduces human assistant policies and improves over a BERT content model by 13 mean reciprocal rank points.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8153,8172,,,,,,,,,,,,,,,,WOS:000855160708030,0
C,"Subramanian, S; Lee, K",,,Assoc Computat Linguist,"Subramanian, Shyam; Lee, Kyumin",,,Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification. Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy. Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification. Our source code is available at https://github.com/ShyamSubramanian/HESM.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7798,7809,,,,,,,,,,,,,,,,WOS:000855160708002,0
C,"Wang, H; Chen, M; Zhang, H; Roth, D",,,Assoc Computat Linguist,"Wang, Haoyu; Chen, Muhao; Zhang, Hongming; Roth, Dan",,,Joint Constrained Learning for Event-Event Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations. Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process. We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,696,706,,,,,,,,,,,,,,,,WOS:000855160700051,0
C,"Wu, CS; Xiong, C",,,Assoc Computat Linguist,"Wu, Chien-Sheng; Xiong, Caiming",,,Probing Task-Oriented Dialogue Representation from Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5036,5051,,,,,,,,,,,,,,,,WOS:000855160705019,0
C,"Xu, L; Li, H; Lu, W; Bing, LD",,,Assoc Computat Linguist,"Xu, Lu; Li, Hao; Lu, Wei; Bing, Lidong",,,Position-Aware Tagging for Aspect Sentiment Triplet Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2339,2349,,,,,,,,,,,,,,,,WOS:000855160702040,0
C,"Yao, JR; Qiu, HL; Min, BA; Xue, NW",,,Assoc Computat Linguist,"Yao, Jiarui; Qiu, Haoling; Min, Bonan; Xue, Nianwen",,,Annotating Temporal Dependency Graphs via Crowdsourcing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora, provide a representation scheme that achieves a good balance between completeness and practicality in temporal annotation. We also provide a crowdsourcing strategy to annotate TDGs, and demonstrate the feasibility of this approach with an evaluation of the quality of the annotation, and the utility of the resulting data set by training a machine learning model on this data set. This data set is publicly available(1).",,,,,,"Xue, Nianwen/0000-0002-4364-3618",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5368,5380,,,,,,,,,,,,,,,,WOS:000855160705042,0
C,"Zhang, R; Reddy, RG; Sultan, MA; Castelli, V; Ferritto, A; Florian, R; Kayi, ES; Roukos, S; Sil, A; Ward, T",,,Assoc Computat Linguist,"Zhang, Rong; Reddy, Revanth Gangi; Sultan, Md Arafat; Castelli, Vittorio; Ferritto, Anthony; Florian, Radu; Kayi, Efsun Sarioglu; Roukos, Salim; Sil, Avirup; Ward, Todd",,,Multi-Stage Pre-training for Low-Resource Domain Adaptation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transfer learning techniques are particularly useful in NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the vocabulary of the LM with domain-specific terms leads to further gains. To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks. We apply these approaches incrementally on a pre-trained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5461,5468,,,,,,,,,,,,,,,,WOS:000855160705050,0
C,"Aggarwall, M; Gupta, H; Sarkari, M; Krishnamurthyl, B",,,Assoc Computat Linguist,"Aggarwall, Milan; Gupta, Hiresh; Sarkari, Mausoom; Krishnamurthyl, Balaji",,,Form2Seq: A Framework for Higher-Order Form Structure Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures. We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms. To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task. We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation. Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines. Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3830,3840,,,,,,,,,,,,,,,,WOS:000855160704002,0
C,"Bar-Haim, R; Kantor, Y; Eden, L; Friedman, R; Lahav, D; Slonim, N",,,Assoc Computat Linguist,"Bar-Haim, Roy; Kantor, Yoav; Eden, Lilach; Friedman, Roni; Lahav, Dan; Slonim, Noam",,,Quantitative Argument Summarization and Beyond: Cross-Domain Key Point Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect. Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments. The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert. Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data. Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews. An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,39,49,,,,,,,,,,,,,,,,WOS:000855160700003,0
C,"Barkan, O; Caciularu, A; Dagan, I",,,Assoc Computat Linguist,"Barkan, Oren; Caciularu, Avi; Dagan, Ido",,,Within-Between Lexical Relation Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3521,3527,,,,,,,,,,,,,,,,WOS:000855160703059,0
C,"Bi, B; Li, CL; Wu, C; Yan, M; Wang, W; Huang, SF; Huang, F; Si, L",,,Assoc Computat Linguist,"Bi, Bin; Li, Chenliang; Wu, Chen; Yan, Ming; Wang, Wei; Huang, Songfang; Huang, Fei; Si, Luo",,,PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Self-supervised pre-training, such as BERT (Devlin et al., 2018), MASS (Song et al., 2019) and BART (Lewis et al., 2019), has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context. This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8681,8691,,,,,,,,,,,,,,,,WOS:000855160708075,0
C,"Brahman, F; Chaturvedi, S",,,Assoc Computat Linguist,"Brahman, Faeze; Chaturvedi, Snigdha",,,Modeling Protagonist Emotions for Emotion-Aware Storytelling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist. Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models. The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning. Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5277,5294,,,,,,,,,,,,,,,,WOS:000855160705036,0
C,"Cai, R; Lapata, M",,,Assoc Computat Linguist,"Cai, Rui; Lapata, Mirella",,,Alignment-free Cross-lingual Semantic Role Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers. We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings. The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence. It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3883,3894,,,,,,,,,,,,,,,,WOS:000855160704007,0
C,"Field, A; Tsvetkov, Y",,,Assoc Computat Linguist,"Field, Anjalie; Tsvetkov, Yulia",,,Unsupervised Discovery of Implicit Gender Bias,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,596,608,,,,,,,,,,,,,,,,WOS:000855160700044,0
C,"Flachs, S; Lacroix, O; Yannakoudakis, H; Rei, M; Sogaard, A",,,Assoc Computat Linguist,"Flachs, Simon; Lacroix, Ophelie; Yannakoudakis, Helen; Rei, Marek; Sogaard, Anders",,,Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of website text generated by English speakers of varying levels of proficiency. Website data is a common and important domain that contains far fewer grammatical errors than learner essays, which we show presents a challenge to state-of-the-art GEC systems. We demonstrate that a factor behind this is the inability of systems to rely on a strong internal language model in low error density domains. We hope this work shall facilitate the development of open-domain GEC models that generalize to different topics and genres.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8467,8478,,,,,,,,,,,,,,,,WOS:000855160708055,0
C,"Fu, J; Liu, P; Neubig, G",,,Assoc Computat Linguist,"Fu, Jinlan; Liu, Pengfei; Neubig, Graham",,,Interpretable Multi-dataset Evaluation for Named Entity Recognition,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6058,6069,,,,,,,,,,,,,,,,WOS:000855160706022,0
C,"Hayati, SA; Kang, D; Zhu, QCY; Shi, WY; Yu, Z",,,Assoc Computat Linguist,"Hayati, Shirley Anugrah; Kang, Dongyeop; Zhu, Qingxiaoyang; Shi, Weiyan; Yu, Zhou",,,INSPIRED: Toward Sociable Recommendation Dialog Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge in developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8142,8152,,,,,,,,,,,,,,,,WOS:000855160708029,0
C,"Hennigen, LT; Williams, A; Cotterell, R",,,Assoc Computat Linguist,"Hennigen, Lucas Torroba; Williams, Adina; Cotterell, Ryan",,,Intrinsic Probing through Dimension Selection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,197,216,,,,,,,,,,,,,,,,WOS:000855160700015,0
C,"Huber, P; Carenini, G",,,Assoc Computat Linguist,"Huber, Patrick; Carenini, Giuseppe",,,MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The lack of large and diverse discourse tree-banks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment-annotated datasets, creating and publishing MEGA-DT, a new large-scale discourse-annotated corpus. Our approach generates discourse trees incorporating structure and nuclearity for documents of arbitrary length by relying on an efficient heuristic beam-search strategy, extended with a stochastic component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7442,7457,,,,,,,,,,,,,,,,WOS:000855160707050,0
C,"Kim, B; Ki, KS; Lee, D; Gweon, G",,,Assoc Computat Linguist,"Kim, Bugeun; Ki, Kyung Seo; Lee, Donggeon; Gweon, Gahgene",,,Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using 'Op (operator/operand)' tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) 'Expression' token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3768,3779,,,,,,,,,,,,,,,,WOS:000855160703083,0
C,"Lee, H; Hudson, DA; Lee, K; Manning, CD",,,Assoc Computat Linguist,"Lee, Haejun; Hudson, Drew A.; Lee, Kangwook; Manning, Christopher D.",,,SLM: Learning a Discourse Language Representation with Sentence Unshuffling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other. However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them. To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1551,1562,,,,,,,,,,,,,,,,WOS:000855160701060,0
C,"Lei, J; Yu, LC; Berg, TL; Bansal, M",,,Assoc Computat Linguist,"Lei, Jie; Yu, Licheng; Berg, Tamara L.; Bansal, Mohit",,,What is More Likely to Happen Next? Video-and-Language Future Event Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore whether AI models are able to learn to make such multimodal commonsense next-event predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. In order to promote the collection of non-trivial challenging examples, we employ an adversarial human-and-model-in-the-loop data collection procedure. We also present a strong baseline incorporating information from video, dialogue, and commonsense knowledge. Experiments show that each type of information is useful for this challenging task, and that compared to the high human performance on VLEP, our model provides a good starting point but leaves large room for future work.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8769,8784,,,,,,,,,,,,,,,,WOS:000855160709001,0
C,"Liang, WX; Zotil, J; Yu, Z",,,Assoc Computat Linguist,"Liang, Weixin; Zotil, James; Yu, Zhou",,,ALICE: Active Learning with Contrastive Natural Language Explanations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points.",,,,,"Zhang, James/HHS-8616-2022",,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4380,4391,,,,,,,,,,,,,,,,WOS:000855160704043,0
C,"Liu, QC; McCarthy, D; Korhonen, A",,,Assoc Computat Linguist,"Liu, Qianchu; McCarthy, Diana; Korhonen, Anna",,,Towards Better Context-aware Lexical Semantics: Adjusting Contextualized Representations through Static Anchors,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors. Our method requires only another pre-trained model and no labeled data is needed. We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context. We demonstrate that while the original contextual representations can be improved by another embedding space from either contextualized or static models, the static embeddings, which have lower computational requirements, provide the most gains.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4066,4075,,,,,,,,,,,,,,,,WOS:000855160704021,0
C,"Ma, JQ; Yan, ZY; Pang, SA; Zhang, Y; Shen, JP",,,Assoc Computat Linguist,"Ma, Jianqiang; Yan, Zeyu; Pang, Shuai; Zhang, Yang; Shen, Jianping",,,Mention Extraction and Linking for SQL Query Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot-filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries. Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6936,6942,,,,,,,,,,,,,,,,WOS:000855160707012,0
C,"Monarch, R; Morrison, A",,,Assoc Computat Linguist,"Monarch, Robert (Munro); Morrison, Alex (Carmen)",,,Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We report that state-of-the-art parsers consistently failed to identify hers and theirs as pronouns but identified the masculine equivalent his. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is amplified in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2011,2017,,,,,,,,,,,,,,,,WOS:000855160702014,0
C,"Reid, M; Marrese-Taylor, E; Matsuo, Y",,,Assoc Computat Linguist,"Reid, Machel; Marrese-Taylor, Edison; Matsuo, Yutaka",,,VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, CAMBRIDGE and the first non-English corpus ROBERT, which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6331,6344,,,,,,,,,,,,,,,,WOS:000855160706046,0
C,"Roy, U; Constant, N; Al-Rfou, R; Barua, A; Phillips, A; Yang, YF",,,Assoc Computat Linguist,"Roy, Uma; Constant, Noah; Al-Rfou, Rami; Barua, Aditya; Phillips, Aaron; Yang, Yinfei",,,LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for strong cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. This level of alignment is important for the practical task of cross-lingual information retrieval. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, model performance on zero-shot variants of our task that only target weak alignment is not predictive of performance on LAReQA. This finding underscores our claim that language-agnostic retrieval is a substantively new kind of cross-lingual evaluation, and suggests that measuring both weak and strong alignment will be important for improving cross-lingual systems going forward.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5919,5930,,,,,,,,,,,,,,,,WOS:000855160706010,0
C,"Tsai, YHH; Ma, MQ; Yang, MQ; Salakhutdinov, R; Morency, LP",,,Assoc Computat Linguist,"Tsai, Yao-Hung Hubert; Ma, Martin Q.; Yang, Muqiao; Salakhutdinov, Ruslan; Morency, Louis-Philippe",,,Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability. In this paper we propose Multimodal Routing, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality features. Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1823,1833,,,,,,,,,,,33969363.0,,,,,WOS:000855160701083,0
C,"Wang, SL; Wang, ZY; Che, WX; Liu, T",,,Assoc Computat Linguist,"Wang, Shaolei; Wang, Zhongyuan; Che, Wanxiang; Liu, Ting",,,Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1813,1822,,,,,,,,,,,,,,,,WOS:000855160701082,0
C,"Wang, T; Wang, X; Qin, Y; Ben Packer; Lee, K; Chen, J; Beutel, A; Chi, E",,,Assoc Computat Linguist,"Wang, Tianlu; Wang, Xuezhi; Qin, Yao; Ben Packer; Lee, Kang; Chen, Jilin; Beutel, Alex; Chi, Ed",,,CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CATGen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be irrelevant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which should not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model retraining and different model architectures.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5141,5146,,,,,,,,,,,,,,,,WOS:000855160705027,0
C,"Wang, ZJ; Duan, ZB; Zhang, H; Wang, CJ; Tian, L; Chen, B; Zhou, MY",,,Assoc Computat Linguist,"Wang, Zhengjue; Duan, Zhibin; Zhang, Hao; Wang, Chaojie; Tian, Long; Chen, Bo; Zhou, Mingyuan",,,Friendly Topic Assistant for Transformer Based Abstractive Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,485,497,,,,,,,,,,,,,,,,WOS:000855160700035,0
C,"Xu, JC; Desai, S; Durrett, G",,,Assoc Computat Linguist,"Xu, Jiacheng; Desai, Shrey; Durrett, Greg",,,Understanding Neural Abstractive Summarization Models via Uncertainty,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6275,6281,,,,,,,,,,,,,,,,WOS:000855160706041,0
C,"Zadeh, A; Cao, YS; Hessner, S; Liang, PP; Poria, S; Morency, LP",,,Assoc Computat Linguist,"Zadeh, Amir; Cao, Yan Sheng; Hessner, Simon; Liang, Paul Pu; Poria, Soujanya; Morency, Louis-Philippe",,,"CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects native speakers of languages other than English. As a step towards building more equitable and inclusive multimodal systems, we introduce the first large-scale multimodal language dataset for Spanish, Portuguese, German and French. The proposed dataset, called CMU-MOSEAS (CMU Multimodal Opinion Sentiment, Emotions and Attributes), is the largest of its kind with 40; 000 total labelled sentences. It covers a diverse set topics and speakers, and carries supervision of 20 labels including sentiment (and subjectivity), emotions, and attributes. Our evaluations on a state-of-the-art multimodal model demonstrates that CMU-MOSEAS enables further research for multilingual studies in multimodal language.",,,,,"Poria, Soujanya/L-8361-2015",,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1801,1812,,,,,,,,,,,33969362.0,,,,,WOS:000855160701081,0
C,"Zhang, JG; Hashimoto, K; Liu, W; Wu, CS; Wan, Y; Yu, PS; Socher, R; Xiong, C",,,Assoc Computat Linguist,"Zhang, Jian-Guo; Hashimoto, Kazuma; Liu, Wenhao; Wu, Chien-Sheng; Wan, Yao; Yu, Philip S.; Socher, Richard; Xiong, Caiming",,,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERTstyle pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embeddingbased nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5064,5082,,,,,,,,,,,,,,,,WOS:000855160705021,0
C,"Zhang, W; Hou, L; Yin, YC; Shang, LF; Chen, X; Jiang, X; Liu, Q",,,Assoc Computat Linguist,"Zhang, Wei; Hou, Lu; Yin, Yichun; Shang, Lifeng; Chen, Xiao; Jiang, Xin; Liu, Qun",,,TernaryBERT: Distillation-aware Ultra-low Bit BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by the lower capacity of low bits, we leverage the knowledge distillation technique (Jiao et al., 2019) in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,509,521,,,,,,,,,,,,,,,,WOS:000855160700037,0
C,"Choi, BJ; Hong, JM; Park, DK; Lee, SW",,,Assoc Computat Linguist,"Choi, Byung-Ju; Hong, Jimin; Park, David Keetae; Lee, Sang Wan",,,F-2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F-2- Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F-2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (i) frequency class, and (ii) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9167,9182,,,,,,,,,,,,,,,,WOS:000855160709032,0
C,"Chu, CX; Razniewski, S; Weikum, G",,,Assoc Computat Linguist,"Chu, Cuong Xuan; Razniewski, Simon; Weikum, Gerhard",,,ENTYFI: A System for Fine-grained Entity Typing in Fictional Texts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-quality type systems for popular fictional domains, and provides recommendations towards reference type systems for given input texts. Users can exploit the richness and diversity of these reference type systems for fine-grained supervised typing, in addition, they can choose among and combine four other typing modules: pre-trained real-world models, unsupervised dependency-based typing, knowledge base lookups, and constraint-based candidate consolidation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,100,106,,,,,,,,,,,,,,,,WOS:000855177700014,0
C,"Ding, XA; Liu, TY; Chang, BB; Sui, ZF; Gimpel, K",,,Assoc Computat Linguist,"Ding, Xiaoan; Liu, Tianyu; Chang, Baobao; Sui, Zhifang; Gimpel, Kevin",,,Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the infinilog loss. Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8189,8202,,,,,,,,,,,,,,,,WOS:000855160708032,0
C,"Duan, S; Zhao, H",,,Assoc Computat Linguist,"Duan, Sufeng; Zhao, Hai",,,Attention Is All You Need for Chinese Word Segmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3862,3872,,,,,,,,,,,,,,,,WOS:000855160704005,0
C,"Ein-Dor, L; Halfon, A; Gera, A; Shnarch, E; Dankin, L; Choshen, L; Danilevsky, M; Aharonov, R; Katz, Y; Slonim, N",,,Assoc Computat Linguist,"Ein-Dor, Liat; Halfon, Alon; Gera, Ariel; Shnarch, Eyal; Dankin, Lena; Choshen, Leshem; Danilevsky, Marina; Aharonov, Ranit; Katz, Yoav; Slonim, Noam",,,Active Learning for BERT: An Empirical Study,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7949,7962,,,,,,,,,,,,,,,,WOS:000855160708013,0
C,"Hua, YC; Li, YF; Haffari, G; Qi, GL; Wu, TT",,,Assoc Computat Linguist,"Hua, Yuncheng; Li, Yuan-Fang; Haffari, Gholamreza; Qi, Guilin; Wu, Tongtong",,,Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set.",,,,,,"Li, Yuan-Fang/0000-0003-4651-2821",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5827,5837,,,,,,,,,,,,,,,,WOS:000855160706002,0
C,"Huang, DD; Cui, LY; Yang, S; Bao, GS; Wang, K; Xie, J; Zhang, Y",,,Assoc Computat Linguist,"Huang, Dandan; Cui, Leyang; Yang, Sen; Bao, Guangsheng; Wang, Kun; Xie, Jun; Zhang, Yue",,,What Have We Achieved on Text Summarization?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric(1) (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,446,469,,,,,,,,,,,,,,,,WOS:000855160700033,0
C,"Ito, T; Kuribayashi, T; Hidaka, M; Suzuki, J; Inui, K",,,Assoc Computat Linguist,"Ito, Takumi; Kuribayashi, Tatsuki; Hidaka, Masatoshi; Suzuki, Jun; Inui, Kentaro",,,Langsmith: An Interactive Academic Text Revision System,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face significant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write English papers, especially in the natural language processing (NLP) field. Our system can suggest fluent, academic-style sentences to writers based on their rough, incomplete phrases or sentences. The system also encourages interaction between human writers and the computerized revision system. The experimental results demonstrated that Langsmith helps non-native English-speaker students write papers in English.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,216,226,,,,,,,,,,,,,,,,WOS:000855177700028,0
C,"Kang, M; Han, M; Hwang, SJ",,,Assoc Computat Linguist,"Kang, Minki; Han, Moonsu; Hwang, Sung Ju",,,Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6102,6120,,,,,,,,,,,,,,,,WOS:000855160706026,0
C,"Keskar, NS; McCann, B; Xiong, C; Socher, R",,,Assoc Computat Linguist,"Keskar, Nitish Shirish; McCann, Bryan; Xiong, Caiming; Socher, Richard",,,The Thieves on Sesame Street are Polyglots-Extracting Multilingual Models from Monolingual APIs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim's labels for that data. We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual. The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages. This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task. We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6203,6207,,,,,,,,,,,,,,,,WOS:000855160706034,0
C,"Ko, WJ; Chen, TY; Huang, YY; Durrett, G; Li, JJ",,,Assoc Computat Linguist,"Ko, Wei-Jen; Chen, Te-Yuan; Huang, Yiyan; Durrett, Greg; Li, Junyi Jessy",,,Inquisitive Question Generation for High Level Text Comprehension,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce INQUISITIVE, a dataset of similar to 19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT2 (Radford et al., 2019) and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6544,6555,,,,,,,,,,,,,,,,WOS:000855160706063,0
C,"Kobbe, J; Hulpus, I; Stuckenschmidt, H",,,Assoc Computat Linguist,"Kobbe, Jonathan; Hulpus, Ioana; Stuckenschmidt, Heiner",,,Unsupervised Stance Detection for Arguments from Consequences,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic. To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences. We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence. Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT. Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.",,,,,,"Stuckenschmidt, Heiner/0000-0002-0209-3859",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,50,60,,,,,,,,,,,,,,,,WOS:000855160700004,0
C,"Lei, DR; Jiang, GR; Gu, XT; Sun, KX; Mao, YN; Ren, X",,,Assoc Computat Linguist,"Lei, Deren; Jiang, Gangrong; Gu, Xiaotao; Sun, Kexuan; Mao, Yuning; Ren, Xiang",,,Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during traversal are often insufficient to guide a sophisticated walk-based reinforcement learning (RL) model. An alternate approach is to use traditional symbolic methods (e.g., rule induction), which achieve good performance but can be hard to generalize due to the limitation of symbolic representation. In this paper, we propose RuleGuider, which leverages high-quality rules generated by symbolic-based methods to provide reward supervision for walk-based agents. Experiments on benchmark datasets show that RuleGuider improves the performance of walk-based models without losing interpretability.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8541,8547,,,,,,,,,,,,,,,,WOS:000855160708063,0
C,"Li, Y; Li, G; He, LH; Zheng, JJ; Li, H; Guan, ZW",,,Assoc Computat Linguist,"Li, Yang; Li, Gang; He, Luheng; Zheng, Jingjie; Li, Hong; Guan, Zhiwei",,,Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a largescale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5495,5510,,,,,,,,,,,,,,,,WOS:000855160705053,0
C,"Majumder, BP; Li, SY; Ni, JM; McAuley, J",,,Assoc Computat Linguist,"Majumder, Bodhisattwa Prasad; Li, Shuyang; Ni, Jianmo; McAuley, Julian",,,INTERVIEW: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and information elicitation in such settings, but has been limited to manual review of small corpora. We introduce INTERVIEW-a large-scale (105K conversations) media dialog dataset collected from news interview transcripts-which allows us to investigate such patterns at scale. We present a dialog model that leverages external knowledge as well as dialog acts via auxiliary losses and demonstrate that our model quantitatively and qualitatively outperforms strong discourse-agnostic baselines for dialog modeling-generating more specific and topical responses in interview-style conversations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8129,8141,,,,,,,,,,,,,,,,WOS:000855160708028,0
C,"Ramnath, S; Nema, P; Sahni, D; Khapra, MM",,,Assoc Computat Linguist,"Ramnath, Sahana; Nema, Preksha; Sahni, Deep; Khapra, Mitesh M.",,,Towards Interpreting BERT for Reading Comprehension Based QA,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3236,3242,,,,,,,,,,,,,,,,WOS:000855160703036,0
C,"Roy, S; Goldwasser, D",,,Assoc Computat Linguist,"Roy, Shamik; Goldwasser, Dan",,,Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we suggest a minimally-supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7698,7716,,,,,,,,,,,,,,,,WOS:000855160707067,0
C,"Sawhney, R; Khanna, P; Aggarwal, A; Jain, T",,,Assoc Computat Linguist,"Sawhney, Ramit; Khanna, Piyush; Aggarwal, Arshiya; Jain, Taru",,,VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies' earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk. Additionally, most existing approaches ignore the correlations between stocks. Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation. Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8001,8013,,,,,,,,,,,,,,,,WOS:000855160708018,0
C,"Sharma, A; Miner, AS; Atkins, DC; Althoff, T",,,Assoc Computat Linguist,"Sharma, Ashish; Miner, Adam S.; Atkins, David C.; Althoff, Tim",,,A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support WARNING: This paper contains content related to suicide and self-harm,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5263,5276,,,,,,,,,,,,,,,,WOS:000855160705035,0
C,"Stahlberg, F; Kumar, S",,,Assoc Computat Linguist,"Stahlberg, Felix; Kumar, Shankar",,,Seq2Edits: Sequence Transduction Using Span-level Edit Operations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5147,5159,,,,,,,,,,,,,,,,WOS:000855160705028,0
C,"Wan, Y; Yang, BS; Wong, DF; Zhou, YK; Chao, LDS; Zhang, HB; Chen, BX",,,Assoc Computat Linguist,"Wan, Yu; Yang, Baosong; Wong, Derek F.; Zhou, Yikai; Chao, Lidia S.; Zhang, Haibo; Chen, Boxing",,,Self-Paced Learning for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1074,1080,,,,,,,,,,,,,,,,WOS:000855160701020,0
C,"Wang, YS; Fan, Z; Rose, CP",,,Assoc Computat Linguist,"Wang, Yansen; Fan, Zhen; Rose, Carolyn P.",,,Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding. Effective designs encode within the layout and formatting signals that point to where the important information can be found. In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task. In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction, and meta-level features that describe pages at a macro-level to aid in strategy selection. Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models. A qualitative post-hoc analysis illustrates how these features function within the model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1790,1800,,,,,,,,,,,,,,,,WOS:000855160701080,0
C,"Wieting, J; Neubig, G; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Wieting, John; Neubig, Graham; Berg-Kirkpatrick, Taylor",,,A Bilingual Generative Transformer for Semantic Sentence Embedding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1581,1594,,,,,,,,,,,,,,,,WOS:000855160701062,0
C,"Xu, QK; Qu, LX; Gao, ZY; Haffari, G",,,Assoc Computat Linguist,"Xu, Qiongkai; Qu, Lizhen; Gao, Zeyu; Haffari, Gholamreza",,,Personal Information Leakage Detection in Conversations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to Markets and Markets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants. The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation. In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on PERSONA-LEAKAGE1. Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems. The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model. In those cases, a significant number of information leaking utterances can be detected by our models with high precision.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6567,6580,,,,,,,,,,,,,,,,WOS:000855160706065,0
C,"Yu, WJ; Xu, C; Xu, J; Pang, L; Gao, XP; Wang, XZ; Wen, JR",,,Assoc Computat Linguist,"Yu, Weijie; Xu, Chen; Xu, Jun; Pang, Liang; Gao, Xiaopeng; Wang, Xiaozhao; Wen, Ji-Rong",,,Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2985,2994,,,,,,,,,,,,,,,,WOS:000855160703014,0
C,"Zhong, HL; Zhou, JS; Qu, WG; Long, YF; Gu, YH",,,Assoc Computat Linguist,"Zhong, Huilin; Zhou, Junsheng; Qu, Weiguang; Long, Yunfei; Gu, Yanhui",,,An Element-aware Multi-representation Model for Law Article Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples. The model uses the labeled elements of law articles to extract fact description features from multiple angles. It generates multiple representations of a fact for classification. Every label has a law-aware fact representation to encode more information. To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations. Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6663,6668,,,,,,,,,,,,,,,,WOS:000855160706073,0
C,"Cao, S; Kitaev, N; Klein, D",,,Assoc Computat Linguist,"Cao, Steven; Kitaev, Nikita; Klein, Dan",,,Unsupervised Parsing via Constituency Tests,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previous best published result.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4798,4808,,,,,,,,,,,,,,,,WOS:000855160704077,0
C,"Chu, C; Valenti, R; Knight, K",,,Assoc Computat Linguist,"Chu, Christopher; Valenti, Raphael; Knight, Kevin",,,Solving Historical Dictionary Codes with a Neural Language Model,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1% of the cipher-word tokens correctly.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5845,5854,,,,,,,,,,,,,,,,WOS:000855160706004,0
C,"Zhang, D; Ju, XC; Li, JH; Li, SS; Zhu, QM; Zhou, GD",,,Assoc Computat Linguist,"Dong Zhang; Ju, Xincheng; Li, Junhui; Li, Shoushan; Zhu, Qiaoming; Zhou, Guodong",,,Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3584,3593,,,,,,,,,,,,,,,,WOS:000855160703066,0
C,"Epure, EV; Salha, G; Moussallam, M; Hennequin, R",,,Assoc Computat Linguist,"Epure, Elena V.; Salha, Guillaume; Moussallam, Manuel; Hennequin, Romain",,,Modeling the Music Genre Perception across Language-Bound Cultures,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we study the feasibility of obtaining relevant cross-lingual, culture-specific music genre annotations based only on language-specific semantic representations, namely distributed concept embeddings and ontologies. Our study, focused on six languages, shows that unsupervised cross-lingual music genre annotation is feasible with high accuracy, especially when combining both types of representations. This approach of studying music genres is the most extensive to date and has many implications in musicology and music information retrieval. Besides, we introduce a new, domain-dependent cross-lingual corpus to benchmark state of the art multilingual pre-trained embedding models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4765,4779,,,,,,,,,,,,,,,,WOS:000855160704074,0
C,"Fu, TJ; Wang, XE; Grafton, ST; Eckstein, MP; Wang, WY",,,Assoc Computat Linguist,"Fu, Tsu-Jui; Wang, Xin Eric; Grafton, Scott T.; Eckstein, Miguel P.; Wang, William Yang",,,SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. However, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking and the ability to think about alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4413,4422,,,,,,,,,,,,,,,,WOS:000855160704045,0
C,"Hardalov, M; Mihaylov, T; Zlatkova, D; Dinkov, Y; Koychev, I; Nakov, P",,,Assoc Computat Linguist,"Hardalov, Momchil; Mihaylov, Todor; Zlatkova, Dimitrina; Dinkov, Yoan; Koychev, Ivan; Nakov, Preslav",,,EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose E chi alpha mu s - a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 highquality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. E chi alpha mu s offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. We perform various experiments with existing top-performing multilingual pre-trained models and we show that E chi alpha mu s offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that E chi alpha mu s will enable researchers to explore challenging reasoning and knowledge transfer methods and pretrained models for school question answering in various languages which was not possible before. The data, code, pre-trained models, and evaluation are available at http://github.com/mhardalov/exams-qa.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5427,5444,,,,,,,,,,,,,,,,WOS:000855160705048,0
C,"Hong, Y; Rodriguez-Opazo, C; Wu, Q; Gould, S",,,Assoc Computat Linguist,"Hong, Yicong; Rodriguez-Opazo, Cristian; Wu, Qi; Gould, Stephen",,,Sub-Instruction Aware Vision-and-Language Navigation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3360,3376,,,,,,,,,,,,,,,,WOS:000855160703046,0
C,"Hope, T; Portenoy, J; Vasan, K; Borchardt, J; Horvitz, E; Weld, DS; Hearst, MA; West, J",,,Assoc Computat Linguist,"Hope, Tom; Portenoy, Jason; Vasan, Kishore; Borchardt, Jonathan; Horvitz, Eric; Weld, Daniel S.; Hearst, Marti A.; West, Jevin",,,SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight(1) has so far served over 15K users with over 42K page views and 13% returns.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,135,143,,,,,,,,,,,,,,,,WOS:000855177700018,0
C,"Ji, CZ; Zhou, X; Zhang, YT; Liu, XZ; Sung, CL; Zhu, CH; Zhao, TJ",,,Assoc Computat Linguist,"Ji, Changzhen; Zhou, Xin; Zhang, Yating; Liu, Xiaozhong; Sung, Changlong; Zhu, Conghui; Zhao, Tiejun",,,Cross Copy Network for Dialogue Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored. Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation. In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances' logical structure simultaneously. Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1900,1910,,,,,,,,,,,,,,,,WOS:000855160702006,0
C,"Lin, ZJ; Madotto, A; Winata, GI; Fung, P",,,Assoc Computat Linguist,"Lin, Zhaojiang; Madotto, Andrea; Winata, Genta Indra; Fung, Pascale",,,MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to carryover the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 (Raffel et al., 2019) and BART (Lewis et al., 2019), and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3391,3405,,,,,,,,,,,,,,,,WOS:000855160703048,0
C,"Marcheggiani, D; Titov, I",,,Assoc Computat Linguist,"Marcheggiani, Diego; Titov, Ivan",,,Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by 'composing' word representations of the first and last words in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are 'decomposed' back into word representations, which are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL2012, and FrameNet.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3915,3928,,,,,,,,,,,,,,,,WOS:000855160704010,0
C,"Micheli, V; D'Hoffschmidt, M; Fleuret, F",,,Assoc Computat Linguist,"Micheli, Vincent; D'Hoffschmidt, Martin; Fleuret, Francois",,,On the importance of pre-training data volume for compact language models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually increasing amounts of French text. Through fine-tuning on the French Question Answering Dataset (FQuAD), we observe that well-performing models are obtained with as little as 100 MB of text. In addition, we show that past critically low amounts of pre-training data, an intermediate pre-training step on the task-specific corpus does not yield substantial improvements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7853,7858,,,,,,,,,,,,,,,,WOS:000855160708007,0
C,"Ning, Q; Wu, H; Han, RJ; Peng, NY; Gardner, M; Roth, D",,,Assoc Computat Linguist,"Ning, Qiang; Wu, Hao; Han, Rujun; Peng, Nanyun; Gardner, Matt; Roth, Dan",,,TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as what happened before/after [some event]? We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1158,1172,,,,,,,,,,,,,,,,WOS:000855160701028,0
C,"Qian, K; Raman, PC; Popa, L; Li, Y",,,Assoc Computat Linguist,"Qian, Kun; Raman, Poornima Chozhiyath; Popa, Lucian; Li, Yunyao",,,Learning Structured Representations of Entity Names using Active Learning and Weak Supervision,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem. Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6376,6383,,,,,,,,,,,,,,,,WOS:000855160706050,0
C,"Scialom, T; Dray, PA; Lamprier, S; Piwowarski, B; Staiano, J",,,Assoc Computat Linguist,"Scialom, Thomas; Dray, Paul-Alexis; Lamprier, Sylvain; Piwowarski, Benjamin; Staiano, Jacopo",,,MLSUM: The Multilingual Summarization Corpus,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages - namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8051,8067,,,,,,,,,,,,,,,,WOS:000855160708022,0
C,"Sen, P; Saffari, A",,,Assoc Computat Linguist,"Sen, Priyanka; Saffari, Amir",,,What do Models Learn from Question Answering Datasets?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release code to convert QA datasets to a shared format for easier experimentation at https: //github.com/amazon- research/ qa- dataset-converter.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2429,2438,,,,,,,,,,,,,,,,WOS:000855160702047,0
C,"Shen, JM; Qiu, WD; Shang, JB; Vanni, M; Ren, X; Han, JW",,,Assoc Computat Linguist,"Shen, Jiaming; Qiu, Wenda; Shang, Jingbo; Vanni, Michelle; Ren, Xiang; Han, Jiawei",,,SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependences. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have similar likelihoods of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities' infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. '11) facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8292,8307,,,,,,,,,,,,,,,,WOS:000855160708041,0
C,"Shin, HC; Zhang, Y; Bakhturina, E; Puri, R; Patwary, M; Shoeybi, M; Mani, R",,,Assoc Computat Linguist,"Shin, Hoo-Chang; Zhang, Yang; Bakhturina, Evelina; Puri, Raul; Patwary, Mostofa; Shoeybi, Mohammad; Mani, Raghav",,,BioMegatron: Larger Biomedical Domain Language Model,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at github.com/NVIDIA/NeMo.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4700,4706,,,,,,,,,,,,,,,,WOS:000855160704067,0
C,"Shmueli, B; Kul, LW; Ray, S",,,Assoc Computat Linguist,"Shmueli, Boaz; Kul, Lun-Wei; Ray, Soumya",,,Reactive Supervision: A New Method for Collecting Sarcasm Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2553,2559,,,,,,,,,,,,,,,,WOS:000855160702058,0
C,"Stacey, J; Minervini, P; Dubossarsky, H; Riedel, S; Rocktaschel, T",,,Assoc Computat Linguist,"Stacey, Joe; Minervini, Pasquale; Dubossarsky, Haim; Riedel, Sebastian; Rocktaschel, Tim",,,Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8281,8291,,,,,,,,,,,,,,,,WOS:000855160708040,0
C,"Sun, M; Hua, W; Liu, Z; Zheng, K; Wang, X; Li, P",,,Assoc Computat Linguist,"Sun, Mingming; Hua, Wenyue; Liu, Zoey; Zheng, Kangjie; Wang, Xin; Li, Ping",,,A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies. The OIX is an OIE friendly expression of a sentence without information loss. The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems. Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased. This paper focuses on the task of OIX and propose a solution Open Information Annotation (OIA). OIA is a predicate-function-argument annotation for sentences. We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences. The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2140,2150,,,,,,,,,,,,,,,,WOS:000855160702024,0
C,"Wang, M; Wang, Y",,,Assoc Computat Linguist,"Wang, Ming; Wang, Yinglin",,,A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses. Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-ofthe-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure. When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6229,6240,,,,,,,,,,,,,,,,WOS:000855160706037,0
C,"Yamada, I; Asai, A; Sakuma, J; Shindo, H; Takeda, H; Takefuji, Y; Matsumoto, Y",,,Assoc Computat Linguist,"Yamada, Ikuya; Asai, Akari; Sakuma, Jin; Shindo, Hiroyuki; Takeda, Hideaki; Takefuji, Yoshiyasu; Matsumoto, Yuji",,,Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a state-of-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies.",,,,,"Takefuji, Yoshiyasu/AAI-8819-2020","Takefuji, Yoshiyasu/0000-0002-1826-742X",,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,23,30,,,,,,,,,,,,,,,,WOS:000855177700004,0
C,"An, B; Lyu, J; Wang, ZY; Li, CY; Hu, CW; Tan, F; Zhang, RY; Hu, YF; Chen, CY",,,Assoc Computat Linguist,"An, Bang; Lyu, Jie; Wang, Zhenyi; Li, Chunyuan; Hu, Changwei; Tan, Fei; Zhang, Ruiyi; Hu, Yifan; Chen, Changyou",,,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,236,255,,,,,,,,,,,,,,,,WOS:000855160700017,0
C,"Botha, JA; Shan, ZF; Gillick, D",,,Assoc Computat Linguist,"Botha, Jan A.; Shan, Zifei; Gillick, Daniel",,,Entity Linking in 100 Languages,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset(1) matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7833,7845,,,,,,,,,,,,,,,,WOS:000855160708005,0
C,"Cao, P; Chen, Y; Zhao, J; Wang, T",,,Assoc Computat Linguist,"Cao, Pengfei; Chen, Yubo; Zhao, Jun; Wang, Taifeng",,,Incremental Event Detection via Knowledge Consolidation Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive. We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes. However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection. In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues. Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively. Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,707,717,,,,,,,,,,,,,,,,WOS:000855160700052,0
C,"Chen, SY; Hou, YT; Cui, YM; Che, WX; Liu, T; Yu, XZ",,,Assoc Computat Linguist,"Chen, Sanyuan; Hou, Yutai; Cui, Yiming; Che, Wanxiang; Liu, Ting; Yu, Xiangzhan",,,Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RECADAM optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7870,7881,,,,,,,,,,,,,,,,WOS:000855160708009,0
C,"Currey, A; Mathur, P; Dinu, G",,,Assoc Computat Linguist,"Currey, Anna; Mathur, Prashant; Dinu, Georgiana",,,Distilling Multiple Domains for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multidomain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4500,4511,,,,,,,,,,,,,,,,WOS:000855160704052,0
C,"Fang, ZY; Gokhale, T; Banerjee, P; Baral, C; Yang, YZ",,,Assoc Computat Linguist,"Fang, Zhiyuan; Gokhale, Tejas; Banerjee, Pratyay; Baral, Chitta; Yang, Yezhou",,,Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are reflected in conventional video captioning. Unlike images, actions in videos are also inherently linked to social aspects such as intentions (why the action is taking place), effects (what changes due to the action), and attributes that describe the agent. Thus for video understanding, such as when captioning videos or when answering questions about videos, one must have an understanding of these commonsense aspects. We present the first work on generating commonsense captions directly from videos, to describe latent aspects such as intentions, effects, and attributes. We present a new dataset Video-to-Commonsense (V2C) that contains similar to 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. Both the generation task and the QA task can be used to enrich video captions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,840,860,,,,,,,,,,,,,,,,WOS:000855160701001,0
C,"Goodman, S; Ding, N; Soricut, R",,,Assoc Computat Linguist,"Goodman, Sebastian; Ding, Nan; Soricut, Radu",,,TeaForN: Teacher-Forcing with N-grams,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8704,8717,,,,,,,,,,,,,,,,WOS:000855160708077,0
C,"Hedderich, MA; Adelani, DI; Zhu, DW; Alabil, J; Markus, U; Klakow, D",,,Assoc Computat Linguist,"Hedderich, Michael A.; Adelani, David I.; Zhu, Dawei; Alabi, Jesujoba; Markus, Udia; Klakow, Dietrich",,,Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and Yor`ub ' a on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2580,2591,,,,,,,,,,,,,,,,WOS:000855160702061,0
C,"Hua, X; Wang, L",,,Assoc Computat Linguist,"Hua, Xinyu; Wang, Lu",,,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often rambling without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,781,793,,,,,,,,,,,,,,,,WOS:000855160700057,0
C,"Jhamtani, H; Clark, P",,,Assoc Computat Linguist,"Jhamtani, Harsh; Clark, Peter",,,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC, contains over 98K explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: X is a Y AND Y has Z IMPLIES X has Z). We find that generalized chains maintain performance while also being more robust to certain perturbations.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,137,150,,,,,,,,,,,,,,,,WOS:000855160700010,0
C,"Jia, C; Shi, YF; Yang, QR; Zhang, Y",,,Assoc Computat Linguist,"Jia, Chen; Shi, Yuefeng; Yang, Qinrong; Zhang, Yue",,,Entity Enhanced BERT Pre-training for Chinese NER,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semisupervised entity enhanced BERT pre-training method. In particular, we first extract an entity lexicon from the relevant raw text using a newword discovery method. We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the selfattention using a combination of character and entity representations. In addition, an entity classification task helps inject the entity information into model parameters in pre-training. The pre-trained models are used for NER finetuning. Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.",,,,,,"Jia, Chen/0000-0002-8666-9930",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6384,6396,,,,,,,,,,,,,,,,WOS:000855160706051,0
C,"Jumel, C; Louis, A; Cheung, JCK",,,Assoc Computat Linguist,"Jumel, Clement; Louis, Annie; Cheung, Jackie C. K.",,,TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as 'London' and 'Paris' with different expressions: the major cities, the capital cities and two European cities. Yet generation, especially, abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content, to the exclusion of such semantic abstraction capabilities. In this paper, we present a new dataset and task aimed at the semantic aggregation of entities. TESA contains a dataset of 5.3K crowd-sourced entity aggregations of PERSON, ORGANIZATION, and LOCATION named entities.(1) The aggregations are document-appropriate, meaning that they are produced by annotators to match the situational context of a given news article from the New York Times. We then build baseline models for generating aggregations given a tuple of entities and document context. We fine-tune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features. Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions, but free-form expressions are understandably harder to generate and evaluate.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8031,8050,,,,,,,,,,,,,,,,WOS:000855160708021,0
C,"Kong, X; Zhang, ZS; Hovy, E",,,Assoc Computat Linguist,"Kong, Xiang; Zhang, Zhisong; Hovy, Eduard",,,Incorporating a Local Translation Mechanism into Non-autoregressive Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM; Ghazvininejad et al., 2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. The code for our model is available at https://github.com/shawnkx/NAT-with-Local-AT.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1067,1073,,,,,,,,,,,,,,,,WOS:000855160701019,0
C,"Kratzwald, B; Feuerriegel, S; Sun, H",,,Assoc Computat Linguist,"Kratzwald, Bernhard; Feuerriegel, Stefan; Sun, Huan",,,Learning a Cost-Effective Annotation Policy for Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3051,3062,,,,,,,,,,,,,,,,WOS:000855160703021,0
C,"Lin, HY; Lu, YJ; Tang, JL; Han, XP; Sun, L; Wei, ZC; Yuan, NJ",,,Assoc Computat Linguist,"Lin, Hongyu; Lu, Yaojie; Tang, Jialong; Han, Xianpei; Sun, Le; Wei, Zhicheng; Yuan, Nicholas Jing",,,A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist. And therefore it raises a critical question of whether previous creditable approaches can still work well when facing these challenges. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. To further verify our conclusions, we also construct a new open NER dataset that focuses on entity types with weaker name regularity and lower mention coverage to verify our conclusion. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is critical for the models to generalize to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained encoders.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7291,7300,,,,,,,,,,,,,,,,WOS:000855160707039,0
C,"Lin, ZH; Pan, X; Wang, MX; Qiu, XP; Feng, JT; Zhou, H; Li, L",,,Assoc Computat Linguist,"Lin, Zehui; Pan, Xiao; Wang, Mingxuan; Qiu, Xipeng; Feng, Jiangtao; Zhou, Hao; Li, Lei",,,Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github.com/linzehui/mRASP.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2649,2663,,,,,,,,,,,,,,,,WOS:000855160702067,0
C,"Loureiro, D; Camacho-Collados, J",,,Assoc Computat Linguist,"Loureiro, Daniel; Camacho-Collados, Jose",,,Don't Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3514,3520,,,,,,,,,,,,,,,,WOS:000855160703058,0
C,"Reddy, BP; Bhushan, S; Sarkar, S; Mukherjee, A",,,Assoc Computat Linguist,"Reddy, Bhanu Prakash; Bhushan, Sasi; Sarkar, Soumya; Mukherjee, Animesh",,,NwQM: A neural quality assessment framework for Wikipedia,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content. This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines. In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation. We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8396,8406,,,,,,,,,,,,,,,,WOS:000855160708049,0
C,"Rei, R; Stewart, C; Farinha, AC; Lavie, A",,,Assoc Computat Linguist,"Rei, Ricardo; Stewart, Craig; Farinha, Ana C.; Lavie, Alon",,,COMET: A Neural Framework for MT Evaluation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-theart levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-ofthe-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2685,2702,,,,,,,,,,,,,,,,WOS:000855160702070,0
C,"Sheng, JW; Guo, S; Chen, ZY; Yue, JW; Wang, LH; Liu, TW; Xu, HB",,,Assoc Computat Linguist,"Sheng, Jiawei; Guo, Shu; Chen, Zhenyu; Yue, Juwei; Wang, Lihong; Liu, Tingwen; Xu, Hongbo",,,Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at https://github.com/JiaweiSheng/FAAN.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1681,1691,,,,,,,,,,,,,,,,WOS:000855160701071,0
C,"Taille, B; Guigue, V; Scoutheeten, G; Gallinari, P",,,Assoc Computat Linguist,"Taille, Bruno; Guigue, Vincent; Scoutheeten, Geoffrey; Gallinari, Patrick",,,Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018a,b), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation. We then propose a small empirical study to quantify the most common mistake's impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05. We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER. This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics. We finally call for unifying the evaluation setting in end-to-end RE (1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3689,3701,,,,,,,,,,,,,,,,WOS:000855160703076,0
C,"Wadden, D; Lin, S; Lo, K; Wang, LL; van Zuylen, M; Cohan, A; Hajishirzi, H",,,Assoc Computat Linguist,"Wadden, David; Lin, Shanchuan; Lo, Kyle; Wang, Lucy Lu; van Zuylen, Madeleine; Cohan, Arman; Hajishirzi, Hannaneh",,,Fact or Fiction: Verifying Scientific Claims,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SCIFACT, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SCIFACT, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SCIFACT will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7534,7550,,,,,,,,,,,,,,,,WOS:000855160707056,0
C,"Wang, QX; Tan, H; Shen, S; Mahoney, MW; Yao, ZW",,,Assoc Computat Linguist,"Wang, Qinxin; Tan, Hao; Shen, Sheng; Mahoney, Michael W.; Yao, Zhewei",,,MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision. We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations. By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios. Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods. With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%. We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2030,2038,,,,,,,,,,,,,,,,WOS:000855160702016,0
C,"Wu, Y; Kuang, K; Zhang, Y; Liu, X; Sun, C; Xiao, J; Zhuang, Y; Si, L; Wu, F",,,Assoc Computat Linguist,"Wu, Yiquan; Kuang, Kun; Zhang, Yating; Liu, Xiaozhong; Sun, Changlong; Xiao, Jun; Zhuang, Yueting; Si, Luo; Wu, Fei",,,De-Biased Court's View Generation with Causality,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Court's view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff's claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court's views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,763,780,,,,,,,,,,,,,,,,WOS:000855160700056,0
C,"Xu, K; Tan, HC; Song, LF; Wu, H; Zhang, HS; Song, LQ; Yu, D",,,Assoc Computat Linguist,"Xu, Kun; Tan, Haochen; Song, Linfeng; Wu, Han; Zhang, Haisong; Song, Linqi; Yu, Dong",,,Semantic Role Labeling Guided Multi-turn Dialogue ReWriter,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting rid of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words. In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model. Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous stateof-the-art systems.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6632,6639,,,,,,,,,,,,,,,,WOS:000855160706070,0
C,"Xu, SL; Semnani, SJ; Campagna, G; Lam, MS",,,Assoc Computat Linguist,"Xu, Silei; Semnani, Sina J.; Campagna, Giovanni; Lam, Monica S.",,,AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,422,434,,,,,,,,,,,,,,,,WOS:000855160700031,0
C,"Yang, K; Yao, V; DeNero, J; Klein, D",,,Assoc Computat Linguist,"Yang, Kevin; Yao, Violet; DeNero, John; Klein, Dan",,,A Streaming Approach For Efficient Batched Beam Search,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically refills the batch before proceeding with a selected subset of candidates. We apply our method to variable-width beam search on a state-of-the-art machine translation model. Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines' BLEU. Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4526,4535,,,,,,,,,,,,,,,,WOS:000855160704054,0
C,"Yavuz, S; Hashimoto, K; Liu, W; Keskar, NS; Socher, R; Xiong, C",,,Assoc Computat Linguist,"Yavuz, Semih; Hashimoto, Kazuma; Liu, Wenhao; Keskar, Nitish Shirish; Socher, Richard; Xiong, Caiming",,,Simple Data Augmentation with the MASK Token Improves Domain Adaptation for Dialog Act Tagging,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of request carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain. In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data. We propose MASKAUGMENT, a controllable mechanism that augments text input by leveraging the pre-trained MASK token from BERT model. Inspired by consistency regularization, we use MASKAUGMENT to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers. Our extensive experiments on the Simulated Dialogue (GSim) and SchemaGuided Dialogue (SGD) datasets show that MASKAUGMENT is useful in improving the cross-domain generalization for DA tagging.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5083,5089,,,,,,,,,,,,,,,,WOS:000855160705022,0
C,"Yu, C; Sie, R; Tedeschi, N; Bergen, L",,,Assoc Computat Linguist,"Yu, Charles; Sie, Ryan; Tedeschi, Nico; Bergen, Leon",,,Word Frequency Does Not Predict Grammatical Knowledge in Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models' accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun's performance on grammatical tasks. Finally, we find that a novel noun's grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4040,4054,,,,,,,,,,,,,,,,WOS:000855160704019,0
C,"Zhang, BW; Hu, HX; Jain, V; Ie, E; Sha, F",,,Assoc Computat Linguist,"Zhang, Bowen; Hu, Hexiang; Jain, Vihan; Ie, Eugene; Sha, Fei",,,Learning to Represent Image and Text with Denotation Graph,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pretraining (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,823,839,,,,,,,,,,,,,,,,WOS:000855160700060,0
C,"Zhang, HZ; Wang, YY; Wang, SR; Cao, XZ; Zhang, FZ; Wang, ZY",,,Assoc Computat Linguist,"Zhang, Hongzhi; Wang, Yingyao; Wang, Sirui; Cao, Xuezhi; Zhang, Fuzheng; Wang, Zhongyuan",,,Table Fact Verification with Structure-Aware Transformer,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1624,1629,,,,,,,,,,,,,,,,WOS:000855160701066,0
C,"Zhou, N; Jurgens, D",,,Assoc Computat Linguist,"Zhou, Naitian; Jurgens, David",,,Condolence and Empathy in Online Communities,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Offering condolence is a natural reaction to hearing someone's distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal-trite responses offer little actual support despite their good intentions. Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online. Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement. Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory. Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,609,626,,,,,,,,,,,,,,,,WOS:000855160700045,0
C,"Boratko, M; Li, XL; O'Gorman, T; Das, R; Le, D; McCallum, A",,,Assoc Computat Linguist,"Boratko, Michael; Li, Xiang Lorraine; O'Gorman, Tim; Das, Rajarshi; Le, Dan; McCallum, Andrew",,,ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Given questions regarding some prototypical situation - such as Name something that people usually do before they leave the house for work? - a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show - FAMILY-FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1122,1136,,,,,,,,,,,,,,,,WOS:000855160701025,0
C,"Brazinskas, A; Lapata, M; Titov, I",,,Assoc Computat Linguist,"Brazinskas, Arthur; Lapata, Mirella; Titov, Ivan",,,Few-Shot Learning for Opinion Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4119,4135,,,,,,,,,,,,,,,,WOS:000855160704025,0
C,"Chalkidis, I; Fergadiotis, M; Kotitsas, S; Malakasiotis, P; Aletras, N; Androutsopoulos, I",,,Assoc Computat Linguist,"Chalkidis, Ilias; Fergadiotis, Manos; Kotitsas, Sotiris; Malakasiotis, Prodromos; Aletras, Nikolaos; Androutsopoulos, Ion",,,An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of LMTC datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7503,7515,,,,,,,,,,,,,,,,WOS:000855160707054,0
C,"Dognin, PL; Melnyk, I; Padhi, I; dos Santos, CN; Das, P",,,Assoc Computat Linguist,"Dognin, Pierre L.; Melnyk, Igor; Padhi, Inkit; dos Santos, Cicero Nogueira; Das, Payel",,,DualTKB: A Dual Learning Bridge between Text and Knowledge Base,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8605,8616,,,,,,,,,,,,,,,,WOS:000855160708069,0
C,"Feng, JY; Shaib, C; Rudzicz, F",,,Assoc Computat Linguist,"Feng, Jinyue; Shaib, Chantal; Rudzicz, Frank",,,Explainable Clinical Decision Support from Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction on the English MIMIC-III dataset, respectively. We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis. Finally, we outline a protocol to evaluate model usability in a clinical decision support context. From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1478,1489,,,,,,,,,,,,,,,,WOS:000855160701055,0
C,"Fevry, T; Soares, LB; FitzGerald, N; Choi, E; Kwiatkowski, T",,,Assoc Computat Linguist,"Fevry, Thibault; Soares, Livio Baldini; FitzGerald, Nicholas; Choi, Eunsol; Kwiatkowski, Tom",,,Entities as Experts: Sparse Memory Access with Entity Supervision,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model-Entities as Experts (EAE)-that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?, outperforming an encoder-generator Transformer model with 10x the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4937,4951,,,,,,,,,,,,,,,,WOS:000855160705011,0
C,"Groenwold, S; Ou, L; Parekh, A; Honnavalli, S; Levy, S; Mirza, D; Wang, WY",,,Assoc Computat Linguist,"Groenwold, Sophie; Ou, Lily; Parekh, Aesha; Honnavalli, Samhita; Levy, Sharon; Mirza, Diba; Wang, William Yang",,,Investigating African-American Vernacular English in Transformer-Based Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to text corpora availability. We investigate the performance of GPT-2 on AAVE text by creating a dataset of intent-equivalent parallel AAVE/SAE tweet pairs, thereby isolating syntactic structure and AAVE- or SAE-specific language for each pair. We evaluate each sample and its GPT-2 generated text with pretrained sentiment classifiers and find that while AAVE text results in more classifications of negative sentiment than SAE, the use of GPT-2 generally increases occurrences of positive sentiment for both. Additionally, we conduct human evaluation of AAVE and SAE text generated with GPT-2 to compare contextual rigor and overall quality.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5877,5883,,,,,,,,,,,,,,,,WOS:000855160706006,0
C,"Jansen, PA",,,Assoc Computat Linguist,"Jansen, Peter A.",,,COSATA: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This work presents COSATA, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text. The stand-alone COSATA solver allows easily expressing complex compositional inference patterns for how knowledge from different tables tends to connect to support inference and explanation construction in question answering and other downstream tasks, while including advanced declarative features and the ability to operate over multiple representations of text (words, lemmas, or part-of-speech tags). COSATA also includes a hybrid imperative/declarative interpreted language for expressing simple models through minimally-specified simulations grounded in constraint patterns, helping bridge the gap between question answering, question explanation, and model simulation. The solver and interpreter are released as open source.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,70,76,,,,,,,,,,,,,,,,WOS:000855177700010,0
C,"Jin, W; Qu, M; Jin, X; Ren, X",,,Assoc Computat Linguist,"Jin, Woojeong; Qu, Meng; Jin, Xisen; Ren, Xiang",,,Y Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE- NET), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE- NET employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RENET, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6669,6683,,,,,,,,,,,,,,,,WOS:000855160706074,0
C,"Kamezawa, H; Nishida, N; Shimizu, N; Miyazaki, T; Nakayama, H",,,Assoc Computat Linguist,"Kamezawa, Hisashi; Nishida, Noriki; Shimizu, Nobuyuki; Miyazaki, Takashi; Nakayama, Hideki",,,A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded first-person dialogue (VFD) dataset with verbal and non-verbal responses. The VFD dataset provides manually annotated (1) first-person images of agents, (2) utterances of human speakers, (3) eye-gaze locations of the speakers, and (4) the agents' verbal and nonverbal responses. We present experimental results obtained using the proposed VFD dataset and recent neural network models (e.g., BERT, ResNet). The results demonstrate that first-person vision helps neural network models correctly understand human intentions, and the production of non-verbal responses is a challenging task like that of verbal responses. Our dataset is publicly available(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3299,3310,,,,,,,,,,,,,,,,WOS:000855160703042,0
C,"Kang, XM; Zhao, Y; Zhang, JJ; Zong, CQ",,,Assoc Computat Linguist,"Kang, Xiaomian; Zhao, Yang; Zhang, Jiajun; Zong, Chengqing",,,Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2242,2254,,,,,,,,,,,,,,,,WOS:000855160702032,0
C,"Kedzie, C; McKeown, K",,,Assoc Computat Linguist,"Kedzie, Chris; McKeown, Kathleen",,,Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness. Additionally, we evaluate how a phrase-based data augmentation method can improve performance. We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model. Data augmentation further improves control on difficult, randomly generated utterance plans.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5160,5185,,,,,,,,,,,,,,,,WOS:000855160705029,0
C,"Kim, S; Yi, J; Kim, E; Yoon, S",,,Assoc Computat Linguist,"Kim, Siwon; Yi, Jihun; Kim, Eunji; Yoon, Sungroh",,,Interpretation of NLP models through input marginalization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"To demystify the black box property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations. In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out. We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3154,3167,,,,,,,,,,,,,,,,WOS:000855160703030,0
C,"Kreutzer, J; Foster, G; Cherry, C",,,Assoc Computat Linguist,"Kreutzer, Julia; Foster, George; Cherry, Colin",,,Inference Strategies for Machine Translation with Conditional Masking,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Conditional masked language model (CMLM) training has proven successful for nonautoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective. We identify a thresholding strategy that has advantages over the standard mask-predict algorithm, and provide analyses of its behavior on machine translation tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5774,5782,,,,,,,,,,,,,,,,WOS:000855160705075,0
C,"Mai, F; Pappas, N; Montero, I; Smith, NA; Henderson, J",,,Assoc Computat Linguist,"Mai, Florian; Pappas, Nikolaos; Montero, Ivan; Smith, Noah A.; Henderson, James",,,Plug and Play Autoencoders for Conditional Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6076,6092,,,,,,,,,,,,,,,,WOS:000855160706024,0
C,"Paranjape, B; Joshi, M; Thickstun, J; Hajishirzi, H; Zettlemoyer, L",,,Assoc Computat Linguist,"Paranjape, Bhargavi; Joshi, Mandar; Thickstun, John; Hajishirzi, Hannaneh; Zettlemoyer, Luke",,,An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text - a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1938,1952,,,,,,,,,,,,,,,,WOS:000855160702010,0
C,"Park, S; Park, K; Ahn, J; Oh, A",,,Assoc Computat Linguist,"Park, Sungjoon; Park, Kiwoong; Ahn, Jaimeen; Oh, Alice",,,Suicidal Risk Detection for Military Personnel,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q&A site, we collect posts containing military-relevant content written by active-duty military personnel. We then annotate the posts with two groups of experts: military experts and mental health experts. Our dataset includes 2,791 posts with 13,955 corresponding expert annotations of suicidal risk levels, and this dataset is available to researchers who consent to research ethics agreement. Using various finetuned state-of-the-art language models, we predict the level of suicide risk, reaching.88 F1 score for classifying the risks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2523,2531,,,,,,,,,,,,,,,,WOS:000855160702055,0
C,"Pasunuru, R; Guo, H; Bansal, M",,,Assoc Computat Linguist,"Pasunuru, Ramakanth; Guo, Han; Bansal, Mohit",,,DORB: Dynamically Optimizing Multiple Rewards with Bandits,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7766,7780,,,,,,,,,,,,,,,,WOS:000855160707072,0
C,"Reimers, N; Gurevych, I",,,Assoc Computat Linguist,"Reimers, Nils; Gurevych, Iryna",,,Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4512,4525,,,,,,,,,,,,,,,,WOS:000855160704053,0
C,"Verma, N; Sharma, A; Madan, D; Contractor, D; Kumar, H; Joshi, S",,,Assoc Computat Linguist,"Verma, Nikhil; Sharma, Abhishek; Madan, Dhiraj; Contractor, Danish; Kumar, Harshit; Joshi, Sachindra",,,Neural Conversational QA: Learning to Reason vs Exploiting Patterns,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural Conversational QA tasks like ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the models learn spurious clues/patterns in the dataset. Furthermore, we show that a heuristic-based program designed to exploit these patterns can have performance comparable to that of the neural models. In this paper we share our findings about four types of patterns found in the ShARC corpus and describe how neural models exploit them. Motivated by the aforementioned findings, we create and share a modified dataset that has fewer spurious patterns, consequently allowing models to learn better.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7263,7269,,,,,,,,,,,,,,,,WOS:000855160707036,0
C,"Webson, A; Chen, ZZ; Eickhoff, C; Pavlick, E",,,Assoc Computat Linguist,"Webson, Albert; Chen, Zhizhong; Eickhoff, Carsten; Pavlick, Ellie",,,Are Undocumented Workers the Same as Illegal Aliens? Disentangling Denotation and Connotation in Vector Spaces,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In politics, neologisms are frequently invented for partisan objectives. For example, undocumented workers and illegal aliens refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists. In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation. In this study, we propose an adversarial neural network that decomposes a pretrained representation as independent denotation and connotation representations. For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., immigrants vs. aliens, estate tax vs. death tax) move closer to each other in denotation space while moving further apart in connotation space. For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4090,4105,,,,,,,,,,,,,,,,WOS:000855160704023,0
C,"Wolf, M; Ruiter, D; D'Sa, AG; Reiners, L; Alexandersson, J; Klakow, D",,,Assoc Computat Linguist,"Wolf, Moritz; Ruiter, Dana; D'Sa, Ashwin Geet; Reiners, Liane; Alexandersson, Jan; Klakow, Dietrich",,,HUMAN: Hierarchical Universal Modular ANnotator,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the high specialisation of most annotation tools can result in having to switch to another tool if the task only slightly changes. We introduce HUMAN, a novel web-based annotation tool that addresses the above problems by a) covering a variety of annotation tasks on both textual and image data, and b) the usage of an internal deterministic state machine, allowing the researcher to chain different annotation tasks in an interdependent manner. Further, the modular nature of the tool makes it easy to define new annotation tasks and integrate machine learning algorithms e.g., for active learning. HUMAN comes with an easy-to-use graphical user interface that simplifies the annotation task and management.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,55,61,,,,,,,,,,,,,,,,WOS:000855177700008,0
C,"Yang, WS; Li, J; Fukumoto, F; Yanming, Y",,,Assoc Computat Linguist,"Yang, Wenshuo; Li, Jiyi; Fukumoto, Fumiyo; Yanming Ye",,,HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6716,6722,,,,,,,,,,,,,,,,WOS:000855160706078,0
C,"Yu, ZW; Zang, HY; Wan, XJ",,,Assoc Computat Linguist,"Yu, Zhiwei; Zang, Hongyu; Wan, Xiaojun",,,Homophonic Pun Generation with Lexically Constrained Rewriting,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with explicit positive and negative constraints. Our model achieves the state-of-the-art results in both automatic and human evaluations. We further make an error analysis and discuss the challenges for the computational pun models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2870,2876,,,,,,,,,,,,,,,,WOS:000855160703004,0
C,"Yuan, M; Lin, HT; Boyd-Graber, J",,,Assoc Computat Linguist,"Yuan, Michelle; Lin, Hsuan-Tien; Boyd-Graber, Jordan",,,Cold-start Active Learning through Self-supervised Language Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7935,7948,,,,,,,,,,,,,,,,WOS:000855160708012,0
C,"Bastianelli, E; Vanzo, A; Swietojanski, P; Rieser, V",,,Assoc Computat Linguist,"Bastianelli, Emanuele; Vanzo, Andrea; Swietojanski, Pawel; Rieser, Verena",,,SLURP: A Spoken Language Understanding Resource Package,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7252,7262,,,,,,,,,,,,,,,,WOS:000855160707035,0
C,"Bommasani, R; Cardie, C",,,Assoc Computat Linguist,"Bommasani, Rishi; Cardie, Claire",,,Intrinsic Evaluation of Summarization Datasets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or post hoc. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many recent summarization datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the datasets employed. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8075,8096,,,,,,,,,,,,,,,,WOS:000855160708024,0
C,"Chan, ZG; Zhang, YC; Chen, XY; Gaol, S; Zhang, ZG; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Chan, Zhangming; Zhang, Yuchi; Chen, Xiuying; Gaol, Shen; Zhang, Zhigiang; Zhao, Dongyan; Yan, Rui",,,Selection and Generation: Learning towards Multi-Product Advertisement Post Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic. A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products. Hence, multi-product AD post generation is meaningful and important. We propose a novel end-to-end model named SMG Net to generate the AD post. Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network). (2) generate a post including selected products via the MGenNet (Multi-Generator Network). Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products. Then, MGenNet generates the description copywriting of each product. Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3818,3829,,,,,,,,,,,,,,,,WOS:000855160704001,0
C,"Chen, JF; Zhang, RC; Mao, YY; Xu, J",,,Assoc Computat Linguist,"Chen, Junfan; Zhang, Richong; Mao, Yongyi; Xu, Jie",,,Parallel Interactive Networks for Multi-Domain Dialogue State Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.",,,,,,"chen, jun fan/0000-0001-6807-0089",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1921,1931,,,,,,,,,,,,,,,,WOS:000855160702008,0
C,"Cheng, LY; Bing, LD; Yu, Q; Lu, W; Si, L",,,Assoc Computat Linguist,"Cheng, Liying; Bing, Lidong; Yu, Qian; Lu, Wei; Si, Luo",,,APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them. We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task. To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task. Thus, we propose a multi-task learning framework based on hierarchical LSTM networks. Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7000,7011,,,,,,,,,,,,,,,,WOS:000855160707018,0
C,"Guo, XX; Yu, M; Gao, Y; Gan, C; Campbell, M; Chang, SY",,,Assoc Computat Linguist,"Guo, Xiaoxiao; Yu, Mo; Gao, Yupeng; Gan, Chuang; Campbell, Murray; Chang, Shiyu",,,Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7755,7765,,,,,,,,,,,,,,,,WOS:000855160707071,0
C,"Huang, K; Huang, D; Liu, Z; Mo, F",,,Assoc Computat Linguist,"Huang, Kaiyu; Huang, Degen; Liu, Zhuang; Mo, Fengran",,,A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora. However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words. In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks. Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters. To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model. Besides, we utilize a transfer learning method to improve the performance of OOV words. Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010). Our method achieves the state-of-the-art performances on all datasets. Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3873,3882,,,,,,,,,,,,,,,,WOS:000855160704006,0
C,"Jawanpuria, P; Meghwanshi, M; Mishra, B",,,Assoc Computat Linguist,"Jawanpuria, Pratik; Meghwanshi, Mayank; Mishra, Bamdev",,,A Simple Approach to Learning Unsupervised Multilingual Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space. In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques. We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing. When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2995,3001,,,,,,,,,,,,,,,,WOS:000855160703015,0
C,"Ji, YJ; Liu, H; He, BL; Xiao, XY; Wu, H; Yu, YH",,,Assoc Computat Linguist,"Ji, Yunjie; Liu, Hao; He, Bolei; Xiao, Xinyan; Wu, Hua; Yu, Yanhua",,,Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7012,7023,,,,,,,,,,,,,,,,WOS:000855160707019,0
C,"Jiao, WX; Wang, X; He, SL; King, I; Lyu, MR; Tu, ZP",,,Assoc Computat Linguist,"Jiao, Wenxiang; Wang, Xing; He, Shilin; King, Irwin; Lyu, Michael R.; Tu, Zhaopeng",,,Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability. (1)",,,,,,"Jiao, Wenxiang/0000-0003-4951-9420",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2255,2266,,,,,,,,,,,,,,,,WOS:000855160702033,0
C,"Kardas, M; Czapla, P; Stenetorp, P; Ruder, S; Riedel, S; Taylor, R; Stojnic, R",,,Assoc Computat Linguist,"Kardas, Marcin; Czapla, Piotr; Stenetorp, Pontus; Ruder, Sebastian; Riedel, Sebastian; Taylor, Ross; Stojnic, Robert",,,AXCELL: Automatic Extraction of Results from Machine Learning Papers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AXCELL, an automatic machine learning pipeline for extracting results from papers. AXCELL uses several novel components, including a table segmentation subtask, to learn relevant structural knowledge that aids extraction. When compared with existing methods, our approach significantly improves the state of the art for results extraction. We also release a structured, annotated dataset for training models for results extraction, and a dataset for evaluating the performance of models on this task. Lastly, we show the viability of our approach enables it to be used for semi-automated results extraction in production, suggesting our improvements make this task practically viable for the first time. Code is available on GitHub.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8580,8594,,,,,,,,,,,,,,,,WOS:000855160708067,0
C,"Majumder, BP; Jhamtani, H; Berg-kirkpatrick, T; McAuley, J",,,Assoc Computat Linguist,"Majumder, Bodhisattwa Prasad; Jhamtani, Harsh; Berg-kirkpatrick, Taylor; McAuley, Julian",,,Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the PERSONACHAT dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9194,9206,,,,,,,,,,,,,,,,WOS:000855160709034,0
C,"Martins, PH; Marinho, Z; Martins, AFT",,,Assoc Computat Linguist,"Martins, Pedro Henrique; Marinho, Zita; Martins, Andre F. T.",,,Sparse Text Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: epsilon-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4252,4273,,,,,,,,,,,,,,,,WOS:000855160704036,0
C,"Min, S; Michael, J; Hajishirzi, H; Zettlemoyer, L",,,Assoc Computat Linguist,"Min, Sewon; Michael, Julian; Hajishirzi, Hannaneh; Zettlemoyer, Luke",,,AMBIGQA: Answering Ambiguous Open-domain Questions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AMBIGQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AMBIGNQ, a dataset covering 14,042 questions from NQ- OPEN, an existing opendomain QA benchmark. We find that over half of the questions in NQ-OPEN are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AMBIGQA which we show benefit from weakly supervised learning that incorporates NQ-OPEN, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5783,5797,,,,,,,,,,,,,,,,WOS:000855160705076,0
C,"Moeller, S; Liu, L; Yang, CB; Kann, K; Hulden, M",,,Assoc Computat Linguist,"Moeller, Sarah; Liu, Ling; Yang, Changbing; Kann, Katharina; Hulden, Mans",,,IGT2P: From Interlinear Glossed Texts to Paradigms,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language's inflectional patterns and to complete inflectional paradigm tables. To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs). We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P). IGT2P generates entire morphological paradigms from IGT input. We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language. We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5251,5262,,,,,,,,,,,,,,,,WOS:000855160705034,0
C,"Morris, JX; Lifland, E; Yoo, JY; Grigsby, J; Jin, D; Qi, YJ",,,Assoc Computat Linguist,"Morris, John X.; Lifland, Eli; Yoo, Jin Yong; Grigsby, Jake; Jin, Di; Qi, Yanjun",,,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack's modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,119,126,,,,,,,,,,,,,,,,WOS:000855177700016,0
C,"Rijhwani, S; Anastasopoulos, A; Neubig, G",,,Assoc Computat Linguist,"Rijhwani, Shruti; Anastasopoulos, Antonios; Neubig, Graham",,,OCR Post Correction for Endangered Language Texts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR postcorrection method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5931,5942,,,,,,,,,,,,,,,,WOS:000855160706011,0
C,"Saharia, C; Chan, W; Saxena, S; Norouzi, M",,,Assoc Computat Linguist,"Saharia, Chitwan; Chan, William; Saxena, Saurabh; Norouzi, Mohammad",,,Non-Autoregressive Machine Translation with Latent Alignments,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates. In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline. Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model. On the competitive WMT'14 En!De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1098,1108,,,,,,,,,,,,,,,,WOS:000855160701023,0
C,"Seonwoo, Y; Kino, JH; Ha, JW; Oh, A",,,Assoc Computat Linguist,"Seonwoo, Yeon; Kin, Ji-Hoon; Ha, Jung -Woo; Oh, Alice",,,Context-Aware Answer Extraction in Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases. To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task. With experiments on reading comprehension, we show that BLANC outperforms the state-ofthe-art QA models, and the performance gap increases as the number of answer text occurrences increases. We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2418,2428,,,,,,,,,,,,,,,,WOS:000855160702046,0
C,"Shwartz, V; Rudinger, R; Tafjord, O",,,Assoc Computat Linguist,"Shwartz, Vered; Rudinger, Rachel; Tafjord, Oyvind",,,You are grounded!: Latent Name Artifacts in Pre-trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for 'Donald is a' substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6850,6861,,,,,,,,,,,,,,,,WOS:000855160707005,0
C,"Sui, D; Chen, YB; Zhao, J; Jia, YT; Xie, YT; Sun, WJ",,,Assoc Computat Linguist,"Sui, Dianbo; Chen, Yubo; Zhao, Jun; Jia, Yantao; Xie, Yuantao; Sun, Weijian",,,FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2118,2128,,,,,,,,,,,,,,,,WOS:000855160702022,0
C,"Tan, S; Joty, S; Varshney, LR; Kan, MY",,,Assoc Computat Linguist,"Tan, Samson; Joty, Shafiq; Varshney, Lav R.; Kan, Min-Yen",,,Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by nonstandard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pre-trained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5647,5663,,,,,,,,,,,,,,,,WOS:000855160705065,0
C,"Toshniwal, S; Wiseman, S; Ettinger, A; Livescu, K; Gimpel, K",,,Assoc Computat Linguist,"Toshniwal, Shubham; Wiseman, Sam; Ettinger, Allyson; Livescu, Karen; Gimpel, Kevin",,,Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8519,8526,,,,,,,,,,,,,,,,WOS:000855160708060,0
C,"Wu, YX; Riedel, S; Minervini, P; Stenetorp, P",,,Assoc Computat Linguist,"Wu, Yuxiang; Riedel, Sebastian; Minervini, Pasquale; Stenetorp, Pontus",,,Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader. However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost. To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read. We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of an early exit probability. We then introduce SKY-LINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning. Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3029,3039,,,,,,,,,,,,,,,,WOS:000855160703019,0
C,"Xue, M; Yu, B; Zhang, ZY; Liu, TW; Zhang, Y; Bin Wang",,,Assoc Computat Linguist,"Xue, Mengge; Yu, Bowen; Zhang, Zhenyu; Liu, Tingwen; Zhang, Yue; Bin Wang",,,Coarse-to-Fine Pre-training for Named Entity Recognition,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"More recently, Named Entity Recognition has achieved great advances aided by pre-training approaches such as BERT. However, current pre-training techniques focus on building language modeling objectives to learn a general representation, ignoring the named entity-related knowledge. To this end, we propose a NER-specific pre-training framework to inject coarse-to-fine automatically mined entity knowledge into pre-trained models. Specifically, we first warm-up the model via an entity span identification task by training it with Wikipedia anchors, which can be deemed as general-typed entities. Then we leverage the gazetteer-based distant supervision strategy to train the model extract coarse-grained typed entities. Finally, we devise a self-supervised auxiliary task to mine the fine-grained named entity knowledge via clustering. Empirical studies on three public NER datasets demonstrate that our framework achieves significant improvements against several pre-trained baselines, establishing the new state-of-the-art performance on three benchmarks. Besides, we show that our framework gains promising results without using human-labeled training data, demonstrating its effectiveness in label-few and low-resource scenarios.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6345,6354,,,,,,,,,,,,,,,,WOS:000855160706047,0
C,"Zhang, RS; Mao, XX; Li, L; Jiang, L; Chen, L; Hu, ZW; Xi, YD; Fan, CJ; Huang, ML",,,Assoc Computat Linguist,"Zhang, Rongsheng; Mao, Xiaoxi; Li, Le; Jiang, Lin; Chen, Lin; Hu, Zhiwei; Xi, Yadong; Fan, Changjie; Huang, Minlie",,,Youling: an AI-Assisted Lyrics Creation System,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates Youling, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, Youling supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, Youling allows users to use multifaceted attributes to control the content and format of generated lyrics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,85,91,,,,,,,,,,,,,,,,WOS:000855177700012,0
C,"Zhang, RZ; Yu, Y; Zhang, C",,,Assoc Computat Linguist,"Zhang, Rongzhi; Yu, Yue; Zhang, Chao",,,SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%-3.75% in terms of F-1 scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8566,8579,,,,,,,,,,,,,,,,WOS:000855160708066,0
C,"Zhao, YF; Xu, C; Wu, W",,,Assoc Computat Linguist,"Zhao, Yufan; Xu, Can; Wu, Wei",,,Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3472,3483,,,,,,,,,,,,,,,,WOS:000855160703054,0
C,"Zhao, ZJ; Papalexakis, EE; Ma, XJ",,,Assoc Computat Linguist,"Zhao, Zhenjie; Papalexakis, Evangelos E.; Ma, Xiaojuan",,,Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3293,3298,,,,,,,,,,,,,,,,WOS:000855160703041,0
C,"Zhong, RQ; Yu, T; Klein, D",,,Assoc Computat Linguist,"Zhong, Ruiqi; Yu, Tao; Klein, Dan",,,Semantic Evaluation for Text-to-SQL with Distilled Test Suites,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,396,411,,,,,,,,,,,,,,,,WOS:000855160700029,0
C,"Aminian, M; Rasooli, MS; Diab, M",,,Assoc Computat Linguist,"Aminian, Maryam; Rasooli, Mohammad Sadegh; Diab, Mona",,,Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.,,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8268,8274,,,,,,,,,,,,,,,,WOS:000855160708038,0
C,"Bornstein, A; Cattan, A; Dagan, I",,,Assoc Computat Linguist,"Bornstein, Aaron; Cattan, Arie; Dagan, Ido",,,CoRefi: A Crowd Sourcing Suite for Coreference Annotation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present COREFI, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, COREFI provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. COREFI is open source and directly embeds into any website, including popular crowdsourcing platforms.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,205,215,,,,,,,,,,,,,,,,WOS:000855177700027,0
C,"Budhraja, A; Pande, M; Nema, P; Kumar, P; Khapra, MM",,,Assoc Computat Linguist,"Budhraja, Aakriti; Pande, Madhura; Nema, Preksha; Kumar, Pratyush; Khapra, Mitesh M.",,,On the weak link between importance and prunability of attention heads,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads. We evaluate this on Transformer and BERT models on multiple NLP tasks. Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy. Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head. On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance. However, strategies that avoid pruning middle layers and consecutive layers perform better. Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads. Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3230,3235,,,,,,,,,,,,,,,,WOS:000855160703035,0
C,"Desai, S; Durrett, G",,,Assoc Computat Linguist,"Desai, Shrey; Durrett, Greg",,,Calibration of Pre-trained Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,295,302,,,,,,,,,,,,,,,,WOS:000855160700021,0
C,"Han, X; Bai, YZ; Qiu, KY; Liu, ZY; Sun, MS",,,Assoc Computat Linguist,"Han, Xu; Bai, Yuzhuo; Qiu, Keyue; Liu, Zhiyuan; Sun, Maosong",,,IsOBS: An Information System for Oracle Bone Script,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ancestor of modern Chinese. As the Chinese writing system is the oldest continuously-used system in the world, the study of OBS plays an important role in both linguistic and historical research. In order to utilize advanced machine learning methods to automatically process OBS, we construct an information system for OBS (IsOBS) to symbolize, serialize, and store OBS data at the character-level, based on efficient databases and retrieval modules. Moreover, we also apply few-shot learning methods to build an effective OBS character recognition module, which can recognize a large number of OBS characters (especially those characters with a handful of examples) and make the system easy to use. The demo system of IsOBS can be found from http://isobs.thunlp.org/. In the future, we will add more OBS data to the system, and hopefully our IsOBS can support further efforts in automatically processing OBS and advance the scientific progress in this field.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,227,233,,,,,,,,,,,,,,,,WOS:000855177700029,0
C,"Jo, Y; Bang, S; Manzoor, E; Hovy, E; Reed, C",,,Assoc Computat Linguist,"Jo, Yohan; Bang, Seojin; Manzoor, Emaad; Hovy, Eduard; Reed, Chris",,,Detecting Attackable Sentences in Arguments,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence's attackability is associated with many of these characteristics regarding the sentence's content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1,23,,,,,,,,,,,,,,,,WOS:000855160700001,0
C,"Ke, P; Ji, HZ; Liu, SY; Zhu, XY; Huang, ML",,,Assoc Computat Linguist,"Ke, Pei; Ji, Haozhe; Liu, Siyang; Zhu, Xiaoyan; Huang, Minlie",,,SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6975,6988,,,,,,,,,,,,,,,,WOS:000855160707016,0
C,"Kenyon-Dean, K; Newell, E; Cheung, JCK",,,Assoc Computat Linguist,"Kenyon-Dean, Kian; Newell, Edward; Cheung, Jackie Chi Kit",,,Deconstructing word embedding algorithms,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Un-contextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8479,8484,,,,,,,,,,,,,,,,WOS:000855160708056,0
C,"Ko, WJ; Ray, A; Shen, YL; Jin, HX",,,Assoc Computat Linguist,"Ko, Wei-Jen; Ray, Avik; Shen, Yilin; Jin, Hongxia",,,Generating Dialogue Responses from a Semantic Latent Space,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4339,4349,,,,,,,,,,,,,,,,WOS:000855160704040,0
C,"Libovicky, J; Fraser, A",,,Assoc Computat Linguist,"Libovicky, Jindrich; Fraser, Alexander",,,Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards highperformance and easy to train character-based models that are not extremely large.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2572,2579,,,,,,,,,,,,,,,,WOS:000855160702060,0
C,"Murawaki, Y",,,Assoc Computat Linguist,"Murawaki, Yugo",,,Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations. However, such ancestral populations are hardly interpretable in the context of the tree model. In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions. We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions. Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,959,976,,,,,,,,,,,,,,,,WOS:000855160701009,0
C,"Narayan, S; Maynez, J; Adamek, J; Pighin, D; Bratankic, B; McDonald, R",,,Assoc Computat Linguist,"Narayan, Shashi; Maynez, Joshua; Adamek, Jakub; Pighin, Daniele; Bratankic, Blaz; McDonald, Ryan",,,Stepwise Extractive Summarization and Planning with Structured Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose encoder-centric stepwise models for extractive summarization using structured transformers - HiBERT (Zhang et al., 2019) and Extended Transformers (Ainslie et al., 2020). We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4143,4159,,,,,,,,,,,,,,,,WOS:000855160704027,0
C,"Pimenter, T; Maudsla, RH; Blasi, D; CotterellA, R",,,Assoc Computat Linguist,"Pimenter, Tiago; Maudsla, Rowan Hall; Blasi, Damian; CotterellA, Ryan",,,Speakers Fill Lexical Semantic Gaps with Context,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear-resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this-one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. rho = 0.40 in English). We then test our main hypothesis-that a word's lexical ambiguity should negatively correlate with its contextual uncertainty-and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4004,4015,,,,,,,,,,,,,,,,WOS:000855160704016,0
C,"Pratapa, A; Jayanthi, SM; Nerella, K",,,Assoc Computat Linguist,"Pratapa, Adithya; Jayanthi, Sai Muralidhar; Nerella, Kavya",,,Constrained Fact Verification for FEVER,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim's factuality, there is little work on understanding the reasoning process. In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence. We present an extensive evaluation of state-of-the-art verification models under these constraints.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7826,7832,,,,,,,,,,,,,,,,WOS:000855160708004,0
C,"Safavi, T; Koutra, D; Meij, E",,,Assoc Computat Linguist,"Safavi, Tara; Koutra, Danai; Meij, Edgar",,,Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner's perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8308,8321,,,,,,,,,,,,,,,,WOS:000855160708042,0
C,"Sawhney, R; Joshi, H; Gandhi, S; Shah, RR",,,Assoc Computat Linguist,"Sawhney, Ramit; Joshi, Harshit; Gandhi, Saumya; Shah, Rajiv Ratn",,,A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social media's ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental depression. The emotional spectrum of a user's historical activity on social media can be indicative of their mental state over time. In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context. We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media. STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment. We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7685,7697,,,,,,,,,,,,,,,,WOS:000855160707066,0
C,"Shao, N; Cui, YM; Liu, T; Wang, SJ; Hu, GP",,,Assoc Computat Linguist,"Shao, Nan; Cui, Yiming; Liu, Ting; Wang, Shijin; Hu, Guoping",,,Is Graph Structure Necessary for Multi-hop Question Answering?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for multi-hop question answering. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for multi-hop question answering. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graph-attention can be considered as a special case of self-attention. Experiments and visualized analysis demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7187,7192,,,,,,,,,,,,,,,,WOS:000855160707030,0
C,"Soler, AG; Apidianaki, M",,,Assoc Computat Linguist,"Soler, Aina Gari; Apidianaki, Marianna",,,"BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7371,7385,,,,,,,,,,,,,,,,WOS:000855160707045,0
C,"Szymanski, P; Gorman, K",,,Assoc Computat Linguist,"Szymanski, Piotr; Gorman, Kyle",,,Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results. We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2203,2212,,,,,,,,,,,,,,,,WOS:000855160702029,0
C,"Wright, D; Augenstein, I",,,Assoc Computat Linguist,"Wright, Dustin; Augenstein, Isabelle",,,Transformer Based Multi-Source Domain Adaptation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple source domains and must make predictions on a domain for which no labelled data has been seen. Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training, to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their performance, suggesting that large transformer-based models are already relatively robust across domains. Additionally, we show that mixture of experts leads to significant performance improvements by comparing several variants of mixing functions, including one novel mixture based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective functions for mixing their predictions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7963,7974,,,,,,,,,,,,,,,,WOS:000855160708014,0
C,"Yang, JF; Yang, DY; Ma, ZR",,,Assoc Computat Linguist,"Yang, Jingfeng; Diyi Yang; Ma, Zhaoran",,,Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies. In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments. We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection. Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus. We have publicly released our code at https://github.com/GT-SALT/Disfluency-Generation-and-Detection.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1450,1460,,,,,,,,,,,,,,,,WOS:000855160701053,0
C,"Zhang, RQ; Zhang, CQ; He, ZJ; Wu, H; Wang, HF",,,Assoc Computat Linguist,"Zhang, Ruiqing; Zhang, Chuanqiang; He, Zhongjun; Wu, Hua; Wang, Haifeng",,,Learning Adaptive Segmentation Policy for Simultaneous Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is essential to segment the ASR output into appropriate units for translation. Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation. The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation. Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2280,2289,,,,,,,,,,,,,,,,WOS:000855160702035,0
C,"Alinejad, A; Sarkar, A",,,Assoc Computat Linguist,"Alinejad, Ashkan; Sarkar, Anoop",,,Effectively pretraining a speech translation decoder with Machine Translation data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8014,8020,,,,,,,,,,,,,,,,WOS:000855160708019,0
C,"Arumae, K; Sun, Q; Bhatia, P",,,Assoc Computat Linguist,"Arumae, Kristjan; Sun, Qing; Bhatia, Parminder",,,An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-training large language models has become a standard in the natural language processing community. Such models are pretrained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0:33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4854,4864,,,,,,,,,,,,,,,,WOS:000855160705005,0
C,"Ballesteros, M; Anubhai, R; Wang, S; Pourdamghani, N; Vyas, Y; Ma, J; Bhatia, P; McKeown, K; Al-Onaizan, Y",,,Assoc Computat Linguist,"Ballesteros, Miguel; Anubhai, Rishita; Wang, Shuai; Pourdamghani, Nima; Vyas, Yogarshi; Ma, Jie; Bhatia, Parminder; McKeown, Kathleen; Al-Onaizan, Yaser",,,Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques. Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5412,5417,,,,,,,,,,,,,,,,WOS:000855160705046,0
C,"Chen, HY; Li, CT",,,Assoc Computat Linguist,"Chen, Hsin-Yu; Li, Cheng-Te",,,HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2543,2552,,,,,,,,,,,,,,,,WOS:000855160702057,0
C,"Chu, C; Fang, S; Knight, K",,,Assoc Computat Linguist,"Chu, Christopher; Fang, Scot; Knight, Kevin",,,Learning to Pronounce Chinese Without a Pronunciation Dictionary,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5687,5693,,,,,,,,,,,,,,,,WOS:000855160705068,0
C,"Chung, HW; Garrette, D; Tan, KC; Riesa, J",,,Assoc Computat Linguist,"Chung, Hyung Won; Garrette, Dan; Tan, Kiat Chuan; Riesa, Jason",,,Improving Multilingual Models with Language-Clustered Vocabularies,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TYDI QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4536,4546,,,,,,,,,,,,,,,,WOS:000855160704055,0
C,"Dinan, E; Fan, A; Wu, L; Weston, J; Kiela, D; Williams, A",,,Assoc Computat Linguist,"Dinan, Emily; Fan, Angela; Wu, Ledell; Weston, Jason; Kiela, Douwe; Williams, Adina",,,Multi-Dimensional Gender Bias Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,314,331,,,,,,,,,,,,,,,,WOS:000855160700023,0
C,"Gonzalez, AV; Barrettl, M; Hvingelby, R; Webster, K; Sogaard, A",,,Assoc Computat Linguist,"Gonzalez, Ana Valeria; Barrettl, Maria; Hvingelby, Rasmus; Webster, Kellie; Sogaard, Anders",,,Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are hallucinatory, e.g., disambiguating genderambiguous occurrences of 'doctor' as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of 'the doctor removed his mask' is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2637,2648,,,,,,,,,,,,,,,,WOS:000855160702066,0
C,"Jen, YH; Huang, CY; Chen, MH; Huang, TH; Ku, LW",,,Assoc Computat Linguist,"Jen, Yun-Hsuan; Huang, Chieh-Yang; Chen, Mei-Hua; Huang, Ting-Hao (Kenneth); Ku, Lun-Wei",,,Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs. little; briefly vs. shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but have difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners' performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent's performance. To enable the agent to behave like a learner, we leverages entailment modeling's capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3807,3817,,,,,,,,,,,,,,,,WOS:000855160703087,0
C,"Khoury, M; Dangovski, R; Ou, LW; Nakov, P; Shen, YC; Jing, L",,,Assoc Computat Linguist,"Khoury, Matthew; Dangovski, Rumen; Ou, Longwu; Nakov, Preslav; Shen, Yichen; Jing, Li",,,Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7975,7984,,,,,,,,,,,,,,,,WOS:000855160708015,0
C,"Kim, H; Kim, B; Kim, G",,,Assoc Computat Linguist,"Kim, Hyunwoo; Kim, Byeongchang; Kim, Gunhee",,,Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,904,916,,,,,,,,,,,,,,,,WOS:000855160701005,0
C,"Kong, L; Li, CY; Ge, JD; Luo, B; Ng, V",,,Assoc Computat Linguist,"Kong, Li; Li, Chuanyi; Ge, Jidong; Luo, Bin; Ng, Vincent",,,Identifying Exaggerated Language,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While exaggeration is one of the most prevalent rhetorical devices, it is arguably one of the least studied in the figurative language processing community. We contribute to the computational study of exaggeration by (1) creating the first Chinese corpus focusing on sentence-level hyperbole detection, with the goal of facilitating a cross-lingual study on this phenomenon, (2) performing a statistical and manual analysis of our corpus, with the goal of gaining insights into the strategies humans employ when creating hyperboles, and (3) addressing the automatic hyperbole detection task with deep learning techniques.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7024,7034,,,,,,,,,,,,,,,,WOS:000855160707020,0
C,"Li, BH; Zhou, H; He, JX; Wang, MX; Yang, YM; Li, L",,,Assoc Computat Linguist,"Li, Bohan; Zhou, Hao; He, Junxian; Wang, Mingxuan; Yang, Yiming; Li, Lei",,,On the Sentence Embeddings from Pre-trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT- flow.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9119,9130,,,,,,,,,,,,,,,,WOS:000855160709028,0
C,"Liu, Y; Zhang, S; Song, R; Feng, S; Xiao, YH",,,Assoc Computat Linguist,"Liu, Ye; Zhang, Sheng; Song, Rui; Feng, Suo; Xiao, Yanghua",,,Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts can be noisy, irrelevant, thus leading to inaccurate answers. Effectively filtering out noisy articles as well as bad answers is the key to improving extraction accuracy. Knowledge graph (KG), which contains rich, well organized information about entities, provides a good resource to address the challenge. In this work, we propose a knowledge-guided reinforcement learning (RL) framework for open attribute value extraction. Informed by relevant knowledge in KG, we trained a deep Q-network to sequentially compare extracted answers to improve extraction accuracy. The proposed framework is applicable to different information extraction system. Our experimental results show that our method outperforms the baselines by 16.5 - 27.8%.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8595,8604,,,,,,,,,,,,,,,,WOS:000855160708068,0
C,"Ma, XY; Sap, M; Rashkin, HN; Choi, YJ",,,Assoc Computat Linguist,"Ma, Xinyao Michelle; Sap, Maarten; Rashkin, Hannah; Choi, Yejin",,,POWERTRANSFORMER: Unsupervised Controllable Revision for Biased Language Correction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (She daydreams about being a doctor) while a man is portrayed as more proactive and powerful (He pursues his dream of being a doctor). We formulate Controllable Debiasing, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce POWERTRANSFORMER as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of POWERTRANSFORMER as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7426,7441,,,,,,,,,,,,,,,,WOS:000855160707049,0
C,"Pappas, N; Mulcaire, P; Smith, NA",,,Assoc Computat Linguist,"Pappas, Nikolaos; Mulcaire, Phoebe; Smith, Noah A.",,,Grounded Compositional Outputs for Adaptive Language Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's vocabulary-typically selected before training and permanently fixed later-affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1252,1267,,,,,,,,,,,,,,,,WOS:000855160701036,0
C,"Qin, JH; Lin, LH; Liang, XD; Zhang, RM; Lin, L",,,Assoc Computat Linguist,"Qin, Jinghui; Lin, Lihui; Liang, Xiaodan; Zhang, Rumin; Lin, Liang",,,Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly. Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation. Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols' semantic meanings like human solving MWPs. Besides, our SAU-Solver also includes a novel subtree-level semantically-aligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information. Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs. Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3780,3789,,,,,,,,,,,,,,,,WOS:000855160703084,0
C,"Ryabinin, M; Popov, S; Prokhorenkova, L; Voita, E",,,Assoc Computat Linguist,"Ryabinin, Max; Popov, Sergei; Prokhorenkova, Liudmila; Voita, Elena",,,Embedding Words in Non-Vector Space with Unsupervised Graph Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce Graph-Glove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7317,7331,,,,,,,,,,,,,,,,WOS:000855160707041,0
C,"Scarlini, B; Pasini, T; Navigli, R",,,Assoc Computat Linguist,"Scarlini, Bianca; Pasini, Tommaso; Navigli, Roberto",,,With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors. ARES representations enable a simple 1-Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only. We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures. ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3528,3539,,,,,,,,,,,,,,,,WOS:000855160703060,0
C,"Shin, T; Razeghi, Y; Logan, RL IV; Wallace, E; Singh, S",,,Assoc Computat Linguist,"Shin, Taylor; Razeghi, Yasaman; Logan, Robert L., IV; Wallace, Eric; Singh, Sameer",,,AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AUTOPROMPT, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTOPROMPT, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4222,4235,,,,,,,,,,,,,,,,WOS:000855160704034,0
C,"Shoeb, AM; de Melo, G",,,Assoc Computat Linguist,"Shoeb, Abu Awal Md; de Melo, Gerard",,,EmoTag1200: Understanding the Association between Emojis and Emotions,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore the connection between emojis and emotions by means of a new dataset consisting of human-solicited association ratings. We additionally conduct experiments to assess to what extent such associations can be inferred from existing data in an unsupervised manner. Our experiments show that this succeeds when high-quality word-level information is available.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8957,8967,,,,,,,,,,,,,,,,WOS:000855160709015,0
C,"Swayamdipta, S; Schwartz, R; Lourie, N; Wang, YZ; Hajishirzi, H; Smith, NA; Choi, Y",,,Assoc Computat Linguist,"Swayamdipta, Swabha; Schwartz, Roy; Lourie, Nicholas; Wang, Yizhong; Hajishirzi, Hannaneh; Smith, Noah A.; Choi, Yejin",,,Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps-a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example-the model's confidence in the true class, and the variability of this confidence across epochs-obtained in a single run of training Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ambiguous regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are easy to learn for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds hard to learn; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9275,9293,,,,,,,,,,,,,,,,WOS:000855160709041,0
C,"Vilares, D; Gomez-Rodriguez, C",,,Assoc Computat Linguist,"Vilares, David; Gomez-Rodriguez, Carlos",,,Discontinuous Constituent Parsing as Sequence Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2771,2785,,,,,,,,,,,,,,,,WOS:000855160702078,0
C,"Vu, T; Wang, T; Munkhdalai, T; Sordoni, A; Trischler, A; Mattarella-Micke, A; Maji, S; Iyyer, M",,,Assoc Computat Linguist,"Vu, Tu; Wang, Tong; Munkhdalai, Tsendsuren; Sordoni, Alessandro; Trischler, Adam; Mattarella-Micke, Andrew; Maji, Subhransu; Iyyer, Mohit",,,Exploring and Predicting Transferability across NLP Tasks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even with low-data source tasks that differ substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as data size, task and domain similarity, and task complexity all play a role in determining transferability.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7882,7926,,,,,,,,,,,,,,,,WOS:000855160708010,0
C,"Wallace, E; Stern, M; Song, D",,,Assoc Computat Linguist,"Wallace, Eric; Stern, Mitchell; Song, Dawn",,,Imitation Attacks and Defenses for Black-box Machine Translation Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semanticallyincorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5531,5546,,,,,,,,,,,,,,,,WOS:000855160705056,0
C,"Wang, J; Lu, W",,,Assoc Computat Linguist,"Wang, Jue; Lu, Wei",,,Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders - a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1706,1721,,,,,,,,,,,,,,,,WOS:000855160701073,0
C,"Wang, Z; Wohlwend, J; Lei, T",,,Assoc Computat Linguist,"Wang, Ziheng; Wohlwend, Jeremy; Lei, Tao",,,Structured Pruning of Large Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6151,6162,,,,,,,,,,,,,,,,WOS:000855160706029,0
C,"Welch, C; Kummerfeld, JK; Perez-Rosas, V; Mihalcea, R",,,Assoc Computat Linguist,"Welch, Charles; Kummerfeld, Jonathan K.; Perez-Rosas, Veronica; Mihalcea, Rada",,,Compositional Demographic Word Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4076,4089,,,,,,,,,,,,,,,,WOS:000855160704022,0
C,"Yang, SQ; Zhang, R; Erfani, S",,,Assoc Computat Linguist,"Yang, Shiquan; Zhang, Rui; Erfani, Sarah",,,GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1878,1888,,,,,,,,,,,,,,,,WOS:000855160702004,0
C,"Zhang, H; Chen, M; Wang, H; Song, Y; Roth, D",,,Assoc Computat Linguist,"Zhang, Hongming; Chen, Muhao; Wang, Haoyu; Song, Yangqiu; Dan Roth",,,Analogous Process Structure Induction for Sub-event Sequence Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a known process such as buying a car can be used in the context of a new but analogous process such as buying a house. Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction. In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes. As our experiments and analysis indicate, APSI(1) supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1541,1550,,,,,,,,,,,,,,,,WOS:000855160701059,0
C,"Zhang, YC; Ou, ZJ; Hu, M; Feng, JL",,,Assoc Computat Linguist,"Zhang, Yichi; Ou, Zhijian; Hu, Min; Feng, Junlan",,,A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES(1). In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABESS2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9207,9219,,,,,,,,,,,,,,,,WOS:000855160709035,0
C,"Zhong, WJ; Tang, DY; Xu, ZN; Wang, RZ; Duan, N; Zhou, M; Wang, JH; Yin, J",,,Assoc Computat Linguist,"Zhong, Wanjun; Tang, Duyu; Xu, Zenan; Wang, Ruize; Nan Duan; Ming Zhou; Wang, Jiahai; Jian Yin",,,Neural Deepfake Detection with Factual Structure of Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they struggle to capture factual structures of documents, which is a discriminative factor between machinegenerated and human-written text according to our statistical analysis. To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text. Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network. Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled. Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa. Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and humanwritten text.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2461,2470,,,,,,,,,,,,,,,,WOS:000855160702050,0
C,"Zhou, X; Nie, Y; Tan, H; Bansal, M",,,Assoc Computat Linguist,"Zhou, Xiang; Nie, Yixin; Tan, Hao; Bansal, Mohit",,,"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8215,8228,,,,,,,,,,,,,,,,WOS:000855160708034,0
C,"Akama, R; Yokoi, S; Suzuki, J; Inui, K",,,Assoc Computat Linguist,"Akama, Reina; Yokoi, Sho; Suzuki, Jun; Inui, Kentaro",,,Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,941,958,,,,,,,,,,,,,,,,WOS:000855160701008,0
C,"Banerjee, P; Baral, C",,,Assoc Computat Linguist,"Banerjee, Pratyay; Baral, Chitta",,,Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,151,162,,,,,,,,,,,,,,,,WOS:000855160700011,0
C,"Bevilacqua, M; Maru, M; Navigli, R",,,Assoc Computat Linguist,"Bevilacqua, Michele; Maru, Marco; Navigli, Roberto",,,Generationary or: How We Went beyondWord Sense Inventories and Learned to Gloss,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses. We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases. The software and reproduction materials are available at http://generationary.org.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7207,7221,,,,,,,,,,,,,,,,WOS:000855160707032,0
C,"Bhandari, M; Gour, P; Ashfaq, A; Liu, PF; Neubig, G",,,Assoc Computat Linguist,"Bhandari, Manik; Gour, Pranav; Ashfaq, Atabak; Liu, Pengfei; Neubig, Graham",,,Re-evaluating Evaluation in Text Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not - for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both systemlevel and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9347,9359,,,,,,,,,,,,,,,,WOS:000855160709046,0
C,"Dazat, A; Frank, A",,,Assoc Computat Linguist,"Dazat, Angel; Frank, Anette",,,X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning. In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages. We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages. We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline. Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3904,3914,,,,,,,,,,,,,,,,WOS:000855160704009,0
C,"Dong, J; Rondeau, MA; Hamilton, WL",,,Assoc Computat Linguist,"Dong, Jin; Rondeau, Marc Antoine; Hamilton, William L.",,,Distilling Structured Knowledge for Text-Based Relational Reasoning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text. In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance. We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation. To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN. We test our approach with two state-of-the-art NLP models on 12 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6782,6791,,,,,,,,,,,,,,,,WOS:000855160706084,0
C,"Dos Santos, CN; Ma, XF; Nallapati, R; Huang, ZH; Xiang, B",,,Assoc Computat Linguist,"Dos Santos, Cicero Nogueira; Ma, Xiaofei; Nallapati, Ramesh; Huang, Zhiheng; Xiang, Bing",,,Beyond [CLS] through Ranking by Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1722,1727,,,,,,,,,,,,,,,,WOS:000855160701074,0
C,"Dugan, L; Ippolito, D; Kirubarajan, A; Callison-Burch, C",,,Assoc Computat Linguist,"Dugan, Liam; Ippolito, Daphne; Kirubarajan, Arun; Callison-Burch, Chris",,,RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present Real or Fake Text (RoFT), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,189,196,,,,,,,,,,,,,,,,WOS:000855177700025,0
C,"Einolghozati, A; Gupta, A; Diedrick, K; Gupta, S",,,Assoc Computat Linguist,"Einolghozati, Arash; Gupta, Anchit; Diedrick, Keith; Gupta, Sonal",,,Sound Natural: Content Rephrasing in Dialog Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intentslot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user. For example, for queries like `ask my wife if she can pick up the kids' or `remind me to take my pills', we need to rephrase the content to `can you pick up the kids' and `take your pills'. In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query. We show that BART, a pre-trained transformers-based masked language model with auto-regressive decoding, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it. We analyze different tradeoffs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5101,5108,,,,,,,,,,,,,,,,WOS:000855160705024,0
C,"Fu, JL; Liu, PF; Zhang, Q; Huang, XJ",,,Assoc Computat Linguist,"Fu, Jinlan; Liu, Pengfei; Zhang, Qi; Huang, Xuanjing",,,RethinkCWS: Is Chinese Word Segmentation a Solved Task?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what's left in the CWS task. Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning. Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research. We make all codes publicly available and release an interface that can quickly evaluate and diagnose user's models: https://github.com/neulab/InterpretEval.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5676,5686,,,,,,,,,,,,,,,,WOS:000855160705067,0
C,"Gokhale, T; Banerjee, P; Baral, C; Yang, YZ",,,Assoc Computat Linguist,"Gokhale, Tejas; Banerjee, Pratyay; Baral, Chitta; Yang, Yezhou",,,MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,878,892,,,,,,,,,,,,,,,,WOS:000855160701003,0
C,"Hahn, M; Krantz, J; Batra, D; Parikh, D; Rehg, JM; Lee, S; Anderson, P",,,Assoc Computat Linguist,"Hahn, Meera; Krantz, Jacob; Batra, Dhruv; Parikh, Devi; Rehg, James M.; Lee, Stefan; Anderson, Peter",,,Where Are You? Localization from Embodied Dialog,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present WHERE ARE YOU? (WAY), a dataset of similar to 6k dialogs in which two humans - an Observer and a Locator - complete a co-operative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task - providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7% success at identifying the Observer's location within 3m in unseen buildings, vs. 70.4% for human Locators.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,806,822,,,,,,,,,,,,,,,,WOS:000855160700059,0
C,"Jwalapuram, P; Joty, S; Shen, Y",,,Assoc Computat Linguist,"Jwalapuram, Prathyusha; Joty, Shafiq; Shen, Youlin",,,Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2267,2279,,,,,,,,,,,,,,,,WOS:000855160702034,0
C,"Khayrallah, H; Thompson, B; Post, M; Koehn, P",,,Assoc Computat Linguist,"Khayrallah, Huda; Thompson, Brian; Post, Matt; Koehn, Philipp",,,Simulated Multiple Reference Training Improves Low-Resource Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser's distribution over possible tokens. We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We also find SMRT is complementary to back-translation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,82,89,,,,,,,,,,,,,,,,WOS:000855160700007,0
C,"Kohita, R; Wachi, A; Zhao, Y; Tachibana, R",,,Assoc Computat Linguist,"Kohita, Ryosuke; Wachi, Akifumi; Zhao, Yang; Tachibana, Ryuki",,,Q-learning with Language Model for Edit-based Unsupervised Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on Q-learning with an edit-based summarization. The method combines two key modules to form an Editorial Agent and Language Model converter (EALM). The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals. Q-learning is leveraged to train the agent to produce proper edit actions. Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set). Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization. We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,470,484,,,,,,,,,,,,,,,,WOS:000855160700034,0
C,"Li, JL; Durmus, E; Cardie, C",,,Assoc Computat Linguist,"Li, Jialu; Durmus, Esin; Cardie, Claire",,,Exploring the Role of Argument Structure in Online Debate Persuasion,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding the characteristics of the audience are both critical in persuasion studies. In this paper, we aim to further investigate the role of discourse structure of the arguments from online debates in their persuasiveness. In particular, we use the factor graph model to obtain features for the argument structure of debates from an online debating platform and incorporate these features to an LSTM-based model to predict the debater that makes the most convincing arguments. We find that incorporating argument structure features play an essential role in achieving the better predictive performance in assessing the persuasiveness of the arguments in online debates.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8905,8912,,,,,,,,,,,,,,,,WOS:000855160709011,0
C,"Li, JQ; Li, CY; Wang, GY; Fu, H; Lin, YC; Chen, LQ; Zhang, YZ; Tao, CY; Zhang, RY; Wang, WL; Shen, DH; Yang, Q; Carin, L",,,Assoc Computat Linguist,"Li, Jianqiao; Li, Chunyuan; Wang, Guoyin; Fu, Hao; Lin, Yuh-Chen; Chen, Liqun; Zhang, Yizhe; Tao, Chenyang; Zhang, Ruiyi; Wang, Wenlin; Shen, Dinghan; Yang, Qian; Carin, Lawrence",,,Improving Text Generation with Student-Forcing Optimal Transport,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. An extension is further proposed to improve the OT learning, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9144,9156,,,,,,,,,,,,,,,,WOS:000855160709030,0
C,"Liu, HC; Wang, WT; Wang, YQ; Liu, H; Liu, ZT; Tang, JL",,,Assoc Computat Linguist,"Liu, Haochen; Wang, Wentao; Wang, Yiqi; Liu, Hui; Liu, Zitao; Tang, Jiliang",,,Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality. The implementation of the proposed framework is released(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,893,903,,,,,,,,,,,,,,,,WOS:000855160701004,0
C,"Ma, XT; Dousti, MJ; Wang, CH; Gu, JT; Pino, J",,,Assoc Computat Linguist,"Ma, Xutai; Dousti, Mohammad Javad; Wang, Changhan; Gu, Jiatao; Pino, Juan",,,SIMULEVAL : An Evaluation Toolkit for Simultaneous Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SIMULEVAL, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SIMULEVAL is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SIMULEVAL has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,144,150,,,,,,,,,,,,,,,,WOS:000855177700019,0
C,"Martinez, VR; Somandepalli, K; Uhls, YT; Narayanan, S",,,Assoc Computat Linguist,"Martinez, Victor R.; Somandepalli, Krishna; Uhls, Yalda T.; Narayanan, Shrikanth",,,Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive. To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production. Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character's language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach. Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that film-makers may otherwise not pick up on.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4780,4790,,,,,,,,,,,,,,,,WOS:000855160704075,0
C,"Michael, J; Botha, JA; Tenney, I",,,Assoc Computat Linguist,"Michael, Julian; Botha, Jan A.; Tenney, Ian",,,Asking without Telling: Exploring Latent Ontologies in Contextual Representations,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6792,6812,,,,,,,,,,,,,,,,WOS:000855160707001,0
C,"Pfeiffer, J; Vulic, I; Gurevych, I; Ruder, S",,,Assoc Computat Linguist,"Pfeiffer, Jonas; Vulic, Ivan; Gurevych, Iryna; Ruder, Sebastian",,,MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot crosslingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal common-sense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7654,7673,,,,,,,,,,,,,,,,WOS:000855160707064,0
C,"Puri, R; Spring, R; Shoeybi, M; Patwary, M; Catanzaro, B",,,Assoc Computat Linguist,"Puri, Raul; Spring, Ryan; Shoeybi, Mohammad; Patwary, Mostofa; Catanzaro, Bryan",,,Training Question Answering Models From Synthetic Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQUAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQUAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic text corpus generated by an 8.3 billion parameter GPT-2 model and achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQUAD1.1 dev set. We further apply our methodology to SQUAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5811,5826,,,,,,,,,,,,,,,,WOS:000855160706001,0
C,"Raganato, A; Pasini, T; Camacho-Collados, J; Pilehvar, MT",,,Assoc Computat Linguist,"Raganato, Alessandro; Pasini, Tommaso; Camacho-Collados, Jose; Pilehvar, Mohammad Taher",,,XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at https://pilehvar.github.io/xlwic/.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7193,7206,,,,,,,,,,,,,,,,WOS:000855160707031,0
C,"Shakeri, S; Dos Santos, CN; Zhu, H; Ng, P; Nan, F; Wang, ZG; Nallapati, R; Xiang, B",,,Assoc Computat Linguist,"Shakeri, Siamak; Dos Santos, Cicero Nogueira; Zhu, Henry; Ng, Patrick; Nan, Feng; Wang, Zhiguo; Nallapati, Ramesh; Xiang, Bing",,,End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoderdecoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token. The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5445,5460,,,,,,,,,,,,,,,,WOS:000855160705049,0
C,"Shen, T; Quach, V; Barzilay, R; Jaakkola, T",,,Assoc Computat Linguist,"Shen, Tianxiao; Quach, Victor; Barzilay, Regina; Jaakkola, Tommi",,,Blank Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5186,5198,,,,,,,,,,,,,,,,WOS:000855160705030,0
C,"Singh, H; Shekhar, S",,,Assoc Computat Linguist,"Singh, Hrituraj; Shekhar, Sumit",,,STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3275,3284,,,,,,,,,,,,,,,,WOS:000855160703039,0
C,"Sogaard, A",,,Assoc Computat Linguist,"Sogaard, Anders",,,Some Languages Seem Easier to Parse Because Their Treebanks Leak,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-language differences in dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. In this paper I point to a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data? I discuss how to compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explains more of the observed variation than standard explanations such as the above.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2765,2770,,,,,,,,,,,,,,,,WOS:000855160702077,0
C,"Sohrab, MG; Duong, KNA; Miwa, M; Topic, G; Ikeda, M; Takamura, H",,,Assoc Computat Linguist,"Sohrab, Mohammad Golam; Duong, Khoa N. A.; Miwa, Makoto; Topic, Goran; Ikeda, Masami; Takamura, Hiroya",,,BENNERD: A Neural Named Entity Linking System for COVID-19,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present a biomedical entity linking (EL) system BENNERD that detects named entities in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BENNERD mainly covers biomedical domain, especially new entity types (e.g., coronavirus, viral proteins, immune responses) by addressing CORD-NER dataset. It includes several NLP tools to process biomedical texts including tokenization, flat and nested entity recognition, and candidate generation and ranking for EL that have been pre-trained using the CORD-NER corpus. To the best of our knowledge, this is the first attempt that addresses NER and EL on COVID-19-related entities, such as COVID-19 virus, potential vaccines, and spreading mechanism, that may benefit research on COVID-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and CORD-NERD dataset for leveraging EL task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,182,188,,,,,,,,,,,,,,,,WOS:000855177700024,0
C,"Staliunaite, I; Iacobacci, I",,,Assoc Computat Linguist,"Staliunaite, Ieva; Iacobacci, Ignacio",,,"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7046,7056,,,,,,,,,,,,,,,,WOS:000855160707022,0
C,"Vu, TT; Phung, D; Haffari, G",,,Assoc Computat Linguist,"Vu, Thuy-Trang; Phung, Dinh; Haffari, Gholamreza",,,Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lower-bound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6163,6173,,,,,,,,,,,,,,,,WOS:000855160706030,0
C,"Wang, KX; Chang, BB; Sui, ZF",,,Assoc Computat Linguist,"Wang, Kexiang; Chang, Baobao; Sui, Zhifang",,,A Spectral Method for Unsupervised Multi-Document Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,435,445,,,,,,,,,,,,,,,,WOS:000855160700032,0
C,"Wang, LH; Zheng, XQ",,,Assoc Computat Linguist,"Wang, Lihao; Zheng, Xiaoqing",,,Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand. We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set. Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2858,2869,,,,,,,,,,,,,,,,WOS:000855160703003,0
C,"Wang, Y; Joty, S; Lyu, MR; King, I; Xiong, C; Hoi, SCH",,,Assoc Computat Linguist,"Wang, Yue; Joty, Shafiq; Lyu, Michael R.; King, Irwin; Xiong, Caiming; Hoi, Steven C. H.",,,VD-BERT: A Unified Vision and Dialog Transformer with BERT,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pre-training on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3325,3338,,,,,,,,,,,,,,,,WOS:000855160703044,0
C,"Wang, ZC; Yang, JJ; Ye, XJ",,,Assoc Computat Linguist,"Wang, Zhichun; Yang, Jinjian; Ye, Xiaoju",,,Knowledge Graph Alignment with Entity-Pair Embedding,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations. Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory. In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment. Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities. To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs. Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1672,1680,,,,,,,,,,,,,,,,WOS:000855160701070,0
C,"Weir, N; Sedoc, J; Van Durme, B",,,Assoc Computat Linguist,"Weir, Nathaniel; Sedoc, Joao; Van Durme, Benjamin",,,COD3S: Diverse Generation with Discrete Semantic Signatures,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seq models typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks. Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity. Though it is generally applicable, we apply COD3S to causal generation, the task of predicting a proposition's plausible causes or effects. We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5199,5211,,,,,,,,,,,,,,,,WOS:000855160705031,0
C,"Wu, L; Petroni, F; Josifoski, M; Riedel, S; Zettlemoyer, L",,,Assoc Computat Linguist,"Wu, Ledell; Petroni, Fabio; Josifoski, Martin; Riedel, Sebastian; Zettlemoyer, Luke",,,Scalable Zero-shot Entity Linking with Dense Entity Retrieval,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a crossencoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zeroshot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive crossencoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github. com/facebookresearch/BLINK.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6397,6407,,,,,,,,,,,,,,,,WOS:000855160706052,0
C,"Wu, X; Cai, Y; Li, Q; Wang, T; Yang, K",,,Assoc Computat Linguist,"Wu, Xin; Cai, Yi; Li, Qing; Wang, Tao; Yang, Kai",,,Task-oriented Domain-specific Meta-Embedding for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains. Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem. In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings. We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3508,3513,,,,,,,,,,,,,,,,WOS:000855160703057,0
C,"Zhang, HP; Zhang, JW",,,Assoc Computat Linguist,"Zhang, Haopeng; Zhang, Jiawei",,,Text Graph Transformer for Document Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8322,8327,,,,,,,,,,,,,,,,WOS:000855160708043,0
C,"Zhang, YZ; Wang, GY; Li, CY; Gan, Z; Brockett, C; Dolan, B",,,Assoc Computat Linguist,"Zhang, Yizhe; Wang, Guoyin; Li, Chunyuan; Gan, Zhe; Brockett, Chris; Dolan, Bill",,,POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER', a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and finetune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields an empirically logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that POINTER achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research(2).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8649,8670,,,,,,,,,,,,,,,,WOS:000855160708073,0
C,"Bak, J; Oh, A",,,Assoc Computat Linguist,"Bak, JinYeong; Oh, Alice",,,Variational Hierarchical User-based Conversation Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating appropriate conversation responses requires careful modeling of the utterances and speakers together. Some recent approaches to response generation model both the utterances and the speakers, but these approaches tend to generate responses that are overly tailored to the speakers. To overcome this limitation, we propose a new model with a stochastic variable designed to capture the speaker information and deliver it to the conversational context. An important part of this model is the network of speakers in which each speaker is connected to one or more conversational partner, and this network is then used to model the speakers better. To test whether our model generates more appropriate conversation responses, we build a new conversation corpus containing approximately 27,000 speakers and 770,000 conversations. With this corpus, we run experiments of generating conversational responses and compare our model with other state-of-the-art models. By automatic evaluation metrics and human evaluation, we show that our model outperforms other models in generating appropriate responses. An additional advantage of our model is that it generates better responses for various new user scenarios, for example when one of the speakers is a known user in our corpus but the partner is a new user. For replicability, we make available all our code and data(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1941,1950,,,,,,,,,,,,,,,,WOS:000854193302010,0
C,"Bi, TC; Xiong, H; He, ZJ; Wu, H; Wang, HF",,,Assoc Computat Linguist,"Bi, Tianchi; Xiong, Hao; He, Zhongjun; Wu, Hua; Wang, Haifeng",,,Multi-agent Learning for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Conventional Neural Machine Translation (NMT) models benefit from the training with an additional agent, e.g., dual learning, and bidirectional decoding with one agent decoding from left to right and the other decoding in the opposite direction. In this paper, we extend the training framework to the multi-agent scenario by introducing diverse agents in an interactive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German-English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline systems and shows competitive performance on all tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,856,865,,,,,,,,,,,,,,,,WOS:000854193301001,0
C,"Cai, R; Lapata, M",,,Assoc Computat Linguist,"Cai, Rui; Lapata, Mirella",,,Semi-Supervised Semantic Role Labeling with Cross-View Training,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The successful application of neural networks to a variety of NLP tasks has provided strong impetus to develop end-to-end models for semantic role labeling which forego the need for extensive feature engineering. Recent approaches rely on high-quality annotations which are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains). Our work aims to reduce the annotation effort involved via semi-supervised learning. We propose an end-toend SRL model and demonstrate it can effectively leverage unlabeled data under the crossview training modeling paradigm. Our LSTM-based semantic role labeler is jointly trained with a sentence learner, which performs POS tagging, dependency parsing, and predicate identification which we argue are critical to learning directly from unlabeled data without recourse to external pre-processing tools. Experimental results on the CoNLL-2009 benchmark dataset show that our model outperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1018,1027,,,,,,,,,,,,,,,,WOS:000854193301016,0
C,"Chen, MY; Zhang, W; Zhang, W; Chen, Q; Chen, HJ",,,Assoc Computat Linguist,"Chen, Mingyang; Zhang, Wen; Zhang, Wei; Chen, Qiang; Chen, Huajun",,,Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Link prediction is an important way to complete knowledge graphs (KGs), while embedding-based methods, effective for link prediction in KGs, perform poorly on relations that only have a few associative triples. In this work, we propose a Meta Relational Learning (MetaR) framework to do the common but challenging few-shot link prediction in KGs, namely predicting new triples about a relation by only observing a few associative triples. We solve few-shot link prediction by focusing on transferring relation-specific meta information to make model learn the most important knowledge and learn faster, corresponding to relation meta and gradientmeta respectively in MetaR. Empirically, our model achieves state-of-the-art results on few-shot link prediction KG benchmarks.",,,,,"chen, qiang/HGU-5418-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4217,4226,,,,,,,,,,,,,,,,WOS:000854193304036,0
C,"Clark, C; Yatskar, M; Zettlemoyer, L",,,Assoc Computat Linguist,"Clark, Christopher; Yatskar, Mark; Zettlemoyer, Luke",,,Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4069,4082,,,,,,,,,,,,,,,,WOS:000854193304023,0
C,"Hale, JT; Kuncoro, A; Hall, KB; Dyer, C; Brennan, JR",,,Assoc Computat Linguist,"Hale, John T.; Kuncoro, Adhiguna; Hall, Keith B.; Dyer, Chris; Brennan, Jonathan R.",,,Text Genre and Training Data Size in Human-Like Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Domain-specific training typically makes NLP systems work better. We show that this extends to cognitive modeling as well by relating the states of a neural phrase-structure parser to electrophysiological measures from human participants. These measures were recorded as participants listened to a spoken recitation of the same literary text that was supplied as input to the neural parser. Given more training data, the system derives a better cognitive model - but only when the training examples come from the same textual genre. This finding is consistent with the idea that humans adapt syntactic expectations to particular genres during language comprehension (Kaan and Chun, 2018; Branigan and Pickering, 2017).",,,,,,"Brennan, Jonathan R/0000-0002-3639-350X",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5846,5852,,,,,,,,,,,,,,,,WOS:000854193306005,0
C,"Han, XD; Schulz, P; Cohn, T",,,Assoc Computat Linguist,"Han, Xudong; Schulz, Philip; Cohn, Trevor",,,Grounding learning of modifier dynamics: An application to color naming,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Grounding is crucial for natural language understanding. An important subtask is to understand modified color expressions, such as dirty blue. We present a model of color modifiers that, compared with previous additive models in RGB space, learns more complex transformations. In addition, we present a model that operates in the HSV color space. We show that certain adjectives are better modeled in that space. To account for all modifiers, we train a hard ensemble model that selects a color space depending on the modifiercolor pair. Experimental results show significant and consistent improvements compared to the state-of-the-art baseline model.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1488,1493,,,,,,,,,,,,,,,,WOS:000854193301080,0
C,"Hsu, TY; Liu, CL; Lee, HY",,,Assoc Computat Linguist,"Hsu, Tsung-Yuan; Liu, Chi-liang; Lee, Hung-yi",,,Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Because it is not feasible to collect training data for every language, there is a growing interest in cross-lingual transfer learning. In this paper, we systematically explore zeroshot cross-lingual transfer learning on reading comprehension tasks with a language representation model pre-trained on multi-lingual corpus. The experimental results show that with pre-trained language representation zeroshot learning is feasible, and translating the source data into the target language is not necessary and even degrades the performance. We further explore what does the model learn in zero-shot setting(0).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5933,5940,,,,,,,,,,,,,,,,WOS:000854193306018,0
C,"Huang, XL; May, J; Peng, NY",,,Assoc Computat Linguist,"Huang, Xiaolei; May, Jonathan; Peng, Nanyun",,,What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages to low-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our model achieves competitive performance with the state-of-the-art. We further analyze how transfer learning works for cross-lingual NER on two transferable factors: sequential order and multilingual embeddings, and investigate how model performance varies across entity lengths. Finally, we conduct a case-study on a non-Latin language, Bengali, which suggests that leveraging knowledge from Wikipedia will be a promising direction to further improve the model performances. Our results can shed light on future research for improving cross-lingual NER.",,,,,,"Huang, Xiaolei/0000-0003-0478-8715",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6395,6401,,,,,,,,,,,,,,,,WOS:000854193306083,0
C,"Huang, XC; Liu, Y; Luan, HB; Xu, JF; Sun, MS",,,Assoc Computat Linguist,"Huang, Xuancheng; Liu, Yang; Luan, Huanbo; Xu, Jingfang; Sun, Maosong",,,Learning to Copy for Automatic Post-Editing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatic post-editing (APE), which aims to correct errors in the output of machine translation systems in a post-processing step, is an important task in natural language processing. While recent work has achieved considerable performance gains by using neural networks, how to model the copying mechanism for APE remains a challenge. In this work, we propose a new method for modeling copying for APE. To better identify translation errors, our method learns the representations of source sentences and system outputs in an interactive way. These representations are used to explicitly indicate which words in the system outputs should be copied, which is useful to help CopyNet (Gu et al., 2016) better generate post-edited translations. Experiments on the datasets of the WMT 2016-2017 APE shared tasks show that our approach outperforms all best published results. (1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6122,6132,,,,,,,,,,,,,,,,WOS:000854193306045,0
C,"Jiang, M; Hu, JJ; Huang, QY; Zhang, L; Diesner, J; Gao, JF",,,Assoc Computat Linguist,"Jiang, Ming; Hu, Junjie; Huang, Qiuyuan; Zhang, Lei; Diesner, Jana; Gao, Jianfeng",,,"REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system's overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1475,1480,,,,,,,,,,,,,,,,WOS:000854193301078,0
C,"Jung, T; Kang, D; Mentch, L; Hovy, E",,,Assoc Computat Linguist,"Jung, Taehee; Kang, Dongyeop; Mentch, Lucas; Hovy, Eduard",,,Earlier Isn't Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that di fferent types of summarization systems (e.g., neural-based) are composed of di fferent degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3324,3335,,,,,,,,,,,,,,,,WOS:000854193303045,0
C,"Kim, DJ; Choi, J; Oh, TH; Kweon, IS",,,Assoc Computat Linguist,"Kim, Dong-Jin; Choi, Jinsoo; Oh, Tae-Hyun; Kweon, In So",,,Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Constructing an organized dataset comprised of a large number of images and several captions for each image is a laborious task, which requires vast human effort. On the other hand, collecting a large number of images and sentences separately may be immensely easier. In this paper, we develop a novel dataefficient semi-supervised framework for training an image captioning model. We leverage massive unpaired image and caption data by learning to associate them. To this end, our proposed semi-supervised learning method assigns pseudo-labels to unpaired samples via Generative Adversarial Networks to learn the joint distribution of image and caption. To evaluate, we construct scarcely-paired COCO dataset, a modified version of MS COCO caption dataset. The empirical results show the effectiveness of our method compared to several strong baselines, especially when the amount of the paired samples are scarce.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2012,2023,,,,,,,,,,,,,,,,WOS:000854193302016,0
C,"Kryscinski, W; Keskar, NS; McCann, B; Xiong, CM; Socher, R",,,Assoc Computat Linguist,"Kryscinski, Wojciech; Keskar, Nitish Shirish; McCann, Bryan; Xiong, Caiming; Socher, Richard",,,Neural Text Summarization: A Critical Evaluation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,540,551,,,,,,,,,,,,,,,,WOS:000854193300051,0
C,"Li, JJ; Wang, XP; Yin, DW; Zong, CQ",,,Assoc Computat Linguist,"Li, Junjie; Wang, Xuepeng; Yin, Dawei; Zong, Chengqing",,,Attribute-aware Sequence Network for Review Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Review summarization aims to generate a condensed summary for a review or multiple reviews. Existing review summarization systems mainly generate summary only based on review content and neglect the authors' attributes (e.g., gender, age, and occupation). In fact, when summarizing a review, users with different attributes usually pay attention to specific aspects and have their own word-using habits or writing styles. Therefore, we propose an Attribute-aware Sequence Network (ASN) to take the aforementioned users' characteristics into account, which includes three modules: an attribute encoder encodes the attribute preferences over the words; an attribute-aware review encoder adopts an attribute-based selective mechanism to select the important information of a review; and an attribute-aware summary decoder incorporates attribute embedding and attribute-specific word-using habits into word prediction. To validate our model, we collect a new dataset TripAtt, comprising 495,440 attribute-review-summary triplets with three kinds of attribute information: gender, age, and travel status. Extensive experiments show that ASN achieves state-of-the-art performance on review summarization in both auto-metric ROUGE and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3000,3010,,,,,,,,,,,,,,,,WOS:000854193303015,0
C,"Liu, C; Liu, K; He, SZ; Nie, ZQ; Zhao, J",,,Assoc Computat Linguist,"Liu, Cao; Liu, Kang; He, Shizhu; Nie, Zaiqing; Zhao, Jun",,,Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We tackle the task of question generation over knowledge bases. Conventional methods for this task neglect two crucial research issues: 1) the given predicate needs to be expressed; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our model achieves state-of-the-art performance. Meanwhile, such generated question can express the given predicate and correspond to a definitive answer.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2431,2441,,,,,,,,,,,,,,,,WOS:000854193302055,0
C,"Parikh, P; Abburi, H; Badjatiya, P; Krishnan, R; Chhaya, N; Gupta, M; Varma, V",,,Assoc Computat Linguist,"Parikh, Pulkit; Abburi, Harika; Badjatiya, Pinkesh; Krishnan, Radhika; Chhaya, Niyati; Gupta, Manish; Varma, Vasudeva",,,Multi-label Categorization of Accounts of Sexism using a Neural Framework,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in utilizing such data to study and counter sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as BERT with distributional and linguistic word embeddings using a flexible, hierarchical architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method outperforms several deep learning as well as traditional machine learning baselines by an appreciable margin.",,,,,"gupta, manish/HIK-2539-2022","Krishnan, Radhika/0000-0001-6225-1989",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1642,1652,,,,,,,,,,,,,,,,WOS:000854193301096,0
C,"Popat, K; Mukherjee, S; Yates, A; Weikum, G",,,Assoc Computat Linguist,"Popat, Kashyap; Mukherjee, Subhabrata; Yates, Andrew; Weikum, Gerhard",,,STANCY: Stance Classification Based on Consistency Cues,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Controversial claims are abundant in online media and discussion forums. A better understanding of such claims requires analyzing them from different perspectives. Stance classification is a necessary step for inferring these perspectives in terms of supporting or opposing the claim. In this work, we present a neural network model for stance classification leveraging BERT representations and augmenting them with a novel consistency constraint. Experiments on the Perspectrum dataset, consisting of claims and users' perspectives from various debate websites, demonstrate the effectiveness of our approach over state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6413,6418,,,,,,,,,,,,,,,,WOS:000854193306086,0
C,"Reimers, N; Gurevych, I",,,Assoc Computat Linguist,"Reimers, Nils; Gurevych, Iryna",,,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (similar to 65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3982,3992,,,,,,,,,,,,,,,,WOS:000854193304015,0
C,"Rosenthal, S; Barker, K; Liang, JZ",,,Assoc Computat Linguist,"Rosenthal, Sara; Barker, Ken; Liang, Jason Zhicheng",,,Leveraging Medical Literature for Section Prediction in Electronic Health Records,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Electronic Health Records (EHRs) contain both structured content and unstructured (text) content about a patient's medical history. In the unstructured text parts, there are common sections such as Assessment and Plan, Social History, and Medications. These sections help physicians find information easily and can be used by an information retrieval system to return specific information sought by a user. However, it is common that the exact format of sections in a particular EHR does not adhere to known patterns. Therefore, being able to predict sections and headers in EHRs automatically is beneficial to physicians. Prior approaches in EHR section prediction have only used text data from EHRs and have required significant manual annotation. We propose using sections from medical literature (e.g., textbooks, journals, web content) that contain content similar to that found in EHR sections. Our approach uses data from a different kind of source where labels are provided without the need of a time-consuming annotation effort. We use this data to train two models: an RNN and a BERTbased model. We apply the learned models along with source data via transfer learning to predict sections in EHRs. Our results show that medical literature can provide helpful supervision signal for this classification task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4864,4873,,,,,,,,,,,,,,,,WOS:000854193305005,0
C,"Song, YF; Jiang, D; Zhao, WW; Xu, Q; Wong, RCW; Yang, Q",,,ASSOC COMPUTAT LINGUIST,"Song, Yuanfeng; Jiang, Di; Zhao, Weiwei; Xu, Qian; Wong, Raymond Chi-Wing; Yang, Qiang",,,Chameleon: A Language Model Adaptation Toolkit for Automatic Speech Recognition of Conversational Speech,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Language model is a vital component in modern automatic speech recognition (ASR) systems. Since one-size-fits-all language model works suboptimally for conversational speeches, language model adaptation (LMA) is considered as a promising solution for solving this problem. In order to compare the state-of-the-art LMA techniques and systematically demonstrate their effect in conversational speech recognition, we develop a novel toolkit named Chameleon, which includes the state-of-the-art cache-based and topic-based LMA techniques. This demonstration does not only vividly visualize underlying working mechanisms of a variety of the state-of-the-art LMA models but also provide an interface for the user to customize the hyperparameters of them. With this demonstration, the audience can experience the effect of LMA in an interactive and real-time fashion. We wish this demonstration would inspire more research on better language model techniques for ASR.",,,,,"yang, qiang/GYJ-0971-2022",,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,37,42,,,,,,,,,,,,,,,,WOS:000855231500007,0
C,"Thompson, B; Knowles, R; Zhang, X; Khayrallah, H; Duh, K; Koehn, P",,,Assoc Computat Linguist,"Thompson, Brian; Knowles, Rebecca; Zhang, Xuan; Khayrallah, Huda; Duh, Kevin; Koehn, Philipp",,,HABLex: Human Annotated Bilingual Lexicons for Experiments in Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the HABLex dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human-generated alignments of words and phrases in machine translation test sets in three language pairs (Russian -> English, Chinese -> English, and Korean -> English), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines-constrained decoding and continued training-and an improvement to continued training to address overfitting.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1382,1387,,,,,,,,,,,,,,,,WOS:000854193301064,0
C,"Wang, BL; Titov, I; Lapata, M",,,Assoc Computat Linguist,"Wang, Bailin; Titov, Ivan; Lapata, Mirella",,,Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial abstract program and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WIKITABLEQUESTIONS and WIKISQL datasets. When compared to a standard attention baseline, we observe that the proposed structuredalignment mechanism is highly beneficial.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3774,3785,,,,,,,,,,,,,,,,WOS:000854193303109,0
C,"Anjum, O; Gong, HY; Bhat, S; Xiong, JJ; Hwu, WM",,,Assoc Computat Linguist,"Anjum, Omer; Gong, Hongyu; Bhat, Suma; Xiong, Jinjun; Hwu, Wen-Mei",,,PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Finding the right reviewers to assess the quality of conference submissions is a time consuming process for conference organizers. Given the importance of this step, various automated reviewer-paper matching solutions have been proposed to alleviate the burden. Prior approaches, including bag-of-words models and probabilistic topic models have been inadequate to deal with the vocabulary mismatch and partial topic overlap between a paper submission and the reviewer's expertise. Our approach, the common topic model, jointly models the topics common to the submission and the reviewer's profile while relying on abstract topic vectors. Experiments and insightful evaluations on two datasets demonstrate that the proposed method achieves consistent improvements compared to available state-of-the-art implementations of paper-reviewer matching.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,518,528,,,,,,,,,,,,,,,,WOS:000854193300049,0
C,"Cai, HY; Chen, HS; Zhang, C; Song, YH; Zhao, XF; Yin, DW",,,Assoc Computat Linguist,"Cai, Hengyi; Chen, Hongshen; Zhang, Cheng; Song, Yonghao; Zhao, Xiaofang; Yin, Dawei",,,Adaptive Parameterization for Neural Dialogue Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural conversation systems generate responses based on the sequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with a single set of learned parameters to generate responses for given input contexts. When confronting diverse conversations, its adaptability is rather limited and the model is hence prone to generate generic responses. In this work, we propose an Adaptive Neural Dialogue generation model, ADAND, which manages various conversations with conversation-specific parameterization. For each conversation, the model generates parameters of the encoder-decoder by referring to the input context. In particular, we propose two adaptive parameterization mechanisms: a context-aware and a topic-aware parameterization mechanism. The context-aware parameterization directly generates the parameters by capturing local semantics of the given context. The topic-aware parameterization enables parameter sharing among conversations with similar topics by first inferring the latent topics of the given context and then generating the parameters with respect to the distributional topics. Extensive experiments conducted on a large-scale real-world conversational dataset show that our model achieves superior performance in terms of both quantitative metrics and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1793,1802,,,,,,,,,,,,,,,,WOS:000854193301110,0
C,"Cho, J; Seo, M; Hajishirzi, H",,,Assoc Computat Linguist,"Cho, Jaemin; Seo, Minjoon; Hajishirzi, Hannaneh",,,Mixture Content Selection for Diverse Sequence Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground-truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/ clovaai/FocusSeq2Seq.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3121,3131,,,,,,,,,,,,,,,,WOS:000854193303026,0
C,"Cohan, A; Beltagy, I; King, D; Dalvi, B; Weld, DS",,,Assoc Computat Linguist,"Cohan, Arman; Beltagy, Iz; King, Daniel; Dalvi, Bhavana; Weld, Daniel S.",,,Pretrained Language Models for Sequential Sentence Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"As a step toward better document-level understanding, we explore classification of a sequence of sentences into their corresponding categories, a task that requires understanding sentences in context of the document. Recent successful models for this task have used hierarchical models to contextualize sentence representations, and Conditional Random Fields (CRFs) to incorporate dependencies between subsequent labels. In this work, we show that pretrained language models, BERT (Devlin et al., 2018) in particular, can be used for this task to capture contextual dependencies without the need for hierarchical encoding nor a CRF. Specifically, we construct a joint sentence representation that allows BERT Transformer layers to directly utilize contextual information from all words in all sentences. Our approach achieves state-of-the-art results on four datasets, including a new dataset of structured scientific abstracts.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3693,3699,,,,,,,,,,,,,,,,WOS:000854193303101,0
C,"Ethayarajh, K",,,Assoc Computat Linguist,"Ethayarajh, Kawin",,,"How Contextual are ContextualizedWord Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,55,65,,,,,,,,,,,,,,,,WOS:000854193300006,0
C,"Gao, TY; Han, X; Zhu, H; Liu, ZY; Li, P; Sun, MS; Zhou, J",,,Assoc Computat Linguist,"Gao, Tianyu; Han, Xu; Zhu, Hao; Liu, Zhiyuan; Li, Peng; Sun, Maosong; Zhou, Jie",,,FewRel 2.0: Towards More Challenging Few-Shot Relation Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present FewRel 2.0, a more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations? To construct FewRel 2.0, we build upon the FewRel dataset (Han et al., 2018) by adding a new test set in a quite different domain, and a NOTA relation choice. With the new dataset and extensive experimental analysis, we found (1) that the state-of-the-art few-shot relation classification models struggle on these two aspects, and (2) that the commonly-used techniques for domain adaptation and NOTA detection still cannot handle the two challenges well. Our research calls for more attention and further efforts to these two real-world issues. All details and resources about the dataset and baselines are released at https://github.com/thunlp/fewrel.",,,,,"li, zhiyuan/HGD-9581-2022","Li, Peng/0000-0003-1374-5979; Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6250,6255,,,,,,,,,,,,,,,,WOS:000854193306060,0
C,"Garg, S; Peitz, S; Nallasamy, U; Paulik, M",,,Assoc Computat Linguist,"Garg, Sarthak; Peitz, Stephan; Nallasamy, Udhyakumar; Paulik, Matthias",,,Jointly Learning to Align and Translate with Transformer Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4453,4462,,,,,,,,,,,,,,,,WOS:000854193304058,0
C,"Gong, M; Shou, LJ; Lin, WT; Sang, ZJ; Yan, QJ; Yang, Z; Chen, FX; Jiang, DX",,,ASSOC COMPUTAT LINGUIST,"Gong, Ming; Shou, Linjun; Lin, Wutao; Sang, Zhijie; Yan, Quanjia; Yang, Ze; Chen, Feixiang; Jiang, Daxin",,,NeuronBlocks: Building Your NLP DNN Models Like Playing Lego,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep Neural Networks (DNN) have been widely employed in industry to address various Natural Language Processing (NLP) tasks. However, many engineers find it a big overhead when they have to choose from multiple frameworks, compare different types of models, and understand various optimization mechanisms. An NLP toolkit for DNN models with both generality and flexibility can greatly improve the productivity of engineers by saving their learning cost and guiding them to find optimal solutions to their tasks. In this paper, we introduce NeuronBlocks1 2, a toolkit encapsulating a suite of neural network modules as building blocks to construct various DNN models with complex architecture. This toolkit empowers engineers to build, train, and test various NLP models through simple configuration of JSON files. The experiments on several NLP datasets such as GLUE, WikiQA and CoNLL-2003 demonstrate the effectiveness of NeuronBlocks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,163,168,,,,,,,,,,,,,,,,WOS:000855231500028,0
C,"Hamalainen, M; Alnajjar, K",,,Assoc Computat Linguist,"Hamalainen, Mika; Alnajjar, Khalid",,,Generating Modern Poetry Automatically in Finnish,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a novel approach for generating poetry automatically for the morphologically rich Finnish language by using a genetic algorithm. The approach improves the state of the art of the previous Finnish poem generators by introducing a higher degree of freedom in terms of structural creativity. Our approach is evaluated and described within the paradigm of computational creativity, where the fitness functions of the genetic algorithm are assimilated with the notion of aesthetics. The output is considered to be a poem 81.5% of the time by human evaluators.",,,,,"Hamalainen, Mika K/V-8920-2018","Hamalainen, Mika K/0000-0001-9315-1278",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5999,6004,,,,,,,,,,,,,,,,WOS:000854193306028,0
C,"Han, XC; Eisenstein, J",,,Assoc Computat Linguist,"Han, Xiaochuang; Eisenstein, Jacob",,,Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a wide range of natural language processing tasks by pretraining on large corpora of unlabeled text. However, the applicability of this approach is unknown when the target domain varies substantially from the pretraining corpus. We are specifically interested in the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled and pretraining texts. To address this scenario, we propose domain-adaptive finetuning, in which the contextualized embeddings are adapted by masked language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-of-vocabulary words. We conclude that domainadaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4238,4248,,,,,,,,,,,,,,,,WOS:000854193304038,0
C,"Jiang, YC; Bansal, M",,,Assoc Computat Linguist,"Jiang, Yichen; Bansal, Mohit",,,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controllerbased Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire network. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our model via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the controller can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4474,4484,,,,,,,,,,,,,,,,WOS:000854193304060,0
C,"Li, RP; Cheng, X",,,Assoc Computat Linguist,"Li, Ruiping; Cheng, Xiang",,,DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Knowledge graphs (KGs) often suffer from sparseness and incompleteness. Knowledge graph reasoning provides a feasible way to address such problems. Recent studies on knowledge graph reasoning have shown that reinforcement learning (RL) based methods can provide state-of-the-art performance. However, existing RL-based methods require numerous trials for path-finding and rely heavily on meticulous reward engineering to fit specific dataset, which is inefficient and laborious to apply to fast-evolving KGs. In this paper, we present DIVINE, a novel plug-and-play framework based on generative adversarial imitation learning for enhancing existing RL-based methods. DIVINE guides the path-finding process, and learns reasoning policies and reward functions self-adaptively through imitating the demonstrations automatically sampled from KGs. Experimental results on two benchmark datasets show that our framework improves the performance of existing RL-based methods without extra reward engineering.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2642,2651,,,,,,,,,,,,,,,,WOS:000854193302074,0
C,"Nema, P; Mohankumar, AK; Khapra, MM; Srinivasan, BV; Ravindran, B",,,Assoc Computat Linguist,"Nema, Preksha; Mohankumar, Akash Kumar; Khapra, Mitesh M.; Srinivasan, Balaji Vasan; Ravindran, Balaraman",,,Let's Ask Again: Refine Network for Automatic Question Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this work, we focus on the task of Automatic Question Generation (AQG) where given a passage and an answer the task is to generate the corresponding question. It is desired that the generated question should be (i) grammatically correct (ii) answerable from the passage and (iii) specific to the given answer. An analysis of existing AQG models shows that they produce questions which do not adhere to one or more of the above-mentioned qualities. In particular, the generated questions look like an incomplete draft of the desired question with a clear scope for refinement. To alleviate this shortcoming, we propose a method which tries to mimic the human process of generating questions by first creating an initial draft and then refining it. More specifically, we propose Refine Network (RefNet) which contains two decoders. The second decoder uses a dual attention network which pays attention to both (i) the original passage and (ii) the question (initial draft) generated by the first decoder. In effect, it refines the question generated by the first decoder, thereby making it more correct and complete. We evaluate RefNet on three datasets, viz., SQuAD, HOTPOT-QA, and DROP, and show that it outperforms existing state-of-the-art methods by 7-16% on all of these datasets. Lastly, we show that we can improve the quality of the second decoder on specific metrics, such as, fluency and answerability by explicitly rewarding revisions that improve on the corresponding metric during training. The code has been made publicly available(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3314,3323,,,,,,,,,,,,,,,,WOS:000854193303044,0
C,"Pethe, C; Skiena, S",,,Assoc Computat Linguist,"Pethe, Charuta; Skiena, Steven",,,The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The sequence of documents produced by any given author varies in style and content, but some documents are more typical or representative of the source than others. We quantify the extent to which a given short text is characteristic of a specific person, using a dataset of tweets from fifteen celebrities. Such analysis is useful for generating excerpts of highvolume Twitter profiles, and understanding how representativeness relates to tweet popularity. We first consider the related task of binary author detection (is x the author of text T?), and report a test accuracy of 90.37% for the best of five approaches to this problem. We then use these models to compute characterization scores among all of an author's texts. A user study shows human evaluators agree with our characterization model for all 15 celebrities in our dataset, each with p-value < 0.05. We use these classifiers to show surprisingly strong correlations between characterization scores and the popularity of the associated texts. Indeed, we demonstrate a statistically significant correlation between this score and tweet popularity (likes/replies/retweets) for 13 of the 15 celebrities in our study.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1653,1663,,,,,,,,,,,,,,,,WOS:000854193301097,0
C,"Wang, MX; Xie, J; Tan, ZX; Su, JS; Xiong, DY; Li, L",,,Assoc Computat Linguist,"Wang, Mingxuan; Xie, Jun; Tan, Zhixing; Su, Jinsong; Xiong, Deyi; Li, Lei",,,Towards Linear Time Neural Machine Translation with Capsule Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this study, we first investigate a novel capsule network with dynamic routing for linear time Neural Machine Translation (NMT), referred as CAPSNMT. CAPSNMT uses an aggregation mechanism to map the source sentence into a matrix with pre-determined size, and then applys a deep LSTM network to decode the target sequence from the source representation. Different from the previous work (Sutskever et al., 2014) to store the source sentence with a passive and bottom-up way, the dynamic routing policy encodes the source sentence with an iterative process to decide the credit attribution between nodes from lower and higher layers. CAPSNMT has two core properties: it runs in time that is linear in the length of the sequences and provides a more flexible way to aggregate the part-whole information of the source sentence. On WMT14 English-German task and a larger WMT14 English-French task, CAPSNMT achieves comparable results with the Transformer system. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for sequence to sequence problems.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,803,812,,,,,,,,,,,,,,,,WOS:000854193300074,0
C,"Washio, K; Sekine, S; Kato, T",,,Assoc Computat Linguist,"Washio, Koki; Sekine, Satoshi; Kato, Tsuneaki",,,Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Definition modeling includes acquiring word embeddings from dictionary definitions and generating definitions of words. While the meanings of defining words are important in dictionary definitions, it is crucial to capture the lexical semantic relations between defined words and defining words. However, thus far, the utilization of such relations has not been explored for definition modeling. In this paper, we propose definition modeling methods that use lexical semantic relations. To utilize implicit semantic relations in definitions, we use unsupervisedly obtained pattern-based word-pair embeddings that represent semantic relations of word pairs. Experimental results indicate that our methods improve the performance in learning embeddings from definitions, as well as definition generation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3521,3527,,,,,,,,,,,,,,,,WOS:000854193303075,0
C,"Yang, ZY; Zhu, CG; Chen, WZ",,,Assoc Computat Linguist,"Yang, Ziyi; Zhu, Chenguang; Chen, Weizhu",,,Parameter-free Sentence Embedding via Orthogonal Basis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word's novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Our model shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,638,648,,,,,,,,,,,,,,,,WOS:000854193300059,0
C,"Zhang, MS; Zhang, Y; Fu, GH",,,Assoc Computat Linguist,"Zhang, Meishan; Zhang, Yue; Fu, Guohong",,,Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,997,1006,,,,,,,,,,,,,,,,WOS:000854193301014,0
C,"Zhu, Y; Liu, HZ; Wu, ZH; Song, Y; Zhang, T",,,Assoc Computat Linguist,"Zhu, Yao; Liu, Hongzhi; Wu, Zhonghai; Song, Yang; Zhang, Tao",,,Representation Learning with Ordered Relation Paths for Knowledge Graph Completion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Incompleteness is a common problem for existing knowledge graphs (KGs), and the completion of KG which aims to predict links between entities is challenging. Most existing KG completion methods only consider the direct relation between nodes and ignore the relation paths which contain useful information for link prediction. Recently, a few methods take relation paths into consideration but pay less attention to the order of relations in paths which is important for reasoning. In addition, these path-based models always ignore nonlinear contributions of path features for link prediction. To solve these problems, we propose a novel KG completion method named OPTransE. Instead of embedding both entities of a relation into the same latent space as in previous methods, we project the head entity and the tail entity of each relation into different spaces to guarantee the order of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear and complex features of different paths to further improve the performance of link prediction. Experimental results on two benchmark datasets show that the proposed model OPTransE performs better than state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2662,2671,,,,,,,,,,,,,,,,WOS:000854193302076,0
C,"Ding, X; Liao, K; Liu, T; Li, ZY; Duan, JW",,,Assoc Computat Linguist,"Ding, Xiao; Liao, Kuo; Liu, Ting; Li, Zhongyang; Duan, Junwen",,,Event Representation Learning Enhanced with External Commonsense Knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as script event prediction. On the other hand, events extracted from raw texts lacks of commonsense knowledge, such as the intents and emotions of the event participants, which are useful for distinguishing event pairs when there are only subtle differences in their surface realizations. To address this issue, this paper proposes to leverage external commonsense knowledge about the intent and sentiment of the event. Experiments on three event-related tasks, i.e., event similarity, script event prediction and stock market prediction, show that our model obtains much better event embeddings for the tasks, achieving 78% improvements on hard similarity task, yielding more precise inferences on subsequent events under given contexts, and better accuracies in predicting the volatilities of the stock market(1).",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4894,4903,,,,,,,,,,,,,,,,WOS:000854193305008,0
C,"Erera, S; Shmueli-Scheuer, M; Feigenblat, G; Nakash, OP; Boni, O; Roitman, H; Cohen, D; Weiner, B; Mass, Y; Rivlin, O; Lev, G; Jerbi, A; Herzig, J; Hou, YF; Jochim, C; Gleize, M; Bonin, F; Ganguly, D; Konopnicki, D",,,ASSOC COMPUTAT LINGUIST,"Erera, Shai; Shmueli-Scheuer, Michal; Feigenblat, Guy; Nakash, Ora Peled; Boni, Odellia; Roitman, Haggai; Cohen, Doron; Weiner, Bar; Mass, Yosi; Rivlin, Or; Lev, Guy; Jerbi, Achiya; Herzig, Jonathan; Hou, Yufang; Jochim, Charles; Gleize, Martin; Bonin, Francesca; Ganguly, Debasis; Konopnicki, David",,,A Summarization System for Scientific Documents,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a novel system providing summaries for Computer Science publications. Through a qualitative user study, we identified the most valuable scenarios for discovery, exploration and understanding of scientific documents. Based on these findings, we built a system that retrieves and summarizes scientific documents for a given information need, either in form of a free-text query or by choosing categorized values such as scientific tasks, datasets and more. Our system ingested 270,000 papers, and its summarization module aims to generate concise yet detailed summaries. We validated our approach with human experts.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,211,216,,,,,,,,,,,,,,,,WOS:000855231500036,0
C,"Gao, J; Bi, W; Liu, XJ; Li, JH; Zhou, GD; Shi, SM",,,Assoc Computat Linguist,"Gao, Jun; Bi, Wei; Liu, Xiaojiang; Li, Junhui; Zhou, Guodong; Shi, Shuming",,,A Discrete CVAE for Response Generation on Short-Text Conversation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural conversation models such as encoder-decoder models are easy to generate bland and generic responses. Some researchers propose to use the conditional variational autoencoder (CVAE) which maximizes the lower bound on the conditional log-likelihood on a continuous latent variable. With different sampled latent variables, the model is expected to generate diverse responses. Although the CVAE-based models have shown tremendous potential, their improvement of generating highquality responses is still unsatisfactory. In this paper, we introduce a discrete latent variable with an explicit semantic meaning to improve the CVAE on short-text conversation. A major advantage of our model is that we can exploit the semantic distance between the latent variables to maintain good diversity between the sampled latent variables. Accordingly, we propose a two-stage sampling approach to enable efficient diverse variable selection from a large latent space assumed in the short-text conversation task. Experimental results indicate that our model outperforms various kinds of generation models under both automatic and human evaluations and generates more diverse and informative responses.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1898,1908,,,,,,,,,,,,,,,,WOS:000854193302006,0
C,"Jin, ZJ; Jin, D; Mueller, J; Matthews, N; Santus, E",,,Assoc Computat Linguist,"Jin, Zhijing; Jin, Di; Mueller, Jonas; Matthews, Nicholas; Santus, Enrico",,,IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text attribute transfer aims to automatically rewrite sentences such that they possess certain linguistic attributes, while simultaneously preserving their semantic content. This task remains challenging due to a lack of supervised parallel data. Existing approaches try to explicitly disentangle content and attribute information, but this is difficult and often results in poor content-preservation and ungrammaticality. In contrast, we propose a simpler approach, Iterative Matching and Translation (IMaT), which: (1) constructs a pseudoparallel corpus by aligning a subset of semantically similar sentences from the source and the target corpora; (2) applies a standard sequence-to-sequence model to learn the attribute transfer; (3) iteratively improves the learned transfer function by refining imperfections in the alignment. In sentiment modification and formality transfer tasks, our method outperforms complex state-of-the-art systems by a large margin. As an auxiliary contribution, we produce a publicly-available test set with human-generated transfer references.(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3097,3109,,,,,,,,,,,,,,,,WOS:000854193303024,0
C,"Li, SY; Lei, DR; Qin, PD; Wang, WY",,,Assoc Computat Linguist,"Li, Siyao; Lei, Deren; Qin, Pengda; Wang, William Yang",,,Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward ROUGE- L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of ROUGE- L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.",,,,,,"Wang, William S-Y./0000-0001-6153-8240",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6038,6044,,,,,,,,,,,,,,,,WOS:000854193306034,0
C,"Maru, M; Scozzafava, F; Martelli, F; Navigli, R",,,Assoc Computat Linguist,"Maru, Marco; Scozzafava, Federico; Martelli, Federico; Navigli, Roberto",,,SyntagNet: Challenging Supervised Word Sense Disambiguation with Lexical-Semantic Combinations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current research in knowledge-based Word Sense Disambiguation (WSD) indicates that performances depend heavily on the Lexical Knowledge Base (LKB) employed. This paper introduces SyntagNet, a novel resource consisting of manually disambiguated lexicalsemantic combinations. By capturing sense distinctions evoked by syntagmatic relations, SyntagNet enables knowledge-based WSD systems to establish a new state of the art which challenges the hitherto unrivaled performances attained by supervised approaches. To the best of our knowledge, SyntagNet is the first large-scale manually-curated resource of this kind made available to the community (at http://syntagnet.org).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3534,3540,,,,,,,,,,,,,,,,WOS:000854193303077,0
C,"Maudslay, RH; Gonen, H; Cotterell, R; Teufel, S",,,Assoc Computat Linguist,"Maudslay, Rowan Hall; Gonen, Hila; Cotterell, Ryan; Teufel, Simone",,,It's All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5267,5275,,,,,,,,,,,,,,,,WOS:000854193305043,0
C,"Peters, ME; Neumann, M; Logan, RL; Schwartz, R; Joshi, V; Singh, S; Smith, NA",,,Assoc Computat Linguist,"Peters, Matthew E.; Neumann, Mark; Logan, Robert L.; Schwartz, Roy; Joshi, Vidur; Singh, Sameer; Smith, Noah A.",,,Knowledge Enhanced Contextual Word Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and selfsupervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert's runtime is comparable to BERT's and it scales to large KBs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,43,54,,,,,,,,,,,,,,,,WOS:000854193300005,0
C,"Prabhakaran, V; Hutchinson, B; Mitchell, M",,,Assoc Computat Linguist,"Prabhakaran, Vinodkumar; Hutchinson, Ben; Mitchell, Margaret",,,Perturbation Sensitivity Analysis to Detect Unintended Model Biases,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Data-driven statistical Natural Language Processing (NLP) techniques leverage large amounts of language data to build models that can understand language. However, most language data reflect the public discourse at the time the data was produced, and hence NLP models are susceptible to learning incidental associations around named referents at a particular point in time, in addition to general linguistic meaning. An NLP system designed to model notions such as sentiment and toxicity should ideally produce scores that are independent of the identity of such entities mentioned in text and their social associations. For example, in a general purpose sentiment analysis system, a phrase such as I hate Katy Perry should be interpreted as having the same sentiment as I hate Taylor Swift. Based on this idea, we propose a generic evaluation framework, Perturbation Sensitivity Analysis, which detects unintended model biases related to named entities, and requires no new annotations or corpora. We demonstrate the utility of this analysis by employing it on two different NLP models - a sentiment model and a toxicity model - applied on online comments in English language from four different genres.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5740,5745,,,,,,,,,,,,,,,,WOS:000854193305091,0
C,"Shi, XF; Xiao, YH",,,Assoc Computat Linguist,"Shi, Xiaofei; Xiao, Yanghua",,,Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Entity alignment aims to find entities in different knowledge graphs (KGs) that refer to the same real-world object. An effective solution for cross-lingual entity alignment is crucial for many cross-lingual AI and NLP applications. Recently many embedding-based approaches were proposed for cross-lingual entity alignment. However, almost all of them are based on TransE or its variants, which have been demonstrated by many studies to be unsuitable for encoding multi-mapping relations such as 1-N, N-1 and N-N relations, thus these methods obtain low alignment precision. To solve this issue, we propose a new embedding-based framework. Through defining dot product-based functions over embeddings, our model can better capture the semantics of both 1-1 and multi-mapping relations. We calibrate embeddings of different KGs via a small set of pre-aligned seeds. We also propose a weighted negative sampling strategy to generate valuable negative samples during training and we regard prediction as a bidirectional problem in the end. Experimental results (especially with the metric Hits@1) on real-world multilingual datasets show that our approach significantly outperforms many other embedding-based approaches with state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,813,822,,,,,,,,,,,,,,,,WOS:000854193300075,0
C,"Sudhakar, A; Upadhyay, B; Maheswaran, A",,,Assoc Computat Linguist,"Sudhakar, Akhilesh; Upadhyay, Bhargav; Maheswaran, Arjun",,,"Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text style transfer is the task of transferring the style of text having certain stylistic attributes, while preserving non-stylistic or content information. In this work we introduce the Generative Style Transformer (GST) - a new approach to rewriting sentences to a target style in the absence of parallel style corpora. GST leverages the power of both, large unsupervised pre-trained language models as well as the Transformer. GST is a part of a larger `Delete Retrieve Generate' framework, in which we also propose a novel method of deleting style attributes from the source sentence by exploiting the inner workings of the Transformer. Our models outperform state-of-art systems across 5 datasets on sentiment, gender and political slant transfer. We also propose the use of the GLEU metric as an automatic metric of evaluation of style transfer, which we found to compare better with human ratings than the predominantly used BLEU score.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3269,3279,,,,,,,,,,,,,,,,WOS:000854193303040,0
C,"Tan, X; Zhang, LY; Xiong, DY; Zhou, GD",,,Assoc Computat Linguist,"Tan, Xin; Zhang, Longyin; Xiong, Deyi; Zhou, Guodong",,,Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Document-level machine translation (MT) remains challenging due to the difficulty in efficiently using document context for translation. In this paper, we propose a hierarchical model to learn the global context for documentlevel neural machine translation (NMT). This is done through a sentence encoder to capture intra-sentence dependencies and a document encoder to model document-level intersentence consistency and coherence. With this hierarchical architecture, we feedback the extracted global document context to each word in a top-down fashion to distinguish different translations of a word according to its specific surrounding context. In addition, since largescale in-domain document-level parallel corpora are usually unavailable, we use a twostep training strategy to take advantage of a large-scale corpus with out-of-domain parallel sentence pairs and a small-scale corpus with in-domain parallel document pairs to achieve the domain adaptability. Experimental results on several benchmark corpora show that our proposed model can significantly improve document-level translation performance over several strong NMT baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1576,1585,,,,,,,,,,,,,,,,WOS:000854193301090,0
C,"Xiao, SY; Ouyang, YX; Rong, WG; Yang, JX; Xiong, Z",,,Assoc Computat Linguist,"Xiao, Shiyuan; Ouyang, Yuanxin; Rong, Wenge; Yang, Jianxin; Xiong, Zhang",,,Similarity Based Auxiliary Classifier for Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The segmentation problem is one of the fundamental challenges associated with name entity recognition (NER) tasks that aim to reduce the boundary error when detecting a sequence of entity words. A considerable number of advanced approaches have been proposed and most of them exhibit performance deterioration when entities become longer. Inspired by previous work in which a multi-task strategy is used to solve segmentation problems, we design a similarity based auxiliary classifier (SAC), which can distinguish entity words from non-entity words. Unlike conventional classifiers, SAC uses vectors to indicate tags. Therefore, SAC can calculate the similarities between words and tags, and then compute a weighted sum of the tag vectors, which can be considered a useful feature for NER tasks. Empirical results are used to verify the rationality of the SAC structure and demonstrate the SAC model's potential in performance improvement against our baseline approaches.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1140,1149,,,,,,,,,,,,,,,,WOS:000854193301027,0
C,"Zhang, B; Zhang, XM",,,Assoc Computat Linguist,"Zhang, Bo; Zhang, Xiaoming",,,Hierarchy Response Learning for Neural Conversation Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The neural encoder-decoder models have shown great promise in neural conversation generation. However, they cannot perceive and express the intention effectively, and hence often generate dull and generic responses. Unlike past work that has focused on diversifying the output at word-level or discourse-level with a flat model to alleviate this problem, we propose a hierarchical generation model to capture the different levels of diversity using the conditional variational autoencoders. Specifically, a hierarchical response generation (HRG) framework is proposed to capture the conversation intention in a natural and coherent way. It has two modules, namely, an expression reconstruction model to capture the hierarchical correlation between expression and intention, and an expression attention model to effectively combine the expressions with contents. Finally, the training procedure of HRG is improved by introducing reconstruction loss. Experiment results show that our model can generate the responses with more appropriate content and expression.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1772,1781,,,,,,,,,,,,,,,,WOS:000854193301108,0
C,"Zhao, Z; Chen, H; Zhang, JB; Zhao, X; Liu, T; Lu, W; Chen, X; Deng, HT; Ju, Q; Du, XY",,,ASSOC COMPUTAT LINGUIST,"Zhao, Zhe; Chen, Hui; Zhang, Jinbin; Zhao, Xin; Liu, Tao; Lu, Wei; Chen, Xi; Deng, Haotang; Ju, Qi; Du, Xiaoyong",,,UER: An Open-Source Toolkit for Pre-training Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing works, including ELMO and BERT, have revealed the importance of pre-training for NLP tasks. While there does not exist a single pre-training model that works best in all cases, it is of necessity to develop a framework that is able to deploy various pre-training models efficiently. For this purpose, we propose an assemble-on-demand pre-training toolkit, namely Universal Encoder Representations (UER). UER is loosely coupled, and encapsulated with rich modules. By assembling modules on demand, users can either reproduce a state-of-the-art pre-training model or develop a pre-training model that remains unexplored. With UER, we have built a model zoo, which contains pre-trained models based on different corpora, encoders, and targets (objectives). With proper pre-trained models, we could achieve new state-of-the-art results on a range of downstream datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,241,246,,,,,,,,,,,,,,,,WOS:000855231500041,0
C,"Zhou, MY; Arnold, J; Yu, Z",,,Assoc Computat Linguist,"Zhou, Mingyang; Arnold, Josh; Yu, Zhou",,,Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Reinforcement learning (RL) is an effective approach to learn an optimal dialog policy for task-oriented visual dialog systems. A common practice is to apply RL on a neural sequence-to-sequence (seq2seq) framework with the action space being the output vocabulary in the decoder. However, it is difficult to design a reward function that can achieve a balance between learning an effective policy and generating a natural dialog response. This paper proposes a novel framework that alternatively trains a RL policy for image guessing and a supervised seq2seq model to improve dialog generation quality. We evaluate our framework on the Guess-Which task and the framework achieves the state-of-the-art performance in both task completion and dialog quality.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,143,153,,,,,,,,,,,,,,,,WOS:000854193300014,0
C,"Dai, HL; Du, DH; Li, X; Song, YQ",,,Assoc Computat Linguist,"Dai, Hongliang; Du, Donghong; Li, Xin; Song, Yangqiu",,,Improving Fine-grained Entity Typing with Entity Linking,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Fine-grained entity typing is a challenging problem since it usually involves a relatively large tag set and may require to understand the context of the entity mention. In this paper, we use entity linking to help with the fine-grained entity type classification process. We propose a deep neural model that makes predictions based on both the context and the information obtained from entity linking results. Experimental results on two commonly used datasets demonstrates the effectiveness of our approach. On both datasets, it achieves more than 5% absolute strict accuracy improvement over the state of the art.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6210,6215,,,,,,,,,,,,,,,,WOS:000854193306054,0
C,"Di Fabio, A; Conia, S; Navigli, R",,,Assoc Computat Linguist,"Di Fabio, Andrea; Conia, Simone; Navigli, Roberto",,,VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present VerbAtlas, a new, hand-crafted lexical-semantic resource whose goal is to bring together all verbal synsets from WordNet into semantically-coherent frames. The frames define a common, prototypical argument structure while at the same time providing new concept-specific information. In contrast to PropBank, which defines enumerative semantic roles, VerbAtlas comes with an explicit, cross-frame set of semantic roles linked to selectional preferences expressed in terms of WordNet synsets, and is the first resource enriched with semantic information about implicit, shadow, and default arguments. We demonstrate the effectiveness of VerbAtlas in the task of dependency-based Semantic Role Labeling and show how its integration into a high-performance system leads to improvements on both the in-domain and out-of-domain test sets of CoNLL-2009. VerbAtlas is available at http://verbatlas.org.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,627,637,,,,,,,,,,,,,,,,WOS:000854193300058,0
C,"Dinan, E; Humeau, S; Chintagunta, B; Weston, J",,,Assoc Computat Linguist,"Dinan, Emily; Humeau, Samuel; Chintagunta, Bharath; Weston, Jason",,,Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Galan-Garcia et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it strategy with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods are all made open source and publicly available.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4537,4546,,,,,,,,,,,,,,,,WOS:000854193304066,0
C,"Dong, W; Su, QL; Shen, DH; Chen, CY",,,Assoc Computat Linguist,"Dong, Wei; Su, Qinliang; Shen, Dinghan; Chen, Changyou",,,Document Hashing with Mixture-Prior Generative Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Hashing is promising for large-scale information retrieval tasks thanks to the efficiency of distance evaluation between binary codes. Generative hashing is often used to generate hashing codes in an unsupervised way. However, existing generative hashing methods only considered the use of simple priors, like Gaussian and Bernoulli priors, which limits these methods to further improve their performance. In this paper, two mixture-prior generative models are proposed, under the objective to produce high-quality hashing codes for documents. Specifically, a Gaussian mixture prior is first imposed onto the variational auto-encoder (VAE), followed by a separate step to cast the continuous latent representation of VAE into binary code. To avoid the performance loss caused by the separate casting, a model using a Bernoulli mixture prior is further developed, in which an end-toend training is admitted by resorting to the straight-through (ST) discrete gradient estimator. Experimental results on several benchmark datasets demonstrate that the proposed methods, especially the one using Bernoulli mixture priors, consistently outperform existing ones by a substantial margin.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5226,5235,,,,,,,,,,,,,,,,WOS:000854193305039,0
C,"Du, CN; Sun, HF; Wang, JY; Qi, Q; Liao, JX; Xu, T; Liu, M",,,Assoc Computat Linguist,"Du, Chunning; Sun, Haifeng; Wang, Jingyu; Qi, Qi; Liao, Jianxin; Xu, Tong; Liu, Ming",,,Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect-level sentiment classification is a crucial task for sentiment analysis, which aims to identify the sentiment polarities of specific targets in their context. The main challenge comes from multi-aspect sentences, which express multiple sentiment polarities towards different targets, resulting in overlapped feature representation. However, most existing neural models tend to utilize static pooling operation or attention mechanism to identify sentimental words, which therefore insufficient for dealing with overlapped features. To solve this problem, we propose to utilize capsule network to construct vector-based feature representation and cluster features by an EM routing algorithm. Furthermore, interactive attention mechanism is introduced in the capsule routing procedure to model the semantic relationship between aspect terms and context. The iterative routing also enables encoding sentence from a global perspective. Experimental results on three datasets show that our proposed model achieves state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5489,5498,,,,,,,,,,,,,,,,WOS:000854193305064,0
C,"Fang, L; Li, CY; Gao, JF; Dong, W; Chen, CY",,,Assoc Computat Linguist,"Fang, Le; Li, Chunyuan; Gao, Jianfeng; Dong, Wen; Chen, Changyou",,,Implicit Deep Latent Variable Models for Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious posterior collapse issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVMto directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the posterior collapse issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3946,3956,,,,,,,,,,,,,,,,WOS:000854193304012,0
C,"Gao, MF; Davis, LS; Socher, R; Xiong, CM",,,Assoc Computat Linguist,"Gao, Mingfei; Davis, Larry S.; Socher, Richard; Xiong, Caiming",,,WSLLN: Weakly Supervised Natural Language Localization Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1481,1487,,,,,,,,,,,,,,,,WOS:000854193301079,0
C,"Graesser, L; Cho, K; Kiela, D",,,Assoc Computat Linguist,"Graesser, Laura; Cho, Kyunghyun; Kiela, Douwe",,,Emergent Linguistic Phenomena in Multi-Agent Communication Games,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We describe a multi-agent communication framework for examining high-level linguistic phenomena at the community-level. We demonstrate that complex linguistic behavior observed in natural language can be reproduced in this simple setting: i) the outcome of contact between communities is a function of inter- and intra-group connectivity; ii) linguistic contact either converges to the majority protocol, or in balanced cases leads to novel creole languages of lower complexity; and iii) a linguistic continuum emerges where neighboring languages are more mutually intelligible than farther removed languages. We conclude that at least some of the intricate properties of language evolution need not depend on complex evolved linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled agents playing communication games.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3700,3710,,,,,,,,,,,,,,,,WOS:000854193303102,0
C,"Herzig, J; Berant, J",,,Assoc Computat Linguist,"Herzig, Jonathan; Berant, Jonathan",,,"Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3810,3820,,,,,,,,,,,,,,,,WOS:000854193303112,0
C,"Huang, PY; Chang, XJ; Hauptmann, A",,,Assoc Computat Linguist,"Huang, Po-Yao; Chang, Xiaojun; Hauptmann, Alexander",,,Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for finegrained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.",,,,,"Chang, Xiaojun/A-2055-2015","Chang, Xiaojun/0000-0002-7778-8807",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1461,1467,,,,,,,,,,,,,,,,WOS:000854193301076,0
C,"Kementchedjhieva, Y; Hartmann, M; Sogaard, A",,,Assoc Computat Linguist,"Kementchedjhieva, Yova; Hartmann, Mareike; Sogaard, Anders",,,Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of bilingual dictionary induction (BDI) is commonly used for intrinsic evaluation of cross-lingual word embeddings. The largest dataset for BDI was generated automatically, so its quality is dubious. We study the composition and quality of the test sets for five diverse languages from this dataset, with concerning findings: (1) a quarter of the data consists of proper nouns, which can be hardly indicative of BDI performance, and (2) there are pervasive gaps in the gold-standard targets. These issues appear to affect the ranking between cross-lingual embedding systems on individual languages, and the overall degree to which the systems differ in performance. With proper nouns removed from the data, the margin between the top two systems included in the study grows from 3.4% to 17.2%. Manual verification of the predictions, on the other hand, reveals that gaps in the gold standard targets artificially inflate the margin between the two systems on English to Bulgarian BDI from 0.1% to 6.7%. We thus suggest that future research either avoids drawing conclusions from quantitative results on this BDI dataset, or accompanies such evaluation with rigorous error analysis.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3336,3341,,,,,,,,,,,,,,,,WOS:000854193303046,0
C,"Liu, LL; Lin, X; Joty, S; Han, SM; Bing, LD",,,Assoc Computat Linguist,"Liu, Linlin; Lin, Xiang; Joty, Shafiq; Han, Simeng; Bing, Lidong",,,Hierarchical Pointer Net Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1007,1017,,,,,,,,,,,,,,,,WOS:000854193301015,0
C,"Liu, ZB; Niu, ZY; Wu, H; Wang, HF",,,Assoc Computat Linguist,"Liu, Zhibin; Niu, Zheng-Yu; Wu, Hua; Wang, Haifeng",,,Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Two types of knowledge, triples from knowledge graphs and texts from documents, have been studied for knowledge aware opendomain conversation generation, in which graph paths can narrow down vertex candidates for knowledge selection decision, and texts can provide rich information for response generation. Fusion of a knowledge graph and texts might yield mutually reinforcing advantages, but there is less study on that. To address this challenge, we propose a knowledge aware chatting machine with three components, an augmented knowledge graph with both triples and texts, knowledge selector, and knowledge aware response generator. For knowledge selection on the graph, we formulate it as a problem of multi-hop graph reasoning to effectively capture conversation flow, which is more explainable and flexible in comparison with previous work. To fully leverage long text information that differentiates our graph from others, we improve a state of the art reasoning algorithm with machine reading comprehension technology. We demonstrate the effectiveness of our system on two datasets in comparison with state-of-the-art models(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1782,1792,,,,,,,,,,,,,,,,WOS:000854193301109,0
C,"Srivastava, S; Labutov, I; Mitchell, T",,,Assoc Computat Linguist,"Srivastava, Shashank; Labutov, Igor; Mitchell, Tom",,,Learning to Ask for Conversational Machine Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Natural language has recently been increasingly explored as a medium of supervision for training machine learning models. Here, we explore learning classification tasks using language in a conversational setting - where the automated learner does not simply receive language input from a teacher, but can proactively engage the teacher by asking template-based questions. We experiment with a reinforcement learning framework, where the learner's actions correspond to question types and the reward for asking a question is based on how the teacher's response changes performance of the resulting machine learning model on the learning task. In this framework, learning good question-asking strategies corresponds to asking sequences of questions that maximize the cumulative (discounted) reward, and hence quickly lead to effective classifiers. Empirical analysis shows that learned question-asking strategies can expedite classifier training by asking appropriate questions at different points in the learning process. The approach allows learning using a blend of strategies, including learning from observations, explanations and clarifications.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4164,4174,,,,,,,,,,,,,,,,WOS:000854193304031,0
C,"Tang, HY; Li, M; Jin, BH",,,Assoc Computat Linguist,"Tang, Hongyin; Li, Miao; Jin, Beihong",,,A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text generation is among the most fundamental tasks in natural language processing. In this paper, we propose a text generation model that learns semantics and structural features simultaneously. This model captures structural features by a sequential variational autoencoder component and leverages a topic modeling component based on a Gaussian distribution to enhance the recognition of text semantics. To make the reconstructed text more coherent to the topics, the model further adapts the encoder of the topic modeling component for a discriminator. The results of experiments over several datasets demonstrate that our model outperforms several states of the art models in terms of text perplexity and topic coherence. Moreover, the latent representations learned by our model is superior to others in a text classification task. Finally, given the input texts, our model can generate meaningful texts which hold similar structures but under different topics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5090,5099,,,,,,,,,,,,,,,,WOS:000854193305026,0
C,"Zhang, Z; Singh, MP",,,Assoc Computat Linguist,"Zhang, Zhe; Singh, Munindar P.",,,Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. We posit that structural and semantic correspondence is both prevalent in opinionated text, especially when associated with attributes, and crucial in accurately revealing its latent aspect and sentiment structure. However, it is not recognized by existing approaches. We propose Trait, an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. To this end, Trait infers and leverages structural and semantic correspondence using a Markov Random Field. We show empirically that by incorporating attributes explicitly Trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions, as shown via visualization, and yielding topics of greater semantic cohesion.",,,,,"Singh, Munindar/U-3881-2019","Singh, Munindar/0000-0003-3599-3893",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5528,5538,,,,,,,,,,,,,,,,WOS:000854193305068,0
C,"Agrawal, S; Carpuat, M",,,Assoc Computat Linguist,"Agrawal, Sweta; Carpuat, Marine",,,Controlling Text Complexity in Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence-to-sequence models that translate Spanish into English targeted at an easier reading grade level than the original Spanish. We show that these multitask models outperform pipeline approaches that translate and simplify text independently.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1549,1564,,,,,,,,,,,,,,,,WOS:000854193301088,0
C,"Ben Noach, M; Goldberg, Y",,,Assoc Computat Linguist,"Ben Noach, Matan; Goldberg, Yoav",,,Transfer Learning Between Related Tasks Using Expected Label Proportions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspectbased Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERTbased Aspect-based Sentiment model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,31,42,,,,,,,,,,,,,,,,WOS:000854193300004,0
C,"Chang, JP; Danescu-Niculescu-Mizil, C",,,Assoc Computat Linguist,"Chang, Jonathan P.; Danescu-Niculescu-Mizil, Cristian",,,Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Online discussions often derail into toxic exchanges between participants. Recent efforts mostly focused on detecting antisocial behavior after the fact, by analyzing single comments in isolation. To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic. This means modeling derailment as an emerging property of a conversation rather than as an isolated utterance-level event. Forecasting emerging conversational properties, however, poses several inherent modeling challenges. First, since conversations are dynamic, a forecasting model needs to capture the flow of the discussion, rather than properties of individual comments. Second, real conversations have an unknown horizon: they can end or derail at any time; thus a practical forecasting model needs to assess the risk in an online fashion, as the conversation develops. In this work we introduce a conversational forecasting model that learns an unsupervised representation of conversational dynamics and exploits it to predict future derailment as the conversation develops. By applying this model to two new diverse datasets of online conversations with labels for antisocial events, we show that it outperforms state-of-the-art systems at forecasting derailment.",,,,,,"Chang, Jonathan/0000-0002-3952-5475",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4743,4754,,,,,,,,,,,,,,,,WOS:000854193304085,0
C,"Chang, S; McKeown, K",,,Assoc Computat Linguist,"Chang, Serina; McKeown, Kathleen",,,Automatically Inferring Gender Associations from Language,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings - one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5746,5752,,,,,,,,,,,,,,,,WOS:000854193305092,0
C,"Collins, E; Rozanov, N; Zhang, BB",,,ASSOC COMPUTAT LINGUIST,"Collins, Edward; Rozanov, Nikolai; Zhang, Bingbing",,,LIDA: Lightweight Interactive Dialogue Annotator,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dialogue systems have the potential to change how people interact with machines but are highly dependent on the quality of the data used to train them. It is therefore important to develop good dialogue annotation tools which can improve the speed and quality of dialogue data annotation. With this in mind, we introduce LIDA, an annotation tool designed specifically for conversation data. As far as we know, LIDA is the first dialogue annotation system that handles the entire dialogue annotation pipeline from raw text, as may be the output of transcription services, to structured conversation data. Furthermore it supports the integration of arbitrary machine learning models as annotation recommenders and also has a dedicated interface to resolve inter-annotator disagreements such as after crowdsourcing annotations for a dataset. LIDA is fully open source, documented and publicly available (1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,121,126,,,,,,,,,,,,,,,,WOS:000855231500021,0
C,"Hashimoto, C",,,Assoc Computat Linguist,"Hashimoto, Chikara",,,Weakly Supervised Multilingual Causality Extraction from Wikipedia,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a method for extracting causality knowledge from Wikipedia, such as Protectionism -> Trade war, where the cause and effect entities correspond to Wikipedia articles. Such causality knowledge is easy to verify by reading corresponding Wikipedia articles, to translate to multiple languages through Wikidata, and to connect to knowledge bases derived from Wikipedia. Our method exploits Wikipedia article sections that describe causality and the redundancy stemming from the multilinguality of Wikipedia. Experiments showed that our method achieved precision and recall above 98% and 64%, respectively. In particular, it could extract causalities whose cause and effect were written distantly in a Wikipedia article. We have released the code and data for further research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2988,2999,,,,,,,,,,,,,,,,WOS:000854193303014,0
C,"Huber, P; Carenini, G",,,Assoc Computat Linguist,"Huber, Patrick; Carenini, Giuseppe",,,Predicting Discourse Structure using Distant Supervision from Sentiment,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Discourse parsing could not yet take full advantage of the neural NLP revolution, mostly due to the lack of annotated datasets. We propose a novel approach that uses distant supervision on an auxiliary task (sentiment classification), to generate abundant data for RSTstyle discourse structure prediction. Our approach combines a neural variant of multipleinstance learning, using document-level supervision, with an optimal CKY-style tree generation algorithm. In a series of experiments, we train a discourse parser (for only structure prediction) on our automatically generated dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2306,2316,,,,,,,,,,,,,,,,WOS:000854193302043,0
C,"Jiang, QN; Chen, L; Xu, RF; Ao, X; Yang, M",,,Assoc Computat Linguist,"Jiang, Qingnan; Chen, Lei; Xu, Ruifeng; Ao, Xiang; Yang, Min",,,A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale MultiAspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this dataset would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new dataset show that the proposed model significantly outperforms the state-of-the-art baseline methods(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6280,6285,,,,,,,,,,,,,,,,WOS:000854193306065,0
C,"Kang, D; Hayashi, H; Black, AW; Hovy, E",,,Assoc Computat Linguist,"Kang, Dongyeop; Hayashi, Hiroaki; Black, Alan W.; Hovy, Eduard",,,Linguistic Versus Latent Relations for Modeling Coherent Flow in Paragraphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating a long, coherent text such as a paragraph requires a high-level control of different levels of relations between sentences (e.g., tense, coreference). We call such a logical connection between sentences as a (paragraph) flow. In order to produce a coherent flow of text, we explore two forms of intersentential relations in a paragraph: one is a human-created linguistical relation that forms a structure (e.g., discourse tree) and the other is a relation from latent representation learned from the sentences themselves. Our two proposed models incorporate each form of relations into document-level language models: the former is a supervised model that jointly learns a language model as well as discourse relation prediction, and the latter is an unsupervised model that is hierarchically conditioned by a recurrent neural network (RNN) over the latent information. Our proposed models with both forms of relations outperform the baselines in partially conditioned paragraph generation task. Our codes and data are publicly available(1).",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5809,5815,,,,,,,,,,,,,,,,WOS:000854193305102,0
C,"Keung, P; Lu, YC; Bhardwaj, V",,,Assoc Computat Linguist,"Keung, Phillip; Lu, Yichao; Bhardwaj, Vikas",,,Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextual word embeddings (e.g. GPT, BERT, ELMo, etc.) have demonstrated state-of-the-art performance on various NLP tasks. Recent work with the multilingual version of BERT has shown that the model performs very well in cross-lingual settings, even when only labeled English data is used to finetune the model. We improve upon multilingual BERT's zero-resource cross-lingual performance via adversarial learning. We report the magnitude of the improvement on the multilingual ML-Doc text classification and CoNLL 2002/2003 named entity recognition tasks. Furthermore, we show that language-adversarial training encourages BERT to align the embeddings of English documents and their translations, which may be the cause of the observed performance gains.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1355,1360,,,,,,,,,,,,,,,,WOS:000854193301060,0
C,"Lai, V; Cai, Z; Tan, CH",,,Assoc Computat Linguist,"Lai, Vivian; Cai, Zheng; Tan, Chenhao",,,Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use, important features from traditional models such as SVM and XGBoost are more similar with each other, than with deep learning models; 2) posthoc methods tend to generate more similar important features for two models than built-in methods. We further demonstrate how such similarity varies across instances. Notably, important features do not always resemble each other better when two models agree on the predicted label than when they disagree.",,,,,,"Tan, Chenhao/0000-0002-3981-2116",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,486,495,,,,,,,,,,,,,,,,WOS:000854193300046,0
C,"Li, J; Tao, CY; Wu, W; Feng, YS; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Li, Jia; Tao, Chongyang; Wu, Wei; Feng, Yansong; Zhao, Dongyan; Yan, Rui",,,Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We study how to sample negative examples to automatically construct a training set for effective model learning in retrieval-based dialogue systems. Following an idea of dynamically adapting negative examples to matching models in learning, we consider four strategies including minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling. Empirical studies on two benchmarks with three matching models indicate that compared with the widely used random sampling strategy, although the first two strategies lead to performance drop, the latter two ones can bring consistent improvement to the performance of all the models on both benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1291,1296,,,,,,,,,,,,,,,,WOS:000854193301050,0
C,"Li, JJ; Gaol, YF; Bing, LD; King, I; Lyul, MR",,,Assoc Computat Linguist,"Li, Jingjing; Gaol, Yifan; Bing, Lidong; King, Irwin; Lyul, Michael R.",,,Improving Question Generation With to the Point Context,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Question generation (QG) is the task of generating a question from a reference sentence and a specified answer within the sentence. A major challenge in QG is to identify answer-relevant context words to finish the declarative-to-interrogative sentence transformation. Existing sequence-to-sequence neural models achieve this goal by proximity-based answer position encoding under the intuition that neighboring words of answers are of high possibility to be answer-relevant. However, such intuition may not apply to all cases especially for sentences with complex answerrelevant relations. Consequently, the performance of these models drops sharply when the relative distance between the answer fragment and other non-stop sentence words that also appear in the ground truth question increases. To address this issue, we propose a method to jointly model the unstructured sentence and the structured answer-relevant relation (extracted from the sentence in advance) for question generation. Specifically, the structured answer-relevant relation acts as the to the point context and it thus naturally helps keep the generated question to the point, while the unstructured sentence provides the full information. Extensive experiments show that to the point context helps our question generation model achieve significant improvements on several automatic evaluation metrics. Furthermore, our model is capable of generating diverse questions for a sentence which conveys multiple relations of its answer fragment.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3216,3226,,,,,,,,,,,,,,,,WOS:000854193303035,0
C,"Morio, G; Egawa, R; Fujita, K",,,Assoc Computat Linguist,"Morio, Gaku; Egawa, Ryo; Fujita, Katsuhide",,,Revealing and Predicting Online Persuasion Strategy with Elementary Units,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In online arguments, identifying how users construct their arguments to persuade others is important in order to understand a persuasive strategy directly. However, existing research lacks empirical investigations on highly semantic aspects of elementary units (EUs), such as propositions for a persuasive online argument. Therefore, this paper focuses on a pilot study, revealing a persuasion strategy using EUs. Our contributions are as follows: (1) annotating five types of EUs in a persuasive forum, the so-called ChangeMyView, (2) revealing both intuitive and non-intuitive strategic insights for the persuasion by analyzing 4612 annotated EUs, and (3) proposing baseline neural models that identify the EU boundary and type. Our observations imply that EUs definitively characterize online persuasion strategies. The annotated dataset, annotation guideline, and implementation of the neural model are available in public.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6274,6279,,,,,,,,,,,,,,,,WOS:000854193306064,0
C,"Pezzelle, S; Fernandez, R",,,Assoc Computat Linguist,"Pezzelle, Sandro; Fernandez, Raquel",,,Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This work aims at modeling how the meaning of gradable adjectives of size ('big', 'small') can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is 'big' or 'small' in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as 'fixed' attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2865,2876,,,,,,,,,,,,,,,,WOS:000854193303003,0
C,"Qi, P; Lin, XW; Mehr, L; Wang, ZJ; Manning, CD",,,Assoc Computat Linguist,"Qi, Peng; Lin, Xiaowen; Mehr, Leo; Wang, Zijian; Manning, Christopher D.",,,Answering Complex Open-domain Questions Through Iterative Query Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"It is challenging for current one-step retrieve-and-read question answering (QA) systems to answer questions like Which novel by the author of `Armada' will be adapted as a feature film by Steven Spielberg? because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GOLDEN (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GOLDEN Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GOLDEN Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GOLDEN Retriever on the recently proposed open-domain multi-hop QA dataset, HOTPOTQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2590,2602,,,,,,,,,,,,,,,,WOS:000854193302069,0
C,"Sarma, PK; Liang, YY; Sethares, WA",,,Assoc Computat Linguist,"Sarma, Prathusha K.; Liang, Yingyu; Sethares, William A.",,,Shallow Domain Adaptive Embeddings for Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper proposes a way to improve the performance of existing algorithms for text classification in domains with strong language semantics. We propose a domain adaptation layer learns weights to combine a generic and a domain specific (DS) word embedding into a domain adapted (DA) embedding. The DA word embeddings are then used as inputs to a generic encoder + classifier framework to perform a downstream task such as classification. This adaptation layer is particularly suited to datasets that are modest in size, and which are, therefore, not ideal candidates for (re)training a deep neural network architecture. Results on binary and multi-class classification tasks using popular encoder architectures, including current state-of-the-art methods (with and without the shallow adaptation layer) show the effectiveness of the proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5549,5558,,,,,,,,,,,,,,,,WOS:000854193305070,0
C,"Shi, WY; Qian, K; Wang, XW; Yu, Z",,,Assoc Computat Linguist,"Shi, Weiyan; Qian, Kun; Wang, Xuewei; Yu, Zhou",,,How to Build User Simulators to Train RL-based Dialog Systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the simulator directly impacts the RL policy. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these simulators both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1990,2000,,,,,,,,,,,,,,,,WOS:000854193302014,0
C,"Sneyd, A; Stevenson, M",,,Assoc Computat Linguist,"Sneyd, Alison; Stevenson, Mark",,,Modelling Stopping Criteria for Search Results using Poisson Processes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text retrieval systems often return large sets of documents, particularly when applied to large collections. Stopping criteria can reduce the number of these documents that need to be manually evaluated for relevance by predicting when a suitable level of recall has been achieved. In this work, a novel method for determining a stopping criterion is proposed that models the rate at which relevant documents occur using a Poisson process. This method allows a user to specify both a minimum desired level of recall to achieve and a desired probability of having achieved it. We evaluate our method on a public dataset and compare it with previous techniques for determining stopping criteria.",,,,,,"Stevenson, Mark/0000-0002-9483-6006",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3484,3489,,,,,,,,,,,,,,,,WOS:000854193303069,0
C,"Sun, SQ; Cheng, Y; Gan, Z; Liu, JJ",,,Assoc Computat Linguist,"Sun, Siqi; Cheng, Yu; Gan, Zhe; Liu, Jingjing",,,Patient Knowledge Distillation for BERT Model Compression,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multilayer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4323,4332,,,,,,,,,,,,,,,,WOS:000854193304046,0
C,"Takanobu, R; Zhu, HL; Huang, ML",,,Assoc Computat Linguist,"Takanobu, Ryuichi; Zhu, Hanlin; Huang, Minlie",,,Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dialog policy decides what and how a taskoriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply Reinforcement Learning to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the reward signal and infers the user goal in the dialog sessions. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,100,110,,,,,,,,,,,,,,,,WOS:000854193300010,0
C,"Wu, RD; Yao, Y; Han, X; Xie, RB; Liu, ZY; Lin, F; Lin, LY; Sun, MS",,,Assoc Computat Linguist,"Wu, Ruidong; Yao, Yuan; Han, Xu; Xie, Ruobing; Liu, Zhiyuan; Lin, Fen; Lin, Leyu; Sun, Maosong",,,Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervised Data,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Open relation extraction (OpenRE) aims to extract relational facts from the open-domain corpus. To this end, it discovers relation patterns between named entities and then clusters those semantically equivalent patterns into a united relation cluster. Most OpenRE methods typically confine themselves to unsupervised paradigms, without taking advantage of existing relational facts in knowledge bases (KBs) and their high-quality labeled instances. To address this issue, we propose Relational Siamese Networks (RSNs) to learn similarity metrics of relations from labeled data of pre-defined relations, and then transfer the relational knowledge to identify novel relations in unlabeled data. Experiment results on two real-world datasets show that our framework can achieve significant improvements as compared with other state-of-the-art methods. Our code is available at https://github. com/thunlp/RSN.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,219,228,,,,,,,,,,,,,,,,WOS:000854193300021,0
C,"Xin, J; Lin, J; Yu, YL",,,Assoc Computat Linguist,"Xin, Ji; Lin, Jimmy; Yu, Yaoliang",,,What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel metric to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed metric. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall accuracy of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5823,5830,,,,,,,,,,,,,,,,WOS:000854193306002,0
C,"Yang, PC; Lin, JY; Xu, JJ; Xie, J; Sun, X; Su, Q",,,Assoc Computat Linguist,"Yang, Pengcheng; Lin, Junyang; Xu, Jingjing; Xie, Jun; Sun, Xu; Su, Qi",,,Specificity-Driven Cascading Approach for Unsupervised Sentiment Modification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11% and 38% relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5508,5517,,,,,,,,,,,,,,,,WOS:000854193305066,0
C,"Yang, YF; Zhang, Y; Tar, C; Baldridge, J",,,Assoc Computat Linguist,"Yang, Yinfei; Zhang, Yuan; Tar, Chris; Baldridge, Jason",,,PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) (Zhang et al., 2019) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT (Devlin et al., 2019) fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3687,3692,,,,,,,,,,,,,,,,WOS:000854193303100,0
C,"Yang, Z; Wu, W; Yang, J; Xu, C; Li, ZJ",,,Assoc Computat Linguist,"Yang, Ze; Wu, Wei; Yang, Jian; Xu, Can; Li, Zhoujun",,,Low-Resource Response Generation with Template Prior,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We study open domain response generation with limited message-response pairs. The problem exists in real-world applications but is less explored by the existing work. Since the paired data now is no longer enough to train a neural generation model, we consider leveraging the large scale of unpaired data that are much easier to obtain, and propose response generation with both paired and unpaired data. The generation model is defined by an encoder-decoder architecture with templates as prior, where the templates are estimated from the unpaired data as a neural hidden semi-markov model. By this means, response generation learned from the small paired data can be aided by the semantic and syntactic knowledge in the large unpaired data. To balance the effect of the prior and the input message to response generation, we propose learning the whole generation model with an adversarial approach. Empirical studies on question response generation and sentiment response generation indicate that when only a few pairs are available, our model can significantly outperform several state-of-the-art response generation models in terms of both automatic and human evaluation.",,,,,,"Li, Zhoujun/0000-0002-9603-9713",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1886,1897,,,,,,,,,,,,,,,,WOS:000854193302005,0
C,"Balkir, E; Naslidnyk, M; Palfrey, D; Mittal, A",,,Assoc Computat Linguist,"Balkir, Esma; Naslidnyk, Masha; Palfrey, Dave; Mittal, Arpit",,,Using Pairwise Occurrence Information to Improve Knowledge Graph Completion on Large-Scale Datasets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Bilinear models such as DistMult and ComplEx are effective methods for knowledge graph (KG) completion. However, they require large batch sizes, which becomes a performance bottleneck when training on large scale datasets due to memory constraints. In this paper we use occurrences of entity-relation pairs in the dataset to construct a joint learning model and to increase the quality of sampled negatives during training. We show on three standard datasets that when these two techniques are combined, they give a significant improvement in performance, especially when the batch size and the number of generated negative examples are low relative to the size of the dataset. We then apply our techniques to a dataset containing 2 million entities and demonstrate that our model outperforms the baseline by 2.8% absolute on hits@1.",,,,,,"Naslidnyk, Masha/0000-0001-9103-2013",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3591,3596,,,,,,,,,,,,,,,,WOS:000854193303086,0
C,"Cai, D; Lam, W",,,Assoc Computat Linguist,"Cai, Deng; Lam, Wai",,,Core Semantic First: A Top-down Approach for AMR Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The core semantic first principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3799,3809,,,,,,,,,,,,,,,,WOS:000854193303111,0
C,"Dankers, V; Rei, M; Lewis, M; Shutova, E",,,Assoc Computat Linguist,"Dankers, Verna; Rei, Marek; Lewis, Martha; Shutova, Ekaterina",,,Modelling the interplay of metaphor and emotion through multitask learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Metaphors allow us to convey emotion by connecting physical experiences and abstract concepts. The results of previous research in linguistics and psychology suggest that metaphorical phrases tend to be more emotionally evocative than their literal counterparts. In this paper, we investigate the relationship between metaphor and emotion within a computational framework, by proposing the first joint model of these phenomena. We experiment with several multitask learning architectures for this purpose, involving both hard and soft parameter sharing. Our results demonstrate that metaphor identification and emotion prediction mutually benefit from joint learning and our models advance the state of the art in both of these tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2218,2229,,,,,,,,,,,,,,,,WOS:000854193302035,0
C,"Du, N; Wang, MQ; Tran, L; Li, G; Shafran, I",,,Assoc Computat Linguist,"Du, Nan; Wang, Mingqiu; Linh Tran; Li, Gang; Shafran, Izhak",,,"Learning to Infer Entities, Properties and their Relations from Clinical Conversations",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently we proposed the Span Attribute Tagging (SAT) Model (Du et al., 2019) to infer clinical entities (e.g., symptoms) and their properties (e.g., duration). It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. We extend the SAT model to jointly infer not only entities and their properties but also relations between them. Most relation extraction models restrict inferring relations between tokens within a few neighboring sentences, mainly to avoid high computational complexity. In contrast, our proposed Relation-SAT (R-SAT) model is computationally efficient and can infer relations over the entire conversation, spanning an average duration of 10 minutes. We evaluate our model on a corpus of clinical conversations. When the entities are given, the R-SAT outperforms baselines in identifying relations between symptoms and their properties by about 32% (0.82 vs 0.62 F-score) and by about 50% (0.60 vs 0.41 F-score) on medications and their properties. On the more difficult task of jointly inferring entities and relations, the R-SAT model achieves a performance of 0.34 and 0.45 for symptoms and medications respectively, which is significantly better than 0.18 and 0.35 for the baseline model. The contributions of different components of the model are quantified using ablation analysis.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4979,4990,,,,,,,,,,,,,,,,WOS:000854193305016,0
C,"Hao, J; Wang, X; Shi, SM; Zhang, JF; Tu, ZP",,,Assoc Computat Linguist,"Hao, Jie; Wang, Xing; Shi, Shuming; Zhang, Jinfeng; Tu, Zhaopeng",,,Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks (RNNs) outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs - Ordered Neurons LSTM (ON-LSTM, Shen et al., 2019), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual architectures and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1336,1341,,,,,,,,,,,,,,,,WOS:000854193301057,0
C,"Hasan, MK; Rahman, W; Zadeh, A; Zhong, JY; Tanveer, MI; Morency, LP; Hoque, M",,,Assoc Computat Linguist,"Hasan, Md Kamrul; Rahman, Wasifur; Zadeh, Amir; Zhong, Jianyuan; Tanveer, Md Iftekhar; Morency, Louis-Philippe; Hoque, Mohammed (Ehsan)",,,UR-FUNNY: A Multimodal Language Dataset for Understanding Humor,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Humor is a unique and creative communicative behavior often displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it has been understudied. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2046,2056,,,,,,,,,,,,,,,,WOS:000854193302019,0
C,"Kang, D; Balakrishnan, A; Shah, P; Crook, P; Boureau, YL; Weston, J",,,Assoc Computat Linguist,"Kang, Dongyeop; Balakrishnan, Anusha; Shah, Pararth; Crook, Paul; Boureau, Y-Lan; Weston, Jason",,,Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Traditional recommendation systems produce static rather than interactive recommendations invariant to a user's specific requests, clarifications, or current mood, and can suffer from the cold-start problem if their tastes are unknown. These issues can be alleviated by treating recommendation as an interactive dialogue task instead, where an expert recommender can sequentially ask about someone's preferences, react to their requests, and recommend more appropriate items. In this work, we collect a goal-driven recommendation dialogue dataset (GoRecDial), which consists of 9,125 dialogue games and 81,260 conversation turns between pairs of human workers recommending movies to each other. The task is specifically designed as a cooperative game between two players working towards a quantifiable common goal. We leverage the dataset to develop an end-to-end dialogue system that can simultaneously converse and recommend. Models are first trained to imitate the behavior of human players without considering the task goal itself (supervised training). We then fine-tune our models on simulated bot-bot conversations between two paired pre-trained models (bot-play), in order to achieve the dialogue goal. Our experiments show that models fine-tuned with bot-play learn improved dialogue strategies, reach the dialogue goal more often when paired with a human, and are rated as more consistent by humans compared to models trained without bot-play. The dataset and code are publicly available through the ParlAI framework(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1951,1961,,,,,,,,,,,,,,,,WOS:000854193302011,0
C,"Kharitonov, E; Chaabouni, R; Bouchacourt, D; Baroni, M",,,ASSOC COMPUTAT LINGUIST,"Kharitonov, Eugene; Chaabouni, Rahma; Bouchacourt, Diane; Baroni, Marco",,,EGG: a toolkit for research on Emergence of lanGuage in Games,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG's modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,55,60,,,,,,,,,,,,,,,,WOS:000855231500010,0
C,"Kumar, S; Wintner, S; Smith, NA; Tsvetkov, Y",,,Assoc Computat Linguist,"Kumar, Sachin; Wintner, Shuly; Smith, Noah A.; Tsvetkov, Yulia",,,Topics to Avoid: Demoting Latent Confounds in Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author's native language is Swedish). We propose a method that represents the latent topical confounds and a model which unlearns confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4153,4163,,,,,,,,,,,,,,,,WOS:000854193304030,0
C,"Kumar, S; Garg, S; Mehta, K; Rasiwasia, N",,,Assoc Computat Linguist,"Kumar, Sawan; Garg, Shweta; Mehta, Kartik; Rasiwasia, Nikhil",,,Improving Answer Selection and Answer Triggering using Hard Negatives,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we establish the effectiveness of using hard negatives, coupled with a siamese network and a suitable loss function, for the tasks of answer selection and answer triggering. We show that the choice of sampling strategy is key for achieving improved performance on these tasks. Evaluating on recent answer selection datasets - InsuranceQA, SelQA, and an internal QA dataset, we show that using hard negatives with relatively simple model architectures (bag of words and LSTM-CNN) drives significant performance gains. On InsuranceQA, this strategy alone improves over previously reported results by a minimum of 1.6 points in P@1. Using hard negatives with a Transformer encoder provides a further improvement of 2.3 points. Further, we propose to use quadruplet loss for answer triggering, with the aim of producing globally meaningful similarity scores. We show that quadruplet loss function coupled with the selection of hard negatives enables bag-of-words models to improve F1 score by 2.3 points over previous baselines, on SelQA answer triggering dataset. Our results provide key insights into answer selection and answer triggering tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5911,5917,,,,,,,,,,,,,,,,WOS:000854193306015,0
C,"Lee, K; Park, S; Han, H; Yeo, J; Hwang, SW; Lee, J",,,Assoc Computat Linguist,"Lee, Kyungjae; Park, Sunghyun; Han, Hojae; Yeo, Jinyoung; Hwang, Seung-won; Lee, Juho",,,Learning with Limited Data for Multilingual Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper studies the problem of supporting question answering in a new language with limited training resources. As an extreme scenario, when no such resource exists, one can (1) transfer labels from another language, and (2) generate labels from unlabeled data, using translator and automatic labeling function respectively. However, these approaches inevitably introduce noises to the training data, due to translation or generation errors, which require a judicious use of data with varying confidence. To address this challenge, we propose a weakly-supervised framework that quantifies such noises from automatically generated labels, to deemphasize or fix noisy data in training. On reading comprehension task, we demonstrate the effectiveness of our model on low-resource languages with varying similarity to English, namely, Korean and French.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2840,2850,,,,,,,,,,,,,,,,WOS:000854193303001,0
C,"Lignos, C; Cohen, D; Lien, YC; Mehta, P; Croft, WB; Miller, S",,,Assoc Computat Linguist,"Lignos, Constantine; Cohen, Daniel; Lien, Yen-Chieh; Mehta, Pratik; Croft, W. Bruce; Miller, Scott",,,The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"When performing cross-language information retrieval (CLIR) for lower-resourced languages, a common approach is to retrieve over the output of machine translation (MT). However, there is no established guidance on how to optimize the resulting MT-IR system. In this paper, we examine the relationship between the performance of MT systems and both neural and term frequency-based IR models to identify how CLIR performance can be best predicted from MT quality. We explore performance at varying amounts of MT training data, byte pair encoding (BPE) merge operations, and across two IR collections and retrieval models. We find that the choice of IR collection can substantially affect the predictive power of MT tuning decisions and evaluation, potentially introducing dissociations between MT-only and overall CLIR performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3497,3502,,,,,,,,,,,,,,,,WOS:000854193303071,0
C,"Liu, YC; Li, QZ; Cifor, M; Liu, XZ; Zhang, Q; Si, L",,,Assoc Computat Linguist,"Liu, Yingchi; Li, Quanzhi; Cifor, Marika; Liu, Xiaozhong; Zhang, Qiong; Si, Luo",,,Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The number of personal stories about sexual harassment shared online has increased exponentially in recent years. This is in part inspired by the #MeToo and #TimesUp movements. Safecity is an online forum for people who experienced or witnessed sexual harassment to share their personal experiences. It has collected >10,000 stories so far. Sexual harassment occurred in a variety of situations, and categorization of the stories and extraction of their key elements will provide great help for the related parties to understand and address sexual harassment. In this study, we manually annotated those stories with labels in the dimensions of location, time, and harassers' characteristics, and marked the key elements related to these dimensions. Furthermore, we applied natural language processing technologies with joint learning schemes to automatically categorize these stories in those dimensions and extract key elements at the same time. We also uncovered significant patterns from the categorized sexual harassment stories. We believe our annotated data set, proposed algorithms, and analysis will help people who have been harassed, authorities, researchers and other related parties in various ways, such as automatically filling reports, enlightening the public in order to prevent future harassment, and enabling more effective, faster action to be taken.",,,,,,"Cifor, Marika/0000-0002-1645-7158",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2328,2337,,,,,,,,,,,,,,,,WOS:000854193302045,0
C,"Neekhara, P; Hussain, S; Dubnov, S; Koushanfar, F",,,Assoc Computat Linguist,"Neekhara, Paarth; Hussain, Shehzeen; Dubnov, Shlomo; Koushanfar, Farinaz",,,Adversarial Reprogramming of Text Classification Neural Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the network architecture or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model's architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.",,,,,,"Koushanfar, Farinaz/0000-0003-0798-3794",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5216,5225,,,,,,,,,,,,,,,,WOS:000854193305038,0
C,"Paz-Argaman, T; Tsarfaty, R",,,Assoc Computat Linguist,"Paz-Argaman, Tzuf; Tsarfaty, Reut",,,RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Following navigation instructions in natural language requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We propose a strong baseline for the task and empirically investigate which aspects of the neural architecture are important for the RUN success. Our results empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6449,6455,,,,,,,,,,,,,,,,WOS:000854193306092,0
C,"Shao, ZH; Huang, ML; Wen, JT; Xu, WF; Zhu, XY",,,Assoc Computat Linguist,"Shao, Zhihong; Huang, Minlie; Wen, Jiangtao; Xu, Wenfei; Zhu, Xiaoyan",,,Long and Diverse Text Generation with Planning-based Hierarchical Variational Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing neural methods for data-to-text generation are still struggling to produce long and diverse texts: they are insufficient to model input data dynamically during generation, to capture inter-sentence coherence, or to generate diversified expressions. To address these issues, we propose a Planning-based Hierarchical Variational Model (PHVM). Our model first plans a sequence of groups (each group is a subset of input items to be covered by a sentence) and then realizes each sentence conditioned on the planning result and the previously generated context, thereby decomposing long text generation into dependent sentence generation sub-tasks. To capture expression diversity, we devise a hierarchical latent structure where a global planning latent variable models the diversity of reasonable planning and a sequence of local latent variables controls sentence realization. Experiments show that our model outperforms state-of-the-art baselines in long and diverse text generation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3257,3268,,,,,,,,,,,,,,,,WOS:000854193303039,0
C,"Shen, LX; Zou, BW; Hong, Y; Zhou, GD; Zhu, QM; Aw, AT",,,Assoc Computat Linguist,"Shen, Longxiang; Zou, Bowei; Hong, Yu; Zhou, Guodong; Zhu, Qiaoming; Aw, Ai Ti",,,Negative Focus Detection via Contextual Attention Mechanism,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture contextual information for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model contextual information. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM'12 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11% over the state-of-the-art. This demonstrates the great effectiveness of the two types of contextual attention mechanisms.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2251,2261,,,,,,,,,,,,,,,,WOS:000854193302038,0
C,"Subramanian, S; Cohn, T; Baldwin, T",,,Assoc Computat Linguist,"Subramanian, Shivashankar; Cohn, Trevor; Baldwin, Timothy",,,Deep Ordinal Regression for Pledge Specificity Prediction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many pledges are made in the course of an election campaign, forming important corpora for political analysis of campaign strategy and governmental accountability. At present, there are no publicly available annotated datasets of pledges, and most political analyses rely on manual analysis. In this paper we collate a novel dataset of manifestos from eleven Australian federal election cycles, with over 12,000 sentences annotated with specificity (e.g., rhetorical vs. detailed pledge) on a fine-grained scale. We propose deep ordinal regression approaches for specificity prediction, under both supervised and semi-supervised settings, and provide empirical results demonstrating the effectiveness of the proposed techniques over several baseline approaches. We analyze the utility of pledge specificity modeling across a spectrum of policy issues in performing ideology prediction, and further provide qualitative analysis in terms of capturing party-specific issue salience across election cycles.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1729,1740,,,,,,,,,,,,,,,,WOS:000854193301104,0
C,"Suntwal, S; Paul, M; Sharp, R; Surdeanu, M",,,Assoc Computat Linguist,"Suntwal, Sandeep; Paul, Mithun; Sharp, Rebecca; Surdeanu, Mihai",,,On the Importance of Delexicalization for Fact Verification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While neural networks produce state-of-the-art performance in many NLP tasks, they generally learn from lexical information, which may transfer poorly between domains. Here, we investigate the importance that a model assigns to various aspects of data while learning and making predictions, specifically, in a recognizing textual entailment (RTE) task. By inspecting the attention weights assigned by the model, we confirm that most of the weights are assigned to noun phrases. To mitigate this dependence on lexicalized information, we experiment with two strategies of masking. First, we replace named entities with their corresponding semantic tags along with a unique identifier to indicate lexical overlap between claim and evidence. Second, we similarly replace other word classes in the sentence (nouns, verbs, adjectives, and adverbs) with their super sense tags (Ciaramita and Johnson, 2003). Our results show that, while performance on the in-domain dataset remains on par with that of the model trained on fully lexicalized data, it improves considerably when tested out of domain. For example, the performance of a state-of-the-art RTE model trained on the masked Fake News Challenge (Pomerleau and Rao, 2017) data and evaluated on Fact Extraction and Verification (Thorne et al., 2018) data improved by over 10% in accuracy score compared to the fully lexicalized model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3413,3418,,,,,,,,,,,,,,,,WOS:000854193303058,0
C,"Tan, H; Bansal, M",,,Assoc Computat Linguist,"Tan, Hao; Bansal, Mohit",,,LXMERT: Learning Cross-Modality Encoder Representations from Transformers,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pretraining strategies significantly contribute to our strong results.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5100,5111,,,,,,,,,,,,,,,,WOS:000854193305027,0
C,"Tebbifakhr, A; Bentivogli, L; Negri, M; Turchi, M",,,Assoc Computat Linguist,"Tebbifakhr, Amirhossein; Bentivogli, Luisa; Negri, Matteo; Turchi, Marco",,,Machine Translation for Machines: the Sentiment Classification Use Case,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a neural machine translation (NMT) approach that, instead of pursuing adequacy and fluency (human-oriented quality criteria), aims to generate translations that are best suited as input to a natural language processing component designed for a specific downstream task (a machine-oriented criterion). Towards this objective, we present a reinforcement learning technique based on a new candidate sampling strategy, which exploits the results obtained on the downstream task as weak feedback. Experiments in sentiment classification of Twitter data in German and Italian show that feeding an English classifier with machine-oriented translations significantly improves its performance. Classification results outperform those obtained with translations produced by general-purpose NMT models as well as by an approach based on reinforcement learning. Moreover, our results on both languages approximate the classification accuracy computed on gold standard English tweets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1368,1374,,,,,,,,,,,,,,,,WOS:000854193301062,0
C,"Tikhonov, A; Shibaev, V; Nagaev, A; Nugmanova, A; Yamshchikov, IP",,,Assoc Computat Linguist,"Tikhonov, Alexey; Shibaev, Viacheslav; Nagaev, Aleksander; Nugmanova, Aigul; Yamshchikov, Ivan P.",,,"Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper shows that standard assessment methodology for style transfer has several significant problems. First, the standard metrics for style accuracy and semantics preservation vary significantly on different re-runs. Therefore one has to report error margins for the obtained results. Second, starting with certain values of bilingual evaluation understudy (BLEU) between input and output and accuracy of the sentiment transfer the optimization of these two standard metrics diverge from the intuitive goal of the style transfer task. Finally, due to the nature of the task itself, there is a specific dependence between these two metrics that could be easily manipulated. Under these circumstances, we suggest taking BLEU between input and human-written reformulations into consideration for benchmarks. We also propose three new architectures that outperform state of the art in terms of this metric.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3936,3945,,,,,,,,,,,,,,,,WOS:000854193304011,0
C,"Wang, HZ; Gan, Z; Liu, XD; Liu, JJ; Gao, JF; Wang, HN",,,Assoc Computat Linguist,"Wang, Huazheng; Gan, Zhe; Liu, Xiaodong; Liu, Jingjing; Gao, Jianfeng; Wang, Hongning",,,Adversarial Domain Adaptation for Machine Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we focus on unsupervised domain adaptation for Machine Reading Comprehension (MRC), where the source domain has a large amount of labeled data, while only unlabeled passages are available in the target domain. To this end, we propose an Adversarial Domain Adaptation framework (AdaMRC), where (i) pseudo questions are first generated for unlabeled passages in the target domain, and then (ii) a domain classifier is incorporated into an MRC model to predict which domain a given passage-question pair comes from. The classifier and the passagequestion encoder are jointly trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semisupervised learning.",,,,,,"Wang, Huazheng/0000-0003-3918-6925",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2510,2520,,,,,,,,,,,,,,,,WOS:000854193302062,0
C,"Yang, W; Yoon, S; Carpenter, A; Park, JC",,,Assoc Computat Linguist,"Yang, Wonsuk; Yoon, Seungwon; Carpenter, Ada; Park, Jong C.",,,Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Annotation quality control is a critical aspect for building reliable corpora through linguistic annotation. In this study, we present a simple but powerful quality control method using two-step reason selection. We gathered sentential annotations of local acceptability and three related attributes through a crowdsourcing platform. For each attribute, the reason for the choice of the attribute value is selected in a two-step manner. The options given for reason selection were designed to facilitate the detection of a nonsensical reason selection. We assume that a reliable annotation may not contain a nonsensical reason selected for the choice of the attribute value, and an annotation that contains a nonsensical reason is less reliable than the one without such reason. Our method, based solely on this assumption, is found to retain the annotations with remarkable quality out of the entire annotationsmixed with those of low quality.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2954,2963,,,,,,,,,,,,,,,,WOS:000854193303011,0
C,"Yu, B; Li, YY; Wang, J",,,Assoc Computat Linguist,"Yu, Bei; Li, Yingya; Wang, Jun",,,Detecting Causal Language Use in Science Findings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Causal interpretation of correlational findings from observational studies has been a major type of misinformation in science communication. Prior studies on identifying inappropriate use of causal language relied on manual content analysis, which is not scalable for examining a large volume of science publications. In this study, we first annotated a corpus of over 3,000 PubMed research conclusion sentences, then developed a BERT-based prediction model that classifies conclusion sentences into no relationship, correlational, conditional causal, and direct causal categories, achieving an accuracy of 0.90 and a macroF1 of 0.88. We then applied the prediction model to measure the causal language use in the research conclusions of about 38,000 observational studies in PubMed. The prediction result shows that 21.7% studies used direct causal language exclusively in their conclusions, and 32.4% used some direct causal language. We also found that the ratio of causal language use differs among authors from different countries, challenging the notion of a shared consensus on causal language use in the global science community. Our prediction model could also be used to help identify the inappropriate use of causal language in science publications.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4664,4674,,,,,,,,,,,,,,,,WOS:000854193304078,0
C,"Zeng, M; Wang, YS; Luo, Y",,,Assoc Computat Linguist,"Zeng, Min; Wang, Yisen; Luo, Yuan",,,Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Variational encoder-decoders have achieved well-recognized performance in the dialogue generation task. Existing works simply assume the Gaussian priors of the latent variable, which are incapable of representing complex latent variables effectively. To address the issues, we propose to use the Dirichlet distribution with flexible structures to characterize the latent variables in place of the traditional Gaussian distribution, called Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that there is redundancy among the dimensions of latent variable, and the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the latent variable. Therefore, controllable responses can be generated through specifying the value of each dimension of the latent variable. Experimental results on benchmarks show that our proposed Dir-VHRED yields substantial improvements on negative log-likelihood, word-embeddingbased and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1267,1272,,,,,,,,,,,,,,,,WOS:000854193301046,0
C,"Ziegler, ZM; Deng, YT; Rush, AM",,,Assoc Computat Linguist,"Ziegler, Zachary M.; Deng, Yuntian; Rush, Alexander M.",,,Neural Linguistic Steganography,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Whereas traditional cryptography encrypts a secret message into an unintelligible form, steganography conceals that communication is taking place by encoding a secret message into a cover signal. Language is a particularly pragmatic cover signal due to its benign occurrence and independence from any one medium. Traditionally, linguistic steganography systems encode secret messages in existing text via synonym substitution or word order rearrangements. Advances in neural language models enable previously impractical generation-based techniques. We propose a steganography technique based on arithmetic coding with large-scale neural language models. We find that our approach can generate realistic looking cover sentences as evaluated by humans, while at the same time preserving security by matching the cover message distribution with the language model distribution.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1210,1215,,,,,,,,,,,,,,,,WOS:000854193301037,0
C,"An, AX; Qian, P; Wilcox, E; Levy, R",,,Assoc Computat Linguist,"An, Aixiu; Qian, Peng; Wilcox, Ethan; Levy, Roger",,,Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural language models have achieved state-of-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2888,2899,,,,,,,,,,,,,,,,WOS:000854193303005,0
C,"Awasthi, A; Sarawagi, S; Goyal, R; Ghosh, S; Piratla, V",,,Assoc Computat Linguist,"Awasthi, Abhijeet; Sarawagi, Sunita; Goyal, Rasna; Ghosh, Sabyasachi; Piratla, Vihari",,,Parallel Iterative Edit Models for Local Sequence Transduction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modelling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1. predicting edits instead of tokens, 2. labeling sequences instead of generating sequences, 3. iteratively refining predictions to capture dependencies, and 4. factorizing logits over edits and their token argument to harness pretrained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4260,4270,,,,,,,,,,,,,,,,WOS:000854193304040,0
C,"Azab, M; Dadian, S; Nastase, V; An, L; Mihalcea, R",,,Assoc Computat Linguist,"Azab, Mahmoud; Dadian, Stephane; Nastase, Vivi; An, Larry; Mihalcea, Rada",,,Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a new dataset consisting of natural language interactions annotated with medical family histories, obtained during interactions with a genetic counselor and through crowdsourcing, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and relation extraction shows promising results - average F-score of 0.87 on complex sentences on the targeted relations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1255,1260,,,,,,,,,,,,,,,,WOS:000854193301044,0
C,"Han, WJ; Wang, G; Jiang, Y; Tu, KW",,,Assoc Computat Linguist,"Han, Wenjuan; Wang, Ge; Jiang, Yong; Tu, Kewei",,,Multilingual Grammar Induction with Continuous Language Identification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The key to multilingual grammar induction is to couple grammar parameters of different languages together by exploiting the similarity between languages. Previous work relies on linguistic phylogenetic knowledge to specify similarity between languages. In this work, we propose a novel universal grammar induction approach that represents language identities with continuous vectors and employs a neural network to predict grammar parameters based on the representation. Without any prior linguistic phylogenetic knowledge, we automatically capture similarity between languages with the vector representations and softly tie the grammar parameters of different languages. In our experiments, we apply our approach to 15 languages across 8 language families and subfamilies in the Universal Dependency Treebank dataset, and we observe substantial performance gain on average over monolingual and multilingual baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5728,5733,,,,,,,,,,,,,,,,WOS:000854193305089,0
C,"He, SX; Li, ZC; Zhao, H",,,Assoc Computat Linguist,"He, Shexia; Li, Zuchao; Zhao, Hai",,,Syntax-aware Multilingual Semantic Role Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, semantic role labeling (SRL) has earned a series of success with even higher performance improvements, which can be mainly attributed to syntactic integration and enhanced word representation. However, most of these efforts focus on English, while SRL on multiple languages more than English has received relatively little attention so that is kept underdevelopment. Thus this paper intends to fill the gap on multilingual SRL with special focus on the impact of syntax and contextualized word representation. Unlike existing work, we propose a novel method guided by syntactic rule to prune arguments, which enables us to integrate syntax into multilingual SRL model simply and effectively. We present a unified SRL model designed for multiple languages together with the proposed uniform syntax enhancement. Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of all seven languages. Besides, we pose a discussion on the syntactic role among different languages and verify the effectiveness of deep enhanced representation for multilingual SRL.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5350,5359,,,,,,,,,,,,,,,,WOS:000854193305051,0
C,"Jin, Q; Dhingra, B; Liu, ZP; Cohen, WW; Lu, XH",,,Assoc Computat Linguist,"Jin, Qiao; Dhingra, Bhuwan; Liu, Zhengping; Cohen, William W.; Lu, Xinghua",,,PubMedQA: A Dataset for Biomedical Research Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.",,,,,,"Jin, Qiao/0000-0002-1268-7239",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2567,2577,,,,,,,,,,,,,,,,WOS:000854193302067,0
C,"Lalor, JP; Wu, H; Yu, H",,,Assoc Computat Linguist,"Lalor, John P.; Wu, Hao; Yu, Hong",,,Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial Crowds,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Incorporating Item Response Theory (IRT) into NLP tasks can provide valuable information about model performance and behavior. Traditionally, IRT models are learned using human response pattern (RP) data, presenting a significant bottleneck for large data sets like those required for training deep neural networks (DNNs). In this work we propose learning IRT models using RPs generated from artificial crowds of DNN models. We demonstrate the effectiveness of learning IRT models using DNN-generated data through quantitative and qualitative analyses for two NLP tasks. Parameters learned from human and machine RPs for natural language inference and sentiment analysis exhibit medium to large positive correlations. We demonstrate a use-case for latent difficulty item parameters, namely training set filtering, and show that using difficulty to sample training data outperforms baseline methods. Finally, we highlight cases where human expectation about item difficulty does not match difficulty as estimated from the machine RPs.",,,,,,"Wu, Hao/0000-0001-6471-1774; Lalor, John/0000-0003-0848-4786",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4249,4259,,,,,,,,,,,31803865.0,,,,,WOS:000854193304039,0
C,"Lertvittayakumjorn, P; Toni, F",,,Assoc Computat Linguist,"Lertvittayakumjorn, Piyawat; Toni, Francesca",,,Human-grounded Evaluations of Explanation Methods for Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Due to the black-box nature of deep learning models, methods for explaining the models' results are crucial to gain trust from humans and support collaboration between AIs and humans. In this paper, we consider several model-agnostic and model-specific explanation methods for CNNs for text classification and conduct three human-grounded evaluations, focusing on different purposes of explanations: (1) revealing model behavior, (2) justifying model predictions, and (3) helping humans investigate uncertain predictions. The results highlight dissimilar qualities of the various explanation methods we consider and show the degree to which these methods could serve for each purpose.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5195,5205,,,,,,,,,,,,,,,,WOS:000854193305036,0
C,"Li, T; Gupta, V; Mehta, M; Srikumar, V",,,Assoc Computat Linguist,"Li, Tao; Gupta, Vivek; Mehta, Maitrey; Srikumar, Vivek",,,A Logic-Driven Framework for Consistency of Neural Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While neural models show remarkable accuracy on individual predictions, their internal beliefs can be inconsistent across examples. In this paper, we formalize such inconsistency as a generalization of prediction error. We propose a learning framework for constraining models using logic rules to regularize them away from inconsistency. Our framework can leverage both labeled and unlabeled examples and is directly compatible with off-the-shelf learning schemes without model redesign. We instantiate our framework on natural language inference, where experiments show that enforcing invariants stated in logic can help make the predictions of neural models both accurate and consistent.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3924,3935,,,,,,,,,,,,,,,,WOS:000854193304010,0
C,"Li, YJ; Caragea, C",,,Assoc Computat Linguist,"Li, Yingjie; Caragea, Cornelia",,,Multi-Task Stance Detection with Sentiment and Stance Lexicons,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Stance detection aims to detect whether the opinion holder is in support of or against a given target. Recent works show improvements in stance detection by using either the attention mechanism or sentiment information. In this paper, we propose a multi-task framework that incorporates target-specific attention mechanism and at the same time takes sentiment classification as an auxiliary task. Moreover, we used a sentiment lexicon and constructed a stance lexicon to provide guidance for the attention layer. Experimental results show that the proposed model significantly outperforms state-of-the-art deep learning methods on the SemEval-2016 dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6299,6305,,,,,,,,,,,,,,,,WOS:000854193306068,0
C,"Lin, Y; Ji, H",,,Assoc Computat Linguist,"Lin, Ying; Ji, Heng",,,An Attentive Fine-Grained Entity Typing Model with Latent Type Representation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a fine-grained entity typing model with a novel attention mechanism and a hybrid type classifier. We advance existing methods in two aspects: feature extraction and type prediction. To capture richer contextual information, we adopt contextualized word representations instead of fixed word embeddings used in previous work. In addition, we propose a two-step mention-aware attention mechanism to enable the model to focus on important words in mentions and contexts. We also present a hybrid classification method beyond binary relevance to exploit type interdependency with latent type representation. Instead of independently predicting each type, we predict a low-dimensional vector that encodes latent type features and reconstruct the type vector from this latent representation. Experiment results on multiple data sets show that our model significantly advances the stateof-the-art on fine-grained entity typing, obtaining up to 6.6% and 5.5% absolute gains in macro averaged F-score and micro averaged Fscore respectively.(1)",,,,,"Lin, Ying/HGE-7388-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6197,6202,,,,,,,,,,,,,,,,WOS:000854193306052,0
C,"Razniewski, S; Jain, N; Mirza, P; Weikum, G",,,Assoc Computat Linguist,"Razniewski, Simon; Jain, Nitisha; Mirza, Paramita; Weikum, Gerhard",,,Coverage of Information Extraction from Sentences and Paragraphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Scalar implicatures are language features that imply the negation of stronger statements, e.g., She was married twice typically implicates that she was not married thrice. In this paper we discuss the importance of scalar implicatures in the context of textual information extraction. We investigate how textual features can be used to predict whether a given text segment mentions all objects standing in a certain relationship with a certain subject. Preliminary results on Wikipedia indicate that this prediction is feasible, and yields informative assessments.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5771,5776,,,,,,,,,,,,,,,,WOS:000854193305096,0
C,"Ribeiro, LFR; Gardent, C; Gurevych, I",,,Assoc Computat Linguist,"Ribeiro, Leonardo F. R.; Gardent, Claire; Gurevych, Iryna",,,Enhancing AMR-to-Text Generation with Dual Graph Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3183,3194,,,,,,,,,,,,,,,,WOS:000854193303032,0
C,"Rosales-Mendez, H; Hogan, A; Poblete, B",,,Assoc Computat Linguist,"Rosales-Mendez, Henry; Hogan, Aidan; Poblete, Barbara",,,Fine-Grained Evaluation for Entity Linking,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with an unambiguous identifier in a Knowledge Base. While much work has been done on the topic, we first present the results of a survey that reveal a lack of consensus in the community regarding what forms of mentions in a text and what forms of links the EL task should consider. We argue that no one definition of the Entity Linking task fits all, and rather propose a fine-grained categorization of different types of entity mentions and links. We then re-annotate three EL benchmark datasets - ACE2004, KORE50, and VoxEL - with respect to these categories. We propose a fuzzy recall metric to address the lack of consensus and conclude with fine-grained evaluation results comparing a selection of online EL systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,718,727,,,,,,,,,,,,,,,,WOS:000854193300066,0
C,"Song, HJ; Park, SB",,,Assoc Computat Linguist,"Song, Hyun-Je; Park, Seong-Bae",,,Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Korean morphological analysis has been considered as a sequence of morpheme processing and POS tagging. Thus, a pipeline model of the tasks has been adopted widely by previous studies. However, the model has a problem that it cannot utilize interactions among the tasks. This paper formulates Korean morphological analysis as a combination of the tasks and presents a tied sequence-to-sequence multi-task model for training the two tasks simultaneously without any explicit regularization. The experiments prove the proposed model achieves the state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1436,1441,,,,,,,,,,,,,,,,WOS:000854193301072,0
C,"Tripodi, R; Navigli, R",,,Assoc Computat Linguist,"Tripodi, Rocco; Navigli, Roberto",,,Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Game-theoretic models, thanks to their intrinsic ability to exploit contextual information, have shown to be particularly suited for the Word Sense Disambiguation task. They represent ambiguous words as the players of a non-cooperative game and their senses as the strategies that the players can select in order to play the games. The interaction among the players is modeled with a weighted graph and the payoff as an embedding similarity function, which the players try to maximize. The impact of the word and sense embedding representations in the framework was tested and analyzed extensively: experiments on standard benchmarks show state-of-art performances and different tests hint at the usefulness of using disambiguation to obtain contextualized word representations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,88,99,,,,,,,,,,,,,,,,WOS:000854193300009,0
C,"Wang, CH; Jain, A; Chen, DL; Gu, JT",,,ASSOC COMPUTAT LINGUIST,"Wang, Changhan; Jain, Anirudh; Chen, Danlu; Gu, Jiatao",,,VizSeq: A Visual Analysis Toolkit for Text Generation Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatic evaluation of text generation tasks (e.g. machine translation, text summarization, image captioning and video description) usually relies heavily on task-specific metrics, such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). They, however, are abstract numbers and are not perfectly aligned with human assessment. This suggests inspecting detailed examples as a complement to identify system error patterns. In this paper, we present VizSeq, a visual analysis toolkit for instance-level and corpus-level system evaluation on a wide variety of text generation tasks. It supports multimodal sources and multiple text references, providing visualization in Jupyter notebook or a web app interface. It can be used locally or deployed onto public servers for centralized data hosting and benchmarking. It covers most common n-gram based metrics accelerated with multiprocessing, and also provides latest embedding-based metrics such as BERTScore (Zhang et al., 2019).",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,253,258,,,,,,,,,,,,,,,,WOS:000855231500043,0
C,"Wang, ZH; Shang, JB; Liu, LY; Lu, LH; Liu, JC; Han, JW",,,Assoc Computat Linguist,"Wang, Zihan; Shang, Jingbo; Liu, Liyuan; Lu, Lihao; Liu, Jiacheng; Han, Jiawei",,,CrossWeigh: Training Named Entity Tagger from Imperfect Annotations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Everyone makes mistakes. So do human annotators when curating labels for named entity recognition (NER). Such label mistakes might hurt model training and interfere model comparison. In this study, we dive deep into one of the widely-adopted NER benchmark datasets, CoNLL03 NER. We are able to identify label mistakes in about 5.38% test sentences, which is a significant ratio considering that the state-of-the-art test F 1 score is already around 93%. Therefore, we manually correct these label mistakes and form a cleaner test set. Our re-evaluation of popular models on this corrected test set leads to more accurate assessments, compared to those on the original test set. More importantly, we propose a simple yet effective framework, CrossWeigh, to handle label mistakes during NER model training. Specifically, it partitions the training data into several folds and train independent NER models to identify potential mistakes in each fold. Then it adjusts the weights of training data accordingly to train the final NER model. Extensive experiments demonstrate significant improvements of plugging various NER models into our proposed framework on three datasets. All implementations and corrected test set are available at our Github repo (1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5154,5163,,,,,,,,,,,,,,,,WOS:000854193305032,0
C,"West, P; Holtzman, A; Buys, J; Choi, YJ",,,Assoc Computat Linguist,"West, Peter; Holtzman, Ari; Buys, Jan; Choi, Yejin",,,BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The principle of the Information Bottleneck (Tishby et al., 1999) is to produce a summary of information X optimized to predict some other relevant information Y. In this paper, we propose a novel approach to unsupervised sentence summarization by mapping the Information Bottleneck principle to a conditional language modelling objective: given a sentence, our approach seeks a compressed sentence that can best predict the next sentence. Our iterative algorithm under the Information Bottleneck objective searches gradually shorter subsequences of the given sentence while maximizing the probability of the next sentence conditioned on the summary. Using only pretrained language models with no direct supervision, our approach can efficiently perform extractive sentence summarization over a large corpus. Building on our unsupervised extractive summarization (BottleSumEx), we then present a new approach to self-supervised abstractive summarization (BottleSumSelf), where a transformer-based language model is trained on the output summaries of our unsupervised method. Empirical results demonstrate that our extractive method outperforms other unsupervised models on multiple automatic metrics. In addition, we find that our selfsupervised abstractive model outperforms unsupervised baselines (including our own) by human evaluation along multiple attributes.",,,,,,"Buys, Jan/0000-0003-1994-5832",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3752,3761,,,,,,,,,,,,,,,,WOS:000854193303107,0
C,"Yang, Q; Huo, ZY; Shen, DH; Chen, Y; Wang, WL; Wang, GY; Carin, L",,,Assoc Computat Linguist,"Yang, Qian; Huo, Zhouyuan; Shen, Dinghan; Chen, Yong; Wang, Wenlin; Wang, Guoyin; Carin, Lawrence",,,An End-to-End Generative Architecture for Paraphrase Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating high-quality paraphrases is a fundamental yet challenging natural language processing task. Despite the effectiveness of previous work based on generative models, there remain problems with exposure bias in recurrent neural networks, and often a failure to generate realistic sentences. To overcome these challenges, we propose the first endto-end conditional generative architecture for generating paraphrases via adversarial training, which does not depend on extra linguistic information. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results, outperforming previous generative architectures on both automatic metrics (BLEU, METEOR, and TER) and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3132,3142,,,,,,,,,,,,,,,,WOS:000854193303027,0
C,"Yuan, CY; Lv, SW; Li, MM; Zhou, W; Zhu, FQ; Han, JZ; Hu, SL",,,Assoc Computat Linguist,"Yuan, Chunyuan; Lv, Shangwen; Li, Mingming; Zhou, Wei; Zhu, Fuqing; Han, Jizhong; Hu, Songlin",,,Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-turn retrieval-based conversation is an important task for building intelligent dialogue systems. Existing works mainly focus on matching candidate responses with every context utterance on multiple levels of granularity, which ignore the side effect of using excessive context information. Context utterances provide abundant information for extracting more matching features, but it also brings noise signals and unnecessary information. In this paper, we will analyze the side effect of using too many context utterances and propose a multi-hop selector network (MSN) to alleviate the problem. Specifically, MSN firstly utilizes a multi-hop selector to select the relevant utterances as context. Then, the model matches the filtered context with the candidate response and obtains a matching score. Experimental results show that MSN outperforms some state-of-the-art methods on three public multi-turn dialogue datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,111,120,,,,,,,,,,,,,,,,WOS:000854193300011,0
C,"Zhao, W; Peyrard, M; Liu, F; Gao, Y; Meyer, CM; Eger, S",,,Assoc Computat Linguist,"Zhao, Wei; Peyrard, Maxime; Liu, Fei; Gao, Yang; Meyer, Christian M.; Eger, Steffen",,,MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,563,578,,,,,,,,,,,,,,,,WOS:000854193300053,0
C,"Agashe, R; Iyer, S; Zettlemoyer, L",,,Assoc Computat Linguist,"Agashe, Rajas; Iyer, Srinivasan; Zettlemoyer, Luke",,,JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5436,5446,,,,,,,,,,,,,,,,WOS:000854193305059,0
C,"Dou, ZY; Yu, KY; Anastasopoulos, A",,,Assoc Computat Linguist,"Dou, Zi-Yi; Yu, Keyi; Anastasopoulos, Antonios",,,Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Learning general representations of text is a fundamental problem for many natural language understanding (NLU) tasks. Previously, researchers have proposed to use language model pre-training and multi-task learning to learn robust representations. However, these methods can achieve sub-optimal performance in low-resource scenarios. Inspired by the recent success of optimization-based meta-learning algorithms, in this paper, we explore the model-agnostic meta-learning algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1192,1197,,,,,,,,,,,,,,,,WOS:000854193301034,0
C,"Hu, MT; Zhao, SW; Zhang, L; Cai, KK; Su, Z; Cheng, RH; Shen, XW",,,Assoc Computat Linguist,"Hu, Mengting; Zhao, Shiwan; Zhang, Li; Cai, Keke; Su, Zhong; Cheng, Renhong; Shen, Xiaowei",,,CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect level sentiment classification is a fine-grained sentiment analysis task. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various attention-based methods for generating aspect-specific sentence representations. However, the attention may inherently introduce noise and downgrade the performance. In this paper, we propose constrained attention networks (CAN), a simple yet effective solution, to regularize the attention for multi-aspect sentiment analysis, which alleviates the drawback of the attention mechanism. Specifically, we introduce orthogonal regularization on multiple aspects and sparse regularization on each single aspect. Experimental results on two public datasets demonstrate the effectiveness of our approach. We further extend our approach to multi-task settings and outperform the state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4601,4610,,,,,,,,,,,,,,,,WOS:000854193304072,0
C,"Khadanga, S; Aggarwal, K; Joty, S; Srivastava, J",,,Assoc Computat Linguist,"Khadanga, Swaraj; Aggarwal, Karan; Joty, Shafiq; Srivastava, Jaideep",,,Using Clinical Notes with Time Series Data for ICU Management,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospital's resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6432,6437,,,,,,,,,,,,,,,,WOS:000854193306089,0
C,"Li, ZH; Lin, Z; He, D; Tian, F; Qin, T; Wang, LW; Liu, TY",,,Assoc Computat Linguist,"Li, Zhuohan; Lin, Zi; He, Di; Tian, Fei; Qin, Tao; Wang, Liwei; Liu, Tie-Yan",,,Hint-Based Training for Non-Autoregressive Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Due to the unparallelizable nature of the autoregressive factorization, AutoRegressive Translation (ART) models have to generate tokens sequentially during decoding and thus suffer from high inference latency. Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time, but could only achieve inferior translation accuracy. In this paper, we proposed a novel approach to leveraging the hints from hidden states and word alignments to help the training of NART models. The results achieve significant improvement over previous NART models for the WMT14 En-De and De-En datasets and are even comparable to a strong LSTM-based ART baseline but one order of magnitude faster in inference.",,,,,,"Qin, Tao/0000-0002-9095-0776",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5708,5713,,,,,,,,,,,,,,,,WOS:000854193305086,0
C,"Liu, J; Chen, YB; Liu, K; Zhao, J",,,Assoc Computat Linguist,"Liu, Jian; Chen, Yubo; Liu, Kang; Zhao, Jun",,,Neural Cross-Lingual Event Detection with Minimal Parallel Resources,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on parallel resources, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on parallel resources. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual cotraining. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,738,748,,,,,,,,,,,,,,,,WOS:000854193300068,0
C,"Liu, Q; Chen, B; Liu, HY; Lou, JG; Fang, L; Zhou, B; Zhang, DM",,,Assoc Computat Linguist,"Liu, Qian; Chen, Bei; Liu, Haoyan; Lou, Jian-Guang; Fang, Lei; Zhou, Bin; Zhang, Dongmei",,,A Split-and-Recombine Approach for Follow-up Query Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Context-dependent semantic parsing has proven to be an important yet challenging task. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with contextual information. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5316,5326,,,,,,,,,,,,,,,,WOS:000854193305048,0
C,"Mesquita, F; Cannaviccio, M; Schmidek, J; Mirza, P; Barbosa, D",,,Assoc Computat Linguist,"Mesquita, Filipe; Cannaviccio, Matteo; Schmidek, Jordan; Mirza, Paramita; Barbosa, Denilson",,,KnowledgeNet: A Benchmark Dataset for Knowledge Base Population,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction). We discuss five baseline approaches, where the best approach achieves an F1 score of 0.50, significantly outperforming a traditional approach by 79% (0.28). However, our best baseline is far from reaching human performance (0.82), indicating our dataset is challenging. The KnowledgeNet dataset and baselines are available at https://github. com/diffbot/knowledge- net",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,749,758,,,,,,,,,,,,,,,,WOS:000854193300069,0
C,"Mosallanezhad, A; Beigi, G; Liu, H",,,Assoc Computat Linguist,"Mosallanezhad, Ahmadreza; Beigi, Ghazaleh; Liu, Huan",,,Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"User-generated textual data is rich in content and has been used in many user behavioral modeling tasks. However, it could also leak user private-attribute information that they may not want to disclose such as age and location. User's privacy concerns mandate data publishers to protect privacy. One effective way is to anonymize the textual data. In this paper, we study the problem of textual data anonymization and propose a novel Reinforcement Learning-based Text Anonymizor, RLTA, which addresses the problem of private-attribute leakage while preserving the utility of textual data. Our approach first extracts a latent representation of the original text w.r.t. a given task, then leverages deep reinforcement learning to automatically learn an optimal strategy for manipulating text representations w.r.t. the received privacy and utility feedback. Experiments show the effectiveness of this approach in terms of preserving both privacy and utility.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2360,2369,,,,,,,,,,,,,,,,WOS:000854193302048,0
C,"Peskov, D; Clarke, N; Krone, J; Fodor, B; Zhang, Y; Youssef, A; Diab, M",,,Assoc Computat Linguist,"Peskov, Denis; Clarke, Nancy; Krone, Jason; Fodor, Brigi; Zhang, Yi; Youssef, Adel; Diab, Mona",,,Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as virtual assistants become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, linguistic diversity, domain coverage, or annotation granularity. In this paper, we present strategies toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the customer) is paired with a trained annotator (the agent). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our strategies on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4526,4536,,,,,,,,,,,,,,,,WOS:000854193304065,0
C,"Qiu, JZ; Xiong, DY",,,Assoc Computat Linguist,"Qiu, Jiazuo; Xiong, Deyi",,,Generating Highly Relevant Questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The neural seq2seq based question generation (QG) is prone to generating generic and undiversified questions that are poorly relevant to the given passage and target answer. In this paper, we propose two methods to address the issue. (1) By a partial copy mechanism, we prioritize words that are morphologically close to words in the input passage when generating questions; (2) By a QA-based reranker, from the n-best list of question candidates, we select questions that are preferred by both the QA and QG model. Experiments and analyses demonstrate that the proposed two methods substantially improve the relevance of generated questions to passages and answers.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5983,5987,,,,,,,,,,,,,,,,WOS:000854193306025,0
C,"Ruckle, A; Moosavi, NS; Gurevych, I",,,Assoc Computat Linguist,"Ruckle, Andreas; Moosavi, Nafise Sadat; Gurevych, Iryna",,,Neural Duplicate Question Detection without Labeled Training Data,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Supervised training of neural models to duplicate question detection in community Question Answering (cQA) requires large amounts of labeled question pairs, which are costly to obtain. To minimize this cost, recent works thus often used alternative methods, e.g., adversarial domain adaptation. In this work, we propose two novel methods: (1) the automatic generation of duplicate questions, and (2) weak supervision using the title and body of a question. We show that both can achieve improved performances even though they do not require any labeled data. We provide comprehensive comparisons of popular training strategies, which provides important insights on how to 'best' train models in different scenarios. We show that our proposed approaches are more effective in many cases because they can utilize larger amounts of unlabeled data from cQA forums. Finally, we also show that our proposed approach for weak supervision with question title and body information is also an effective method to train cQA answer selection models without direct answer supervision.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1607,1617,,,,,,,,,,,,,,,,WOS:000854193301093,0
C,"Saito, J; Murawaki, Y; Kurohashi, S",,,Assoc Computat Linguist,"Saito, Jun; Murawaki, Yugo; Kurohashi, Sadao",,,Minimally Supervised Learning of Affective Events Using Discourse Relations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5758,5765,,,,,,,,,,,,,,,,WOS:000854193305094,0
C,"Schmidt, FD; Dietsche, M; Ponzetto, SP; Glavas, G",,,ASSOC COMPUTAT LINGUIST,"Schmidt, Fabian David; Dietsche, Markus; Ponzetto, Simone Paolo; Glavas, Goran",,,SEAGLE: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce SEAGLE,1 a platform for comparative evaluation of semantic text encoding models on information retrieval (IR) tasks. SEAGLE implements (1) word embedding aggregators, which represent texts as algebraic aggregations of pretrained word embeddings and (2) pretrained semantic encoders, and allows for their comparative evaluation on arbitrary (monolingual and cross-lingual) IR collections. We benchmark SEAGLE's models on monolingual document retrieval and crosslingual sentence retrieval. SEAGLE functionality can be exploited via an easy-to-use web interface and its modular backend (microservice architecture) can easily be extended with additional semantic search models.",,,,,,"Ponzetto, Simone Paolo/0000-0001-7484-2049",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,199,204,,,,,,,,,,,,,,,,WOS:000855231500034,0
C,"Sun, HT; Bedrax-Weiss, T; Cohen, WW",,,Assoc Computat Linguist,"Sun, Haitian; Bedrax-Weiss, Tania; Cohen, William W.",,,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We consider open-domain question answering (QA) where answers are drawn from either a corpus, a knowledge base (KB), or a combination of both of these. We focus on a setting in which a corpus is supplemented with a large but incomplete KB, and on questions that require non-trivial (e.g., multi-hop) reasoning. We describe PullNet, an integrated framework for (1) learning what to retrieve and (2) reasoning with this heterogeneous information to find the best answer. PullNet uses an iterative process to construct a question-specific subgraph that contains information relevant to the question. In each iteration, a graph convolutional network (graph CNN) is used to identify subgraph nodes that should be expanded using retrieval (or pull) operations on the corpus and/or KB. After the subgraph is complete, another graph CNN is used to extract the answer from the subgraph. This retrieve-and-reason process allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-ofthe art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting.",,,,,"wu, william/HII-5817-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2380,2390,,,,,,,,,,,,,,,,WOS:000854193302050,0
C,"Tan, XW; Cai, Y; Zhu, CX",,,Assoc Computat Linguist,"Tan, Xingwei; Cai, Yi; Zhu, Changxi",,,Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect-level sentiment classification, which is a fine-grained sentiment analysis task, has received lots of attention these years. There is a phenomenon that people express both positive and negative sentiments towards an aspect at the same time. Such opinions with conflicting sentiments, however, are ignored by existing studies, which design models based on the absence of them. We argue that the exclusion of conflict opinions is problematic, for the reason that it represents an important style of human thinking - dialectic thinking. If a realworld sentiment classification system ignores the existence of conflict opinions when it is designed, it will incorrectly mixed conflict opinions into other sentiment polarity categories in action. Existing models have problems when recognizing conflicting opinions, such as data sparsity. In this paper, we propose a multilabel classification model with dual attention mechanism to address these problems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3426,3431,,,,,,,,,,,,,,,,WOS:000854193303060,0
C,"Tandon, N; Mishra, BD; Sakaguchi, K; Bosselut, A; Clark, P",,,Assoc Computat Linguist,"Tandon, Niket; Mishra, Bhavana Dalvi; Sakaguchi, Keisuke; Bosselut, Antoine; Clark, Peter",,,WIQA: A dataset for What if. . . reasoning over procedural text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce WIQA, the first large-scale dataset of What if... questions over procedural text. WIQA contains three parts: a collection of paragraphs each describing a process, e.g., beach erosion; a set of crowdsourced influenct graphs for each paragraph, describing how one change a ffects another; and a large (40k) collection of What if...? multiple-choice questions derived from the graphs. For example, given a paragraph about beach erosion, would stormy weather result in more or less erosion (or have no e ffect)? The task is to answer the questions, given their associated paragraph. WIQA contains three kinds of questions: perturbations to steps mentioned in the paragraph; external (out-of-paragraph) perturbations requiring commonsense knowledge; and irrelevant (no effect) perturbations. We find that state-of-the-art models achieve 73.8% accuracy, well below the human performance of 96.3%. We analyze the challenges, in particular tracking chains of influences, and present the dataset as an open challenge to the community.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6076,6085,,,,,,,,,,,,,,,,WOS:000854193306040,0
C,"Thompson, B; Koehn, P",,,Assoc Computat Linguist,"Thompson, Brian; Koehn, Philipp",,,Vecalign: Improved Sentence Alignment in Linear Time and Space,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce Vecalign, a novel bilingual sentence alignment method which is linear in time and space with respect to the number of sentences being aligned and which requires only bilingual sentence embeddings. On a standard German-French test set, Vecalign outperforms the previous state-of-the-art method (which has quadratic time complexity and requires a machine translation system) by 5 F-1 points. It substantially outperforms the popular Hunalign toolkit at recovering Bible verse alignments in medium- to low-resource language pairs, and it improves downstream MT quality by 1.7 and 1.6 BLEU in Sinhala!English and Nepali!English, respectively, compared to the Hunalign-based Paracrawl pipeline.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1342,1348,,,,,,,,,,,,,,,,WOS:000854193301058,0
C,"Wallace, E; Feng, S; Kandpal, N; Gardner, M; Singh, S",,,Assoc Computat Linguist,"Wallace, Eric; Feng, Shi; Kandpal, Nikhil; Gardner, Matt; Singh, Sameer",,,Universal Adversarial Triggers for Attacking and Analyzing NLP WARNING: This paper contains model outputs which are offensive in nature,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradientguided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of why questions in SQuAD to be answered to kill american people, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2153,2162,,,,,,,,,,,,,,,,WOS:000854193302029,0
C,"Zhou, CT; Ma, XZ; Hu, JJ; Neubig, G",,,Assoc Computat Linguist,"Zhou, Chunting; Ma, Xuezhe; Hu, Junjie; Neubig, Graham",,,Handling Syntactic Divergence in Low-resource Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of trainingtime supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1388,1394,,,,,,,,,,,,,,,,WOS:000854193301065,0
C,"Alberti, C; Ling, J; Collins, M; Reitter, D",,,Assoc Computat Linguist,"Alberti, Chris; Ling, Jeffrey; Collins, Michael; Reitter, David",,,Fusion of Detected Objects in Text for Visual Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"To advance models of multimodal context, we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The Bounding Boxes in Text Transformer (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark1, achieving a new state-of-the-art with a 25% relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided(2).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2131,2140,,,,,,,,,,,,,,,,WOS:000854193302027,0
C,"Amplayo, RK",,,Assoc Computat Linguist,"Amplayo, Reinald Kim",,,Rethinking Attribute Representation and Injection for Sentiment Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text attributes, such as user and product information in product reviews, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the attention mechanism, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above method is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the model. We propose to represent attributes as chunk-wise importance weight matrices and consider four locations in the model (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also outperform the state-of-the-art despite our use of a simple base model. Finally, we show that these representations transfer well to other tasks(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5602,5613,,,,,,,,,,,,,,,,WOS:000854193305075,0
C,"Chen, MD; Chu, ZW; Gimpel, K",,,Assoc Computat Linguist,"Chen, Mingda; Chu, Zewei; Gimpel, Kevin",,,Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Prior work on pretrained sentence embeddings and benchmarks focuses on the capabilities of representations for stand-alone sentences. We propose DiscoEval, a test suite of tasks to evaluate whether sentence representations include information about the role of a sentence in its discourse context. We also propose a variety of training objectives that make use of natural annotations from Wikipedia to build sentence encoders capable of modeling discourse information. We benchmark sentence encoders trained with our proposed objectives, as well as other popular pretrained sentence encoders, on DiscoEval and other sentence evaluation tasks. Empirically, we show that these training objectives help to encode different aspects of information from the surrounding document structure. Moreover, BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018a) demonstrate strong performance across DiscoEval tasks with individual hidden layers showing different characteristics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,649,662,,,,,,,,,,,,,,,,WOS:000854193300060,0
C,"Hu, LM; Zhang, LH; Shi, CA; Nie, LQ; Guan, WL; Yang, C",,,Assoc Computat Linguist,"Hu, Linmei; Zhang, Luhao; Shi, Chuan; Nie, Liqiang; Guan, Weili; Yang, Cheng",,,Improving Distantly-Supervised Relation Extraction with Joint Label Embedding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Distantly-supervised relation extraction has proven to be effective to find relational facts from texts. However, the existing approaches treat labels as independent and meaningless one-hot vectors, which cause a loss of potential label information for selecting valid instances. In this paper, we propose a novel multi-layer attention-based model to improve relation extraction with joint label embedding. The model makes full use of both structural information from Knowledge Graphs and textual information from entity descriptions to learn label embeddings through gating integration, while avoiding the imposed noise with an attention mechanism. Then the learned label embeddings are used as another attention over the instances (whose embeddings are also enhanced with the entity descriptions) for improving relation extraction. Extensive experiments demonstrate that our model significantly outperforms state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3821,3829,,,,,,,,,,,,,,,,WOS:000854193303113,0
C,"Huang, YY; Du, JH",,,Assoc Computat Linguist,"Huang, Yuyun; Du, Jinhua",,,Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Distance supervision is widely used in relation extraction tasks, particularly when large-scale manual annotations are virtually impossible to conduct. Although Distantly Supervised Relation Extraction (DSRE) benefits from automatic labelling, it suffers from serious mislabelling issues, i.e. some or all of the instances for an entity pair (head and tail entities) do not express the labelled relation. In this paper, we propose a novel model that employs a collaborative curriculum learning framework to reduce the effects of mislabelled data. Specifically, we firstly propose an internal self-attention mechanism between the convolution operations in convolutional neural networks (CNNs) to learn a better sentence representation from the noisy inputs. Then we define two sentence selection models as two relation extractors in order to collaboratively learn and regularise each other under a curriculum scheme to alleviate noisy effects, where the curriculum could be constructed by conflicts or small loss. Finally, experiments are conducted on a widely-used public dataset and the results indicate that the proposed model significantly outperforms baselines including the state-of-the-art in terms of P@N and PR curve metrics, thus evidencing its capability of reducing noisy effects for DSRE.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,389,398,,,,,,,,,,,,,,,,WOS:000854193300037,0
C,"Joshi, M; Levy, O; Weld, DS; Zettlemoyer, L",,,Assoc Computat Linguist,"Joshi, Mandar; Levy, Omer; Weld, Daniel S.; Zettlemoyer, Luke",,,BERT for Coreference Resolution: Baselines and Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We apply BERT to coreference resolution, achieving strong improvements on the OntoNotes (+3.9 F1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5803,5808,,,,,,,,,,,,,,,,WOS:000854193305101,0
C,"Kadowaki, K; Iida, R; Torisawa, K; Oh, JH; Kloetzer, J",,,Assoc Computat Linguist,"Kadowaki, Kazuma; Iida, Ryu; Torisawa, Kentaro; Oh, Jong-Hoon; Kloetzer, Julien",,,Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose new BERT-based methods for recognizing event causality such as smoke cigarettes -> die of lung cancer written in web texts. In our methods, we grasp each annotator's policy by training multiple classifiers, each of which predicts the labels given by a single annotator, and combine the resulting classifiers' outputs to predict the final labels determined by majority vote. Furthermore, we investigate the effect of supplying background knowledge to our classifiers. Since BERT models are pre-trained with a large corpus, some sort of background knowledge for event causality may be learned during pre-training. Our experiments with a Japanese dataset suggest that this is actually the case: Performance improved when we pre-trained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5816,5822,,,,,,,,,,,,,,,,WOS:000854193306001,0
C,"Kondratyuk, D; Straka, M",,,Assoc Computat Linguist,"Kondratyuk, Dan; Straka, Milan",,,"75 Languages, 1 Model: Parsing Universal Dependencies Universally",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on. Code for UDify is available at https:// github.com/hyperparticle/udify.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2779,2795,,,,,,,,,,,,,,,,WOS:000854193302087,0
C,"Kumar, V; Muneeswaran, S; Ramakrishnan, G; Li, YF",,,ASSOC COMPUTAT LINGUIST,"Kumar, Vishwajeet; Muneeswaran, Sivaanandh; Ramakrishnan, Ganesh; Li, Yuan-Fang",,,ParaQG: A System for Generating Questions and Answers from Paragraphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating syntactically and semantically valid and relevant questions from paragraphs is useful with many applications. Manual generation is a labour-intensive task, as it requires the reading, parsing and understanding of long passages of text. A number of question generation models based on sequence-to-sequence techniques have recently been proposed. Most of them generate questions from sentences only, and none of them is publicly available as an easy-to-use service. In this paper, we demonstrate ParaQG, a Web-based system for generating questions from sentences and paragraphs. ParaQG incorporates a number of novel functionalities to make the question generation process user-friendly. It provides an interactive interface for a user to select answers with visual insights on generation of questions. It also employs various faceted views to group similar questions as well as filtering techniques to eliminate unanswerable questions.",,,,,,"Li, Yuan-Fang/0000-0003-4651-2821",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,175,180,,,,,,,,,,,,,,,,WOS:000855231500030,0
C,"Li, PF; Mao, KZ; Yang, XF; Li, Q",,,Assoc Computat Linguist,"Li, Pengfei; Mao, Kezhi; Yang, Xuefeng; Li, Qi",,,Improving Relation Extraction with Knowledge-attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While attention mechanisms have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into deep neural networks for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. Stateof-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,229,239,,,,,,,,,,,,,,,,WOS:000854193300022,0
C,"Liang, D; Zhang, FB; Zhang, Q; Huang, XJ",,,Assoc Computat Linguist,"Liang, Di; Zhang, Fubao; Zhang, Qi; Huang, Xuanjing",,,Asynchronous Deep Interaction Network for Natural Language Inference,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Natural language inference aims to predict whether a premise sentence can infer another hypothesis sentence. Existing methods typically have framed the reasoning problem as a semantic matching task. The both sentences are encoded and interacted symmetrically and in parallel. However, in the process of reasoning, the role of the two sentences is obviously different, and the sentence pairs for NLI are asymmetrical corpora. In this paper, we propose an asynchronous deep interaction network (ADIN) to complete the task. ADIN is a neural network structure stacked with multiple inference sub-layers, and each sub-layer consists of two local inference modules in an asymmetrical manner. Different from previous methods, this model deconstructs the reasoning process and implements the asynchronous and multi-step reasoning. Experiment results show that ADIN achieves competitive performance and outperforms strong baselines on three popular benchmarks: SNLI, MultiNLI, and SciTail.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2692,2700,,,,,,,,,,,,,,,,WOS:000854193302079,0
C,"Mishra, A; Tater, T; Sankaranarayanan, K",,,Assoc Computat Linguist,"Mishra, Abhijit; Tater, Tarun; Sankaranarayanan, Karthik",,,A Modular Architecture for Unsupervised Sarcasm Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we propose a novel framework for sarcasm generation; the system takes a literal negative opinion as input and translates it into a sarcastic version. Our framework does not require any paired data for training. Sarcasm emanates from context-incongruity which becomes apparent as the sentence unfolds. Our framework introduces incongruity into the literal input version through modules that: (a) filter factual content from the input opinion, (b) retrieve incongruous phrases related to the filtered facts and (c) synthesize sarcastic text from the filtered and incongruous phrases. The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non-sarcastic and sarcastic opinions. Since no labeled dataset exists for such a task, for evaluation, we manually prepare a benchmark dataset containing literal opinions and their sarcastic paraphrases. Qualitative and quantitative performance analyses on the data reveal our system's superiority over baselines, built using known unsupervised statistical and neural machine translation and style transfer techniques.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6144,6154,,,,,,,,,,,,,,,,WOS:000854193306047,0
C,"Pfeiffer, J; Meyer, CM; Schulz, C; Kiesewetter, J; Zottmann, J; Sailer, M; Bauer, E; Fischer, F; Fischer, MR; Gurevych, I",,,ASSOC COMPUTAT LINGUIST,"Pfeiffer, Jonas; Meyer, Christian M.; Schulz, Claudia; Kiesewetter, Jan; Zottmann, Jan; Sailer, Michael; Bauer, Elisabeth; Fischer, Frank; Fischer, Martin R.; Gurevych, Iryna",,,FAMULUS: Interactive Annotation and Feedback Generation for Teaching Diagnostic Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Our proposed system FAMULUS helps students learn to diagnose based on automatic feedback in virtual patient simulations, and it supports instructors in labeling training data. Diagnosing is an exceptionally difficult skill to obtain but vital for many different professions (e.g., medical doctors, teachers). Previous case simulation systems are limited to multiple-choice questions and thus cannot give constructive individualized feedback on a student's diagnostic reasoning process. Given initially only limited data, we leverage a (replaceable) NLP model to both support experts in their further data annotation with automatic suggestions, and we provide automatic feedback for students. We argue that because the central model consistently improves, our interactive approach encourages both students and instructors to recurrently use the tool, and thus accelerate the speed of data creation and annotation. We show results from two user studies on diagnostic reasoning in medicine and teacher education and outline how our system can be extended to further use cases.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,73,78,,,,,,,,,,,,,,,,WOS:000855231500013,0
C,"Ponti, EM; Vulic, I; Cotterell, R; Reichart, R; Korhonen, A",,,Assoc Computat Linguist,"Ponti, Edoardo M.; Vulic, Ivan; Cotterell, Ryan; Reichart, Roi; Korhonen, Anna",,,Towards Zero-shot Language Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Can we construct a neural model that is inductively biased towards learning human languages? Motivated by this question, we aim at constructing an informative prior over neural weights, in order to adapt quickly to held-out languages in the task of character-level language modeling. We infer this distribution from a sample of typologically diverse training languages via Laplace approximation. The use of such a prior outperforms baseline models with an uninformative prior (so-called 'finetuning') in both zero-shot and few-shot settings. This shows that the prior is imbued with universal phonological knowledge. Moreover, we harness additional language-specific side information as distant supervision for held-out languages. Specifically, we condition language models on features from typological databases, by concatenating them to hidden states or generating weights with hyper-networks. These features appear beneficial in the few-shot setting, but not in the zero-shot setting. Since the paucity of digital texts affects the majority of the world's languages, we hope that these findings will help broaden the scope of applications for language technology.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2900,+,,,,,,,,,,,,,,,,WOS:000854193303006,0
C,"Ren, LL; Ni, JM; McAuley, J",,,Assoc Computat Linguist,"Ren, Liliang; Ni, Jianmo; McAuley, Julian",,,Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing approaches to dialogue state tracking rely on pre-defined ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-defined slots that need tracking. This issue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investigate how to approach DST using a generation framework without the pre-defined ontology list. Given each turn of user utterance and system response, we directly generate a sequence of belief states by applying a hierarchical encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre-defined slots. Experiments on both the multidomain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-defined domains and slots but also reaches the state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1876,1885,,,,,,,,,,,,,,,,WOS:000854193302004,0
C,"Subburathinam, A; Lu, D; Ji, H; May, J; Chang, SF; Sil, A; Voss, C",,,Assoc Computat Linguist,"Subburathinam, Ananya; Lu, Di; Ji, Heng; May, Jonathan; Chang, Shih-Fu; Sil, Avirup; Voss, Clare",,,Cross-lingual Structure Transfer for Relation and Event Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of crosslingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6% F-score for Relation Extraction, and 63.1% F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,313,325,,,,,,,,,,,,,,,,WOS:000854193300030,0
C,"Sun, SM; Nenkova, A",,,Assoc Computat Linguist,"Sun, Simeng; Nenkova, Ani",,,The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"ROUGE is widely used to automatically evaluate summarization systems. However, ROUGE measures semantic overlap between a system summary and a human reference on word-string level, much at odds with the contemporary treatment of semantic meaning. Here we present a suite of experiments on using distributed representations for evaluating summarizers, both in reference-based and in reference-free setting. Our experimental results show that the max value over each dimension of the summary ELMo word embeddings is a good representation that results in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1216,1221,,,,,,,,,,,,,,,,WOS:000854193301038,0
C,"Tafjord, O; Gardner, M; Lin, K; Clark, P",,,Assoc Computat Linguist,"Tafjord, Oyvind; Gardner, Matt; Lin, Kevin; Clark, Peter",,,QUARTZ: An Open-Domain Dataset of Qualitative Relationship Questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce the first open-domain dataset, called QUARTZ, for reasoning about textual qualitative relationships. QUARTZ contains general qualitative statements, e.g., A sunscreen with a higher SPF protects the skin longer., twinned with 3864 crowdsourced situated questions, e.g., Billy is wearing sunscreen with a lower SPF than Lucy. Who will be best protected from the sun?, plus annotations of the properties being compared. Unlike previous datasets, the general knowledge is textual and not tied to a fixed set of relationships, and tests a system's ability to comprehend and apply textual qualitative knowledge in a novel setting. We find state-of-the-art results are substantially (20%) below human performance, presenting an open challenge to the NLP community.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5941,5946,,,,,,,,,,,,,,,,WOS:000854193306019,0
C,"van Schijndel, M; Mueller, A; Linzen, T",,,Assoc Computat Linguist,"van Schijndel, Marten; Mueller, Aaron; Linzen, Tal",,,Quantity doesn't buy quality syntax with neural language models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5831,5837,,,,,,,,,,,,,,,,WOS:000854193306003,0
C,"Wang, J; Yu, LC; Lai, KR; Zhang, XJ",,,Assoc Computat Linguist,"Wang, Jin; Yu, Liang-Chih; Lai, K. Robert; Zhang, Xuejie",,,Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep neural network models such as long short-term memory (LSTM) and treeLSTM have been proven to be effective for sentiment analysis. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the parse tree will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement.",,,,,,"Wang, Jin/0000-0002-8298-4378",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3432,3437,,,,,,,,,,,,,,,,WOS:000854193303061,0
C,"Wang, ZY; Wang, BX; Duan, XY; Wu, DY; Wang, SJ; Hu, GP; Hu, T",,,ASSOC COMPUTAT LINGUIST,"Wang, Ziyue; Wang, Baoxin; Duan, Xingyi; Wu, Dayong; Wang, Shijin; Hu, Guoping; Hu, Ting",,,"IFlyLegal: A Chinese Legal System for Consultation, Law Searching, and Document Analysis",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Legal Tech is developed to help people with legal services and solve legal problems via machines. To achieve this, one of the key requirements for machines is to utilize legal knowledge and comprehend legal context. This can be fulfilled by natural language processing (NLP) techniques, for instance, text representation, text categorization, question answering (QA) and natural language inference, etc. To this end, we introduce a freely available Chinese Legal Tech system (IFlyLegal) that benefits from multiple NLP tasks. It is an integrated system that performs legal consulting, multiway law searching, and legal document analysis by exploiting techniques such as deep contextual representations and various attention mechanisms. To our knowledge, IFlyLegal is the first Chinese legal system that employs upto-date NLP techniques and caters for needs of different user groups, such as lawyers, judges, procurators, and clients. Since Jan, 2019, we have gathered 2,349 users and 28,238 page views (till June, 23, 2019).",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,97,102,,,,,,,,,,,,,,,,WOS:000855231500017,0
C,"Xia, QR; Li, ZH; Zhang, M",,,Assoc Computat Linguist,"Xia, Qingrong; Li, Zhenghua; Zhang, Min",,,A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic role labeling (SRL) aims to identify the predicate-argument structure of a sentence. Inspired by the strong correlation between syntax and semantics, previous works pay much attention to improve SRL performance on exploiting syntactic knowledge, achieving significant results. Pipeline methods based on automatic syntactic trees and multi-task learning (MTL) approaches using standard syntactic trees are two common research orientations. In this paper, we adopt a simple unified span-based model for both span-based and word-based Chinese SRL as a strong baseline. Besides, we present a MTL framework that includes the basic SRL module and a dependency parser module. Different from the commonly used hard parameter sharing strategy in MTL, the main idea is to extract implicit syntactic representations from the dependency parser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5382,5392,,,,,,,,,,,,,,,,WOS:000854193305054,0
C,"Xiao, XY; Wang, LF; Fan, B; Xiang, SM; Pan, CH",,,Assoc Computat Linguist,"Xiao, Xinyu; Wang, Lingfeng; Fan, Bin; Xiang, Shiming; Pan, Chunhong",,,Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In the current video captioning models, the video frames are collected in one network and the semantics are mixed into one feature, which not only increase the difficulty of the caption decoding, but also decrease the interpretability of the captioning models. To address these problems, we propose an Adaptive Semantic Guidance Network (ASGN), which instantiates the whole video semantics to different POS-aware semantics with the supervision of part of speech (POS) tag. In the encoding process, the POS tag activates the related neurons and parses the whole semantic information into corresponding encoded video representations. Furthermore, the potential of the model is stimulated by the POS-aware video features. In the decoding process, the related video features of noun and verb are used as the supervision to construct a new adaptive attention model which can decide whether to attend to the video feature or not. With the explicit improving of the interpretability of the network, the learning process is more transparent and the results are more predictable. Extensive experiments demonstrate the effectiveness of our model when compared with stateof-the-art models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2068,2077,,,,,,,,,,,,,,,,WOS:000854193302021,0
C,"Yang, XY; Gu, XT; Lin, S; Tang, SL; Zhuang, YT; Wu, F; Chen, ZG; Hu, GP; Ren, X",,,Assoc Computat Linguist,"Yang, Xiyuan; Gu, Xiaotao; Lin, Sheng; Tang, Siliang; Zhuang, Yueting; Wu, Fei; Chen, Zhigang; Hu, Guoping; Ren, Xiang",,,Learning Dynamic Context Augmentation for Global Entity Linking,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite of the recent success of collective entity linking (EL) methods, these global inference methods may yield sub-optimal results when the all-mention coherence assumption breaks, and often suffer from high computational cost at the inference stage, due to the complex search space. In this paper, we propose a simple yet effective solution, called Dynamic Context Augmentation (DCA), for collective EL, which requires only one pass through the mentions in a document. DCA sequentially accumulates context information to make efficient, collective inference, and can cope with different local EL models as a plugand-enhance module. We explore both supervised and reinforcement learning strategies for learning the DCA model. Extensive experiments1 show the effectiveness of our model with different learning settings, base models, decision orders and attention mechanisms.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,271,281,,,,,,,,,,,,,,,,WOS:000854193300026,0
C,"Zhelezniak, V; Shen, A; Busbridge, D; Savkov, A; Hammerla, N",,,Assoc Computat Linguist,"Zhelezniak, Vitalii; Shen, April; Busbridge, Daniel; Savkov, Aleksandar; Hammerla, Nils",,,Correlations between Word Vector Sets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Similarity measures based purely on word embeddings are comfortably competing with much more sophisticated deep learning and expert-engineered systems on unsupervised semantic textual similarity (STS) tasks. In contrast to commonly used geometric approaches, we treat a single word embedding as e.g. 300 observations from a scalar random variable. Using this paradigm, we first illustrate that similarities derived from elementary pooling operations and classic correlation coefficients yield excellent results on standard STS benchmarks, outperforming many recently proposed methods while being much faster and trivial to implement. Next, we demonstrate how to avoid pooling operations altogether and compare sets of word embeddings directly via correlation operators between reproducing kernel Hilbert spaces. Just like cosine similarity is used to compare individual word vectors, we introduce a novel application of the centered kernel alignment (CKA) as a natural generalisation of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy to implement and enjoys very strong empirical results.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,77,87,,,,,,,,,,,,,,,,WOS:000854193300008,0
C,"Zheng, RJ; Ma, MB; Zheng, BG; Huang, L",,,Assoc Computat Linguist,"Zheng, Renjie; Ma, Mingbo; Zheng, Baigong; Huang, Liang",,,Speculative Beam Search for Simultaneous Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Beam search is universally used in full-sentence translation but its application to simultaneous translation remains non-trivial, where output words are committed on the fly. In particular, the recently proposed wait-k policy (Ma et al., 2019a) is a simple and effective method that (after an initial wait) commits one output word on receiving each input word, making beam search seemingly impossible. To address this challenge, we propose a speculative beam search algorithm that hallucinates several steps into the future in order to reach a more accurate decision, implicitly benefiting from a target language model. This makes beam search applicable for the first time to the generation of a single word in each step. Experiments over diverse language pairs show large improvements over previous work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1395,1402,,,,,,,,,,,,,,,,WOS:000854193301066,0
C,"Balashankar, A; Chakraborty, S; Fraiberger, S; Subramanian, L",,,Assoc Computat Linguist,"Balashankar, Ananth; Chakraborty, Sunandan; Fraiberger, Samuel; Subramanian, Lakshminarayanan",,,Identifying Predictive Causal Factors from News Streams,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a new framework to uncover the relationship between news events and real world phenomena. We present the Predictive Causal Graph (PCG) which allows to detect latent relationships between events mentioned in news streams. This graph is constructed by measuring how the occurrence of a word in the news influences the occurrence of another (set of) word(s) in the future. We show that PCG can be used to extract latent features from news streams, outperforming other graph-based methods in prediction error of 10 stock price time series for 12 months. We then extended PCG to be applicable for longer time windows by allowing time-varying factors, leading to stock price prediction error rates between 1.5% and 5% for about 4 years. We then manually validated PCG, finding that 67% of the causation semantic frame arguments present in the news corpus were directly connected in the PCG, the remaining being connected through a semantically relevant intermediate node.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2338,2348,,,,,,,,,,,,,,,,WOS:000854193302046,0
C,"Chen, JF; Zhang, RC; Mao, YY; Guo, HY; Xu, J",,,Assoc Computat Linguist,"Chen, Junfan; Zhang, Richong; Mao, Yongyi; Guo, Hongyu; Xu, Jie",,,Uncover the Ground-Truth Relations in Distant Supervision: A Neural Expectation-Maximization Framework,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Distant supervision for relation extraction enables one to effectively acquire structured relations out of very large text corpora with less human efforts. Nevertheless, most of the prior-art models for such tasks assume that the given text can be noisy, but their corresponding labels are clean. Such unrealistic assumption is contradictory with the fact that the given labels are often noisy as well, thus leading to significant performance degradation of those models on real-world data. To cope with this challenge, we propose a novel label-denoising framework that combines neural network with probabilistic modelling, which naturally takes into account the noisy labels during learning. We empirically demonstrate that our approach significantly improves the current art in uncovering the ground-truth relation labels.",,,,,,"chen, jun fan/0000-0001-6807-0089",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,326,336,,,,,,,,,,,,,,,,WOS:000854193300031,0
C,"Chen, QB; Lin, JY; Zhang, YC; Ding, M; Cen, YK; Yang, HX; Tang, J",,,Assoc Computat Linguist,"Chen, Qibin; Lin, Junyang; Zhang, Yichang; Ding, Ming; Cen, Yukuo; Yang, Hongxia; Tang, Jie",,,Towards Knowledge-Based Recommender Dialog System,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we propose a novel end-to-end framework called KBRD, which stands for Knowledge-Based Recommender Dialog System. It integrates the recommender system and the dialog generation system. The dialog system can enhance the performance of the recommendation system by introducing knowledge-grounded information about users' preferences, and the recommender system can improve that of the dialog generation system by providing recommendation-aware vocabulary bias. Experimental results demonstrate that our proposed model has significant advantages over the baselines in both the evaluation of dialog generation and recommendation. A series of analyses show that the two systems can bring mutual benefits to each other, and the introduced knowledge contributes to both their performances.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1803,1813,,,,,,,,,,,,,,,,WOS:000854193301111,0
C,"Hao, YR; Dong, L; Wei, FR; Xu, K",,,Assoc Computat Linguist,"Hao, Yaru; Dong, Li; Wei, Furu; Xu, Ke",,,Visualizing and Understanding the Effectiveness of BERT,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pretraining-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4143,4152,,,,,,,,,,,,,,,,WOS:000854193304029,0
C,"Kozareva, Z; Ravi, S",,,Assoc Computat Linguist,"Kozareva, Zornitsa; Ravi, Sujith",,,ProSeqo: Projection Sequence Networks for On-Device Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multiple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text classification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text classification neural model. It achieved comparable results to previous neural approaches for news article, answers and product categorization, while preserving small memory footprint and maintaining high accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3894,3903,,,,,,,,,,,,,,,,WOS:000854193304007,0
C,"Mehri, S; Eskenazi, M",,,Assoc Computat Linguist,"Mehri, Shikib; Eskenazi, Maxine",,,Multi-Granularity Representations of Dialog,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural models of dialog rely on generalized latent representations of language. This paper introduces a novel training procedure which explicitly learns multiple representations of language at several levels of granularity. The multi-granularity training algorithm modifies the mechanism by which negative candidate responses are sampled in order to control the granularity of learned latent representations. Strong performance gains are observed on the next utterance retrieval task using both the MultiWOZ dataset and the Ubuntu dialog corpus. Analysis significantly demonstrates that multiple granularities of representation are being learned, and that multi-granularity training facilitates better transfer to downstream tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1752,1761,,,,,,,,,,,,,,,,WOS:000854193301106,0
C,"Mubarak, H; Abdelali, A; Darwish, K; Eldesouki, M; Samih, Y; Sajjad, H",,,ASSOC COMPUTAT LINGUIST,"Mubarak, Hamdy; Abdelali, Ahmed; Darwish, Kareem; Eldesouki, Mohamed; Samih, Younes; Sajjad, Hassan",,,A System for Diacritizing Four Varieties of Arabic,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisian. The system uses a character level sequenceto-sequence deep learning model that requires no feature engineering and beats all previous SOTA systems for all the Arabic varieties that we test on.",,,,,,"Samih, Younes/0000-0002-0485-7920",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,217,222,,,,,,,,,,,,,,,,WOS:000855231500037,0
C,"Qin, LB; Liu, YJ; Che, WX; Wen, HY; Li, YM; Liu, T",,,Assoc Computat Linguist,"Qin, Libo; Liu, Yijia; Che, Wanxiang; Wen, Haoyang; Li, Yangming; Liu, Ting",,,Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Querying the knowledge base (KB) has long been a challenge in the end-to-end taskoriented dialogue system. Previous sequenceto-sequence (Seq2Seq) dialogue generation work treats the KB query as an attention over the entire KB, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel framework which queries the KB in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single KB row, we introduce a KB retrieval component which explicitly returns the most relevant KB row given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the attention mechanism to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,133,142,,,,,,,,,,,,,,,,WOS:000854193300013,0
C,"Searle, T; Kraljevic, Z; Bendayan, R; Bean, D; Dobson, R",,,ASSOC COMPUTAT LINGUIST,"Searle, Thomas; Kraljevic, Zeljko; Bendayan, Rebecca; Bean, Daniel; Dobson, Richard",,,MedCATTrainer: A Biomedical Free Text Annotation Interface with Active Learning and Research Use Case Specific Customisation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present MedCATTrainer(1) an interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text. NER+L is often used as a first step in deriving value from clinical text. Collecting labelled data for training models is difficult due to the need for specialist domain knowledge. MedCATTrainer offers an interactive web-interface to inspect and improve recognised entities from an underlying NER+L model via active learning. Secondary use of data for clinical research often has task and context specific criteria. MedCATTrainer provides a further interface to define and collect supervised learning training data for researcher specific use cases. Initial results suggest our approach allows for efficient and accurate collection of research use case specific training data.",,,,,"; dobson, richard/C-9269-2011","Searle, Thomas/0000-0001-8424-3218; dobson, richard/0000-0003-4224-9245",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,139,144,,,,,,,,,,,,,,,,WOS:000855231500024,0
C,"Srivatsan, N; Barron, JT; Klein, D; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Srivatsan, Nikita; Barron, Jonathan T.; Klein, Dan; Berg-Kirkpatrick, Taylor",,,A Deep Factorization of Style and Structure in Fonts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2195,2205,,,,,,,,,,,,,,,,WOS:000854193302033,0
C,"Sui, DAB; Chen, YB; Liu, K; Zhao, J; Liu, SP",,,Assoc Computat Linguist,"Sui, Dianbo; Chen, Yubo; Liu, Kang; Zhao, Jun; Liu, Shengping",,,Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The lack of word boundaries information has been seen as one of the main obstacles to develop a high performance Chinese named entity recognition (NER) system. Fortunately, the automatically constructed lexicon contains rich word boundaries information and word semantic information. However, integrating lexical knowledge in Chinese NER tasks still faces challenges when it comes to self-matched lexical words as well as the nearest contextual lexical words. We present a Collaborative Graph Network to solve these challenges. Experiments on various datasets show that our model not only outperforms the stateof-the-art (SOTA) results, but also achieves a speed that is six to fifteen times faster than that of the SOTA model.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3830,3840,,,,,,,,,,,,,,,,WOS:000854193304001,0
C,"Tan, M; Yu, Y; Wang, HY; Wang, DK; Potdar, S; Chang, SY; Yu, M",,,Assoc Computat Linguist,"Tan, Ming; Yu, Yang; Wang, Haoyu; Wang, Dakuo; Potdar, Saloni; Chang, Shiyu; Yu, Mo",,,Out-of-Domain Detection for Low-Resource Text Classification Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Out-of-domain (OOD) detection for low-resource text classification is a realistic but understudied task. The goal is to detect the OOD cases with limited in-domain (ID) training data, since we observe that training data is often insufficient in machine learning applications. In this work, we propose an OOD-resistant Prototypical Network to tackle this zero-shot OOD detection and few-shot ID classification task. Evaluation on real-world datasets show that the proposed solution out-performs state-of-the-art methods in zero-shot OOD detection task, while maintaining a competitive performance on ID classification task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3566,3572,,,,,,,,,,,,,,,,WOS:000854193303082,0
C,"Toledo, A; Gretz, S; Cohen-Karlik, E; Friedman, R; Venezian, E; Lahav, D; Jacovi, M; Aharonov, R; Slonim, N",,,Assoc Computat Linguist,"Toledo, Assaf; Gretz, Shai; Cohen-Karlik, Edo; Friedman, Roni; Venezian, Elad; Lahav, Dan; Jacovi, Michal; Aharonov, Ranit; Slonim, Noam",,,Automatic Argument Quality Assessment - New Datasets and Methods,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We explore the task of automatic assessment of argument quality. To that end, we actively collected 6:3k arguments, more than a factor of five compared to previously examined data. Each argument was explicitly and carefully annotated for its quality. In addition, 14k pairs of arguments were annotated independently, identifying the higher quality argument in each pair. In spite of the inherent subjective nature of the task, both annotation schemes led to surprisingly consistent results. We release the labeled datasets to the community. Furthermore, we suggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5625,5635,,,,,,,,,,,,,,,,WOS:000854193305077,0
C,"Tsai, YHH; Bai, SJ; Yamada, M; Morency, LP; Salakhutdinov, R",,,Assoc Computat Linguist,"Tsai, Yao-Hung Hubert; Bai, Shaojie; Yamada, Makoto; Morency, Louis-Philippe; Salakhutdinov, Ruslan",,,Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer's attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer's attention. As an example, we propose a new variant of Transformer's attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",,,,,"Liang, Paul/AAL-4346-2020",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4344,4353,,,,,,,,,,,32362720.0,,,,,WOS:000854193304048,0
C,"Voita, E; Sennrich, R; Titov, I",,,Assoc Computat Linguist,"Voita, Elena; Sennrich, Rico; Titov, Ivan",,,Context-Aware Monolingual Repair for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling roundtrip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English -> Russian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.(1)",,,,,,"Sennrich, Rico/0000-0002-1438-4741",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,877,886,,,,,,,,,,,,,,,,WOS:000854193301003,0
C,"Vyas, Y; Carpuat, M",,,Assoc Computat Linguist,"Vyas, Yogarshi; Carpuat, Marine",,,Weakly Supervised Cross-lingual Semantic Relation Classification via Knowledge Distillation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Words in different languages rarely cover the exact same semantic space. This work characterizes differences in meaning between words across languages using semantic relations that have been used to relate the meaning of English words. However, because of translation ambiguity, semantic relations are not always preserved by translation. We introduce a cross-lingual relation classifier trained only with English examples and a bilingual dictionary. Our classifier relies on a novel attention-based distillation approach to account for translation ambiguity when transferring knowledge from English to crosslingual settings. On new English-Chinese and English-Hindi test sets, the resulting models largely outperform baselines that more naively rely on bilingual embeddings or dictionaries for cross-lingual transfer, and approach the performance of fully supervised systems on English tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5285,5296,,,,,,,,,,,,,,,,WOS:000854193305045,0
C,"Wang, ZH; Lai, KP; Li, PJ; Bing, LD; Lam, W",,,Assoc Computat Linguist,"Wang, Zihao; Lai, Kwun Ping; Li, Piji; Bing, Lidong; Lam, Wai",,,Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"For large-scale knowledge graphs (KGs), recent research has been focusing on the large proportion of infrequent relations which have been ignored by previous studies. For example few-shot learning paradigm for relations has been investigated. In this work, we further advocate that handling uncommon entities is inevitable when dealing with infrequent relations. Therefore, we propose a meta-learning framework that aims at handling infrequent relations with few-shot learning and uncommon entities by using textual descriptions. We design a novel model to better extract key information from textual descriptions. Besides, we also develop a novel generative model in our framework to enhance the performance by generating extra triplets during the training stage. Experiments are conducted on two datasets from real-world KGs, and the results show that our framework 1 outperforms previous methods when dealing with infrequent relations and their accompanying uncommon entities.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,250,260,,,,,,,,,,,,,,,,WOS:000854193300024,0
C,"Wei, J; Zou, K",,,Assoc Computat Linguist,"Wei, Jason; Zou, Kai",,,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6382,6388,,,,,,,,,,,,,,,,WOS:000854193306081,0
C,"Wu, JW; Xiong, WH; Wang, LY",,,Assoc Computat Linguist,"Wu, Jiawei; Xiong, Wenhan; Wang, William Yang",,,Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many tasks in natural language processing can be viewed as multi-label classification problems. However, most of the existing models are trained with the standard cross-entropy loss function and use a fixed prediction policy (e.g., a threshold of 0.5) for all the labels, which completely ignores the complexity and dependencies among different labels. In this paper, we propose a meta-learning method to capture these complex label dependencies. More specifically, our method utilizes a metalearner to jointly learn the training policies and prediction policies for different labels. The training policies are then used to train the classifier with the cross-entropy loss function, and the prediction policies are further implemented for prediction. Experimental results on fine-grained entity typing and text classification demonstrate that our proposed method can obtain more accurate multi-label classification results.",,,,,,"Wang, William S-Y./0000-0001-6153-8240",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4354,4364,,,,,,,,,,,,,,,,WOS:000854193304049,0
C,"Xu, P; Wu, CS; Madotto, A; Fung, P",,,Assoc Computat Linguist,"Xu, Peng; Wu, Chien-Sheng; Madotto, Andrea; Fung, Pascale",,,Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Sensational headlines are headlines that capture people's attention and generate reader interest. Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention. In this paper, we propose a model that generates sensational headlines without labeled data. We first train a sensationalism scorer by classifying online headlines with many comments (clickbait) against a baseline of headlines generated from a summarization model. The score from the sensationalism scorer is used as the reward for a reinforcement learner. However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines. To effectively leverage this noisy reward, we propose a novel loss function, Auto-tuned Reinforcement Learning (ARL), to dynamically balance reinforcement learning (RL) with maximum likelihood estimation (MLE). Human evaluation shows that 60.8% of samples generated by our model are sensational, which is significantly better than the Pointer-Gen baseline (See et al., 2017) and other RL models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3065,3075,,,,,,,,,,,,,,,,WOS:000854193303021,0
C,"Zhou, YC; Jiang, JY; Chang, KW; Wang, W",,,Assoc Computat Linguist,"Zhou, Yichao; Jiang, Jyun-Yu; Chang, Kai-Wei; Wang, Wei",,,Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis. In this paper, we propose a novel framework, learning to discriminate perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models. To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations. For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search. DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure. Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification. In addition, in-depth analysis shows the robustness of DISP across different situations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4904,4913,,,,,,,,,,,,,,,,WOS:000854193305009,0
C,"Dabre, R; Fujita, A; Chu, CH",,,Assoc Computat Linguist,"Dabre, Raj; Fujita, Atsushi; Chu, Chenhui",,,Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper highlights the impressive utility of multi-parallel corpora for transfer learning in a one-to-many low-resource neural machine translation (NMT) setting. We report on a systematic comparison of multistage fine-tuning configurations, consisting of (1) pretraining on an external large (209k-440k) parallel corpus for English and a helping target language, (2) mixed pre-training or fine-tuning on a mixture of the external and low-resource (18k) target parallel corpora, and (3) pure finetuning on the target parallel corpora. Our experiments confirm that multi-parallel corpora are extremely useful despite their scarcity and content-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage finetuning can give 3-9 BLEU score gains over a simple one-to-one model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1410,1416,,,,,,,,,,,,,,,,WOS:000854193301068,0
C,"Ding, CC; Utiyama, M; Sumita, E",,,ASSOC COMPUTAT LINGUIST,"Ding, Chenchen; Utiyama, Masao; Sumita, Eiichiro",,,MY-AKKHARA: A Romanization-based Burmese (Myanmar) Input Method,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"MY-AKKHARA is a method used to input Burmese texts encoded in the Unicode standard, based on commonly accepted Latin transcription. By using this method, arbitrary Burmese strings can be accurately inputted with 26 lowercase Latin letters. Meanwhile, the 26 uppercase Latin letters are designed as shortcuts of lowercase letter sequences. The frequency of Burmese characters is considered in MY-AKKHARA to realize an efficient keystroke distribution on a QWERTY keyboard. Given that the Unicode standard has not been extensively used in digitization of Burmese, we hope that MY-AKKHARA can contribute to the widespread use of Unicode in Myanmar and can provide a platform for smart input methods for Burmese in the future. An implementation of MY-AKKHARA running in Windows is released at http: //www2.nict.go.jp/astrec- att/ member/ding/my-akkhara.html",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,157,162,,,,,,,,,,,,,,,,WOS:000855231500027,0
C,"Gao, X; Zhang, YZ; Lee, SJ; Galley, M; Brockett, C; Gao, JF; Dolan, B",,,Assoc Computat Linguist,"Gao, Xiang; Zhang, Yizhe; Lee, Sungjin; Galley, Michel; Brockett, Chris; Gao, Jianfeng; Dolan, Bill",,,Structuring Latent Spaces for Stylized Response Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating responses in a targeted style is a useful yet challenging task, especially in the absence of parallel data. With limited data, existing methods tend to generate responses that are either less stylized or less context-relevant. We propose STYLEFUSION, which bridges conversation modeling and non-parallel style transfer by sharing a structured latent space. This structure allows the system to generate stylized relevant responses by sampling in the neighborhood of the conversation model prediction, and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sher-lock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. (1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1814,1823,,,,,,,,,,,,,,,,WOS:000854193301112,0
C,"Garcia, A; Colombo, P; Essid, S; d'Alche-Buc, F; Clavel, C",,,Assoc Computat Linguist,"Garcia, Alexandre; Colombo, Pierre; Essid, Slim; d'Alche-Buc, Florence; Clavel, Chloe",,,From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of predicting fine grained user opinion based on spontaneous spoken language is a key problem arising in the development of Computational Agents as well as in the development of social network based opinion miners. Unfortunately, gathering reliable data on which a model can be trained is notoriously difficult and existing works rely only on coarsely labeled opinions. In this work we aim at bridging the gap separating fine grained opinion models already developed for written language and coarse grained models developed for spontaneous multimodal opinion mining. We take advantage of the implicit hierarchical structure of opinions to build a joint fine and coarse grained opinion model that exploits different views of the opinion expression. The resulting model shares some properties with attentionbased models and is shown to provide competitive results on a recently released multimodal fine grained annotated corpus.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5539,5548,,,,,,,,,,,,,,,,WOS:000854193305069,0
C,"Gonen, H; Goldberg, Y",,,Assoc Computat Linguist,"Gonen, Hila; Goldberg, Yoav",,,"Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons: (1) lack of available large-scale codeswitched data for training; (2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4175,4185,,,,,,,,,,,,,,,,WOS:000854193304032,0
C,"Jiang, NJ; de Marneffe, MC",,,Assoc Computat Linguist,"Jiang, Nanjiang; de Marneffe, Marie-Catherine",,,Evaluating BERT for natural language inference: A case study on the CommitmentBank,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Natural language inference (NLI) datasets (e.g., MultiNLI) were collected by soliciting hypotheses for a given premise from annotators. Such data collection led to annotation artifacts: systems can identify the premise-hypothesis relationship without observing the premise (e.g., negation in hypothesis being indicative of contradiction). We address this problem by recasting the CommitmentBank for NLI, which contains items involving reasoning over the extent to which a speaker is committed to complements of clause-embedding verbs under entailment-canceling environments (conditional, negation, modal and question). Instead of being constructed to stand in certain relationships with the premise, hypotheses in the recast CommitmentBank are the complements of the clause-embedding verb in each premise, leading to no annotation artifacts in the hypothesis. A state-of-the-art BERT-based model performs well on the CommitmentBank with 85% F1. However analysis of model behavior shows that the BERT models still do not capture the full complexity of pragmatic reasoning, nor encode some of the linguistic generalizations, highlighting room for improvement.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6086,6091,,,,,,,,,,,,,,,,WOS:000854193306041,0
C,"Jwalapuram, P; Joty, S; Temnikova, I; Nakov, P",,,Assoc Computat Linguist,"Jwalapuram, Prathyusha; Joty, Shafiq; Temnikova, Irina; Nakov, Preslav",,,Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures such as BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2964,2975,,,,,,,,,,,,,,,,WOS:000854193303012,0
C,"Lin, ZJ; Madotto, A; Shin, J; Xu, P; Fung, P",,,Assoc Computat Linguist,"Lin, Zhaojiang; Madotto, Andrea; Shin, Jamin; Xu, Peng; Fung, Pascale",,,MoEL: Mixture of Empathetic Listeners,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous research on empathetic dialogue systems has mostly focused on generating responses given certain emotions. However, being empathetic not only requires the ability of generating emotional responses, but more importantly, requires the understanding of user emotions and replying appropriately. In this paper, we propose a novel end-toend approach for modeling empathy in dialogue systems: Mixture of Empathetic Listeners (MoEL). Our model first captures the user emotions and outputs an emotion distribution. Based on this, MoEL will softly combine the output states of the appropriate Listener(s), which are each optimized to react to certain emotions, and generate an empathetic response. Human evaluations on empathetic-dialogues (Rashkin et al., 2018) dataset confirm that MoEL outperforms multitask training baseline in terms of empathy, relevance, and fluency. Furthermore, the case study on generated responses of different Listeners shows high interpretability of our model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,121,132,,,,,,,,,,,,,,,,WOS:000854193300012,0
C,"Liu, MT; Zhang, YJ; Xu, JN; Chen, YF",,,Assoc Computat Linguist,"Liu, Mingtong; Zhang, Yujie; Xu, Jinan; Chen, Yufeng",,,Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Sentence matching is a key issue in natural language inference and paraphrase identification. Despite the recent progress on multi-layered neural network with cross sentence attention, one sentence learns attention to the intermediate representations of another sentence, which are propagated from preceding layers and therefore are uncertain and unstable for matching, particularly at the risk of error propagation. In this paper, we present an original semantics-oriented attention and deep fusion network (OSOA-DFN) for sentence matching. Unlike existing models, each attention layer of OSOA-DFN is oriented to the original semantic representation of another sentence, which captures the relevant information from a fixed matching target. The multiple attention layers allow one sentence to repeatedly read the important information of another sentence for better matching. We then additionally design deep fusion to propagate the attention information at each matching layer. At last, we introduce a self-attention mechanism to capture global context to enhance attention-aware representation within each sentence. Experiment results on three sentence matching benchmark datasets SNLI, SciTail and Quora show that OSOA-DFN has the ability to model sentence matching more precisely.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2652,2661,,,,,,,,,,,,,,,,WOS:000854193302075,0
C,"Lv, X; Gu, YX; Han, X; Hou, L; Li, JZ; Liu, ZY",,,Assoc Computat Linguist,"Lv, Xin; Gu, Yuxian; Han, Xu; Hou, Lei; Li, Juanzi; Liu, Zhiyuan",,,Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough training triples, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms the current state-of-the-art methods in few-shot scenarios. Our code and datasets can be obtained from https://github.com/THU-KEG/MetaKGR.",,,,,,"Hou, Lei/0000-0002-8907-3526; Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3376,3381,,,,,,,,,,,,,,,,WOS:000854193303052,0
C,"Merullo, J; Yeh, L; Handler, A; Grissom, A; O'Connor, B; Iyyer, M",,,Assoc Computat Linguist,"Merullo, Jack; Yeh, Luke; Handler, Abram; Grissom, Alvin, II; O'Connor, Brendan; Iyyer, Mohit",,,Investigating Sports Commentator Bias within a Large Corpus of American Football Broadcasts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Sports broadcasters inject drama into play-by-play commentary by building team and player narratives through subjective analyses and anecdotes. Prior studies based on small datasets and manual coding show that such theatrics evince commentator bias in sports broadcasts. To examine this phenomenon, we assemble FOOTBALL, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250K player mentions and linked with racial metadata. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.",,,,,,"Grissom II, Alvin/0000-0002-6503-2703",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6355,6361,,,,,,,,,,,,,,,,WOS:000854193306077,0
C,"Patro, J; Bansal, S; Mukherjee, A",,,Assoc Computat Linguist,"Patro, Jasabanta; Bansal, Srijan; Mukherjee, Animesh",,,A deep-learning framework to detect sarcasm targets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper we propose a deep learning framework for sarcasm target detection in pre-defined sarcastic texts. Identification of sarcasm targets can help in many core natural language processing tasks such as aspect based sentiment analysis, opinion mining etc. To begin with, we perform an empirical study of the socio-linguistic features and identify those that are statistically significant in indicating sarcasm targets (p-values in the range (0:05; 0:001)). Finally, we present a deeplearning framework augmented with sociolinguistic features to detect sarcasm targets in sarcastic book-snippets and tweets. We achieve a huge improvement in the performance in terms of exact match and dice score as compared to the current state-of-the-art baseline.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6336,6342,,,,,,,,,,,,,,,,WOS:000854193306074,0
C,"Perez, E; Karamcheti, S; Fergus, R; Weston, J; Kiela, D; Cho, K",,,Assoc Computat Linguist,"Perez, Ethan; Karamcheti, Siddharth; Fergus, Rob; Weston, Jason; Kiela, Douwe; Cho, Kyunghyun",,,Finding Generalizable Evidence by Learning to Convince Q&A Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only.20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2402,2411,,,,,,,,,,,,,,,,WOS:000854193302052,0
C,"Shang, MY; Li, PJ; Fu, ZX; Bing, LD; Zhao, DY; Shi, SM; Yan, R",,,Assoc Computat Linguist,"Shang, Mingyue; Li, Piji; Fu, Zhenxin; Bing, Lidong; Zhao, Dongyan; Shi, Shuming; Yan, Rui",,,Semi-supervised Text Style Transfer: Cross Projection in Latent Space,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text style transfer task requires the model to transfer a sentence of one style to another style while retaining its original content meaning, which is a challenging problem that has long suffered from the shortage of parallel data. In this paper, we first propose a semi-supervised text style transfer model that combines the small-scale parallel data with the large-scale nonparallel data. With these two types of training data, we introduce a projection function between the latent space of different styles and design two constraints to train it. We also introduce two other simple but effective semi-supervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4937,4946,,,,,,,,,,,,,,,,WOS:000854193305012,0
C,"Shen, XY; Zhao, Y; Su, H; Klakowi, D",,,Assoc Computat Linguist,"Shen, Xiaoyu; Zhao, Yang; Su, Hui; Klakowi, Dietrich",,,Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks: Firstly, the pointer is limited to copying the exact words while ignoring possible inflections or abstractions, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the model to edit pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned relation embedding. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higherquality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3762,3773,,,,,,,,,,,,,,,,WOS:000854193303108,0
C,"Song, LF; Zhang, Y; Glidea, D; Yu, M; Wang, ZG; Su, JS",,,Assoc Computat Linguist,"Song, Linfeng; Zhang, Yue; Glidea, Daniel; Yu, Mo; Wang, Zhiguo; Su, Jinsong",,,Leveraging Dependency Forest for Neural Medical Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Medical relation extraction discovers relations between entity mentions in text, such as research articles. For this task, dependency syntax has been recognized as a crucial source of features. Yet in the medical domain, 1-best parse trees suffer from relatively low accuracies, diminishing their usefulness. We investigate a method to alleviate this problem by utilizing dependency forests. Forests contain many possible decisions and therefore have higher recall but more noise compared with 1-best outputs. A graph neural network is used to represent the forests, automatically distinguishing the useful syntactic information from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,208,218,,,,,,,,,,,,,,,,WOS:000854193300020,0
C,"Wang, WY; Wang, YC; Chen, SZ; Jin, Q",,,Assoc Computat Linguist,"Wang, Weiying; Wang, Yongcheng; Chen, Shizhe; Jin, Qin",,,YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multimodal semantic comprehension has attracted increasing research interests in recent years, such as visual question answering and caption generation. However, due to the data limitation, fine-grained semantic comprehension which requires to capture semantic details of multimodal contents has not been well investigated. In this work, we introduce YouMakeup, a large-scale multimodal instructional video dataset to support finegrained semantic comprehension research in specific domain. YouMakeup contains 2,800 videos from YouTube, spanning more than 420 hours in total. Each video is annotated with a sequence of natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. The annotated steps in a video involve subtle difference in actions, products and regions, which require fine-grained understanding and reasoning both temporally and spatially. In order to evaluate models' ability for fined-grained comprehension, we further propose two groups of tasks including generation tasks and visual question answering tasks from different aspects. We also establish a baseline of step caption generation for future comparison. The dataset will be publicly available at https:// github.com/AIM3-RUC/YouMakeup to support research investigation in fine-grained semantic comprehension.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5133,5143,,,,,,,,,,,,,,,,WOS:000854193305030,0
C,"Wu, LJ; Wang, YR; Xia, YC; Qin, T; Lai, JH; Liu, TY",,,Assoc Computat Linguist,"Wu, Lijun; Wang, Yiren; Xia, Yingce; Qin, Tao; Lai, Jianhuang; Liu, Tie-Yan",,,Exploiting Monolingual Data at Scale for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While target-side monolingual data has been proven to be very useful to improve neural machine translation (briefly, NMT) through back translation, source-side monolingual data is not well investigated. In this work, we study how to use both the source-side and targetside monolingual data for NMT, and propose an effective strategy leveraging both of them. First, we generate synthetic bitext by translating monolingual data from the two domains into the other domain using the models pretrained on genuine bitext. Next, a model is trained on a noised version of the concatenated synthetic bitext where each source sequence is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext and a clean version of a subset of the synthetic bitext without adding any noise. Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 English <-> German translations and WMT19 German <-> French translations, which demonstrate the effectiveness of our method. We also conduct a comprehensive study on how each part in the pipeline works.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4207,4216,,,,,,,,,,,,,,,,WOS:000854193304035,0
C,"Yuan, XD; Cote, MA; Fu, J; Lin, ZH; Pal, C; Bengio, Y; Trischler, A",,,Assoc Computat Linguist,"Yuan, Xingdi; Cote, Marc-Alexandre; Fu, Jie; Lin, Zhouhan; Pal, Christopher; Bengio, Yoshua; Trischler, Adam",,,Interactive Language Learning by Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit)1. In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2796,2813,,,,,,,,,,,,,,,,WOS:000854193302088,0
C,"Zeng, XR; He, SZ; Zeng, DJ; Liu, K; Zhao, J",,,Assoc Computat Linguist,"Zeng, Xiangrong; He, Shizhu; Zeng, Daojian; Liu, Kang; Zhao, Jun",,,Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The multiple relation extraction task tries to extract all relational facts from a sentence. Existing works didn't consider the extraction order of relational facts in a sentence. In this paper we argue that the extraction order is important in this task. To take the extraction order into consideration, we apply the reinforcement learning into a sequence-to-sequence model. The proposed model could generate relational facts freely. Widely conducted experiments on two public datasets demonstrate the efficacy of the proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,367,377,,,,,,,,,,,,,,,,WOS:000854193300035,0
C,"Assael, Y; Sommerschield, T; Prag, J",,,Assoc Computat Linguist,"Assael, Yannis; Sommerschield, Thea; Prag, Jonathan",,,Restoring ancient text using deep learning: a case study on Greek epigraphy,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Ancient History relies on disciplines such as Epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, inscriptions, are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists. This work presents PYTHIA, the first ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. Its architecture is carefully designed to handle longterm context information, and deal efficiently with missing or corrupted character and word representations. To train it, we wrote a nontrivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML. On PHI-ML, PYTHIA's predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of PYTHIA, which effectively demonstrates the impact of this assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration.",,,,,"Assael, Yannis/E-8160-2013","Assael, Yannis/0000-0001-7408-3847",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6368,6375,,,,,,,,,,,,,,,,WOS:000854193306079,0
C,"Beltagy, I; Lo, K; Cohan, A",,,Assoc Computat Linguist,"Beltagy, Iz; Lo, Kyle; Cohan, Arman",,,SCIBERT: A Pretrained Language Model for Scientific Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3615,3620,,,,,,,,,,,,,,,,WOS:000854193303089,0
C,"Breitfeller, LM; Ahn, E; Jurgens, D; Tsvetkov, Y",,,Assoc Computat Linguist,"Breitfeller, Luke M.; Ahn, Emily; Jurgens, David; Tsvetkov, Yulia",,,Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and well-defined criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of microaggression data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective criterion for annotation and an activelearning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets.",,,,,,"Tsvetkov, Yulia/0000-0002-4634-7128",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1664,1674,,,,,,,,,,,,,,,,WOS:000854193301098,0
C,"Chen, FX; Hwang, SW; Choo, J; Ha, JW; Kim, S",,,Assoc Computat Linguist,"Chen, Fuxiang; Hwang, Seung-won; Choo, Jaegul; Ha, Jung-Woo; Kim, Sunghun",,,NL2pSQL: Generating Pseudo-SQL Queries from Under-Specified Natural Language Questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating SQL codes from natural language questions (NL2SQL) is an emerging research area. Existing studies have mainly focused on clear scenarios where specified information is fully given to generate a SQL query. However, in developer forums such as Stack Overflow,1 questions cover more diverse tasks including table manipulation or performance issues, where a table is not specified. The SQL query posted in Stack Overflow, PseudoSQL (pSQL), does not usually contain table schemas and is not necessarily executable, is sufficient to guide developers. Here we describe a new NL2pSQL task to generate pSQL codes from natural language questions on under-specified database issues, in short, NL2pSQL. In addition, we define two new metrics suitable for the proposed NL2pSQL task, Canonical-BLEU and SQL-BLEU, instead of the conventional BLEU. With a baseline model using sequence-to-sequence architecture integrated with denoising autoencoder, we confirm the validity of our task. Experiments show that the proposed NL2pSQL approach yields well-formed queries (up to 43% more than a standard Seq2Seq model). Our code and datasets are publicly available at http: //github.com/clovaai/nl2psql.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2603,2613,,,,,,,,,,,,,,,,WOS:000854193302070,0
C,"Cui, LY; Zhang, Y",,,Assoc Computat Linguist,"Cui, Leyang; Zhang, Yue",,,Hierarchically-Refined Label Attention Network for Sequence Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"CRF has been used as a powerful model for statistical sequence labeling. For neural sequence labeling, however, BiLSTM-CRF does not always lead to better results compared with BiLSTM-softmax local classification. This can be because the simple Markov label transition model of CRF does not give much information gain over strong neural encoding. For better representing label sequences, we investigate a hierarchically-refined label attention network, which explicitly leverages label embeddings and captures potential long-term label dependency by giving each word incrementally refined label distributions with hierarchical attention. Results on POS tagging, NER and CCG supertagging show that the proposed model not only improves the overall tagging accuracy with similar number of parameters, but also significantly speeds up the training and testing compared to BiLSTM-CRF.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4115,4128,,,,,,,,,,,,,,,,WOS:000854193304027,0
C,"Davani, AM; Yeh, L; Atari, M; Portillo-Wightman, BKG; Gonzalez, E; Delong, N; Bhatia, R; Mirinjian, A; Ren, X; Dehghani, M",,,Assoc Computat Linguist,"Davani, Aida Mostafazadeh; Yeh, Leigh; Atari, Mohammad; Portillo-Wightman, Brendan Kennedy Gwenyth; Gonzalez, Elaine; Delong, Natalie; Bhatia, Rhea; Mirinjian, Arineh; Ren, Xiang; Dehghani, Morteza",,,Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Official reports of hate crimes in the US are under-reported relative to the actual number of such incidents. Further, despite statistical approximations, there are no official reports from a large number of US cities regarding incidents of hate. Here, we first demonstrate that event extraction and multi-instance learning, applied to a corpus of local news articles, can be used to predict instances of hate crime. We then use the trained model to detect incidents of hate in cities for which the FBI lacks statistics. Lastly, we train models on predicting homicide and kidnapping, compare the predictions to FBI reports, and establish that incidents of hate are indeed under-reported, compared to other types of crimes, in local press.",,,,,,"Dehghani, Morteza/0000-0002-9478-4365",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5753,5757,,,,,,,,,,,,,,,,WOS:000854193305093,0
C,"Elgohary, A; Peskov, D; Boyd-Graber, J",,,Assoc Computat Linguist,"Elgohary, Ahmed; Peskov, Denis; Boyd-Graber, Jordan",,,Can You Unpack That? Learning to Rewrite Questions-in-Context,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Question answering is an AI-complete problem, but existing datasets lack key elements of language understanding such as coreference and ellipsis resolution. We consider sequential question answering: multiple questions are asked one-by-one in a conversation between a questioner and an answerer. Answering these questions is only possible through understanding the conversation history. We introduce the task of question-in-context rewriting: given the context of a conversation's history, rewrite a context-dependent into a selfcontained question with the same answer. We construct, CANARD, a dataset of 40,527 questions based on QUAC (Choi et al., 2018) and train Seq2Seq models for incorporating context into standalone questions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5918,5924,,,,,,,,,,,,,,,,WOS:000854193306016,0
C,"Ghazvininejad, M; Levy, O; Liu, YH; Zettlemoyer, L",,,Assoc Computat Linguist,"Ghazvininejad, Marjan; Levy, Omer; Liu, Yinhan; Zettlemoyer, Luke",,,Mask-Predict: Parallel Decoding of Conditional Masked Language Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6112,6121,,,,,,,,,,,,,,,,WOS:000854193306044,0
C,"Gui, L; Leng, J; Pergola, G; Zhou, Y; Xu, RF; He, YL",,,Assoc Computat Linguist,"Gui, Lin; Leng, Jia; Pergola, Gabriele; Zhou, Yu; Xu, Ruifeng; He, Yulan",,,Neural Topic Model with Reinforcement Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In recent years, advances in neural variational inference have achieved many successes in text processing. Examples include neural topic models which are typically built upon variational autoencoder (VAE) with an objective of minimising the error of reconstructing original documents based on the learned latent topic vectors. However, minimising reconstruction errors does not necessarily lead to high quality topics. In this paper, we borrow the idea of reinforcement learning and incorporate topic coherence measures as reward signals to guide the learning of a VAE-based topic model. Furthermore, our proposed model is able to automatically separating background words dynamically from topic words, thus eliminating the pre-processing step of filtering infrequent and/or top frequent words, typically required for learning traditional topic models. Experimental results on the 20 Newsgroups and the NIPS datasets show superior performance both on perplexity and topic coherence measure compared to state-of-the-art neural topic models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3478,3483,,,,,,,,,,,,,,,,WOS:000854193303068,0
C,"Gupta, S; Kenkre, S; Talukdar, P",,,Assoc Computat Linguist,"Gupta, Swapnil; Kenkre, Sreyash; Talukdar, Partha",,,CaRe: Open Knowledge Graph Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Open Information Extraction (OpenIE) methods are effective at extracting (noun phrase, relation phrase, noun phrase) triples from text, e.g., (Barack Obama, took birth in, Honolulu). Organization of such triples in the form of a graph with noun phrases (NPs) as nodes and relation phrases (RPs) as edges results in the construction of Open Knowledge Graphs (OpenKGs). In order to use such OpenKGs in downstream tasks, it is often desirable to learn embeddings of the NPs and RPs present in the graph. Even though several Knowledge Graph (KG) embedding methods have been recently proposed, all of those methods have targeted Ontological KGs, as opposed to OpenKGs. Straightforward application of existing Ontological KG embedding methods to OpenKGs is challenging, as unlike Ontological KGs, OpenKGs are not canonicalized, i.e., a real-world entity may be represented using multiple nodes in the OpenKG, with each node corresponding to a different NP referring to the entity. For example, nodes with labels Barack Obama, Obama, and President Obama may refer to the same real-world entity Barack Obama. Even though canonicalization of OpenKGs has received some attention lately, output of such methods has not been used to improve OpenKG embeddings. We fill this gap in the paper and propose Canonicalization-infused Representations (CaRe) for OpenKGs. Through extensive experiments, we observe that CaRe enables existing models to adapt to the challenges in OpenKGs and achieve substantial improvements for the link prediction task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,378,388,,,,,,,,,,,,,,,,WOS:000854193300036,0
C,"Jiang, ZR; Gao, Z; He, GX; Kang, YY; Sun, CL; Zhang, Q; Si, L; Liu, XZ",,,Assoc Computat Linguist,"Jiang, Zhuoren; Gao, Zhe; He, Guoxiu; Kang, Yangyang; Sun, Changlong; Zhang, Qiong; Si, Luo; Liu, Xiaozhong",,,Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the graph embeddings of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6187,6196,,,,,,,,,,,,,,,,WOS:000854193306051,0
C,"Lai, YH; Chang, J",,,ASSOC COMPUTAT LINGUIST,"Lai, Yi-Huei; Chang, Jason",,,TellMeWhy: Learning to Explain Corrective Feedback for Second Language Learners,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a writing prototype feedback system, TellMeWhy, to provide explanations of errors in submitted essays. In our approach, the sentence with corrections is analyzed to identify error types and problem words, aimed at customizing explanations based on the context of the error. The method involves learning the relation of errors and problem words, generating common feedback patterns, and extracting grammar patterns, collocations and example sentences. At run-time, a sentence with corrections is classified, and the problem word and template are identified to provide detailed explanations. Preliminary evaluation shows that the method has potential to improve existing commercial writing services.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,235,240,,,,,,,,,,,,,,,,WOS:000855231500040,0
C,"Liu, LQ; Yang, W; Rao, JF; Tang, R; Lin, J",,,Assoc Computat Linguist,"Liu, Linqing; Yang, Wei; Rao, Jinfeng; Tang, Raphael; Lin, Jimmy",,,Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. However, such structure priors have not been well exploited in previous work for semantic modeling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1204,1209,,,,,,,,,,,,,,,,WOS:000854193301036,0
C,"Lu, ZC; Arabshahi, F; Labutov, I; Mitchell, T",,,Assoc Computat Linguist,"Lu, Zhichu; Arabshahi, Forough; Labutov, Igor; Mitchell, Tom",,,Look-up and Adapt: A One-shot Semantic Parser,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Computing devices have recently become capable of interacting with their end users via natural language. However, they can only operate within a limited supported domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their semantic parser. In this paper, we propose a semantic parser that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8% on one-shot parsing under two different evaluation settings compared to the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1129,1139,,,,,,,,,,,,,,,,WOS:000854193301026,0
C,"Mishra, BD; Tandon, N; Bosselut, A; Yih, WT; Clark, P",,,Assoc Computat Linguist,"Mishra, Bhavana Dalvi; Tandon, Niket; Bosselut, Antoine; Yih, Wen-tau; Clark, Peter",,,Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Our goal is to better comprehend procedural text, e.g., a paragraph about photosynthesis, by not only predicting what happens, but why some actions need to happen before others. Our approach builds on a prior process comprehension framework for predicting actions' e ffects, to also identify subsequent steps that those e ffects enable. We present our new model (XPAD) that biases e ffect predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. We also extend an existing benchmark dataset for procedural text comprehension, ProPara, by adding the new task of explaining actions by predicting their dependencies. We find that XPAD significantly outperforms prior systems on this task, while maintaining the performance on the original task in ProPara. The dataset is available at http://data.allenai.org/propara",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4496,4505,,,,,,,,,,,,,,,,WOS:000854193304062,0
C,"Moon, S; Shah, P; Kumar, A; Subba, R",,,ASSOC COMPUTAT LINGUIST,"Moon, Seungwhan; Shah, Pararth; Kumar, Anuj; Subba, Rajen",,,Memory Grounded Conversational Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We demonstrate a conversational system which engages the user through a multi-modal, multi-turn dialog over the user's memories. The system can perform QA over memories by responding to user queries to recall specific attributes and associated media (e.g. photos) of past episodic memories. The system can also make proactive suggestions to surface related events or facts from past memories to make conversations more engaging and natural. To implement such a system, we collect a new corpus of memory grounded conversations, which comprises human-to-human role-playing dialogs given synthetic memory graphs with simulated attributes. Our proofof-concept system operates on these synthetic memory graphs, however it can be trained and applied to real-world user memory data (e.g. photo albums, etc.) We present the architecture of the proposed conversational system, and example queries that the system supports.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,145,150,,,,,,,,,,,,,,,,WOS:000855231500025,0
C,"Moshtaghi, M",,,Assoc Computat Linguist,"Moshtaghi, Masud",,,Supervised and Nonlinear Alignment of Two Embedding Spaces for Dictionary Induction in Low Resourced Languages,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Enabling cross-lingual NLP tasks by leveraging multilingual word embedding has recently attracted much attention. An important motivation is to support lower resourced languages, however, most efforts focus on demonstrating the effectiveness of the techniques using embeddings derived from similar languages to English with large parallel content. In this study, we present a noise tolerant piecewise linear technique to learn a non-linear mapping between two monolingual word embedding vector spaces. We evaluate our approach on inferring bilingual dictionaries. We show that our technique outperforms the state of the art in lower resourced settings with an average improvement of 3.7% for precision @10 across 14 mostly low resourced languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,823,832,,,,,,,,,,,,,,,,WOS:000854193300076,0
C,"Orbach, M; Bilu, Y; Gera, A; Kantor, Y; Dankin, L; Lavee, T; Kotlerman, L; Mirkin, S; Jacovi, M; Aharonov, R; Slonim, N",,,Assoc Computat Linguist,"Orbach, Matan; Bilu, Yonatan; Gera, Ariel; Kantor, Yoav; Dankin, Lena; Lavee, Tamar; Kotlerman, Lili; Mirkin, Shachar; Jacovi, Michal; Aharonov, Ranit; Slonim, Noam",,,A Dataset of General-Purpose Rebuttal,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In Natural Language Understanding, the task of response generation is usually focused on responses to short texts, such as tweets or a turn in a dialog. Here we present a novel task of producing a critical response to a long argumentative text, and suggest a method based on general rebuttal arguments to address it. We do this in the context of the recently-suggested task of listening comprehension over argumentative content: given a speech on some specified topic, and a list of relevant arguments, the goal is to determine which of the arguments appear in the speech. The general rebuttals we describe here (written in English) overcome the need for topic-specific arguments to be provided, by proving to be applicable for a large set of topics. This allows creating responses beyond the scope of topics for which specific arguments are available. All data collected during this work is freely available for research(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5591,5601,,,,,,,,,,,,,,,,WOS:000854193305074,0
C,"Qian, LH; Qiu, L; Zhang, WN; Jiang, X; Yu, Y",,,Assoc Computat Linguist,"Qian, Lihua; Qiu, Lin; Zhang, Weinan; Jiang, Xin; Yu, Yong",,,Exploring Diverse Expressions for Paraphrase Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Paraphrasing plays an important role in various natural language processing (NLP) tasks, such as question answering, information retrieval and sentence simplification. Recently, neural generative models have shown promising results in paraphrase generation. However, prior work mainly focused on single paraphrase generation, while ignoring the fact that diversity is essential for enhancing generalization capability and robustness of downstream applications. Few works have been done to solve diverse paraphrase generation. In this paper, we propose a novel approach with two discriminators and multiple generators to generate a variety of different paraphrases. A reinforcement learning algorithm is applied to train our model. Our experiments on two realworld datasets demonstrate that our model not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3173,3182,,,,,,,,,,,,,,,,WOS:000854193303031,0
C,"Shen, XY; Suzuki, J; Inui, K; Su, H; Klakow, D; Sekine, S",,,Assoc Computat Linguist,"Shen, Xiaoyu; Suzuki, Jun; Inui, Kentaro; Su, Hui; Klakow, Dietrich; Sekine, Satoshi",,,Select and Attend: Towards Controllable Content Selection in Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many text generation tasks naturally contain two steps: content selection and surface realization. Current neural encoder-decoder models conflate both steps into a black-box architecture. As a result, the content to be described in the text cannot be explicitly controlled. This paper tackles this problem by decoupling content selection from the decoder. The decoupled content selection is human interpretable, whose value can be manually manipulated to control the content of generated text. The model can be trained end-to-end without human annotations by maximizing a lower bound of the marginal likelihood. We further propose an effective way to trade-off between performance and controllability with a single adjustable hyperparameter. In both data-to-text and headline generation tasks, our model achieves promising results, paving the way for controllable content selection in text generation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,579,590,,,,,,,,,,,,,,,,WOS:000854193300054,0
C,"Taitelbaum, H; Chechik, G; Goldberger, J",,,Assoc Computat Linguist,"Taitelbaum, Hagai; Chechik, Gal; Goldberger, Jacob",,,A Multi-Pairwise Extension of Procrustes Analysis for Multilingual Word Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper we present a novel approach to simultaneously representing multiple languages in a common space. Procrustes Analysis (PA) is commonly used to find the optimal orthogonal word mapping in the bilingual case. The proposed Multi Pairwise Procrustes Analysis (MPPA) is a natural extension of the PA algorithm to multilingual word mapping. Unlike previous PA extensions that require a k-way dictionary, this approach requires only pairwise bilingual dictionaries that are much easier to construct in either a supervised or an unsupervised way. The improved performance of the MPPA algorithm is demonstrated on two standard multilingual tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3560,3565,,,,,,,,,,,,,,,,WOS:000854193303081,0
C,"Wang, H; Liu, B; Li, CZ; Yang, Y; Li, TR",,,Assoc Computat Linguist,"Wang, Hao; Liu, Bing; Li, Chaozhuo; Yang, Yan; Li, Tianrui",,,Learning with Noisy Labels for Sentence-level Sentiment Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep neural networks (DNNs) can fit (or even over-fit) the training data very well. If a DNN model is trained using data with noisy labels and tested on data with clean labels, the model may perform poorly. This paper studies the problem of learning with noisy labels for sentence-level sentiment classification. We propose a novel DNN model called NETAB (as shorthand for convolutional neural NETworks with AB-networks) to handle noisy labels during training. NETAB consists of two convolutional neural networks, one with a noise transition layer for dealing with the input noisy labels and the other for predicting `clean' labels. We train the two networks using their respective loss functions in a mutual reinforcement manner. Experimental results demonstrate the effectiveness of the proposed model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6286,6292,,,,,,,,,,,,,,,,WOS:000854193306066,0
C,"Wang, YX; Che, WX; Guo, J; Liu, YJ; Liu, T",,,Assoc Computat Linguist,"Wang, Yuxuan; Che, Wanxiang; Guo, Jiang; Liu, Yijia; Liu, Ting",,,Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. (1)",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5721,5727,,,,,,,,,,,,,,,,WOS:000854193305088,0
C,"Williams, A; Cotterell, R; Wolf-Sonkin, L; Blasi, D; Wallach, H",,,Assoc Computat Linguist,"Williams, Adina; Cotterell, Ryan; Wolf-Sonkin, Lawrence; Blasi, Damian; Wallach, Hanna",,,Quantifying the Semantic Core of Gender Systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many of the world's languages employ grammatical gender on the lexeme. For example, in Spanish, the word for house (casa) is feminine, whereas the word for paper (papel) is masculine. To a speaker of a genderless language, this assignment seems to exist with neither rhyme nor reason. But is the assignment of inanimate nouns to grammatical genders truly arbitrary? We present the first large-scale investigation of the arbitrariness of noun-gender assignments. To that end, we use canonical correlation analysis to correlate the grammatical gender of inanimate nouns with an externally grounded definition of their lexical semantics. We find that 18 languages exhibit a significant correlation between grammatical gender and lexical semantics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5734,5739,,,,,,,,,,,,,,,,WOS:000854193305090,0
C,"Yang, HY; Huang, SJ; Dai, XY; Chen, JJ",,,Assoc Computat Linguist,"Yang, Huiyun; Huang, Shujian; Dai, Xinyu; Chen, Jiajun",,,Fine-grained Knowledge Fusion for Sequence Labeling Domain Adaptation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In sequence labeling, previous domain adaptation methods focus on the adaptation from the source domain to the entire target domain without considering the diversity of individual target domain samples, which may lead to negative transfer results for certain samples. Besides, an important characteristic of sequence labeling tasks is that different elements within a given sample may also have diverse domain relevance, which requires further consideration. To take the multi-level domain relevance discrepancy into account, in this paper, we propose a fine-grained knowledge fusion model with the domain relevance modeling scheme to control the balance between learning from the target domain data and learning from the source domain model. Experiments on three sequence labeling tasks show that our fine-grained knowledge fusion model outperforms strong baselines and other state-of-the-art sequence labeling domain adaptation methods. (1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4197,4206,,,,,,,,,,,,,,,,WOS:000854193304034,0
C,"Zhang, HM; Bai, JX; Song, Y; Xu, K; Yu, CL; Song, YQ; Ng, W; Yu, D",,,Assoc Computat Linguist,"Zhang, Hongming; Bai, Jiaxin; Song, Yan; Xu, Kun; Yu, Changlong; Song, Yangqiu; Ng, Wilfred; Yu, Dong",,,Multiplex Word Embeddings for Selectional Preference Acquisition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Conventional word embeddings represent words with fixed vectors, which are usually trained based on co-occurrence patterns among words. In doing so, however, the power of such representations is limited, where the same word might be functionalized separately under different syntactic relations. To address this limitation, one solution is to incorporate relational dependencies of different words into their embeddings. Therefore, in this paper, we propose a multiplex word embedding model, which can be easily extended according to various relations among words. As a result, each word has a center embedding to represent its overall semantics, and several relational embeddings to represent its relational dependencies. Compared to existing models, our model can effectively distinguish words with respect to different relations without introducing unnecessary sparseness. Moreover, to accommodate various relations, we use a small dimension for relational embeddings and our model is able to keep their effectiveness. Experiments on selectional preference acquisition and word similarity demonstrate the effectiveness of the proposed model, and a further study of scalability also proves that our embeddings only need 1/20 of the original embedding size to achieve better performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5247,5256,,,,,,,,,,,,,,,,WOS:000854193305041,0
C,"Byrne, B; Krishnamoorthi, K; Sankar, C; Neelakantan, A; Duckworth, D; Yavuz, S; Goodrich, B; Dubey, A; Cedilnik, A; Kim, KY",,,Assoc Computat Linguist,"Byrne, Bill; Krishnamoorthi, Karthik; Sankar, Chinnadhurai; Neelakantan, Arvind; Duckworth, Daniel; Yavuz, Semih; Goodrich, Ben; Dubey, Amit; Cedilnik, Andy; Kim, Kyu-Young",,,Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A significant barrier to progress in data-driven approaches to building dialog systems is the lack of high quality, goal-oriented conversational data. To help satisfy this elementary requirement, we introduce the initial release of the Taskmaster-1 dataset which includes 13,215 task-based dialogs comprising six domains. Two procedures were used to create this collection, each with unique advantages. The first involves a two-person, spoken Wizard of Oz (WOz) approach in which trained agents and crowdsourced workers interact to complete the task while the second is self-dialog in which crowdsourced workers write the entire dialog themselves. We do not restrict the workers to detailed scripts or to a small knowledge base and hence we observe that our dataset contains more realistic and diverse conversations in comparison to existing datasets. We offer several baseline models including state of the art neural seq2seq architectures with benchmark performance as well as qualitative human evaluations. Dialogs are labeled with API calls and arguments, a simple and cost effective approach which avoids the requirement of complex annotation schema. The layer of abstraction between the dialog model and the service provider API allows for a given model to interact with multiple services that provide similar functionally. Finally, the dataset will evoke interest in written vs. spoken language, discourse patterns, error handling and other linguistic phenomena related to dialog system research, development and design.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4516,4525,,,,,,,,,,,,,,,,WOS:000854193304064,0
C,"Chang, JX; He, RF; Wang, LBA; Zhao, XY; Yang, T; Wang, RF",,,Assoc Computat Linguist,"Chang, Jinxin; He, Ruifang; Wang, Longbiao; Zhao, Xiangyu; Yang, Ting; Wang, Ruifang",,,A Semi-Supervised Stable Variational Network for Promoting Replier-Consistency in Dialogue Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural sequence-to-sequence models for dialog systems suffer from the problem of favoring uninformative and non replier-specific responses due to lacking of the global and relevant information guidance. The existing methods model the generation process by leveraging the neural variational network with simple Gaussian. However, the sampled information from latent space usually becomes useless due to the KL divergence vanishing issue, and the highly abstractive global variables easily dilute the personal features of replier, leading to a non replier-specific response. Therefore, a novel Semi-Supervised Stable Variational Network (SSVN) is proposed to address these issues. We use a unit hypersperical distribution, namely the von Mises-Fisher (vMF), as the latent space of a semi-supervised model, which can obtain the stable KL performance by setting a fixed variance and hence enhance the global information representation. Meanwhile, an unsupervised extractor is introduced to automatically distill the replier-tailored feature which is then injected into a supervised generator to encourage the replier-consistency. Experimental results on two large conversation datasets show that our model outperforms the competitive baseline models significantly, and can generate diverse and replier-specific responses.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1920,1930,,,,,,,,,,,,,,,,WOS:000854193302008,0
C,"Chen, MD; Chu, ZW; Chen, Y; Stratos, K; Gimpel, K",,,Assoc Computat Linguist,"Chen, Mingda; Chu, Zewei; Chen, Yang; Stratos, Karl; Gimpel, Kevin",,,EntEval: A Holistic Evaluation Benchmark for Entity Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Rich entity representations are useful for a wide class of problems involving entities. Despite their importance, there is no standardized benchmark that evaluates the overall quality of entity representations. In this work, we propose EntEval: a test suite of diverse tasks that require nontrivial understanding of entities including entity typing, entity similarity, entity relation prediction, and entity disambiguation. In addition, we develop training techniques for learning better entity representations by using natural hyperlink annotations in Wikipedia. We identify effective objectives for incorporating the contextual information in hyperlinks into state-of-the-art pretrained language models (Peters et al., 2018a) and show that they improve strong baselines on multiple EntEval tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,421,433,,,,,,,,,,,,,,,,WOS:000854193300040,0
C,"Christopoulou, F; Miwa, M; Ananiadou, S",,,Assoc Computat Linguist,"Christopoulou, Fenia; Miwa, Makoto; Ananiadou, Sophia",,,Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4925,4936,,,,,,,,,,,,,,,,WOS:000854193305011,0
C,"Del Tredici, M; Marcheggiani, D; Walde, SSI; Fernandez, R",,,Assoc Computat Linguist,"Del Tredici, Marco; Marcheggiani, Diego; Walde, Sabine Schulte Im; Fernandez, Raquel",,,You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Information about individuals can help to better understand what they say, particularly in social media where texts are short. Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. This ignores the fact, well known in sociolinguistics, that an individual may be part of several communities which are not equally relevant in all communicative situations. We present a model based on Graph Attention Networks that captures this observation. It dynamically explores the social graph of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4707,4717,,,,,,,,,,,,,,,,WOS:000854193304082,0
C,"Feng, SY; Li, AW; Hoey, J",,,Assoc Computat Linguist,"Feng, Steven Y.; Li, Aaron W.; Hoey, Jesse",,,Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic Text Exchange,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we present a novel method for measurably adjusting the semantics of text while preserving its sentiment and fluency, a task we call semantic text exchange. This is useful for text data augmentation and the semantic correction of text generated by chatbots and virtual assistants. We introduce a pipeline called SMERTI that combines entity replacement, similarity masking, and text infilling. We measure our pipeline's success by its Semantic Text Exchange Score (STES): the ability to preserve the original text's sentiment and fluency while adjusting semantic content. We propose to use masking (replacement) rate threshold as an adjustable parameter to control the amount of semantic change in the text. Our experiments demonstrate that SMERTI can outperform baseline models on Yelp reviews, Amazon reviews, and news headlines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2701,2711,,,,,,,,,,,,,,,,WOS:000854193302080,0
C,"Gupta, A; Zhang, P; Lalwani, G; Diab, M",,,Assoc Computat Linguist,"Gupta, Arshit; Zhang, Peng; Lalwani, Garima; Diab, Mona",,,CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Natural Language Understanding (NLU) is a core component of dialog systems. It typically involves two tasks - intent classification (IC) and slot labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of context management to DM. However, contextual information is critical to the correct prediction of intents and slots in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the model. In this work, we propose a context-aware self-attentive NLU (CASA- NLU) model that uses multiple signals, such as previous intents, slots, dialog acts and utterances over a variable context window, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7% on the IC task for one of the datasets. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance for IC task on standard public datasets - SNIPS and ATIS.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1285,1290,,,,,,,,,,,,,,,,WOS:000854193301049,0
C,"Guzman, F; Chen, PJ; Ott, M; Pino, J; Lample, G; Koehn, P; Chaudhary, V; Ranzato, M",,,Assoc Computat Linguist,"Guzman, Francisco; Chen, Peng-Jen; Ott, Myle; Pino, Juan; Lample, Guillaume; Koehn, Philipp; Chaudhary, Vishrav; Ranzato, Marc'Aurelio",,,The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for Nepali-English and SinhalaEnglish, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6098,6111,,,,,,,,,,,,,,,,WOS:000854193306043,0
C,"Hu, LM; Yang, TC; Shi, CA; Ji, HY; Li, XL",,,Assoc Computat Linguist,"Hu, Linmei; Yang, Tianchi; Shi, Chuan; Ji, Houye; Li, Xiaoli",,,Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4821,4830,,,,,,,,,,,,,,,,WOS:000854193305001,0
C,"Huang, PS; Stanforth, R; Welbl, J; Dyer, C; Yogatama, D; Gowal, S; Dvijotham, K; Kohli, P",,,Assoc Computat Linguist,"Huang, Po-Sen; Stanforth, Robert; Welbl, Johannes; Dyer, Chris; Yogatama, Dani; Gowal, Sven; Dvijotham, Krishnamurthy; Kohli, Pushmeet",,,Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation - a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4083,4093,,,,,,,,,,,,,,,,WOS:000854193304024,0
C,"Jing, YM; Xiong, DY; Zhen, Y",,,Assoc Computat Linguist,"Jing, Yimin; Xiong, Deyi; Zhen, Yan",,,BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper presents BiPaR, a bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support multilingual and cross-lingual reading comprehension. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written parallelly in two languages. We collect 3,667 bilingual parallel paragraphs from Chinese and English novels, from which we construct 14,668 parallel question-answer pairs via crowdsourced workers following a strict quality control procedure. We analyze BiPaR in depth and find that BiPaR offers good diversification in prefixes of questions, answer types and relationships between questions and passages. We also observe that answering questions of novels requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality, etc. With BiPaR, we build monolingual, multilingual, and cross-lingual MRC baseline models. Even for the relatively simple monolingual MRC on this dataset, experiments show that a strong BERT baseline is over 30 points behind human in terms of both EM and F1 score, indicating that BiPaR provides a challenging testbed for monolingual, multilingual and cross-lingual MRC on novels. The dataset is available at https://multinlp.github.io/BiPaR/.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2452,2462,,,,,,,,,,,,,,,,WOS:000854193302057,0
C,"Kocijan, V; Camburu, OM; Cretu, AM; Yordanov, Y; Blunsom, P; Lukasiewicz, T",,,Assoc Computat Linguist,"Kocijan, Vid; Camburu, Oana-Maria; Cretu, Ana-Maria; Yordanov, Yordan; Blunsom, Phil; Lukasiewicz, Thomas",,,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce WIKICREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our WIKICREM dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-theart approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WINOBIAS, and WINOGENDER. We release our model to be used off-the-shelf for solving pronoun disambiguation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4303,4312,,,,,,,,,,,,,,,,WOS:000854193304044,0
C,"Labeau, M; Cohen, SB",,,Assoc Computat Linguist,"Labeau, Matthieu; Cohen, Shay B.",,,Experimenting with Power Divergences for Language Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural language models are usually trained using Maximum-Likelihood Estimation (MLE). The corresponding objective function for MLE is derived from the Kullback-Leibler (KL) divergence between the empirical probability distribution representing the data and the parametric probability distribution output by the model. However, the word frequency discrepancies in natural language make performance extremely uneven: while the perplexity is usually very low for frequent words, it is especially difficult to predict rare words. To address that, we experiment with several families (alpha, beta and gamma) of power divergences, generalized from the KL divergence, for learning language models with an objective different than standard MLE. Intuitively, these divergences should affect the way the probability mass is spread during learning, notably by prioritizing performance on high or low-frequency words. In addition, we implement and experiment with various sampling-based objectives, where the computation of the output layer is only done on a small subset of the vocabulary. They are derived as power generalizations of a softmax approximated via Importance Sampling, and Noise Contrastive Estimation, for accelerated learning. Our experiments on the Penn Treebank andWikitext-2 show that these power divergences can indeed be used to prioritize learning on the frequent or rare words, and lead to general performance improvements in the case of sampling-based learning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4104,4114,,,,,,,,,,,,,,,,WOS:000854193304026,0
C,"Le, R; Hu, WP; Shang, MY; You, ZJ; Bing, LD; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Le, Ran; Hu, Wenpeng; Shang, Mingyue; You, Zhenjun; Bing, Lidong; Zhao, Dongyan; Yan, Rui",,,Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous research on dialogue systems generally focuses on the conversation between two participants, yet multi-party conversations which involve more than two participants within one session bring up a more complicated but realistic scenario. In real multiparty conversations, we can observe who is speaking, but the addressee information is not always explicit. In this paper, we aim to tackle the challenge of identifying all the missing addressees in a conversation session. To this end, we introduce a novel who-to-whom (W2W) model which models users and utterances in the session jointly in an interactive way. We conduct experiments on the benchmark Ubuntu Multi-Party Conversation Corpus and the experimental results demonstrate that our model outperforms baselines with consistent improvements.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1909,1919,,,,,,,,,,,,,,,,WOS:000854193302007,0
C,"Lee, M; Cho, SJ; Bindel, D; Mimno, D",,,Assoc Computat Linguist,"Lee, Moontae; Cho, Sungjun; Bindel, David; Mimno, David",,,Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite great scalability on large data and their ability to understand correlations between topics, spectral topic models have not been widely used due to the absence of reliability in real data and lack of practical implementations. This paper aims to solidify the foundations of spectral topic inference and provide a practical implementation for anchor-based topic modeling. Beginning with vocabulary curation, we scrutinize every single inference step with other viable options. We also evaluate our matrix-based approach against popular alternatives including a tensor-based spectral method as well as probabilistic algorithms. Our quantitative and qualitative experiments demonstrate the power of Rectified Anchor Word algorithm in various real datasets, providing a complete guide to practical correlated topic modeling.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4991,5001,,,,,,,,,,,,,,,,WOS:000854193305017,0
C,"Li, H; Lu, W",,,Assoc Computat Linguist,"Li, Hao; Lu, Wei",,,Learning Explicit and Implicit Structures for Targeted Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Targeted sentiment analysis is the task of jointly predicting target entities and their associated sentiment information. Existing research efforts mostly regard this joint task as a sequence labeling problem, building models that can capture explicit structures in the output space. However, the importance of capturing implicit global structural information that resides in the input space is largely unexplored. In this work, we argue that both types of information (implicit and explicit structural information) are crucial for building a successful targeted sentiment analysis model. Our experimental results show that properly capturing both information is able to lead to better performance than competitive existing approaches. We also conduct extensive experiments to investigate our model's effectiveness and robustness(1).",,,,,"Lu, Wei/AHA-5606-2022","Lu, Wei/0000-0003-0827-0382",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5478,5488,,,,,,,,,,,,,,,,WOS:000854193305063,0
C,"Pereg, O; Korat, D; Wasserblat, M; Mamou, J; Dagan, I",,,ASSOC COMPUTAT LINGUIST,"Pereg, Oren; Korat, Daniel; Wasserblat, Moshe; Mamou, Jonathan; Dagan, Ido",,,ABSApp: A Portable Weakly-Supervised Aspect-Based Sentiment Extraction System,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present ABSApp, a portable system for weakly-supervised aspect-based sentiment extraction 1. The system is interpretable and user friendly and does not require labeled training data, hence can be rapidly and costeffectively used across different domains in applied setups. The system flow includes three stages: First, it generates domain-specific aspect and opinion lexicons based on an unlabeled dataset; second, it enables the user to view and edit those lexicons (weak supervision); and finally, it enables the user to select an unlabeled target dataset from the same domain, classify it, and generate an aspect-based sentiment report. ABSApp has been successfully used in a number of real-life use cases, among them movie review analysis and convention impact analysis.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,1,6,,,,,,,,,,,,,,,,WOS:000855231500001,0
C,"Ravichander, A; Black, A; Wilson, S; Norton, T; Sadeh, N",,,Assoc Computat Linguist,"Ravichander, Abhilasha; Black, Alan; Wilson, Shomir; Norton, Thomas; Sadeh, Norman",,,Question Answering for Privacy Policies: Combining Computational and Legal Perspectives,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Privacy policies are long and complex documents that are difficult for users to read and understand, and yet, they have legal effects on how user data is collected, managed and used. Ideally, we would like to empower users to inform themselves about issues that matter to them, and enable them to selective explore those issues. We present PRIVACYQA, a corpus consisting of 1750 questions about the privacy policies of mobile applications, and over 3500 expert annotations of relevant answers. We observe that a strong neural baseline underperforms human performance by almost 0.3 F1 on PRIVACYQA, suggesting considerable room for improvement for future systems. Further, we use this dataset to shed light on challenges to question answerability, with domain-general implications for any question answering system. The PRIVACYQA corpus offers a challenging corpus for question answering, with genuine real-world utility.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4947,4958,,,,,,,,,,,,,,,,WOS:000854193305013,0
C,"Shen, LH; Tai, PL; Wu, CC; Lin, SD",,,ASSOC COMPUTAT LINGUIST,"Shen, Liang-Hsin; Tai, Pei-Lun; Wu, Chao-Chung; Lin, Shou-De",,,Controlling Sequence-to-Sequence Models - A Demonstration on Neural-based Acrostic Generator,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"An acrostic is a form of writing for which the first token of each line (or other recurring features in the text) forms a meaningful sequence. In this paper we present a generalized acrostic generation system that can hide certain message in a flexible pattern specified by the users. Different from previous works that focus on rule-based solutions, here we adopt a neural-based sequence-to-sequence model to achieve this goal. Besides acrostic, users can also specify the rhyme and length of the output sequences. To the best of our knowledge, this is the first neural-based natural language generation system that demonstrates the capability of performing micro-level control over output sentences.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,43,48,,,,,,,,,,,,,,,,WOS:000855231500008,0
C,"Shi, WJ; Chen, MH; Zhou, P; Chang, KW",,,Assoc Computat Linguist,"Shi, Weijia; Chen, Muhao; Zhou, Pei; Chang, Kai-Wei",,,Retrofitting Contextualized Word Embeddings with Paraphrases,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextualized word embedding models, such as ELMo, generate meaningful representations of words and their context. These models have been shown to have a great impact on downstream applications. However, in many cases, the contextualized embedding of a word changes drastically when the context is paraphrased. As a result, the downstream model is not robust to paraphrasing and other linguistic variations. To enhance the stability of contextualized word embedding models, we propose an approach to retrofitting contextualized embedding models with paraphrase contexts. Our method learns an orthogonal transformation on the input space, which seeks to minimize the variance of word representations on paraphrased contexts. Experiments show that the retrofitted model significantly outperforms the original ELMo on various sentence classification and language inference tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1198,1203,,,,,,,,,,,,,,,,WOS:000854193301035,0
C,"Wang, BL; Lu, W",,,Assoc Computat Linguist,"Wang, Bailin; Lu, Wei",,,Combining Spans into Entities: A Neural Two-Stage Approach for Recognizing Discontiguous Entities,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In medical documents, it is possible that an entity of interest not only contains a discontiguous sequence of words but also overlaps with another entity. Entities of such structures are intrinsically hard to recognize due to the large space of possible entity combinations. In this work, we propose a neural two-stage approach to recognizing discontiguous and overlapping entities by decomposing this problem into two subtasks: 1) it first detects all the overlapping spans that either form entities on their own or present as segments of discontiguous entities, based on the representation of segmental hypergraph, 2) next it learns to combine these segments into discontiguous entities with a classifier, which filters out other incorrect combinations of segments. Two neural components are designed for these subtasks respectively and they are learned jointly using a shared encoder for text. Our model achieves the state-of-the-art performance in a standard dataset, even in the absence of external features that previous methods used.",,,,,"Lu, Wei/AHA-5606-2022","Lu, Wei/0000-0003-0827-0382",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6216,6224,,,,,,,,,,,,,,,,WOS:000854193306055,0
C,"Warstadt, A; Cao, Y; Grosu, I; Peng, W; Blix, H; Nie, YN; Alsop, A; Bordia, S; Liu, HK; Parrish, A; Wang, SF; Phang, J; Mohananey, A; Htut, PM; Jeretic, P; Bowman, SR",,,Assoc Computat Linguist,"Warstadt, Alex; Cao, Yu; Grosu, Ioana; Peng, Wei; Blix, Hagen; Nie, Yining; Alsop, Anna; Bordia, Shikha; Liu, Haokun; Parrish, Alicia; Wang, Sheng-Fu; Phang, Jason; Mohananey, Anhad; Htut, Phu Mon; Jeretic, Paloma; Bowman, Samuel R.",,,Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing in English, as a case study for our experiments. NPIs like any are grammatical only if they appear in a licensing environment like negation (Sue doesn't have any cats vs. *Sue has any cats). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model's grammatical knowledge in a given domain.",,,,,,"Wang, Sheng-Fu/0000-0001-6805-6965; Parrish, Alicia/0000-0002-1054-0516; Nie, Yining/0000-0002-0985-4251",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2877,2887,,,,,,,,,,,,,,,,WOS:000854193303004,0
C,"Yadav, V; Bethard, S; Surdeanu, M",,,Assoc Computat Linguist,"Yadav, Vikas; Bethard, Steven; Surdeanu, Mihai",,,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose an unsupervised strategy for the selection of justification sentences for multihop question answering (QA) that (a) maximizes the relevance of the selected sentences, (b) minimizes the overlap between the selected facts, and (c) maximizes the coverage of both question and answer. This unsupervised sentence selection method can be coupled with any supervised QA approach. We show that the sentences selected by our method improve the performance of a state-of-the-art supervised QA model on two multi-hop QA datasets: AI2's Reasoning Challenge (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art performance on both datasets among approaches that do not use external resources for training the QA system: 56.82% F1 on ARC (41.24% on Challenge and 64.49% on Easy) and 26.1% EM0 on MultiRC. Our justification sentences have higher quality than the justifications selected by a strong information retrieval baseline, e.g., by 5.4% F1 in MultiRC. We also show that our unsupervised selection of justification sentences is more stable across domains than a state-of-the-art supervised sentence selection method.",,,,,,"Bethard, Steven/0000-0001-9560-6491",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2578,2589,,,,,,,,,,,,,,,,WOS:000854193302068,0
C,"Zhang, C; Li, QC; Song, DW",,,Assoc Computat Linguist,"Zhang, Chen; Li, Qiuchi; Song, Dawei",,,Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models1, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4568,4578,,,,,,,,,,,,,,,,WOS:000854193304069,0
C,"Zhong, RQ; Chen, YD; Patton, D; Selous, C; McKeown, K",,,Assoc Computat Linguist,"Zhong, Ruiqi; Chen, Yanda; Patton, Desmond; Selous, Charlotte; McKeown, Kathleen",,,Detecting and Reducing Bias in a High Stakes Domain,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Gang-involved youth in cities such as Chicago sometimes post on social media to express their aggression towards rival gangs and previous research has demonstrated that a deep learning approach can predict aggression and loss in posts. To address the possibility of bias in this sensitive application, we developed an approach to systematically interpret the state of the art model. We found, surprisingly, that it frequently bases its predictions on stop words such as a or on, an approach that could harm social media users who have no aggressive intentions. To tackle this bias, domain experts annotated the rationales, highlighting words that explain why a tweet is labeled as aggression. These new annotations enable us to quantitatively measure how justified the model predictions are, and build models that drastically reduce bias. Our study shows that in high stake scenarios, accuracy alone cannot guarantee a good system and we need new evaluation methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4765,4775,,,,,,,,,,,,,,,,WOS:000854193304087,0
C,"Aji, AF; Heafield, K; Bogoychev, N",,,Assoc Computat Linguist,"Aji, Alham Fikri; Heafield, Kenneth; Bogoychev, Nikolay",,,Combining Global Sparse Gradients with Local Gradients in Distributed Neural Network Training,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"One way to reduce network traffic in multinode data-parallel stochastic gradient descent is to only exchange the largest gradients. However, doing so damages the gradient and degrades the model's performance. Tranformer models degrade dramatically while the impact on RNNs is smaller. We restore gradient quality by combining the compressed global gradient with the node's locally computed uncompressed gradient. Neural machine translation experiments show that Transformer convergence is restored while RNNs converge faster. With our method, training on 4 nodes converges up to 1.5x as fast as with uncompressed gradients and scales 3.5x relative to singlenode training.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3626,3631,,,,,,,,,,,,,,,,WOS:000854193303091,0
C,"Andor, D; He, LH; Lee, K; Pitler, E",,,Assoc Computat Linguist,"Andor, Daniel; He, Luheng; Lee, Kenton; Pitler, Emily",,,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable `programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5947,5952,,,,,,,,,,,,,,,,WOS:000854193306020,0
C,"Augenstein, I; Lioma, C; Wang, DS; Lima, LC; Hansen, C; Hansen, C; Simonsen, JG",,,Assoc Computat Linguist,"Augenstein, Isabelle; Lioma, Christina; Wang, Dongsheng; Lima, Lucas Chaves; Hansen, Casper; Hansen, Christian; Simonsen, Jakob Grue",,,MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that this is a challenging testbed for claim veracity prediction.",,,,,,"Augenstein, Isabelle/0000-0003-1562-7909",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4685,4697,,,,,,,,,,,,,,,,WOS:000854193304080,0
C,"Caragea, C; Uban, AS; Dinu, LP",,,Assoc Computat Linguist,"Caragea, Cornelia; Uban, Ana Sabina; Dinu, Liviu P.",,,The Myth of Double-Blind Review Revisited: ACL vs. EMNLP,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The review and selection process for scientific paper publication is essential for the quality of scholarly publications in a scientific field. The double-blind review system, which enforces author anonymity during the review period, is widely used by prestigious conferences and journals to ensure the integrity of this process. Although the notion of anonymity in the double-blind review has been questioned before, the availability of full text paper collections brings new opportunities for exploring the question: Is the double-blind review process really double-blind? We study this question on the ACL and EMNLP paper collections and present an analysis on how well deep learning techniques can infer the authors of a paper. Specifically, we explore Convolutional Neural Networks trained on various aspects of a paper, e.g., content, style features, and references, to understand the extent to which we can infer the authors of a paper and what aspects contribute the most. Our results show that the authors of a paper can be inferred with accuracy as high as 87% on ACL and 78% on EMNLP for the top 100 most prolific authors.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2317,2327,,,,,,,,,,,,,,,,WOS:000854193302044,0
C,"Chen, KH; Wang, R; Utiyama, M; Sumita, E",,,Assoc Computat Linguist,"Chen, Kehai; Wang, Rui; Utiyama, Masao; Sumita, Eiichiro",,,Recurrent Positional Embedding for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In the Transformer network architecture, positional embeddings are used to encode order dependencies into the input representation. However, this input representation only involves static order dependencies based on discrete numerical information, that is, are independent of word content. To address this issue, this work proposes a recurrent positional embedding approach based on word vector. In this approach, these recurrent positional embeddings are learned by a recurrent neural network, encoding word content-based order dependencies into the input representation. They are then integrated into the existing multi-head selfattention model as independent heads or part of each head. The experimental results revealed that the proposed approach improved translation performance over that of the state-of-the-art Transformer baseline in WMT'14 English-to-German and NIST Chinese-to-English translation tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1361,1367,,,,,,,,,,,,,,,,WOS:000854193301061,0
C,"Chen, S; Wang, JP; Feng, XC; Jiang, F; Qin, B; Lin, CY",,,Assoc Computat Linguist,"Chen, Shuang; Wang, Jinpeng; Feng, Xiaocheng; Jiang, Feng; Qin, Bing; Lin, Chin-Yew",,,Enhancing Neural Data-To-Text Generation Models with External Background Knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent neural models for data-to-text generation rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that writing knowledge can be acquired from the training data alone. However, when people are writing, they not only rely on the data but also consider related knowledge. In this paper, we enhance neural data-to-text models with external knowledge in a simple but effective way to improve the fidelity of generated text. Besides relying on parallel data and text as in previous work, our model attends to relevant external knowledge, encoded as a temporary memory, and combines this knowledge with the context representation of data before generating words. This allows the model to infer relevant facts which are not explicitly stated in the data table from an external knowledge source. Experimental results on twenty-one Wikipedia infobox-to-text datasets show our model, KBAtt, consistently improves a state-of-the-art model on most of the datasets. In addition, to quantify when and why external knowledge is effective, we design a metric, KBGain, which shows a strong correlation with the observed performance boost. This result demonstrates the relevance of external knowledge and sparseness of original data are the main factors affecting system performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3022,3032,,,,,,,,,,,,,,,,WOS:000854193303017,0
C,"Daza, A; Frank, A",,,Assoc Computat Linguist,"Daza, Angel; Frank, Anette",,,Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for lever-aging SRL data in multiple languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,603,615,,,,,,,,,,,,,,,,WOS:000854193300056,0
C,"Dufter, P; Schutze, H",,,Assoc Computat Linguist,"Dufter, Philipp; Schuetze, Hinrich",,,Analytical Methods for Interpretable Ultradense Word Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameterfree and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1185,1191,,,,,,,,,,,,,,,,WOS:000854193301033,0
C,"Elsahar, H; Galle, M",,,Assoc Computat Linguist,"Elsahar, Hady; Galle, Matthias",,,To Annotate or Not? Predicting Performance Drop under Domain Shift,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods ( H-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2:15% and 0:89% for sentiment analysis and POS tagging respectively.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2163,2173,,,,,,,,,,,,,,,,WOS:000854193302030,0
C,"Fu, C; Chen, T; Qu, M; Jin, W; Ren, X",,,Assoc Computat Linguist,"Fu, Cong; Chen, Tong; Qu, Meng; Jin, Woojeong; Ren, Xiang",,,Collaborative Policy Learning for Open Knowledge Graph Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In recent years, there has been a surge of interests in interpretable graph reasoning methods. However, these models often suffer from limited performance when working on sparse and incomplete graphs, due to the lack of evidential paths that can reach target entities. Here we study open knowledge graph reasoning-a task that aims to reason for missing facts over a graph augmented by a background text corpus. A key challenge of the task is to filter out ir-relevant facts extracted from corpus, in order to maintain an effective search space during path inference. We propose a novel reinforcement learning framework to train two collaborative agents jointly, i.e., a multi-hop graph reasoner and a fact extractor. The fact extraction agent generates fact triples from corpora to enrich the graph on the fly; while the reasoning agent provides feedback to the fact extractor and guides it towards promoting facts that are helpful for the interpretable reasoning. Experiments on two public datasets demonstrate the effectiveness of the proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2672,2681,,,,,,,,,,,,,,,,WOS:000854193302077,0
C,"Gooding, S; Kochmar, E",,,Assoc Computat Linguist,"Gooding, Sian; Kochmar, Ekaterina",,,Recursive Context-Aware Lexical Simplification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper presents a novel architecture for recursive context-aware lexical simplification, REC-LS, that is capable of (1) making use of the wider context when detecting the words in need of simplification and suggesting alternatives, and (2) taking previous simplification steps into account. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and outperforms the current state-of-theart systems in lexical simplification.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4853,4863,,,,,,,,,,,,,,,,WOS:000854193305004,0
C,"Hewitt, J; Liang, P",,,Assoc Computat Linguist,"Hewitt, John; Liang, Percy",,,Designing and Interpreting Probes with Control Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2733,2743,,,,,,,,,,,,,,,,WOS:000854193302083,0
C,"Liu, H; Zhang, XT; Fan, L; Fu, XD; Li, QM; Wu, XM; Lam, AYS",,,Assoc Computat Linguist,"Liu, Han; Zhang, Xiaotong; Fan, Lu; Fu, Xuandi; Li, Qimai; Wu, Xiao-Ming; Lam, Albert Y. S.",,,Reconstructing Capsule Networks for Zero-shot Intent Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Intent classification is an important building block of dialogue systems. With the burgeoning of conversational AI, existing systems are not capable of handling numerous fast-emerging intents, which motivates zero-shot intent classification. Nevertheless, research on this problem is still in the incipient stage and few methods are available. A recently proposed zero-shot intent classification method, IntentCapsNet, has been shown to achieve state-of-the-art performance. However, it has two unaddressed limitations: (1) it cannot deal with polysemy when extracting semantic capsules; (2) it hardly recognizes the utterances of unseen intents in the generalized zero-shot intent classification setting. To overcome these limitations, we propose to reconstruct capsule networks for zero-shot intent classification. First, we introduce a dimensional attention mechanism to fight against polysemy. Second, we reconstruct the transformation matrices for unseen intents by utilizing abundant latent information of the labeled utterances, which significantly improves the model generalization ability. Experimental results on two task-oriented dialogue datasets in different languages show that our proposed method outperforms IntentCapsNet and other strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4799,4809,,,,,,,,,,,,,,,,WOS:000854193304090,0
C,"Lyu, CC; Cohen, SB; Titov, I",,,Assoc Computat Linguist,"Lyu, Chunchuan; Cohen, Shay B.; Titov, Ivan",,,Semantic Role Labeling with Iterative Structure Refinement,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Modern state-of-the-art Semantic Role Labeling (SRL) methods rely on expressive sentence encoders (e.g., multi-layer LSTMs) but tend to model only local (if any) interactions between individual argument labeling decisions. This contrasts with earlier work and also with the intuition that the labels of individual arguments are strongly interdependent. We model interactions between argument labeling decisions through iterative refinement. Starting with an output produced by a factorized model, we iteratively refine it using a refinement network. Instead of modeling arbitrary interactions among roles and words, we encode prior knowledge about the SRL problem by designing a restricted network architecture capturing non-local interactions. This modeling choice prevents overfitting and results in an effective model, outperforming strong factorized baseline models on all 7 CoNLL-2009 languages, and achieving state-of-the-art results on 5 of them, including English.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1071,1082,,,,,,,,,,,,,,,,WOS:000854193301021,0
C,"Ma, XZ; Zhou, CT; Li, X; Neubig, G; Hovy, E",,,Assoc Computat Linguist,"Ma, Xuezhe; Zhou, Chunting; Li, Xian; Neubig, Graham; Hovy, Eduard",,,FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art nonautoregressive NMT models and almost constant decoding time w.r.t the sequence length.(1)",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4282,4292,,,,,,,,,,,,,,,,WOS:000854193304042,0
C,"Ning, Q; Subramanian, S; Roth, D",,,Assoc Computat Linguist,"Ning, Qiang; Subramanian, Sanjay; Roth, Dan",,,An Improved Neural Baseline for Temporal Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Determining temporal relations (e.g., before or after) between events has been a challenging natural language understanding task, partly due to the difficulty to generate large amounts of high-quality training data. Consequently, neural approaches have not been widely used on it, or showed only moderate improvements. This paper proposes a new neural system that achieves about 10% absolute improvement in accuracy over the previous best system (25% error reduction) on two benchmark datasets. The proposed system is trained on the state-of-the-art MATRES dataset and applies contextualized word embeddings, a Siamese encoder of a temporal common sense knowledge base, and global inference via integer linear programming (ILP). We suggest that the new approach could serve as a strong baseline for future research in this area.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6203,6209,,,,,,,,,,,,,,,,WOS:000854193306053,0
C,"Nishino, T; Misawa, S; Kano, R; Taniguchi, T; Miura, Y; Ohkuma, T",,,Assoc Computat Linguist,"Nishino, Toru; Misawa, Shotaro; Kano, Ryuji; Taniguchi, Tomoki; Miura, Yasuhide; Ohkuma, Tomoko",,,Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The automated generation of information indicating the characteristics of articles such as headlines, key phrases, summaries and categories helps writers to alleviate their workload. Previous research has tackled these tasks using neural abstractive summarization and classification methods. However, the outputs may be inconsistent if they are generated individually. The purpose of our study is to generate multiple outputs consistently. We introduce a multi-task learning model with a shared encoder and multiple decoders for each task. We propose a novel loss function called hierarchical consistency loss to maintain consistency among the attention weights of the decoders. To evaluate the consistency, we employ a human evaluation. The results show that our model generates more consistent headlines, key phrases and categories. In addition, our model outperforms the baseline model on the ROUGE scores, and generates more adequate and fluent headlines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3195,3205,,,,,,,,,,,,,,,,WOS:000854193303033,0
C,"Parida, S; Motlicek, P",,,Assoc Computat Linguist,"Parida, Shantipriya; Motlicek, Petr",,,Abstract Text Summarization: A Low Resource Challenge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text summarization is considered as a challenging task in the NLP community. The availability of datasets for the task of multilingual text summarization is rare, and such datasets are difficult to construct. In this work, we build an abstract text summarizer for the German language text using the state-of-the-art Transformer model. We propose an iterative data augmentation approach which uses synthetic data along with the real summarization data for the German language. To generate synthetic data, the Common Crawl (German) dataset is exploited, which covers different domains. The synthetic data is effective for the low resource condition and is particularly helpful for our multilingual scenario where availability of summarizing data is still a challenging issue. The data are also useful in deep learning scenarios where the neural models require a large amount of training data for utilization of its capacity. The obtained summarization performance is measured in terms of ROUGE and BLEU score. We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 (R1 F1) on the development and test sets, respectively, compared to the system which does not rely on data augmentation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5994,5998,,,,,,,,,,,,,,,,WOS:000854193306027,0
C,"Petroni, F; Rocktaschel, T; Lewis, P; Bakhtin, A; Wu, YX; Miller, AH; Riedel, S",,,Assoc Computat Linguist,"Petroni, Fabio; Rocktaschel, Tim; Lewis, Patrick; Bakhtin, Anton; Wu, Yuxiang; Miller, Alexander H.; Riedel, Sebastian",,,Language Models as Knowledge Bases?,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as fillin-the-blank cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-theart pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https: //github.com/facebookresearch/LAMA",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2463,2473,,,,,,,,,,,,,,,,WOS:000854193302058,0
C,"Tuan, YL; Chen, YN; Lee, HY",,,Assoc Computat Linguist,"Tuan, Yi-Lin; Chen, Yun-Nung; Lee, Hung-yi",,,DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1855,1865,,,,,,,,,,,,,,,,WOS:000854193302002,0
C,"Yee, K; Dauphin, YN; Auli, M",,,Assoc Computat Linguist,"Yee, Kyra; Dauphin, Yann N.; Auli, Michael",,,Simple and Effective Noisy Channel Modeling for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous work on neural noisy channel modeling relied on latent variable models that incrementally process the source and target sentence. This makes decoding decisions based on partial source prefixes even though the full source is available. We pursue an alternative approach based on standard sequence to sequence models which utilize the entire source. These models perform remarkably well as channel models, even though they have neither been trained on, nor designed to factor over incomplete target sentences. Experiments with neural language models trained on billions of words show that noisy channel models can outperform a direct model by up to 3.2 BLEU on WMT'17 German-English translation. We evaluate on four language-pairs and our channel models consistently outperform strong alternatives such right-to-left reranking models and ensembles of direct models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5696,5701,,,,,,,,,,,,,,,,WOS:000854193305084,0
C,"Zhou, WJ; Zhang, MH; Wu, YF",,,Assoc Computat Linguist,"Zhou, Wenjie; Zhang, Minghua; Wu, Yunfang",,,Multi-Task Learning with Language Modeling for Question Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper explores the task of answer-aware questions generation. Based on the attention-based pointer generator model, we propose to incorporate an auxiliary task of language modeling to help question generation in a hierarchical multi-task learning structure. Our joint-learning model enables the encoder to learn a better representation of the input sequence, which will guide the decoder to generate more coherent and fluent questions. On both SQuAD and MARCO datasets, our multitask learning model boosts the performance, achieving state-of-the-art results. Moreover, human evaluation further proves the high quality of our generated questions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3394,3399,,,,,,,,,,,,,,,,WOS:000854193303055,0
C,"Bogin, B; Gardner, M; Berant, J",,,Assoc Computat Linguist,"Bogin, Ben; Gardner, Matt; Berant, Jonathan",,,Global Reasoning over Database Structures for Text-to-SQL Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zeroshot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextuallyinformed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-ofthe-art model for SPIDER, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4% to 47.4%.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3659,3664,,,,,,,,,,,,,,,,WOS:000854193303096,0
C,"Croce, D; Rossini, D; Basili, R",,,Assoc Computat Linguist,"Croce, Danilo; Rossini, Daniele; Basili, Roberto",,,Auditing Deep Learning processes through Kernel-based Explanatory Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While NLP systems become more pervasive, their accountability gains value as a focal point of effort. Epistemological opaqueness of nonlinear learning methods, such as deep learning models, can be a major drawback for their adoptions. In this paper, we discuss the application of Layerwise Relevance Propagation over a linguistically motivated neural architecture, the Kernel-based Deep Architecture, in order to trace back connections between linguistic properties of input instances and system decisions. Such connections then guide the construction of argumentations on the network's inferences, i.e., explanations based on real examples that are semantically related to the input. We also propose here a methodology to evaluate the transparency and coherence of analogy-based explanations modeling an audit stage for the system. Quantitative analysis on two semantic tasks, i.e., question classification and semantic role labeling, shows that the explanatory capabilities (native in KDAs) are effective and they pave the way to more complex argumentation methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4037,4046,,,,,,,,,,,,,,,,WOS:000854193304020,0
C,"Cui, YM; Che, WX; Liu, T; Qin, B; Wang, SJ; Hu, GP",,,Assoc Computat Linguist,"Cui, Yiming; Che, Wanxiang; Liu, Ting; Qin, Bing; Wang, Shijin; Hu, Guoping",,,Cross-Lingual Machine Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt. However, to accurately align the answer into another language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in a bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. (1)",,,,,"Cui, Yiming/AAA-5499-2022; liu, ting/GZM-3326-2022","Cui, Yiming/0000-0002-2452-375X; ",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1586,1595,,,,,,,,,,,,,,,,WOS:000854193301091,0
C,"Fancellu, F; Gilroy, S; Lopez, A; Lapata, M",,,Assoc Computat Linguist,"Fancellu, Federico; Gilroy, Sorcha; Lopez, Adam; Lapata, Mirella",,,Semantic Graph Parsing with Recurrent Neural Network DAG Grammars,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the linearized graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be wellformed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that ensures only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Bank-a multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2769,2778,,,,,,,,,,,,,,,,WOS:000854193302086,0
C,"Geva, M; Goldberg, Y; Berant, J",,,Assoc Computat Linguist,"Geva, Mor; Goldberg, Yoav; Berant, Jonathan",,,AreWe Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1161,1166,,,,,,,,,,,,,,,,WOS:000854193301029,0
C,"He, SL; Tu, ZP; Wang, X; Wang, LY; Lyu, MR; Shi, SM",,,Assoc Computat Linguist,"He, Shilin; Tu, Zhaopeng; Wang, Xing; Wang, Longyue; Lyu, Michael R.; Shi, Shuming",,,Towards Understanding Neural Machine Translation with Word Importance,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Although neural machine translation (NMT) has advanced the state-of-the-art on various language pairs, the interpretability of NMT remains unsatisfactory. In this work, we propose to address this gap by focusing on understanding the input-output behavior of NMT models. Specifically, we measure the word importance by attributing the NMT output to every input word through a gradient-based method. We validate the approach on a couple of perturbation operations, language pairs, and model architectures, demonstrating its superiority on identifying input words with higher influence on translation performance. Encouragingly, the calculated importance can serve as indicators of input words that are under-translated by NMT models. Furthermore, our analysis reveals that words of certain syntactic categories have higher importance while the categories vary across language pairs, which can inspire better design principles of NMT architectures for multi-lingual translation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,953,962,,,,,,,,,,,,,,,,WOS:000854193301010,0
C,"Henderson, M; Vulic, I; Casanueva, I; Budzianowski, P; Gerz, D; Coope, S; Spithourakis, G; Wen, TH; Mrksic, N; Su, PH",,,ASSOC COMPUTAT LINGUIST,"Henderson, Matthew; Vulic, Ivan; Casanueva, Iiiigo; Budzianowski, Pawel; Gerz, Daniela; Coope, Sam; Spithourakis, Georgios; Wen, Tsung-Hsien; Mrksic, Nikola; Su, Pei-Hao",,,PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations: it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,181,186,,,,,,,,,,,,,,,,WOS:000855231500031,0
C,"Huang, LZ; Ma, DH; Li, SJ; Zhang, XD; Wang, HF",,,Assoc Computat Linguist,"Huang, Lianzhe; Ma, Dehong; Li, Sujian; Zhang, Xiaodong; Wang, Houfeng",,,Text Level Graph Neural Network for Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, researches have explored the graph neural network (GNN) techniques on text classification, since GNN does well in handling complex structures and preserving global information. However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which do not support online testing and high memory consumption. To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus. This method removes the burden of dependence between an individual text and entire corpus which support online testing, but still preserve global information. Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3444,3450,,,,,,,,,,,,,,,,WOS:000854193303063,0
C,"Huang, LF; Le Bras, R; Bhagavatula, C; Choi, YJ",,,Assoc Computat Linguist,"Huang, Lifu; Le Bras, Ronan; Bhagavatula, Chandra; Choi, Yejin",,,COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce COSMOS QA, a large-scale dataset of 35, 600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people's everyday narratives, asking such questions as what might be the possible reason of...?, or what would have happened if... that require reasoning beyond the exact text spans in the context. To establish baseline performances on COSMOS QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4%) and human performance (94%), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone. github.io/cosmos.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2391,2401,,,,,,,,,,,,,,,,WOS:000854193302051,0
C,"Jang, Y; Lee, J; Park, J; Lee, KH; Lison, P; Kim, KE",,,ASSOC COMPUTAT LINGUIST,"Jang, Youngsoo; Lee, Jongmin; Park, Jaeyoung; Lee, Kyeng-Hun; Lison, Pierre; Kim, Kee-Eung",,,PyOpenDial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present PyOpenDial, a Python-based domain-independent, open-source toolkit for spoken dialogue systems. Recent advances in core components of dialogue systems, such as speech recognition, language understanding, dialogue management, and language generation, harness deep learning to achieve state-ofthe-art performance. The original OpenDial, implemented in Java, provides a plugin architecture to integrate external modules, but lacks Python bindings, making it difficult to interface with popular deep learning frameworks such as Tensorflow or PyTorch. To this end, we re-implemented OpenDial in Python and extended the toolkit with a number of novel functionalities for neural dialogue state tracking and action planning. We describe the overall architecture and its extensions, and illustrate their use on an example where the system response model is implemented with a recurrent neural network.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,187,192,,,,,,,,,,,,,,,,WOS:000855231500032,0
C,"Kann, K; Cho, K; Bowman, SR",,,Assoc Computat Linguist,"Kann, Katharina; Cho, Kyunghyun; Bowman, Samuel R.",,,Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for models obtained by training with and without development sets. On average over languages, absolute accuracy differs by up to 1.4%. However, for some languages and tasks, differences are as big as 18.0% accuracy. Our results highlight the importance of realistic experimental setups in the publication of lowresource NLP research results.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3342,3349,,,,,,,,,,,,,,,,WOS:000854193303047,0
C,"Kreutzer, J; Bastings, J; Riezler, S",,,ASSOC COMPUTAT LINGUIST,"Kreutzer, Julia; Bastings, Joost; Riezler, Stefan",,,Joey NMT: A Minimalist NMT Toolkit for Novices,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present Joey NMT, a minimalist neural machine translation toolkit based on PyTorch that is specifically designed for novices. Joey NMT provides many popular NMT features in a small and simple code base, so that novices can easily and quickly learn to use it and adapt it to their needs. Despite its focus on simplicity, Joey NMT supports classic architectures (RNNs, transformers), fast beam search, weight tying, and more, and achieves performance comparable to more complex toolkits on standard benchmarks. We evaluate the accessibility of our toolkit in a user study where novices with general knowledge about Pytorch and NMT and experts work through a self-contained Joey NMT tutorial, showing that novices perform almost as well as experts in a subsequent code quiz. Joey NMT is available at https://github. com/joeynmt/joeynmt.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,109,114,,,,,,,,,,,,,,,,WOS:000855231500019,0
C,"Kruengkrai, C",,,Assoc Computat Linguist,"Kruengkrai, Canasai",,,Learning to Flip the Sentiment of Reviews from Non-Parallel Corpora,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,Flipping sentiment while preserving sentence meaning is challenging because parallel sentences with the same content but different sentiment polarities are not always available for model learning. We introduce a method for acquiring imperfectly aligned sentences from non-parallel corpora and propose a model that learns to minimize the sentiment and content losses in a fully end-to-end manner. Our model is simple and offers well-balanced results across two domains: Yelp restaurant and Amazon product reviews.(1),,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6311,6316,,,,,,,,,,,,,,,,WOS:000854193306070,0
C,"Li, Z; Li, X; Wei, Y; Bing, LD; Zhang, Y; Yang, Q",,,Assoc Computat Linguist,"Li, Zheng; Li, Xin; Wei, Ying; Bing, Lidong; Zhang, Yu; Yang, Qiang",,,Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Joint extraction of aspects and sentiments can be effectively formulated as a sequence labeling problem. However, such formulation hinders the effectiveness of supervised methods due to the lack of annotated sequence data in many domains. To address this issue, we firstly explore an unsupervised domain adaptation setting for this task. Prior work can only use common syntactic relations between aspect and opinion words to bridge the domain gaps, which highly relies on external linguistic resources. To resolve it, we propose a novel Selective Adversarial Learning (SAL) method to align the inferred correlation vectors that automatically capture their latent relations. The SAL method can dynamically learn an alignment weight for each word such that more important words can possess higher alignment weights to achieve finegrained (word-level) adaptation. Empirically, extensive experiments1 demonstrate the effectiveness of the proposed SAL method.",,,,,,"Wei, Ying/0000-0003-1662-4443",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4590,4600,,,,,,,,,,,,,,,,WOS:000854193304071,0
C,"Liao, M; Li, J; Zhang, HS; Wang, LZ; Wu, XX; Wong, KF",,,Assoc Computat Linguist,"Liao, Ming; Li, Jing; Zhang, Haisong; Wang, Lingzhi; Wu, Xixin; Wong, Kam-Fai",,,Coupling Global and Local Context for Unsupervised Aspect Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4579,4589,,,,,,,,,,,,,,,,WOS:000854193304070,0
C,"Martino, GDS; Yu, S; Barron-Cedeno, A; Petrov, R; Nakov, P",,,Assoc Computat Linguist,"Martino, Giovanni Da San; Yu, Seunghak; Barron-Cedeno, Alberto; Petrov, Rostislav; Nakov, Preslav",,,Fine-Grained Analysis of Propaganda in News Articles,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Propaganda aims at influencing people's mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection at the document level, typically labelling all articles from a propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect the quality of any learning system trained on them. A further issue with most existing systems is the lack of explainability. To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at the fragment level with eighteen propaganda techniques and we propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5636,5646,,,,,,,,,,,,,,,,WOS:000854193305078,0
C,"Ni, JM; Li, JC; McAuley, J",,,Assoc Computat Linguist,"Ni, Jianmo; Li, Jiacheng; McAuley, Julian",,,Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Several recent works have considered the problem of generating reviews (or 'tips') as a form of explanation as to why a recommendation might match a user's interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users' decision-making process. We seek to introduce new datasets and methods to address this recommendation justification task. In terms of data, we first propose an 'extractive' approach to identify review segments which justify users' intentions; this approach is then used to distantly label massive review corpora and construct largescale personalized recommendation justification datasets. In terms of generation, we design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,188,197,,,,,,,,,,,,,,,,WOS:000854193300018,0
C,"Ouchi, H; Suzuki, J; Inui, K",,,Assoc Computat Linguist,"Ouchi, Hiroki; Suzuki, Jun; Inui, Kentaro",,,Transductive Learning of Neural Language Models for Syntactic and Semantic Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In transductive learning, an unlabeled test set is used for model training. While this setting deviates from the common assumption of a completely unseen test set, it is applicable in many real-world scenarios, where the texts to be processed are known in advance. However, despite its practical advantages, transductive learning is underexplored in natural language processing. Here, we conduct an empirical study of transductive learning for neural models and demonstrate its utility in syntactic and semantic tasks. Specifically, we fine-tune language models (LMs) on an unlabeled test set to obtain test-set-specific word representations. Through extensive experiments, we demonstrate that despite its simplicity, transductive LM fine-tuning consistently improves state-ofthe-art neural models in both in-domain and out-of-domain settings.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3665,3671,,,,,,,,,,,,,,,,WOS:000854193303097,0
C,"Renduchintala, A; Koehn, P; Eisner, J",,,Assoc Computat Linguist,"Renduchintala, Adithya; Koehn, Philipp; Eisner, Jason",,,Spelling-Aware Construction of Macaronic Texts for Teaching Foreign-Language Vocabulary,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a machine foreign-language teacher that modifies text in a student's native language (L1) by replacing some word tokens with glosses in a foreign language (L2), in such a way that the student can acquire L2 vocabulary simply by reading the resulting macaronic text. The machine teacher uses no supervised data from human students. Instead, to guide the machine teacher's choice of which words to replace, we equip a cloze language model with a training procedure that can incrementally learn representations for novel words, and use this model as a proxy for the word guessing and learning ability of real human students. We use Mechanical Turk to evaluate two variants of the student model: (i) one that generates a representation for a novel word using only surrounding context and (ii) an extension that also uses the spelling of the novel word.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6438,6443,,,,,,,,,,,,,,,,WOS:000854193306090,0
C,"Taitelbaum, H; Chechik, G; Goldberger, J",,,Assoc Computat Linguist,"Taitelbaum, Hagai; Chechik, Gal; Goldberger, Jacob",,,Multilingual Word Translation using Auxiliary Languages,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current multilingual word translation methods are focused on jointly learning mappings from each language to a shared space. The actual translation, however, is still performed as an isolated bilingual task. In this study we propose a multilingual translation procedure that uses all the learned mappings to translate a word from one language to another. For each source word, we first search for the most relevant languages. We then use the auxiliary translations to these languages to form an improved representation of the source word. Finally, this representation is used for the actual translation to the target language. Experiments on a standard multilingual word translation benchmark demonstrate that our model outperforms state of the art results.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1330,1335,,,,,,,,,,,,,,,,WOS:000854193301056,0
C,"Zeng, XS; Li, J; Wang, L; Wong, KF",,,Assoc Computat Linguist,"Zeng, Xingshan; Li, Jing; Wang, Lu; Wong, Kam-Fai",,,Neural Conversation Recommendation with Online Interaction Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The prevalent use of social media leads to a vast amount of online conversations being produced on a daily basis. It presents a concrete challenge for individuals to better discover and engage in social media discussions. In this paper, we present a novel framework to automatically recommend conversations to users based on their prior conversation behaviors. Built on neural collaborative filtering, our model explores deep semantic features that measure how a user's preferences match an ongoing conversation's context. Furthermore, to identify salient characteristics from interleaving user interactions, our model incorporates graph-structured networks, where both replying relations and temporal features are encoded as conversation context. Experimental results on two large-scale datasets collected from Twitter and Reddit show that our model yields better performance than previous stateof-the-art models, which only utilize lexical features and ignore past user interactions in the conversations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4633,4643,,,,,,,,,,,,,,,,WOS:000854193304075,0
C,"Zheng, S; Cao, W; Xu, W; Bian, J",,,Assoc Computat Linguist,"Zheng, Shun; Cao, Wei; Xu, Wei; Bian, Jiang",,,Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/ dolphin-zs/Doc2EDAG.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,337,346,,,,,,,,,,,,,,,,WOS:000854193300032,0
C,"Zhou, P; Shi, WJ; Zhao, JY; Huang, KH; Chen, MH; Cotterell, R; Chang, KW",,,Assoc Computat Linguist,"Zhou, Pei; Shi, Weijia; Zhao, Jieyu; Huang, Kuan-Hao; Chen, Muhao; Cotterell, Ryan; Chang, Kai-Wei",,,Examining Gender Bias in Languages with Grammatical Gender,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches effectively reduce the gender bias while preserving the utility of the embeddings.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5276,5284,,,,,,,,,,,,,,,,WOS:000854193305044,0
C,"Alqahtani, S; Mishra, A; Diab, M",,,Assoc Computat Linguist,"Alqahtani, Sawsan; Mishra, Ajay; Diab, Mona",,,Efficient Convolutional Neural Networks for Diacritic Restoration,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Diacritic restoration has gained importance with the growing need for machines to understand written texts. The task is typically modeled as a sequence labeling problem and currently Bidirectional Long Short Term Memory (BiLSTM) models provide state-of-the-art results. Recently, Bai et al. (2018) show the advantages of Temporal Convolutional Neural Networks (TCN) over Recurrent Neural Networks (RNN) for sequence modeling in terms of performance and computational resources. As diacritic restoration benefits from both previous as well as subsequent timesteps, we further apply and evaluate a variant of TCN, Acausal TCN (A-TCN), which incorporates context from both directions (previous and future) rather than strictly incorporating previous context as in the case of TCN. A-TCN yields significant improvement over TCN for diacritization in three different languages: Arabic, Yoruba, and Vietnamese. Furthermore, A-TCN and BiLSTM have comparable performance, making A-TCN an efficient alternative over BiLSTM since convolutions can be trained in parallel. A-TCN is significantly faster than BiLSTM at inference time (270%similar to 334% improvement in the amount of text diacritized per minute).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1442,1448,,,,,,,,,,,,,,,,WOS:000854193301073,0
C,"Cui, BY; Li, YM; Chen, M; Zhang, ZF",,,Assoc Computat Linguist,"Cui, Baiyun; Li, Yingming; Chen, Ming; Zhang, Zhongfei",,,Fine-tune BERT with Sparse Self-Attention Mechanism,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into self-attention mechanism to enhance the finetuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when fine-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed model is evaluated on sentiment analysis, question answering, and natural language inference tasks. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the baseline methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3548,3553,,,,,,,,,,,,,,,,WOS:000854193303079,0
C,"Derby, S; Miller, P; Devereux, B",,,Assoc Computat Linguist,"Derby, Steven; Miller, Paul; Devereux, Barry",,,Feature2Vec: Distributional semantic modelling of human property knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Feature norm datasets of human conceptual knowledge, collected in surveys of human volunteers, yield highly interpretable models of word meaning and play an important role in neurolinguistic research on semantic cognition. However, these datasets are limited in size due to practical obstacles associated with exhaustively listing properties for a large number of words. In contrast, the development of distributional modelling techniques and the availability of vast text corpora have allowed researchers to construct effective vector space models of word meaning over large lexicons. However, this comes at the cost of interpretable, human-like information about word meaning. We propose a method for mapping human property knowledge onto a distributional semantic space, which adapts the word2vec architecture to the task of modelling concept features. Our approach gives a measure of concept and feature affinity in a single semantic space, which makes for easy and efficient ranking of candidate human-derived semantic properties for arbitrary words. We compare our model with a previous approach, and show that it performs better on several evaluation tasks. Finally, we discuss how our method could be used to develop efficient sampling techniques to extend existing feature norm datasets in a reliable way.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5853,5859,,,,,,,,,,,,,,,,WOS:000854193306006,0
C,"Du, L; Ding, X; Liu, T; Li, ZY",,,Assoc Computat Linguist,"Du, Li; Ding, Xiao; Liu, Ting; Li, Zhongyang",,,Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Understanding event and event-centered commonsense reasoning are crucial for natural language processing (NLP). Given an observed event, it is trivial for human to infer its intents and effects, while this type of If-Then reasoning still remains challenging for NLP systems. To facilitate this, a If-Then commonsense reasoning dataset Atomic is proposed, together with an RNN-based Seq2Seq model to conduct such reasoning. However, two fundamental problems still need to be addressed: first, the intents of an event may be multiple, while the generations of RNN-based Seq2Seq models are always semantically close; second, external knowledge of the event background may be necessary for understanding events and conducting the If-Then reasoning. To address these issues, we propose a novel context-aware variational autoencoder effectively learning event background information to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2682,2691,,,,,,,,,,,,,,,,WOS:000854193302078,0
C,"Ferreira, W; Vlachos, A",,,Assoc Computat Linguist,"Ferreira, William; Vlachos, Andreas",,,Incorporating Label Dependencies in Multilabel Stance Detection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Stance detection in social media is a well-studied task in a variety of domains. Nevertheless, previous work has mostly focused on multiclass versions of the problem, where the labels are mutually exclusive, and typically positive, negative or neutral. In this paper, we address versions of the task in which an utterance can have multiple labels, thus corresponding to multilabel classification. We propose a method that explicitly incorporates label dependencies in the training objective and compare it against a variety of baselines, as well as a reduction of multilabel to multiclass learning. In experiments with three datasets, we find that our proposed method improves upon all baselines on two out of three datasets. We also show that the reduction of multilabel to multiclass classification can be very competitive, especially in cases where the output consists of a small number of labels and one can enumerate over all label combinations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6350,6354,,,,,,,,,,,,,,,,WOS:000854193306076,0
C,"Garbacea, C; Carton, S; Yan, SY; Mei, QZ",,,Assoc Computat Linguist,"Garbacea, Cristina; Carton, Samuel; Yan, Shiyan; Mei, Qiaozhu",,,Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We conduct a large-scale, systematic study to evaluate the existing evaluation methods for natural language generation in the context of generating online product reviews. We compare human-based evaluators with a variety of automated evaluation procedures, including discriminative evaluators that measure how well machine-generated text can be distinguished from human-written text, as well as word overlap metrics that assess how similar the generated text compares to human-written references. We determine to what extent these different evaluators agree on the ranking of a dozen of state-of-the-art generators for online product reviews. We find that human evaluators do not correlate well with discriminative evaluators, leaving a bigger question of whether adversarial accuracy is the correct objective for natural language generation. In general, distinguishing machine-generated text is challenging even for human evaluators, and human decisions correlate better with lexical overlaps. We find lexical diversity an intriguing metric that is indicative of the assessments of different evaluators. A post-experiment survey of participants provides insights into how to evaluate and improve the quality of natural language generation systems(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3968,3981,,,,,,,,,,,,,,,,WOS:000854193304014,0
C,"Jain, A; Paranjape, B; Lipton, ZC",,,Assoc Computat Linguist,"Jain, Alankar; Paranjape, Bhargavi; Lipton, Zachary C.",,,Entity Projection via Machine-Translation for Cross-Lingual NER,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Although over 100 languages are supported by strong off-the-shelf machine translation systems, only a subset of them possess large annotated corpora for named entity recognition. Motivated by this fact, we leverage machine translation to improve annotation-projection approaches to cross-lingual named entity recognition. We propose a system that improves over prior entity-projection methods by: (a) leveraging machine translation systems twice: first for translating sentences and subsequently for translating entities; (b) matching entities based on orthographic and phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Our approach improves upon current state-of-the-art methods for cross-lingual named entity recognition on 5 diverse languages by an average of 4:1 points. Further, our method achieves state-of-the-art F-1 scores for Armenian, outperforming even a monolingual model trained on Armenian source data.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1083,+,,,,,,,,,,,,,,,,WOS:000854193301022,0
C,"Karamanolakis, G; Hsu, D; Gravano, L",,,Assoc Computat Linguist,"Karamanolakis, Giannis; Hsu, Daniel; Gravano, Luis",,,Leveraging Just a Few Keywords for Fine-Grained Aspect Detection ThroughWeakly Supervised Co-Training,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"User-generated reviews can be decomposed into fine-grained segments (e.g., sentences, clauses), each evaluating a different aspect of the principal entity (e.g., price, quality, appearance). Automatically detecting these aspects can be useful for both users and downstream opinion mining applications. Current supervised approaches for learning aspect classifiers require many fine-grained aspect labels, which are labor-intensive to obtain. And, unfortunately, unsupervised topic models often fail to capture the aspects of interest. In this work, we consider weakly supervised approaches for training aspect classifiers that only require the user to provide a small set of seed words (i.e., weakly positive indicators) for the aspects of interest. First, we show that current weakly supervised approaches do not effectively leverage the predictive power of seed words for aspect detection. Next, we propose a student-teacher approach that effectively leverages seed words in a bag-of-words classifier (teacher); in turn, we use the teacher to train a second model (student) that is potentially more powerful (e.g., a neural network that uses pre-trained word embeddings). Finally, we show that iterative co-training can be used to cope with noisy seed words, leading to both improved teacher and student models. Our proposed approach consistently outperforms previous weakly supervised approaches (by 14.1 absolute F1 points on average) in six different domains of product reviews and six multilingual datasets of restaurant reviews.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4611,4621,,,,,,,,,,,,,,,,WOS:000854193304073,0
C,"Ke, P; Huang, F; Huang, ML; Zhu, XY",,,Assoc Computat Linguist,"Ke, Pei; Huang, Fei; Huang, Minlie; Zhu, Xiaoyan",,,ARAML: A Stable Adversarial Training Framework for Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most of the existing generative adversarial networks (GAN) for text generation suffer from the instability of reinforcement learning training algorithms such as policy gradient, leading to unstable performance. To tackle this problem, we propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML). During adversarial training, the discriminator assigns rewards to samples which are acquired from a stationary distribution near the data rather than the generator's distribution. The generator is optimized with maximum likelihood estimation augmented by the discriminator's rewards instead of policy gradient. Experiments show that our model can outperform state-of-the-art text GANs with a more stable training process.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4271,4281,,,,,,,,,,,,,,,,WOS:000854193304041,0
C,"Lange, L; Hedderich, MA; Klakow, D",,,Assoc Computat Linguist,"Lange, Lukas; Hedderich, Michael A.; Klakow, Dietrich",,,Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Noisy Labels,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In low-resource settings, the performance of supervised labeling models can be improved with automatically annotated or distantly supervised data, which is cheap to create but often noisy. Previous works have shown that significant improvements can be reached by injecting information about the confusion between clean and noisy labels in this additional training data into the classifier training. However, for noise estimation, these approaches either do not take the input features (in our case word embeddings) into account, or they need to learn the noise modeling from scratch which can be difficult in a low-resource setting. We propose to cluster the training data using the input features and then compute different confusion matrices for each cluster. To the best of our knowledge, our approach is the first to leverage feature-dependent noise modeling with pre-initialized confusion matrices. We evaluate on low-resource named entity recognition settings in several languages, showing that our methods improve upon other confusion-matrix based methods by up to 9%.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3554,3559,,,,,,,,,,,,,,,,WOS:000854193303080,0
C,"Li, P; Tuzhilin, A",,,Assoc Computat Linguist,"Li, Pan; Tuzhilin, Alexander",,,Towards Controllable and Personalized Review Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we propose a novel model RevGAN that automatically generates controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information. RevGAN utilizes the combination of three novel components, including selfattentive recursive autoencoders, conditional discriminators, and personalized decoders. We test its performance on the several real-world datasets, where our model significantly outperforms state-of-the-art generation models in terms of sentence quality, coherence, personalization and human evaluations. We also empirically show that the generated reviews could not be easily distinguished from the organically produced reviews and that they follow the same statistical linguistics laws.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3237,3245,,,,,,,,,,,,,,,,WOS:000854193303037,0
C,"Li, XL; Eisner, J",,,Assoc Computat Linguist,"Li, Xiang Lisa; Eisner, Jason",,,SpecializingWord Embeddings (for Parsing) by Information Bottleneck,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2744,2754,,,,,,,,,,,,,,,,WOS:000854193302084,0
C,"McCarthy, AD; Wu, W; Mueller, A; Watson, B; Yarowsky, D",,,Assoc Computat Linguist,"McCarthy, Arya D.; Wu, Winston; Mueller, Aaron; Watson, Bill; Yarowsky, David",,,Modeling Color Terminology Across Thousands of Languages,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"There is an extensive history of scholarship into what constitutes a basic color term, as well as a broadly attested acquisition sequence of basic color terms across many languages, as articulated in the seminal work of Berlin and Kay (1969). This paper employs a set of diverse measures on massively cross-linguistic data to operationalize and critique the Berlin and Kay color term hypotheses. Collectively, the 14 empirically-grounded computational linguistic metrics we design-as well as their aggregation-correlate strongly with both the Berlin and Kay basic/secondary color term partition (gamma = 0.96) and their hypothesized universal acquisition sequence. The measures and result provide further empirical evidence from computational linguistics in support of their claims, as well as additional nuance: they suggest treating the partition as a spectrum instead of a dichotomy.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2241,2250,,,,,,,,,,,,,,,,WOS:000854193302037,0
C,"Min, BN; Zhao, XX",,,Assoc Computat Linguist,"Min, Bonan; Zhao, Xiaoxi",,,Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Socio-economic conditions are difficult to measure. For example, the U.S. Bureau of Labor Statistics needs to conduct large-scale household surveys regularly to track the unemployment rate, an indicator widely used by economists and policy makers. We argue that events reported in streaming news can be used as micro-sensors for measuring socio-economic conditions. Similar to collecting surveys and then counting answers, it is possible to measure a socio-economic indicator by counting related events. In this paper, we propose Event-Centric Indicator Measure (ECIM), a novel approach to measure socioeconomic indicators with events. We empirically demonstrate strong correlations between ECIM values to several representative indicators in socio-economic research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1249,1254,,,,,,,,,,,,,,,,WOS:000854193301043,0
C,"Moon, HC; Mohiuddin, T; Joty, S; Xu, C",,,Assoc Computat Linguist,"Moon, Han Cheol; Mohiuddin, Tasnim; Joty, Shafiq; Xu, Chi",,,A Unified Neural Coherence Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models under-perform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.",,,,,,"Xu, Chi/0000-0001-5480-3974",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2262,2272,,,,,,,,,,,,,,,,WOS:000854193302039,0
C,"Stahlberg, F; Byrne, B",,,Assoc Computat Linguist,"Stahlberg, Felix; Byrne, Bill",,,On NMT Search Errors and Model Errors: Cat Got Your Tongue?,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",,,,,"Sharipov, Felix/B-5059-2013","Sharipov, Felix/0000-0001-9372-2915",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3356,3362,,,,,,,,,,,,,,,,WOS:000854193303049,0
C,"Tsai, H; Riesa, J; Johnson, M; Arivazhagan, N; Li, X; Archer, A",,,Assoc Computat Linguist,"Tsai, Henry; Riesa, Jason; Johnson, Melvin; Arivazhagan, Naveen; Li, Xin; Archer, Amelia",,,Small and Practical BERT Models for Sequence Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-theart multilingual baseline. We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3632,3636,,,,,,,,,,,,,,,,WOS:000854193303092,0
C,"Wang, WK; Zhang, JJ; Li, Q; Zong, CQ; Li, ZF",,,Assoc Computat Linguist,"Wang, Weikang; Zhang, Jiajun; Li, Qian; Zong, Chengqing; Li, Zhifei",,,Are You for Real? Detecting Identity Fraud via Dialogue Interactions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Identity fraud detection is of great importance in many real-world scenarios such as the financial industry. However, few studies addressed this problem before. In this paper, we focus on identity fraud detection in loan applications and propose to solve this problem with a novel interactive dialogue system which consists of two modules. One is the knowledge graph (KG) constructor organizing the personal information for each loan applicant. The other is structured dialogue management that can dynamically generate a series of questions based on the personal KG to ask the applicants and determine their identity states. We also present a heuristic user simulator based on problem analysis to evaluate our method. Experiments have shown that the trainable dialogue system can effectively detect fraudsters, and achieve higher recognition accuracy compared with rule-based systems. Furthermore, our learned dialogue strategies are interpretable and flexible, which can help promote real-world applications.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1762,1771,,,,,,,,,,,,,,,,WOS:000854193301107,0
C,"Wang, ZJ; Potts, C",,,Assoc Computat Linguist,"Wang, Zijian; Potts, Christopher",,,TalkDown: A Corpus for Condescension Detection in Context,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Condescending language use is caustic; it can bring dialogues to an end and bifurcate communities. Thus, systems for condescension detection could have a large positive impact. A challenge here is that condescension is often impossible to detect from isolated utterances, as it depends on the discourse and social context. To address this, we present TALKDOWN, a new labeled dataset of condescending linguistic acts in context. We show that extending a language-only model with representations of the discourse improves performance, and we motivate techniques for dealing with the low rates of condescension overall. We also use our model to estimate condescension rates in various online communities and relate these differences to differing community norms.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3711,3719,,,,,,,,,,,,,,,,WOS:000854193303103,0
C,"Wu, YT; Liu, X; Feng, YS; Wang, Z; Zhao, DY",,,Assoc Computat Linguist,"Wu, Yuting; Liu, Xiao; Feng, Yansong; Wang, Zheng; Zhao, Dongyan",,,Jointly Learning Entity and Relation Representations for Entity Alignment,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Entity alignment is a viable means for integrating heterogeneous knowledge among different knowledge graphs (KGs). Recent developments in the field often take an embeddingbased approach to model the structural information of KGs so that entity alignment can be easily performed in the embedding space. However, most existing works do not explicitly utilize useful relation representations to assist in entity alignment, which, as we will show in the paper, is a simple yet effective way for improving entity alignment. This paper presents a novel joint learning framework for entity alignment. At the core of our approach is a Graph Convolutional Network (GCN) based framework for learning both entity and relation representations. Rather than relying on pre-aligned relation seeds to learn relation representations, we first approximate them using entity embeddings learned by the GCN. We then incorporate the relation approximation into entities to iteratively learn better representations for both. Experiments performed on three real-world cross-lingual datasets show that our approach substantially outperforms state-of-the-art entity alignment methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,240,249,,,,,,,,,,,,,,,,WOS:000854193300023,0
C,"Xiao, L; Huang, X; Chen, BL; Jing, LP",,,Assoc Computat Linguist,"Xiao, Lin; Huang, Xin; Chen, Boli; Jing, Liping",,,Label-Specific Document Representation for Multi-Label Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-label text classification (MLTC) aims to tag most relevant labels for the given document. In this paper, we propose a LabelSpecific Attention Network (LSAN) to learn the new document representation. LSAN takes advantage of label semantic information to determine the semantic connection between labels and document for constructing label-specific document representation. Meanwhile, the self-attention mechanism is adopted to identify the label-specific document representation from document content information. In order to seamlessly integrate the above two parts, an adaptive fusion strategy is designed, which can effectively output the comprehensive document representation to build multilabel text classifier. Extensive experimental results on four benchmark datasets demonstrate that LSAN consistently outperforms the state-of-the-art methods, especially on the prediction of low-frequency labels. The code and hyper-parameter settings are released to facilitate other researchers (1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,466,475,,,,,,,,,,,,,,,,WOS:000854193300044,0
C,"An, B; Chen, B; Han, XP; Sun, L",,,ASSOC COMPUTAT LINGUIST,"An, Bo; Chen, Bo; Han, Xianpei; Sun, Le",,,EUSP: An Easy-to-Use Semantic Parsing PlatForm,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic parsing aims to map natural language utterances into structured meaning representations. We present a modular platform, EUSP (Easy-to-Use Semantic Parsing PlatForm), that facilitates developers to build semantic parser from scratch. Instead of requiring a large amount of training data or complex grammar knowledge, in our platform developers can build grammar-based semantic parser or neural-based semantic parser through configure files which specify the modules and components that compose semantic parsing system. A high quality grammar-based semantic parsing system only requires domain lexicons rather than costly training data for a semantic parser. Furthermore, we provide a browser-based method to generate the semantic parsing system to minimize the difficulty of development. Experimental results show that the neural-based semantic parser system achieves competitive performance on semantic parsing task, and grammar-based semantic parsers significantly improve the performance of a business search engine.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,67,72,,,,,,,,,,,,,,,,WOS:000855231500012,0
C,"Bhardwaj, S; Aggarwal, S; Mausam",,,Assoc Computat Linguist,"Bhardwaj, Sangnie; Aggarwal, Samarth; Mausam",,,CaRB: A Crowdsourced Benchmark for Open IE,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Open Information Extraction (Open IE) systems have been traditionally evaluated via manual annotation. Recently, an automated evaluator with a benchmark dataset (OIE2016) was released - it scores Open IE systems automatically by matching system predictions with predictions in the benchmark dataset (Stanovsky and Dagan, 2016). Unfortunately, our analysis reveals that its data is rather noisy, and the tuple matching in the evaluator has issues, making the results of automated comparisons less trustworthy. We contribute CaRB, an improved dataset and framework for testing Open IE systems. To the best of our knowledge, CaRB is the first crowdsourced Open IE dataset and it also makes substantive changes in the matching code and metrics. NLP experts annotate CaRB's dataset to be more accurate than OIE2016. Moreover, we find that on one pair of Open IE systems, CaRB framework provides contradictory results to OIE2016. Human assessment verifies that CaRB's ranking of the two systems is the accurate ranking. We release the CaRB framework along with its crowdsourced dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6262,6267,,,,,,,,,,,,,,,,WOS:000854193306062,0
C,"Cao, L; Zhang, HJ; Feng, L; Wei, ZH; Wang, X; Li, NY; He, XH",,,Assoc Computat Linguist,"Cao, Lei; Zhang, Huijun; Feng, Ling; Wei, Zihan; Wang, Xin; Li, Ningyun; He, Xiaohao",,,Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite detection of suicidal ideation on social media has made great progress in recent years, people's implicitly and anti-real contrarily expressed posts still remain as an obstacle, constraining the detectors to acquire higher satisfactory performance. Enlightened by the hidden tree holes phenomenon on microblog, where people at suicide risk tend to disclose their inner real feelings and thoughts to the microblog space whose authors have committed suicide, we explore the use of tree holes to enhance microblog-based suicide risk detection from the following two perspectives. (1) We build suicide-oriented word embeddings based on tree hole contents to strength the sensibility of suicide-related lexicons and context based on tree hole contents. (2) A two-layered attention mechanism is deployed to grasp intermittently changing points from individual's open blog streams, revealing one's inner emotional world more or less. Our experimental results show that with suicide-oriented word embeddings and attention, microblog-based suicide risk detection can achieve over 91% accuracy. A large-scale well-labelled suicide data set is also reported in the paper.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1718,1728,,,,,,,,,,,,,,,,WOS:000854193301103,0
C,"Chan, ZM; Li, JT; Yang, XP; Chen, XY; Hu, WP; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Chan, Zhangming; Li, Juntao; Yang, Xiaopeng; Chen, Xiuying; Hu, Wenpeng; Zhao, Dongyan; Yan, Rui",,,Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoencoders,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Variational autoencoders (VAEs) and Wasserstein autoencoders (WAEs) have achieved noticeable progress in open-domain response generation. Through introducing latent variables in continuous space, these models are capable of capturing utterance-level semantics, e.g., topic, syntactic properties, and thus can generate informative and diversified responses. In this work, we improve the WAE for response generation. In addition to the utterance-level information, we also model user-level information in latent continue space. Specifically, we embed user-level and utterance-level information into two multimodal distributions, and combine these two multimodal distributions into a mixed distribution. This mixed distribution will be used as the prior distribution of WAE in our proposed model, named as Persona WAE. Experimental results on a large-scale real-world dataset confirm the superiority of our model for generating informative and personalized responses, where both automatic and human evaluations outperform state-of-the-art models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1931,1940,,,,,,,,,,,,,,,,WOS:000854193302009,0
C,"Cui, YM; Liu, T; Che, WX; Xiao, L; Chen, ZP; Ma, WT; Wang, SJ; Hu, GP",,,Assoc Computat Linguist,"Cui, Yiming; Liu, Ting; Che, Wanxiang; Xiao, Li; Chen, Zhipeng; Ma, Wentao; Wang, Shijin; Hu, Guoping",,,A Span-Extraction Dataset for Chinese Machine Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a SpanExtraction dataset for Chinese machine reading comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several baseline systems as well as anonymous submissions for demonstrating the difficulties in this dataset. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research.(1)",,,,,"liu, ting/GZM-3326-2022; Cui, Yiming/AAA-5499-2022","Cui, Yiming/0000-0002-2452-375X",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5883,5889,,,,,,,,,,,,,,,,WOS:000854193306011,0
C,"Deruyttere, T; Vandenhende, S; Grujicic, D; Van Gool, L; Moens, MF",,,Assoc Computat Linguist,"Deruyttere, Thierry; Vandenhende, Simon; Grujicic, Dusan; Van Gool, Luc; Moens, Marie-Francine",,,Talk2Car: Taking Control of Your Self-Driving Car,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A long-term goal of artificial intelligence is to have an agent execute commands communicated through natural language. In many cases the commands are grounded in a visual environment shared by the human who gives the command and the agent. Execution of the command then requires mapping the command into the physical visual space, after which the appropriate action can be taken. In this paper we consider the former. Or more specifically, we consider the problem in an autonomous driving setting, where a passenger requests an action that can be associated with an object found in a street scene. Our work presents the Talk2Car dataset, which is the first object referral dataset that contains commands written in natural language for self-driving cars. We provide a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance analysis using strong state-ofthe-art models. The results show that the proposed object referral task is a challenging one for which the models show promising results but still require additional research in natural language processing, computer vision and the intersection of these fields. The dataset can be found on our website: http:// macchina-ai.eu/",,,,,,"Deruyttere, Thierry/0000-0003-1348-983X",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2088,2098,,,,,,,,,,,,,,,,WOS:000854193302023,0
C,"Du, YP; Wu, YB; Lan, M",,,Assoc Computat Linguist,"Du, Yupei; Wu, Yuanbin; Lan, Man",,,Exploring Human Gender Stereotypes with Word Association Test,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6133,6143,,,,,,,,,,,,,,,,WOS:000854193306046,0
C,"Gauthier, J; Levy, RP",,,Assoc Computat Linguist,"Gauthier, Jon; Levy, Roger P.",,,Linking artificial and human neural representations of language,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,529,539,,,,,,,,,,,,,,,,WOS:000854193300050,0
C,"Han, X; Gao, TY; Yao, Y; Ye, DM; Liu, ZY; Sun, MS",,,ASSOC COMPUTAT LINGUIST,"Han, Xu; Gao, Tianyu; Yao, Yuan; Ye, Demin; Liu, Zhiyuan; Sun, Maosong",,,OpenNRE: An Open and Extensible Toolkit for Neural Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"OpenNRE is an open-source and extensible toolkit that provides a unified framework to implement neural models for relation extraction (RE). Specifically, by implementing typical RE methods, OpenNRE not only allows developers to train custom models to extract structured relational facts from the plain text but also supports quick model validation for researchers. Besides, OpenNRE provides various functional RE modules based on both TensorFlow and PyTorch to maintain sufficient modularity and extensibility, making it becomes easy to incorporate new models into the framework. Besides the toolkit, we also release an online system to meet real-time extraction without any training and deploying. Meanwhile, the online system can extract facts in various scenarios as well as aligning the extracted facts to Wikidata, which may benefit various downstream knowledge-driven applications (e.g., information retrieval and question answering). More details of the toolkit and online system can be obtained from http://github.com/ thunlp/OpenNRE.",,,,,"li, zhiyuan/HGD-9581-2022","Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,169,174,,,,,,,,,,,,,,,,WOS:000855231500029,0
C,"Huang, LY; Sun, C; Qiu, XP; Huang, XJ",,,Assoc Computat Linguist,"Huang, Luyao; Sun, Chi; Qiu, Xipeng; Huang, Xuanjing",,,GlossBERT: BERT forWord Sense Disambiguation with Gloss Knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT-based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3509,3514,,,,,,,,,,,,,,,,WOS:000854193303073,0
C,"Jiang, M; Huang, QY; Zhang, L; Wang, X; Zhang, PCA; Gan, Z; Diesner, J; Gao, JF",,,Assoc Computat Linguist,"Jiang, Ming; Huang, Qiuyuan; Zhang, Lei; Wang, Xin; Zhang, Pengchuan; Gan, Zhe; Diesner, Jana; Gao, Jianfeng",,,TIGEr: Text-to-Image Grounding for Image Caption Evaluation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machinegenerated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric's effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2141,2152,,,,,,,,,,,,,,,,WOS:000854193302028,0
C,"Jiang, YF; Hu, C; Xiao, T; Zhang, CL; Zhu, JB",,,Assoc Computat Linguist,"Jiang, Yufan; Hu, Chi; Xiao, Tong; Zhang, Chunliang; Zhu, Jingbo",,,Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-of-the-art on the NER task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3585,3590,,,,,,,,,,,,,,,,WOS:000854193303085,0
C,"Jo, H; Cinarel, C",,,Assoc Computat Linguist,"Jo, Hwiyeol; Cinarel, Ceyda",,,Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a novel and simple method for semi-supervised text classification. The method stems from the hypothesis that a classifier with pretrained word embeddings always outperforms the same classifier with randomly initialized word embeddings, as empirically observed in NLP tasks. Our method first builds two sets of classifiers as a form of model ensemble, and then initializes their word embeddings differently: one using random, the other using pretrained word embeddings. We focus on different predictions between the two classifiers on unlabeled data while following the self-training framework. We also use earlystopping in meta-epoch to improve the performance of our method. Our method, Deltatraining, outperforms the self-training and the co-training framework in 4 different text classification datasets, showing robustness against error accumulation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3458,3463,,,,,,,,,,,,,,,,WOS:000854193303065,0
C,"Kulmizev, A; de Lhoneux, M; Gontrum, J; Fano, E; Nivre, J",,,Assoc Computat Linguist,"Kulmizev, Artur; de Lhoneux, Miryam; Gontrum, Johannes; Fano, Elena; Nivre, Joakim",,,Deep ContextualizedWord Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Transition-based and graph-based dependency parsers have previously been shown to have complementary strengths and weaknesses: transition-based parsers exploit rich structural features but suffer from error propagation, while graph-based parsers benefit from global optimization but have restricted feature scope. In this paper, we show that, even though some details of the picture have changed after the switch to neural networks and continuous representations, the basic trade-off between rich features and global optimization remains essentially the same. Moreover, we show that deep contextualized word embeddings, which allow parsers to pack information about global sentence structure into local feature representations, benefit transition-based parsers more than graph-based parsers, making the two approaches virtually equivalent in terms of both accuracy and error profile. We argue that the reason is that these representations help prevent search errors and thereby allow transitionbased parsers to better exploit their inherent strength of making accurate local decisions. We support this explanation by an error analysis of parsing experiments on 13 languages.",,,,,,"de Lhoneux, Miryam/0000-0001-8844-2126",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2755,2768,,,,,,,,,,,,,,,,WOS:000854193302085,0
C,"Li, DQ; Zhang, YZ; Gan, Z; Cheng, Y; Brockett, C; Sun, MT; Dolan, B",,,Assoc Computat Linguist,"Li, Dianqi; Zhang, Yizhe; Gan, Zhe; Cheng, Yu; Brockett, Chris; Sun, Ming-Ting; Dolan, Bill",,,Domain Adaptive Text Style Transfer,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text style transfer without parallel data has achieved some practical success. However, in the scenario where less data is available, these methods may yield poor performance. In this paper, we examine domain adaptation for text style transfer to leverage massively available data from other domains. These data may demonstrate domain shift, which impedes the benefits of utilizing such data for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to: (i) distinguish stylized information and generic content information; (ii) maximally preserve content information; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3304,3313,,,,,,,,,,,,,,,,WOS:000854193303043,0
C,"Mayhew, S; Tsygankova, T; Roth, D",,,Assoc Computat Linguist,"Mayhew, Stephen; Tsygankova, Tatiana; Roth, Dan",,,ner and pos when nothing is capitalized,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"For those languages which use it, capitalization is an important signal for the fundamental NLP tasks of Named Entity Recognition (NER) and Part of Speech (POS) tagging. In fact, it is such a strong signal that model performance on these tasks drops sharply in common lowercased scenarios, such as noisy web text or machine translation outputs. In this work, we perform a systematic analysis of solutions to this problem in English, modifying only the casing of the train or test data using lowercasing and truecasing methods. While prior work and first impressions might suggest training a caseless model, or using a truecaser at test time, we show that the most effective strategy is a concatenation of cased and lowercased training data, producing a single model with high performance on both cased and uncased text. As shown in our experiments, this result holds across tasks and input representations. Finally, we show that our proposed solution gives an 8% F1 improvement in mention detection on noisy out-of-domain Twitter data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6256,6261,,,,,,,,,,,,,,,,WOS:000854193306061,0
C,"Papalampidi, P; Keller, F; Lapata, M",,,Assoc Computat Linguist,"Papalampidi, Pinelopi; Keller, Frank; Lapata, Mirella",,,Movie Plot Analysis via Turning Point Identification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"According to screenwriting theory, turning points (e.g., change of plans, major setback, climax) are crucial narrative moments within a screenplay: they define the plot structure, determine its progression and thematic units (e.g., setup, complications, aftermath). We propose the task of turning point identification in movies as a means of analyzing their narrative structure. We argue that turning points and the segmentation they provide can facilitate processing long, complex narratives, such as screenplays, for summarization and question answering. We introduce a dataset consisting of screenplays and plot synopses annotated with turning points and present an end-to-end neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays. Our model outperforms strong baselines based on state-of-the-art sentence representations and the expected position of turning points.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1707,1717,,,,,,,,,,,,,,,,WOS:000854193301102,0
C,"Park, JU; Ko, SK; Cognetta, M; Han, YS",,,Assoc Computat Linguist,"Park, Jun-U; Ko, Sang-Ki; Cognetta, Marco; Han, Yo-Sub",,,SoftRegex: Generating Regex from Natural Language Descriptions using Softened Regex Equivalence,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We continue the study of generating semantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model, SemRegex, produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ Reg model for computing the similarity of two regular expressions using deep neural networks. Our EQ Reg model essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, using the EQ Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6425,6431,,,,,,,,,,,,,,,,WOS:000854193306088,0
C,"Quan, J; Xiong, DY; Webber, B; Hu, CJ",,,Assoc Computat Linguist,"Quan, Jun; Xiong, Deyi; Webber, Bonnie; Hu, Changjian",,,GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Ellipsis and co-reference are common and ubiquitous especially in multi-turn dialogues. In this paper, we treat the resolution of ellipsis and co-reference in dialogue as a problem of generating omitted or referred expressions from the dialogue context. We therefore propose a unified end-to-end Generative Ellipsis and CO-reference Resolution model (GECOR) in the context of dialogue. The model can generate a new pragmatically complete user utterance by alternating the generation and copy mode for each user utterance. A multi-task learning framework is further proposed to integrate the GECOR into an end-to-end task-oriented dialogue. In order to train both the GECOR and the multitask learning framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequence-to-sequence (seq2seq) baseline model in terms of EM, BLEU and F-1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model (Lei et al., 2018).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4547,4557,,,,,,,,,,,,,,,,WOS:000854193304067,0
C,"Do, Q; Gaspers, J",,,Assoc Computat Linguist,"Quynh Do; Gaspers, Judith",,,Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A typical cross-lingual transfer learning approach boosting model performance on a resource-poor language is to pre-train the model on all available supervised data from another resource-rich language. However, in large-scale systems, this leads to high training times and computational requirements. In addition, characteristic differences between the source and target languages raise a natural question of whether source-language data selection can improve the knowledge transfer. In this paper, we address this question and propose a simple but effective language model based source-language data selection method for cross-lingual transfer learning in largescale spoken language understanding. The experimental results show that with data selection i) the source data amount and hence training speed is reduced significantly and ii) model performance is improved.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1455,1460,,,,,,,,,,,,,,,,WOS:000854193301075,0
C,"Ray, A; Sikka, K; Divakaran, A; Lee, S; Burachas, G",,,Assoc Computat Linguist,"Ray, Arijit; Sikka, Karan; Divakaran, Ajay; Lee, Stefan; Burachas, Giedrius",,,Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers red to What color is the balloon?, it might answer no if asked, Is the balloon red?. These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5860,5865,,,,,,,,,,,,,,,,WOS:000854193306007,0
C,"Santy, S; Dandapat, S; Choudhury, M; Bali, K",,,ASSOC COMPUTAT LINGUIST,"Santy, Sebastin; Dandapat, Sandipan; Choudhury, Monojit; Bali, Kalika",,,INMT: Interactive Neural Machine Translation Prediction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient, and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.(12)",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,103,108,,,,,,,,,,,,,,,,WOS:000855231500018,0
C,"Schmidt, F; Mandt, S; Hofmann, T",,,Assoc Computat Linguist,"Schmidt, Florian; Mandt, Stephan; Hofmann, Thomas",,,Autoregressive Text Generation Beyond Feedback Loops,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of autoregressive models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3400,3406,,,,,,,,,,,,,,,,WOS:000854193303056,0
C,"Schwab, M; Jaschke, R; Fischer, F; Strotgen, J",,,Assoc Computat Linguist,"Schwab, Michel; Jaeschke, Robert; Fischer, Frank; Stroetgen, Jannik",,,A Buster Keaton of Linguistics: First Automated Approaches for the Extraction of Vossian Antonomasia,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Attributing a particular property to a person by naming another person, who is typically well-known for the respective property, is called a Vossian Antonomasia (VA). This subtpye of metonymy, which overlaps with metaphor, has a specific syntax and is especially frequent in journalistic texts. While identifying Vossian Antonomasia is of particular interest in the study of stylistics, it is also a source of errors in relation and fact extraction as an explicitly mentioned entity occurs only metaphorically and should not be associated with respective contexts. Despite rather simple syntactic variations, the automatic extraction of VA was never addressed as yet since it requires a deeper semantic understanding of mentioned entities and underlying relations. In this paper, we propose a first method for the extraction of VAs that works completely automatically. Our approaches use named entity recognition, distant supervision based on Wikidata, and a bi-directional LSTM for postprocessing. The evaluation on 1.8 million articles of the New York Times corpus shows that our approach significantly outperforms the only existing semi-automatic approach for VA identification by more than 30 percentage points in precision.",,,,,,"Jaschke, Robert/0000-0003-3271-9653",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6238,6243,,,,,,,,,,,,,,,,WOS:000854193306058,0
C,"Sharma, E; Huang, LY; Hu, Z; Wang, L",,,Assoc Computat Linguist,"Sharma, Eva; Huang, Luyang; Hu, Zhe; Wang, Lu",,,An Entity-Driven Framework for Abstractive Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The Abstractive summarization systems aim to produce more coherent and concise summaries than their extractive counterparts. Popular neural models have achieved impressive results for single-document summarization, yet their outputs are often incoherent and unfaithful to the input. In this paper, we introduce SENECA, a novel System for ENtitydrivEn Coherent Abstractive summarization framework that leverages entity information to generate informative and coherent abstracts. Our framework takes a two-step approach: (1) an entity-aware content selection module first identifies salient sentences from the input, then (2) an abstract generation module conducts cross-sentence information compression and abstraction to generate the final summary, which is trained with rewards to promote coherence, conciseness, and clarity. The two components are further connected using reinforcement learning. Automatic evaluation shows that our model significantly outperforms previous state-of-the-art on ROUGE and our proposed coherence measures on New York Times and CNN/Daily Mail datasets. Human judges further rate our system summaries as more informative and coherent than those by popular summarization models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3280,3291,,,,,,,,,,,,,,,,WOS:000854193303041,0
C,"Sun, K; Zhang, RC; Mensah, S; Mao, YY; Liu, XD",,,Assoc Computat Linguist,"Sun, Kai; Zhang, Richong; Mensah, Samuel; Mao, Yongyi; Liu, Xudong",,,Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a method based on neural networks to identify the sentiment polarity of opinion words expressed on a specific aspect of a sentence. Although a large majority of works typically focus on leveraging the expressive power of neural networks in handling this task, we explore the possibility of integrating dependency trees with neural networks for representation learning. To this end, we present a convolution over a dependency tree (CDT) model which exploits a Bi-directional Long Short Term Memory (Bi-LSTM) to learn representations for features of a sentence, and further enhance the embeddings with a graph convolutional network (GCN) which operates directly on the dependency tree of the sentence. Our approach propagates both contextual and dependency information from opinion words to aspect words, offering discriminative properties for supervision. Experimental results ranks our approach as the new state-of-the-art in aspect-based sentiment classification.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5679,5688,,,,,,,,,,,,,,,,WOS:000854193305082,0
C,"Xu, ZN; Su, QL; Quan, XJ; Zhang, WJ",,,Assoc Computat Linguist,"Xu, Zenan; Su, Qinliang; Quan, Xiaojun; Zhang, Weijia",,,A Deep Neural Information Fusion Architecture for Textual Network Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Textual network embeddings aim to learn a low-dimensional representation for every node in the network so that both the structural and textual information from the networks can be well preserved in the representations. Traditionally, the structural and textual embeddings were learned by models that rarely take the mutual influences between them into account. In this paper, a deep neural architecture is proposed to effectively fuse the two kinds of informations into one representation. The novelties of the proposed architecture are manifested in the aspects of a newly defined objective function, the complementary information fusion method for structural and textual features, and the mutual gate mechanism for textual feature extraction. Experimental results show that the proposed model outperforms the comparing methods on all three datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4698,4706,,,,,,,,,,,,,,,,WOS:000854193304081,0
C,"Yang, SW; Zou, YY; Shi, P; Lu, W; Lin, J; Sun, X",,,Assoc Computat Linguist,"Yang, Hsiu-Wei; Zou, Yanyan; Shi, Peng; Lu, Wei; Lin, Jimmy; Sun, Xu",,,Aligning Cross-Lingual Entities with Multi-Aspect Information,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multiaspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERTbased modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4431,4441,,,,,,,,,,,,,,,,WOS:000854193304056,0
C,"Yang, ZX; Zhang, JC; Meng, FD; Gu, SH; Feng, Y; Zhou, J",,,Assoc Computat Linguist,"Yang, Zhengxin; Zhang, Jinchao; Meng, Fandong; Gu, Shuhao; Feng, Yang; Zhou, Jie",,,Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However, this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem, we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1527,1537,,,,,,,,,,,,,,,,WOS:000854193301086,0
C,"Yang, ZC; Cai, PS; Feng, YS; Li, F; Feng, WJ; Chiu, ESY; Yu, H",,,Assoc Computat Linguist,"Yang, Zhichao; Cai, Pengshan; Feng, Yansong; Li, Fei; Feng, Weijiang; Chiu, Elena Suet-Ying; Yu, Hong",,,Generating Classical Chinese Poems from Vernacular Chinese,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Classical Chinese poetry is a jewel in the treasure house of Chinese culture. Previous poem generation models only allow users to employ keywords to interfere the meaning of generated poems, leaving the dominion of generation to the model. In this paper, we propose a novel task of generating classical Chinese poems from vernacular, which allows users to have more control over the semantic of generated poems. We adapt the approach of unsupervised machine translation (UMT) to our task. We use segmentation-based padding and reinforcement learning to address under-translation and over-translation respectively. According to experiments, our approach significantly improve the perplexity and BLEU compared with typical UMT models. Furthermore, we explored guidelines on how to write the input vernacular to generate better poems. Human evaluation showed our approach can generate high-quality poems which are comparable to amateur poems.",,,,,,"Yang, Zhichao/0000-0002-2797-4257",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6155,6164,,,,,,,,,,,32467928.0,,,,,WOS:000854193306048,0
C,"Yin, WP; Hay, J; Roth, D",,,Assoc Computat Linguist,"Yin, Wenpeng; Hay, Jamaal; Roth, Dan",,,"Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Zero-shot text classification (0SHOT-TC) is a challenging NLU problem to which little attention has been paid by the research community. 0SHOT- TC aims to associate an appropriate label with a piece of text, irrespective of the text domain and the aspect (e.g., topic, emotion, event, etc.) described by the label. And there are only a few articles studying 0SHOT-TC, all focusing only on topical categorization which, we argue, is just the tip of the iceberg in 0SHOT-TC. In addition, the chaotic experiments in literature make no uniform comparison, which blurs the progress. This work benchmarks the 0SHOT-TC problem by providing unified datasets, standardized evaluations, and state-of-the-art baselines. Our contributions include: i) The datasets we provide facilitate studying 0SHOT- TC relative to conceptually different and diverse aspects: the topic aspect includes sports and politics as labels; the emotion aspect includes joy and anger; the situation aspect includes medical assistance and water shortage. ii) We extend the existing evaluation setup (labelpartially-unseen) - given a dataset, train on some labels, test on all labels - to include a more challenging yet realistic evaluation label-fully-unseen 0SHOT- TC (Chang et al., 2008), aiming at classifying text snippets without seeing task specific training data at all. iii) We unify the 0SHOT- TC of diverse aspects within a textual entailment formulation and study it this way. (1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3914,3923,,,,,,,,,,,,,,,,WOS:000854193304009,0
C,"Alva-Manchego, F; Martin, L; Scarton, C; Specia, L",,,ASSOC COMPUTAT LINGUIST,"Alva-Manchego, Fernando; Martin, Louis; Scarton, Carolina; Specia, Lucia",,,EASSE: Easier Automatic Sentence Simplification Evaluation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce EASSE, a Python package aiming to facilitate and standardise automatic evaluation and comparison of Sentence Simplification (SS) systems. EASSE provides a single access point to a broad range of evaluation resources: standard automatic metrics for assessing SS outputs (e.g. SARI), wordlevel accuracy scores for certain simplification transformations, reference-independent quality estimation features (e.g. compression ratio), and standard test data for SS evaluation (e.g. TurkCorpus). Finally, EASSE generates easy-to-visualise reports on the various metrics and features above and on how a particular SS output fares against reference simplifications. Through experiments, we show that these functionalities allow for better comparison and understanding of the performance of SS systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,49,54,,,,,,,,,,,,,,,,WOS:000855231500009,0
C,"Arase, Y; Tsujii, J",,,Assoc Computat Linguist,"Arase, Yuki; Tsujii, Junichi",,,Transfer Fine-Tuning: A BERT Case Study,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5393,5404,,,,,,,,,,,,,,,,WOS:000854193305055,0
C,"Badene, S; Thompson, K; Lorre, JP; Asher, N",,,Assoc Computat Linguist,"Badene, Sonia; Thompson, Kate; Lorre, Jean-Pierre; Asher, Nicholas",,,Weak Supervision for Learning Discourse Structure,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper provides a detailed comparison of a data programming approach with (i) off-the-shelf, state-of-the-art deep learning architectures that optimize their representations (BERT) and (ii) handcrafted-feature approaches previously used in the discourse analysis literature. We compare these approaches on the task of learning discourse structure for multi-party dialogue. The data programming paradigm offered by the Snorkel framework allows a user to label training data using expert-composed heuristics, which are then transformed via the generative step into probability distributions of the class labels given the data. We show that on our task the generative model outperforms both deep learning architectures as well as more traditional ML approaches when learning discourse structure-it even outperforms the combination of deep learning methods and handcrafted features. We also implement several strategies for decoding our generative model output in order to improve our results. We conclude that weak supervision methods hold great promise as a means for creating and improving data sets for discourse structure.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2296,2305,,,,,,,,,,,,,,,,WOS:000854193302042,0
C,"Cao, M; Cheung, JCK",,,Assoc Computat Linguist,"Cao, Meng; Cheung, Jackie Chi Kit",,,Referring Expression Generation Using Entity Profiles,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Referring Expression Generation (REG) is the task of generating contextually appropriate references to entities. A limitation of existing REG systems is that they rely on entityspecific supervised training, which means that they cannot handle entities not seen during training. In this study, we address this in two ways. First, we propose task setups in which we specifically test a REG system's ability to generalize to entities not seen during training. Second, we propose a profile-based deep neural network model, PROFILEREG, which encodes both the local context and an external profile of the entity to generate reference realizations. Our model generates tokens by learning to choose between generating pronouns, generating from a fixed vocabulary, or copying a word from the profile. We evaluate our model on three different splits of theWebNLG dataset, and show that it outperforms competitive baselines in all settings according to automatic and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3163,3172,,,,,,,,,,,,,,,,WOS:000854193303030,0
C,"Charoenphakdee, N; Lee, J; Jin, YP; Wanvarie, D; Sugiyama, M",,,Assoc Computat Linguist,"Charoenphakdee, Nontawat; Lee, Jongyeong; Jin, Yiping; Wanvarie, Dittaya; Sugiyama, Masashi",,,Learning Only from Relevant Keywords and Unlabeled Documents,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We consider a document classification problem where document labels are absent but only relevant keywords of a target class and unlabeled documents are given. Although heuristic methods based on pseudo-labeling have been considered, theoretical understanding of this problem has still been limited. Moreover, previous methods cannot easily incorporate welldeveloped techniques in supervised text classification. In this paper, we propose a theoretically guaranteed learning framework that is simple to implement and has flexible choices of models, e.g., linear models or neural networks. We demonstrate how to optimize the area under the receiver operating characteristic curve (AUC) effectively and also discuss how to adjust it to optimize other well-known evaluation metrics such as the accuracy and F-1-measure. Finally, we show the effectiveness of our framework using benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3993,4002,,,,,,,,,,,,,,,,WOS:000854193304016,0
C,"Durmus, E; Ladhak, F; Cardie, C",,,Assoc Computat Linguist,"Durmus, Esin; Ladhak, Faisal; Cardie, Claire",,,The Role of Pragmatic and Discourse Context in Determining Argument Impact,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Research in the social sciences and psychology has shown that the persuasiveness of an argument depends not only the language employed, but also on attributes of the source/communicator, the audience, and the appropriateness and strength of the argument's claims given the pragmatic and discourse context of the argument. Among these characteristics of persuasive arguments, prior work in NLP does not explicitly investigate the effect of the pragmatic and discourse context when determining argument quality. This paper presents a new dataset to initiate the study of this aspect of argumentation: it consists of a diverse collection of arguments covering 741 controversial topics and comprising over 47,000 claims. We further propose predictive models that incorporate the pragmatic and discourse context of argumentative claims and show that they outperform models that rely only on claim-specific linguistic features for predicting the perceived impact of individual claims within a particular line of argument.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5668,5678,,,,,,,,,,,,,,,,WOS:000854193305081,0
C,"Fan, M; Feng, C; Sun, MM; Li, P",,,Assoc Computat Linguist,"Fan, Miao; Feng, Chao; Sun, Mingming; Li, Ping",,,Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"To automatically assess the helpfulness of a customer review online, conventional approaches generally acquire various linguistic and neural embedding features solely from the textual content of the review itself as the evidence. We, however, find out that a helpful review is largely concerned with the metadata (such as the name, the brand, the category, etc.) of its target product. It leaves us with a challenge of how to choose the correct key-value product metadata to help appraise the helpfulness of free-text reviews more precisely. To address this problem, we propose a novel framework composed of two mutual-benefit modules. Given a product, a selector (agent) learns from both the keys in the product metadata and one of its reviews to take an action that selects the correct value, and a successive predictor (network) makes the free-text review attend to this value to obtain better neural representations for helpfulness assessment. The predictor is directly optimized by SGD with the loss of helpfulness prediction, and the selector could be updated via policy gradient rewarded with the performance of the predictor. We use two real-world datasets from Amazon.com and Yelp.com, respectively, to compare the performance of our framework with other mainstream methods under two application scenarios: helpfulness identification and regression of customer reviews. Extensive results demonstrate that our framework can achieve state-of-the-art performance with substantial improvements.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1675,1683,,,,,,,,,,,,,,,,WOS:000854193301099,0
C,"Hao, J; Wang, X; Shi, SM; Zhang, JF; Tu, ZP",,,Assoc Computat Linguist,"Hao, Jie; Wang, Xing; Shi, Shuming; Zhang, Jinfeng; Tu, Zhaopeng",,,Multi-Granularity Self-Attention for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current state-of-the-art neural machine translation (NMT) uses a deep multi-head self-attention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced substantial improvements, suggesting the possibility of improving NMT performance from explicit modeling of phrases. In this work, we present multi-granularity self-attention (MG-SA): a neural network that combines multi-head self-attention and phrase modeling. Specifically, we train several attention heads to attend to phrases in either n-gram or syntactic formalism. Moreover, we exploit interactions among phrases to enhance the strength of structure modeling - a commonly-cited weakness of self-attention. Experimental results on WMT14 English-to-German and NIST Chinese-to-English translation tasks show the proposed approach consistently improves performance. Targeted linguistic analysis reveals that MG-SA indeed captures useful phrase information at various levels of granularities.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,887,897,,,,,,,,,,,,,,,,WOS:000854193301004,0
C,"He, JL; Madhyastha, P; Specia, L",,,Assoc Computat Linguist,"He, Julia; Madhyastha, Pranava; Specia, Lucia",,,Deep copycat Networks for Text-to-Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most text-to-text generation tasks, for example text summarisation and text simplification, require copying words from the input to the output. We introduce copycat, a transformerbased pointer network for such tasks which obtains competitive results in abstractive text summarisation and generates more abstractive summaries. We propose a further extension of this architecture for automatic post-editing, where generation is conditioned over two inputs (source language and machine translation), and the model is capable of deciding where to copy information from. This approach achieves competitive performance when compared to state-of-the-art automated post-editing systems. More importantly, we show that it addresses a well-known limitation of automatic post-editing - overcorrecting translations - and that our novel mechanism for copying source language words improves the results.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3227,3236,,,,,,,,,,,,,,,,WOS:000854193303036,0
C,"Kang, D; Gangal, V; Hovy, E",,,Assoc Computat Linguist,"Kang, Dongyeop; Gangal, Varun; Hovy, Eduard",,,"(Male, Bachelor) and (Female, Ph.D) have different connotations: Parallelly Annotated Stylistic Language Dataset with Multiple Personas",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Stylistic variation in text needs to be studied with different aspects including the writer's personal traits, interpersonal relations, rhetoric, and more. Despite recent attempts on computational modeling of the variation, the lack of parallel corpora of style language makes it difficult to systematically control the stylistic change as well as evaluate such models. We release PASTEL, the parallel and annotated stylistic language dataset, that contains similar to 41K parallel sentences (8:3K parallel stories) annotated across different personas. Each persona has different styles in conjunction: gender, age, country, political view, education, ethnic, and time-of-writing. The dataset is collected from human annotators with solid control of input denotation: not only preserving original meaning between text, but promoting stylistic diversity to annotators. We test the dataset on two interesting applications of style language, where PASTEL helps design appropriate experiment and evaluation. First, in predicting a target style (e.g., male or female in gender) given a text, multiple styles of PASTEL make other external style variables controlled (or fixed), which is a more accurate experimental design. Second, a simple supervised model with our parallel text outperforms the unsupervised models using nonparallel text in style transfer. Our dataset is publicly available(1).",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1696,1706,,,,,,,,,,,,,,,,WOS:000854193301101,0
C,"Kudugunta, S; Bapna, A; Caswell, I; Firat, O",,,Assoc Computat Linguist,"Kudugunta, Sneha; Bapna, Ankur; Caswell, Isaac; Firat, Orhan",,,Investigating Multilingual NMT Representations at Scale,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major conclusions from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1565,1575,,,,,,,,,,,,,,,,WOS:000854193301089,0
C,"Li, GL; Liu, LM; Huang, GP; Zhu, CH; Zhao, TJ; Shi, SM",,,Assoc Computat Linguist,"Li, Guanlin; Liu, Lemao; Huang, Guoping; Zhu, Conghui; Zhao, Tiejun; Shi, Shuming",,,Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many Data Augmentation (DA) methods have been proposed for neural machine translation. Existing works measure the superiority of DA methods in terms of their performance on a specific test set, but we find that some DA methods do not exhibit consistent improvements across translation tasks. Based on the observation, this paper makes an initial attempt to answer a fundamental question: what benefits, which are consistent across different methods and tasks, does DA in general obtain? Inspired by recent theoretic advances in deep learning, the paper understands DA from two perspectives towards the generalization ability of a model: input sensitivity and prediction margin, which are defined independent of specific test set thereby may lead to findings with relatively low variance. Extensive experiments show that relatively consistent benefits across five DA methods and four translation tasks are achieved regarding both perspectives.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5689,5695,,,,,,,,,,,,,,,,WOS:000854193305083,0
C,"Li, YP; Zhao, L; Wang, JY; Hestness, J",,,Assoc Computat Linguist,"Li, Yuanpeng; Zhao, Liang; Wang, Jianyu; Hestness, Joel",,,Compositional Generalization for Primitive Substitutions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Compositional generalization is a basic mechanism in human language learning, but current neural networks lack such ability. In this paper, we conduct fundamental research for encoding compositionality in neural networks. Conventional methods use a single representation for the input sentence, making it hard to apply prior knowledge of compositionality. In contrast, our approach leverages such knowledge with two representations, one generating attention maps, and the other mapping attended input words to output symbols. We reduce the entropy in each representation to improve generalization. Our experiments demonstrate significant improvements over the conventional methods in five NLP tasks including instruction learning and machine translation. In the SCAN domain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0% to 99.7% in TurnLeft task. It also beats human performance on a few-shot learning task. We hope the proposed approach can help ease future research towards human-level compositional language learning. Source code is available online(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4293,4302,,,,,,,,,,,,,,,,WOS:000854193304043,0
C,"Liu, QY; Guan, WY; Li, SJ; Kawahara, D",,,Assoc Computat Linguist,"Liu, Qianying; Guan, Wenyu; Li, Sujian; Kawahara, Daisuke",,,Tree-structured Decoding for Solving MathWord Problems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatically solving math word problems is an interesting research topic that needs to bridge natural language descriptions and formal math equations. Previous studies introduced end-to-end neural network methods, but these approaches did not efficiently consider an important characteristic of the equation, i.e., an abstract syntax tree. To address this problem, we propose a tree-structured decoding method that generates the abstract syntax tree of the equation in a top-down manner. In addition, our approach can automatically stop during decoding without a redundant stop token. The experimental results show that our method achieves single model state-of-the-art performance on Math23K, which is the largest dataset on this task.",,,,,"liu, qian/HDM-2936-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2370,2379,,,,,,,,,,,,,,,,WOS:000854193302049,0
C,"Malmi, E; Krause, S; Rothe, S; Mirylenka, D; Severyn, A",,,Assoc Computat Linguist,"Malmi, Eric; Krause, Sebastian; Rothe, Sascha; Mirylenka, Daniil; Severyn, Aliaksei",,,"Encode, Tag, Realize: High-Precision Text Editing",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose LASERTAGGER-a sequence tagging approach that casts text generation as a text editing task. Target texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. To predict the edit operations, we propose a novel model, which combines a BERT encoder with an autoregressive Transformer decoder. This approach is evaluated on English text on four tasks: sentence fusion, sentence splitting, abstractive summarization, and grammar correction. LASERTAGGER achieves new state-of-the-art results on three of these tasks, performs comparably to a set of strong seq2seq baselines with a large number of training examples, and outperforms them when the number of examples is limited. Furthermore, we show that at inference time tagging can be more than two orders of magnitude faster than comparable seq2seq models, making it more attractive for running in a live environment.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5054,5065,,,,,,,,,,,,,,,,WOS:000854193305023,0
C,"Mohtarami, M; Glass, J; Nakov, P",,,Assoc Computat Linguist,"Mohtarami, Mitra; Glass, James; Nakov, Preslav",,,Contrastive Language Adaptation for Cross-Lingual Stance Detection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective (or stance) of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4442,4452,,,,,,,,,,,,,,,,WOS:000854193304057,0
C,"Perkins, H; Yang, Y",,,Assoc Computat Linguist,"Perkins, Hugh; Yang, Yi",,,Dialog Intent Induction with Deep Multi-View Clustering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce the dialog intent induction task and present a novel deep multi-view clustering approach to tackle the problem. Dialog intent induction aims at discovering user intents from user query utterances in humanhuman conversations such as dialogs between customer support agents and customers.(1) Motivated by the intuition that a dialog intent is not only expressed in the user query utterance but also captured in the rest of the dialog, we split a conversation into two independent views and exploit multi-view clustering techniques for inducing the dialog intent. In particular, we propose alternating-view k-means ( AV-KMEANS) for joint multi-view representation learning and clustering analysis. The key innovation is that the instance-view representations are updated iteratively by predicting the cluster assignment obtained from the alternative view, so that the multi-view representations of the instances lead to similar cluster assignments. Experiments on two public datasets show that AV-KMEANS can induce better dialog intent clusters than stateof-the-art unsupervised representation learning methods and standard multi-view clustering approaches.(2)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4016,4025,,,,,,,,,,,,,,,,WOS:000854193304018,0
C,"Song, KS; Bing, LD; Gao, W; Lin, J; Zhao, LJ; Wang, JC; Sun, CL; Liu, XZ; Zhang, Q",,,Assoc Computat Linguist,"Song, Kaisong; Bing, Lidong; Gao, Wei; Lin, Jun; Zhao, Lujun; Wang, Jiancheng; Sun, Changlong; Liu, Xiaozhong; Zhang, Qiong",,,Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Customers ask questions and customer service staffs answer their questions, which is the basic service model via multi-turn customer service (CS) dialogues on E-commerce platforms. Existing studies fail to provide comprehensive service satisfaction analysis, namely satisfaction polarity classification (e.g., well satisfied, met and unsatisfied) and sentimental utterance identification (e.g., positive, neutral and negative). In this paper, we conduct a pilot study on the task of service satisfaction analysis (SSA) based on multi-turn CS dialogues. We propose an extensible Context-Assisted Multiple Instance Learning (CAMIL) model to predict the sentiments of all the customer utterances and then aggregate those sentiments into service satisfaction polarity. After that, we propose a novel Context Clue Matching Mechanism (CCMM) to enhance the representations of all customer utterances with their matched context clues, i.e., sentiment and reasoning clues. We construct two CS dialogue datasets from a top E-commerce platform. Extensive experimental results are presented and contrasted against a few previous models to demonstrate the efficacy of our model.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,198,207,,,,,,,,,,,,,,,,WOS:000854193300019,0
C,"Wallace, E; Wang, YZ; Li, SJ; Singh, S; Gardner, M",,,Assoc Computat Linguist,"Wallace, Eric; Wang, Yizhong; Li, Sujian; Singh, Sameer; Gardner, Matt",,,Do NLP Models Know Numbers? Probing Numeracy in Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens-they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise-ELMo captures numeracy the best for all pre-trained methods-but BERT, which uses sub-word units, is less exact.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5307,5315,,,,,,,,,,,,,,,,WOS:000854193305047,0
C,"Wang, L; Zhao, W; Jia, RY; Li, SJ; Liu, JM",,,Assoc Computat Linguist,"Wang, Liang; Zhao, Wei; Jia, Ruoyu; Li, Sujian; Liu, Jingming",,,Denoising based Sequence-to-Sequence Pre-training for Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pretrains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4003,4015,,,,,,,,,,,,,,,,WOS:000854193304017,0
C,"Wang, LY; Tu, ZP; Wang, X; Shi, SM",,,Assoc Computat Linguist,"Wang, Longyue; Tu, Zhaopeng; Wang, Xing; Shi, Shuming",,,One Model to Learn Both: Zero Pronoun Prediction and Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Zero pronouns (ZPs) are frequently omitted in pro-drop languages, but should be recalled in non-pro-drop languages. This discourse phenomenon poses a significant challenge for machine translation (MT) when translating texts from pro-drop to non-pro-drop languages. In this paper, we propose a unified and discourse-aware ZP translation approach for neural MT models. Specifically, we jointly learn to predict and translate ZPs in an end-to-end manner, allowing both components to interact with each other. In addition, we employ hierarchical neural networks to exploit discourse-level context, which is beneficial for ZP prediction and thus translation Experimental results on both Chinese)English and Japanese)English data show that our approach significantly and accumulatively improves both translation performance and ZP prediction accuracy over not only baseline but also previous works using external ZP prediction models. Extensive analyses confirm that the performance improvement comes from the alleviation of different kinds of errors especially caused by subjective ZPs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,921,930,,,,,,,,,,,,,,,,WOS:000854193301007,0
C,"Wang, YN; Zhang, JJ; Zhou, L; Liu, YC; Zong, CQ",,,Assoc Computat Linguist,"Wang, Yining; Zhang, Jiajun; Zhou, Long; Liu, Yuchen; Zong, Chengqing",,,Synchronously Generating Two Languages with Interactive Decoding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we introduce a novel interactive approach to translate a source language into two different languages simultaneously and interactively. Specifically, the generation of one language relies on not only previously generated outputs by itself, but also the outputs predicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3350,3355,,,,,,,,,,,,,,,,WOS:000854193303048,0
C,"Xu, JJ; Zhao, L; Yan, HQ; Zeng, Q; Liang, Y; Sun, X",,,Assoc Computat Linguist,"Xu, Jingjing; Zhao, Liang; Yan, Hanqi; Zeng, Qi; Liang, Yun; Sun, Xu",,,LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent work has shown that current sentiment classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current sentiment classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or supersuperior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.(1)",,,,,"Zeng, Qi/GLT-7682-2022; yan, hanqi/GQQ-5461-2022","Zeng, Qi/0000-0002-3696-1538; ",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5518,5527,,,,,,,,,,,,,,,,WOS:000854193305067,0
C,"Yan, HR; Jin, XL; Meng, XB; Guo, JF; Cheng, XQ",,,Assoc Computat Linguist,"Yan, Haoran; Jin, Xiaolong; Meng, Xiangbin; Guo, Jiafeng; Cheng, Xueqi",,,Event Detection with Multi-Order Graph Convolution and Aggregated Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Syntactic relations are broadly used in many NLP tasks. For event detection, syntactic relation representations based on dependency tree can better capture the interrelations between candidate trigger words and related entities than sentence representations. But, existing studies only use first-order syntactic relations (i.e., the arcs) in dependency trees to identify trigger words. For this reason, this paper proposes a new method for event detection, which uses a dependency tree based graph convolution network with aggregative attention to explicitly model and aggregate multi-order syntactic representations in sentences. Experimental comparison with state-of-the-art baselines shows the superiority of the proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5766,5770,,,,,,,,,,,,,,,,WOS:000854193305095,0
C,"Yang, WW; Boyd-Graber, J; Resnik, P",,,Assoc Computat Linguist,"Yang, Weiwei; Boyd-Graber, Jordan; Resnik, Philip",,,A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multilingual topic models (MTMs) learn topics on documents in multiple languages. Past models align topics across languages by implicitly assuming the documents in different languages are highly comparable, often a false assumption. We introduce a new model that does not rely on this assumption, particularly useful in important low-resource language scenarios. Our MTM learns weighted topic links and connects cross-lingual topics only when the dominant words defining them are similar, outperforming LDA and previous MTMs in classification tasks using documents' topic posteriors as features. It also learns coherent topics on documents with low comparability.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1243,1248,,,,,,,,,,,,,,,,WOS:000854193301042,0
C,"Yeh, YT; Chen, YN",,,Assoc Computat Linguist,"Yeh, Yi-Ting; Chen, Yun-Nung",,,QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Standard accuracy metrics indicate that modern reading comprehension systems have achieved strong performance in many question answering datasets. However, the extent these systems truly understand language remains unknown, and existing systems are not good at distinguishing distractor sentences, which look related but do not actually answer the question. To address this problem, we propose QAInfomax as a regularizer in reading comprehension systems by maximizing mutual information among passages, a question, and its answer. QAInfomax helps regularize the model to not simply learn the superficial correlation for answering questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3370,3375,,,,,,,,,,,,,,,,WOS:000854193303051,0
C,"Bao, ZY; Huang, R; Li, C; Zhu, K",,,Assoc Computat Linguist,"Bao, Zuyi; Huang, Rui; Li, Chen; Zhu, Kenny Q.",,,Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous work on cross-lingual sequence labeling tasks either requires parallel data or bridges the two languages through word-byword matching. Such requirements and assumptions are infeasible for most languages, especially for languages with large linguistic distances, e.g., English and Chinese. In this work, we propose a Multilingual Language Model with deep semantic Alignment (MLMA) to generate language-independent representations for cross-lingual sequence labeling. Our methods require only monolingual corpora with no bilingual resources at all and take advantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1028,1039,,,,,,,,,,,,,,,,WOS:000854193301017,0
C,"Bordes, P; Zablocki, E; Soulier, L; Piwowarski, B; Gallinari, P",,,Assoc Computat Linguist,"Bordes, Patrick; Zablocki, Eloi; Soulier, Laure; Piwowarski, Benjamin; Gallinari, Patrick",,,Incorporating Visual Semantics into Sentence Representations within a Grounded Space,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Language grounding is an active field aiming at enriching textual representations with visual information. Generally, textual and visual elements are embedded in the same representation space, which implicitly assumes a one-to-one correspondence between modalities. This hypothesis does not hold when representing words, and becomes problematic when used to learn sentence representations - the focus of this paper - as a visual scene can be described by a wide variety of sentences. To overcome this limitation, we propose to transfer visual information to textual representations by learning an intermediate representation space: the grounded space. We further propose two new complementary objectives ensuring that (1) sentences associated with the same visual content are close in the grounded space and (2) similarities between related elements are preserved across modalities. We show that this model outperforms the previous state-of-the-art on classification and semantic relatedness tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,696,707,,,,,,,,,,,,,,,,WOS:000854193300064,0
C,"Chakrabarty, T; Hidey, C; Muresan, S; Mckeown, K; Hwang, A",,,Assoc Computat Linguist,"Chakrabarty, Tuhin; Hidey, Christopher; Muresan, Smaranda; Mckeown, Kathleen; Hwang, Alyssa",,,AMPERSAND: Argument Mining for PERSuAsive oNline Discussions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Argumentation is a type of discourse where speakers try to persuade their audience about the reasonableness of a claim by presenting supportive arguments. Most work in argument mining has focused on modeling arguments in monologues. We propose a computational model for argument mining in online persuasive discussion forums that brings together the micro-level (argument as product) and macro-level (argument as process) models of argumentation. Fundamentally, this approach relies on identifying relations between components of arguments in a discussion thread. Our approach for relation prediction uses contextual information in terms of fine-tuning a pre-trained language model and leveraging discourse relations based on Rhetorical Structure Theory. We additionally propose a candidate selection method to automatically predict what parts of one's argument will be targeted by other participants in the discussion. Our models obtain significant improvements compared to recent state-of-the-art approaches using pointer networks and a pre-trained language model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2933,2943,,,,,,,,,,,,,,,,WOS:000854193303009,0
C,"Dai, ZY; Huang, RH",,,Assoc Computat Linguist,"Dai, Zeyu; Huang, Ruihong",,,A Regularization Approach for Incorporating Event Knowledge and Coreference Relations into Neural Discourse Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We argue that external commonsense knowledge and linguistic constraints need to be incorporated into neural network models for mitigating data sparsity issues and further improving the performance of discourse parsing. Realizing that external knowledge and linguistic constraints may not always apply in understanding a particular context, we propose a regularization approach that tightly integrates these constraints with contexts for deriving word representations. Meanwhile, it balances attentions over contexts and constraints through adding a regularization term into the objective function. Experiments show that our knowledge regularization approach outperforms all previous systems on the benchmark dataset PDTB for discourse parsing.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2976,2987,,,,,,,,,,,,,,,,WOS:000854193303013,0
C,"Ferreira, TC; van der Lee, C; van Miltenburg, E; Krahmer, E",,,Assoc Computat Linguist,"Ferreira, Thiago Castro; van der Lee, Chris; van Miltenburg, Emiel; Krahmer, Emiel",,,Neural data-to-text generation: A comparison between pipeline and end-to-end architectures,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. By contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer, two state-of-the art deep learning methods. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.(1)",,,,,,"van Miltenburg, Emiel/0000-0002-7143-8961",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,552,562,,,,,,,,,,,,,,,,WOS:000854193300052,0
C,"Hu, MH; Peng, YX; Huang, Z; Li, DS",,,Assoc Computat Linguist,"Hu, Minghao; Peng, Yuxing; Huang, Zhen; Li, Dongsheng",,,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Rapid progress has been made in the field of reading comprehension and question answering, where several systems have achieved human parity in some simplified settings. However, the performance of these models degrades significantly when they are applied to more realistic scenarios, where answers are involved with various types, multiple text strings are correct answers, or discrete reasoning abilities are required. In this paper, we introduce the Multi-Type Multi-Span Network (MTMSN), a neural reading comprehension model that combines a multi-type answer predictor designed to support various answer types (e.g., span, count, negation, and arithmetic expression) with a multi-span extraction method for dynamically producing one or multiple text spans. In addition, an arithmetic expression reranking mechanism is proposed to rank expression candidates for further confirming the prediction. Experiments show that our model achieves 79.9 F1 on the DROP hidden test set, creating new state-of-the-art results. Source code(1) is released to facilitate future work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1596,1606,,,,,,,,,,,,,,,,WOS:000854193301092,0
C,"Jhamtani, H; Mehta, SV; Carbonell, J; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Jhamtani, Harsh; Mehta, Sanket Vaibhav; Carbonell, Jaime; Berg-Kirkpatrick, Taylor",,,Learning Rhyming Constraints using Structured Adversaries,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing recurrent neural language models often fail to capture higher-level structure present in text: for example, rhyming patterns present in poetry. Much prior work on poetry generation uses manually defined constraints which are satisfied during decoding using either specialized decoding procedures or rejection sampling. The rhyming constraints themselves are typically not learned by the generator. We propose an alternate approach that uses a structured discriminator to learn a poetry generator that directly captures rhyming constraints in a generative adversarial setup. By causing the discriminator to compare poems based only on a learned similarity matrix of pairs of line ending words, the proposed approach is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6025,6031,,,,,,,,,,,,,,,,WOS:000854193306032,0
C,"Khot, T; Sabharwal, A; Clark, P",,,Assoc Computat Linguist,"Khot, Tushar; Sabharwal, Ashish; Clark, Peter",,,What's Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-hop textual question answering requires combining information from multiple sentences. We focus on a natural setting where, unlike typical reading comprehension, only partial information is provided with each question. The model must retrieve and use additional knowledge to correctly answer the question. To tackle this challenge, we develop a novel approach that explicitly identifies the knowledge gap between a key span in the provided knowledge and the answer choices. The model, GapQA, learns to fill this gap by determining the relationship between the span and an answer choice, based on retrieved knowledge targeting this gap. We propose jointly training a model to simultaneously fill this knowledge gap and compose it with the provided partial knowledge. On the Open-BookQA dataset, given partial knowledge, explicitly identifying what's missing substantially outperforms previous approaches.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2814,2828,,,,,,,,,,,,,,,,WOS:000854193302089,0
C,"Lee, J; Cho, K; Kiela, D",,,Assoc Computat Linguist,"Lee, Jason; Cho, Kyunghyun; Kiela, Douwe",,,Countering Language Drift via Visual Grounding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a nonlinguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4385,4395,,,,,,,,,,,,,,,,WOS:000854193304052,0
C,"Luo, FL; Li, SY; Yang, PC; Li, L; Chang, BB; Sui, ZF; Sun, X",,,Assoc Computat Linguist,"Luo, Fuli; Li, Shunyao; Yang, Pengcheng; Li, Lei; Chang, Baobao; Sui, Zhifang; Sun, Xu",,,Pun-GAN: Generative Adversarial Network for Pun Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of largescale pun corpus to guide the supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN), which does not require any pun corpus. It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3388,3393,,,,,,,,,,,,,,,,WOS:000854193303054,0
C,"Meng, T; Peng, NY; Chang, KW",,,Assoc Computat Linguist,"Meng, Tao; Peng, Nanyun; Chang, Kai-Wei",,,Target Language-Aware Constrained Inference for Cross-lingual Dependency Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Prior work on cross-lingual dependency parsing often focuses on capturing the commonalities between source and target languages and overlooks the potential of leveraging linguistic properties of the languages to facilitate the transfer. In this paper, we show that weak supervisions of linguistic knowledge for the target languages can improve a cross-lingual graph-based dependency parser substantially. Specifically, we explore several types of corpus linguistic statistics and compile them into corpus-wise constraints to guide the inference process during the test time. We adapt two techniques, Lagrangian relaxation and posterior regularization, to conduct inference with corpus-statistics constraints. Experiments show that the Lagrangian relaxation and posterior regularization inference improve the performances on 15 and 17 out of 19 target languages, respectively. The improvements are especially significant for target languages that have different word order features from the source language.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1117,1128,,,,,,,,,,,,,,,,WOS:000854193301025,0
C,"Pasupae, P; Gupta, S; Mandyam, K; Shah, R; Lewis, M; Zettlemoyer, L",,,Assoc Computat Linguist,"Pasupae, Panupong; Gupta, Sonal; Mandyam, Karishma; Shah, Rushin; Lewis, Mike; Zettlemoyer, Luke",,,Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a semantic parser for parsing compositional utterances into Task Oriented Parse (TOP), a tree representation that has intents and slots as labels of nesting tree nodes. Our parser is span-based: it scores labels of the tree nodes covering each token span independently, but then decodes a valid tree globally. In contrast to previous sequence decoding approaches and other span-based parsers, we (1) improve the training speed by removing the need to run the decoder at training time; and (2) introduce edge scores, which model relations between parent and child labels, to mitigate the independence assumption between node labels and improve accuracy. Our best parser outperforms previous methods on the TOP dataset of mixed-domain task-oriented utterances in both accuracy and training speed.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1520,1526,,,,,,,,,,,,,,,,WOS:000854193301085,0
C,"Roy, A; Park, Y; Lee, T; Pan, SM",,,Assoc Computat Linguist,"Roy, Arpita; Park, Youngja; Lee, Taesung; Pan, Shimei",,,Supervising Unsupervised Open Information Extraction Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a novel supervised open information extraction (Open IE) framework that leverages an ensemble of unsupervised Open IE systems and a small amount of labeled data to improve system performance. It uses the outputs of multiple unsupervised Open IE systems plus a diverse set of lexical and syntactic information such as word embedding, part-of-speech embedding, syntactic role embedding and dependency structure as its input features and produces a sequence of word labels indicating whether the word belongs to a relation, the arguments of the relation or irrelevant. Comparing with existing supervised Open IE systems, our approach leverages the knowledge in existing unsupervised Open IE systems to overcome the problem of insufficient training data. By employing multiple unsupervised Open IE systems, our system learns to combine the strength and avoid the weakness in each individual Open IE system. We have conducted experiments on multiple labeled benchmark data sets. Our evaluation results have demonstrated the superiority of the proposed method over existing supervised and unsupervised models by a significant margin.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,728,737,,,,,,,,,,,,,,,,WOS:000854193300067,0
C,"Stengel-Estrin, E; Su, TR; Post, M; Van Durme, B",,,Assoc Computat Linguist,"Stengel-Estrin, Elias; Su, Tzu-Ray; Post, Matt; Van Durme, Benjamin",,,A Discriminative Neural Model for Cross-Lingual Word Alignment,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a novel discriminative word alignment model, which we integrate into a Transformer-based machine translation model. In experiments based on a small number of labeled examples (similar to 1.7K-5K sentences) we evaluate its performance intrinsically on both English-Chinese and English-Arabic alignment, where we achieve major improvements over unsupervised baselines (11-27 F1). We evaluate the model extrinsically on data projection for Chinese NER, showing that our alignments lead to higher performance when used to project NER tags from English to Chinese. Finally, we perform an ablation analysis and an annotation experiment that jointly support the utility and feasibility of future manual alignment elicitation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,910,920,,,,,,,,,,,,,,,,WOS:000854193301006,0
C,"Wang, WB; Gao, Y; Huang, HY; Zhou, YX",,,Assoc Computat Linguist,Wang Wenbo; Gao Yang; Huang Heyan; Zhou Yuxiang,,,Concept Pointer Network for Abstractive Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-tosequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distantly-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC2004 and Gigaword datasets. A human evaluation of the model's abstractive abilities also supports the quality of the summaries produced within this framework.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3076,3085,,,,,,,,,,,,,,,,WOS:000854193303022,0
C,"Yoo, KM; Kim, T; Lee, SG",,,Assoc Computat Linguist,"Yoo, Kang Min; Kim, Taeuk; Lee, Sang-goo",,,Don't Just Scratch the Surface: EnhancingWord Representations for Korean with Hanja,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a simple yet effective approach for improving Korean word representations using additional linguistic annotation (i.e. Hanja). We employ cross-lingual transfer learning in training word representations by leveraging the fact that Hanja is closely related to Chinese. We evaluate the intrinsic quality of representations learned through our approach using the word analogy and similarity tests. In addition, we demonstrate their effectiveness on several downstream tasks, including a novel Korean news headline generation task.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3528,3533,,,,,,,,,,,,,,,,WOS:000854193303076,0
C,"Zhang, S; Ma, XT; Duh, K; Van Durme, B",,,Assoc Computat Linguist,"Zhang, Sheng; Ma, Xutai; Duh, Kevin; Van Durme, Benjamin",,,Broad-Coverage Semantic Parsing as Transduction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We unify different broad-coverage semantic parsing tasks under a transduction paradigm, and propose an attention-based neural framework that incrementally builds a meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the transducer can be effectively trained without relying on a pre-trained aligner. Experiments conducted on three separate broadcoverage semantic parsing tasks - AMR, SDP and UCCA - demonstrate that our attentionbased neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3786,3798,,,,,,,,,,,,,,,,WOS:000854193303110,0
C,"Zhang, SY; Bansal, M",,,Assoc Computat Linguist,"Zhang, Shiyue; Bansal, Mohit",,,Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a semantic drift problem, i.e., the semantics of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model's ability to mimic human annotators in generating QA training data. Experiments show that our method achieves the new state-of-theart performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable semi-supervised QA. We propose two ways to generate synthetic QA pairs: generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2495,2509,,,,,,,,,,,,,,,,WOS:000854193302061,0
C,"Zhang, YF; Martino, GD; Barron-Cedeno, A; Romeo, S; An, JS; Kwak, H; Staykovski, T; Jaradat, I; Karadzhov, G; Baly, R; Darwish, K; Glass, J; Nakov, P",,,ASSOC COMPUTAT LINGUIST,"Zhang, Yifan; Martino, Giovanni Da San; Barron-Cedeno, Alberto; Romeo, Salvatore; An, Jisun; Kwak, Haewoon; Staykovski, Todor; Jaradat, Israa; Karadzhov, Georgi; Baly, Ramy; Darwish, Kareem; Glass, James; Nakov, Preslav",,,Tanbih: Get To Know What You Are Reading,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce Tanbih, a news aggregator with intelligent analysis tools to help readers understanding what is behind a news story. Our system displays news grouped into events and generates media profiles that show the general factuality of reporting, the degree of propagandistic content, hyper-partisanship, leading political ideology, general frame of reporting, and stance with respect to various claims and topics of a news outlet. In addition, we automatically analyze each article in order to detect whether it is propagandistic and to determine its stance with respect to a number of controversial topics.",,,,,,"Kwak, Haewoon/0000-0003-1418-0834",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,223,228,,,,,,,,,,,,,,,,WOS:000855231500038,0
C,"Zhu, CG; Zeng, M; Huang, XD",,,Assoc Computat Linguist,"Zhu, Chenguang; Zeng, Michael; Huang, Xuedong",,,Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In task-oriented dialogues, Natural Language Generation (NLG) is the final and crucial step to produce user-facing system utterances. The result of NLG is directly related to the perceived quality and usability of a dialogue system. While most existing systems provide semantically correct responses given goals to present, they struggle to match the variation and fluency in the human language. In this paper, we propose a novel multi-task learning framework, NLG-LM, for natural language generation. In addition to generating highquality responses conveying the required information, it also explicitly targets for naturalness in generated responses via an unconditioned language model. This can significantly improve the learning of style and variation in human language. Empirical results show that this multi-task learning framework outperforms previous models across multiple datasets. For example, it improves the previous best BLEU score on the E2E-NLG dataset by 2.2%, and on the Laptop dataset by 6.1%.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1261,1266,,,,,,,,,,,,,,,,WOS:000854193301045,0
C,"Zhu, JN; Wang, Q; Wang, YN; Zhou, Y; Zhang, JJ; Wang, SN; Zong, CQ",,,Assoc Computat Linguist,"Zhu, Junnan; Wang, Qian; Wang, Yining; Zhou, Yu; Zhang, Jiajun; Wang, Shaonan; Zong, Chengqing",,,NCLS: Neural Cross-Lingual Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Cross-lingual summarization (CLS) is the task to produce a summary in one particular language for a source document in a different language. Existing methods simply divide this task into two steps: summarization and translation, leading to the problem of error propagation. To handle that, we present an end-to-end CLS framework, which we refer to as Neural Cross-Lingual Summarization (NCLS), for the first time. Moreover, we propose to further improve NCLS by incorporating two related tasks, monolingual summarization and machine translation, into the training process of CLS under multi-task learning. Due to the lack of supervised CLS data, we propose a round-trip translation strategy to acquire two high-quality large-scale CLS datasets based on existing monolingual summarization datasets. Experimental results have shown that our NCLS achieves remarkable improvement over traditional pipeline methods on both English-to-Chinese and Chinese-toEnglish CLS human-corrected test sets. In addition, NCLS with multi-task learning can further significantly improve the quality of generated summaries. We make our dataset and code publicly available here: http://www. nlpr.ia.ac.cn/cip/dataset.htm.",,,,,,"Zhu, Junnan/0000-0002-9856-2946",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3054,3064,,,,,,,,,,,,,,,,WOS:000854193303020,0
C,"Bian, SQ; Zhao, WNX; Song, Y; Zhang, T; Wen, JR",,,Assoc Computat Linguist,"Bian, Shuqing; Zhao, Wayne Xin; Song, Yang; Zhang, Tao; Wen, Ji-Rong",,,Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Person-job fit has been an important task which aims to automatically match job positions with suitable candidates. Previous methods mainly focus on solving the match task in single-domain setting, which may not work well when labeled data is limited. We study the domain adaptation problem for person-job fit. We first propose a deep global match network for capturing the global semantic interactions between two sentences from a job posting and a candidate resume respectively. Furthermore, we extend the match network and implement domain adaptation in three levels, i.e., sentence-level representation, sentence-level match, and global match. Extensive experiment results on a large real-world dataset consisting of six domains have demonstrated the effectiveness of the proposed model, especially when there is not sufficient labeled data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4810,4820,,,,,,,,,,,,,,,,WOS:000854193304091,0
C,"Cai, D; Wang, Y; Bi, W; Tut, ZP; Liu, XJ; Shi, SM",,,Assoc Computat Linguist,"Cai, Deng; Wang, Yan; Bi, Wei; Tut, Zhaopeng; Liu, Xiaojiang; Shi, Shuming",,,Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the safe response problem. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1866,1875,,,,,,,,,,,,,,,,WOS:000854193302003,0
C,"Czarnowska, P; Ruder, S; Grave, E; Cotterell, R; Copestake, A",,,Assoc Computat Linguist,"Czarnowska, Paula; Ruder, Sebastian; Grave, Edouard; Cotterell, Ryan; Copestake, Ann",,,Don't Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Human translators routinely have to translate rare inflections of words-due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as hablar ' amos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages1 and evaluate three of the state-of-the-art models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,974,983,,,,,,,,,,,,,,,,WOS:000854193301012,0
C,"Deutsch, D; Roth, D",,,Assoc Computat Linguist,"Deutsch, Daniel; Roth, Dan",,,Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A key challenge in topic-focused summarization is determining what information should be included in the summary, a problem known as content selection. In this work, we propose a new method for studying content selection in topic-focused summarization called the summary cloze task. The goal of the summary cloze task is to generate the next sentence of a summary conditioned on the beginning of the summary, a topic, and a reference document(s). The main challenge is deciding what information in the references is relevant to the topic and partial summary and should be included in the summary. Although the cloze task does not address all aspects of the traditional summarization problem, the more narrow scope of the task allows us to collect a large-scale datset of nearly 500k summary cloze instances fromWikipedia. We report experimental results on this new dataset using various extractive models and a two-step abstractive model that first extractively selects a small number of sentences and then abstractively summarizes them. Our results show that the topic and partial summary help the models identify relevant content, but the task remains a significant challenge.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3720,3729,,,,,,,,,,,,,,,,WOS:000854193303104,0
C,"Du, CN; Sun, HF; Wang, JY; Qi, Q; Liao, JX; Wang, C; Ma, B",,,Assoc Computat Linguist,"Du, Chunning; Sun, Haifeng; Wang, Jingyu; Qi, Qi; Liao, Jianxin; Wang, Chun; Ma, Bing",,,Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"As an essential component of natural language processing, text classification relies on deep learning in recent years. Various neural networks are designed for text classification on the basis of word embedding. However, polysemy is a fundamental feature of the natural language, which brings challenges to text classification. One polysemic word contains more than one sense, while the word embedding procedure conflates different senses of a polysemic word into a single vector. Extracting the distinct representation for the specific sense could thus lead to fine-grained models with strong generalization ability. It has been demonstrated that multiple senses of a word actually reside in linear superposition within the word embedding so that specific senses can be extracted from the original word embedding. Therefore, we propose to use capsule networks to construct the vectorized representation of semantics and utilize hyperplanes to decompose each capsule to acquire the specific senses. A novel dynamic routing mechanism named 'routing-on-hyperplane' will select the proper sense for the downstream classification task. Our model is evaluated on 6 different datasets, and the experimental results show that our model is capable of extracting more discriminative semantic features and yields a significant performance gain compared to other baseline methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,456,465,,,,,,,,,,,,,,,,WOS:000854193300043,0
C,"Gui, M; Tian, JF; Wang, R; Yang, ZL",,,Assoc Computat Linguist,"Gui, Min; Tian, Junfeng; Wang, Rui; Yang, Zhenglu",,,Attention Optimization for Abstractive Document Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Attention plays a key role in the improvement of sequence-to-sequence-based document summarization models. To obtain a powerful attention helping with reproducing the most salient information and avoiding repetitions, we augment the vanilla attention model from both local and global aspects. We propose an attention refinement unit paired with local variance loss to impose supervision on the attention model at each decoding step, and a global variance loss to optimize the attention distributions of all decoding steps from the global perspective. The performances on the CNN/Daily Mail dataset verify the effectiveness of our methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1222,1228,,,,,,,,,,,,,,,,WOS:000854193301039,0
C,"Jia, R; Raghunathan, A; Goksel, K; Liang, P",,,Assoc Computat Linguist,"Jia, Robin; Raghunathan, Aditi; Goksel, Kerem; Liang, Percy",,,Certified Robustness to Adversarial Word Substitutions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain 75% adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only 8% and 35%, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4129,4142,,,,,,,,,,,,,,,,WOS:000854193304028,0
C,"Khalil, T; Kielczewski, K; Chouliaras, GC; Keldibek, A; Versteegh, M",,,Assoc Computat Linguist,"Khalil, Talaat; Kielczewski, Kornel; Chouliaras, Georgios Christos; Keldibek, Amina; Versteegh, Maarten",,,Cross-lingual intent classification in a low resource industrial setting,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper explores different approaches to multilingual intent classification in a low resource setting. Recent advances in multi-lingual text representations promise cross-lingual transfer for classifiers. We investigate the potential for this transfer in an applied industrial setting and compare to multilingual classification using machine translated text. Our results show that while the recently developed methods show promise, practical application calls for a combination of techniques for useful results.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6419,6424,,,,,,,,,,,,,,,,WOS:000854193306087,0
C,"Min, QK; Shi, YF; Zhang, Y",,,Assoc Computat Linguist,"Min, Qingkai; Shi, Yuefeng; Zhang, Yue",,,A Pilot Study for Chinese SQL Semantic Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides crossdomain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and wordbased encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3652,3658,,,,,,,,,,,,,,,,WOS:000854193303095,0
C,"Parvez, MR; Bolukbasi, T; Chang, KW; Saligrama, V",,,Assoc Computat Linguist,"Parvez, Md Rizwan; Bolukbasi, Tolga; Chang, Kai-Wei; Saligrama, Venkatesh",,,Robust Text Classifier on Test-Time Budgets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We design a generic framework for learning a robust text classification model that achieves high accuracy under different selection budgets (a.k.a selection rates) at test-time. We take a different approach from existing methods and learn to dynamically filter a large fraction of unimportant words by a low-complexity selector such that any high-complexity classifier only needs to process a small fraction of text, relevant for the target task. To this end, we propose a data aggregation method for training the classifier, allowing it to achieve competitive performance on fractured sentences. On four benchmark text classification tasks, we demonstrate that the framework gains consistent speedup with little degradation in accuracy on various selection budgets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1167,1172,,,,,,,,,,,,,,,,WOS:000854193301030,0
C,"Redmiles, EM; Maszkiewicz, L; Hwang, E; Kuchhal, D; Liu, E; Morales, M; Peskov, D; Rao, S; Stevens, R; Gligoric, K; Kross, S; Mazurek, ML; Daume, H",,,Assoc Computat Linguist,"Redmiles, Elissa M.; Maszkiewicz, Lisa; Hwang, Emily; Kuchhal, Dhruv; Liu, Everest; Morales, Miraida; Peskov, Denis; Rao, Sudha; Stevens, Rock; Gligoric, Kristina; Kross, Sean; Mazurek, Michelle L.; Daume, Hal, III",,,Comparing and Developing Tools to Measure the Readability of Domain-Speeifie Texts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The readability of a digital text can influence people's ability to learn new things about a range topics from digital resources (e.g., Wikipedia, WebMD). Readability also impacts search rankings, and is used to evaluate the performance of NLP systems. Despite this, we lack a thorough understanding of how to validly measure readability at scale, especially for domain-specific texts. In this work, we present a comparison of the validity of well-known readability measures and introduce a novel approach, Smart Cloze, which is designed to address shortcomings of existing measures. We compare these approaches across four different corpora: crowdworker-generated stories, Wikipedia articles, security and privacy advice, and health information. On these corpora, we evalu ate the convergent and content validity of each measure, and detail tradeoffs in score precision, domain -specificity, and participant burden. These results provide a foundation for mom accurate readability measurements and better evaluation of new natural -languageprocessing systems and tools",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4831,4842,,,,,,,,,,,,,,,,WOS:000854193305002,0
C,"Shalyminov, I; Lee, SJ; Eshghi, A; Lemon, O",,,Assoc Computat Linguist,"Shalyminov, Igor; Lee, Sungjin; Eshghi, Arash; Lemon, Oliver",,,Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Goal-oriented dialogue systems are now being widely adopted in industry where it is of key importance to maintain a rapid prototyping cycle for new products and domains. Datadriven dialogue system development has to be adapted to meet this requirement - therefore, reducing the amount of data and annotations necessary for training such systems is a central research problem. In this paper, we present the Dialogue Knowledge Transfer Network (DiKTNet), a stateof-the-art approach to goal-oriented dialogue generation which only uses a few example dialogues (i.e. few-shot learning), none of which has to be annotated. We achieve this by performing a 2-stage training. Firstly, we perform unsupervised dialogue representation pre-training on a large source of goal-oriented dialogues in multiple domains, the MetaLWOz corpus. Secondly, at the transfer stage, we train DiKTNet using this representation together with 2 other textual knowledge sources with different levels of generality: ELMo encoder and the main dataset's source domains. Our main dataset is the Stanford MultiDomain dialogue corpus. We evaluate our model on it in terms of BLEU and Entity F1 scores, and show that our approach significantly and consistently improves upon a series of baseline models as well as over the previous state-of-the-art dialogue generation model, ZSDG. The improvement upon the latter - up to 10% in Entity F1 and the average of 3% in BLEU score - is achieved using only 10% equivalent of ZSDG's in-domain training data.",,,,,,"Lemon, Oliver/0000-0001-9497-4743",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1741,1751,,,,,,,,,,,,,,,,WOS:000854193301105,0
C,"Sinha, K; Sodhani, S; Dong, J; Pineau, J; Hamilton, WL",,,Assoc Computat Linguist,"Sinha, Koustuv; Sodhani, Shagun; Dong, Jin; Pineau, Joelle; Hamilton, William L.",,,CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs-with the graph-based model exhibiting both stronger generalization and greater robustness.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4506,4515,,,,,,,,,,,,,,,,WOS:000854193304063,0
C,"Tan, SL; Zhou, ZX; Xu, ZZ; Li, P",,,Assoc Computat Linguist,"Tan, Shulong; Zhou, Zhixin; Xu, Zhaozhuo; Li, Ping",,,On Efficient Retrieval of Top Similarity Vectors,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Retrieval of relevant vectors produced by representation learning critically influences the efficiency in natural language processing (NLP) tasks. In this paper we demonstrate an efficient method for searching vectors via a typical nonmetric matching function: inner product. Our method, which constructs an approximate Inner Product Delaunay Graph (IPDG) for top-1 Maximum Inner Product Search (MIPS), transforms retrieving the most suitable latent vectors into a graph search problem with great benefits of efficiency. Experiments on data representations learned for different machine learning tasks verify the outperforming effectiveness and efficiency of the proposed IPDG.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5236,5246,,,,,,,,,,,,,,,,WOS:000854193305040,0
C,"Tang, JZ; Feng, YS; Zhao, DY",,,Assoc Computat Linguist,"Tang, Jizhi; Feng, Yansong; Zhao, Dongyan",,,Learning to Update Knowledge Graphs by Reading News,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"News streams contain rich up-to-date information which can be used to update knowledge graphs (KGs). Most current text-based KG updating methods rely on elaborately designed information extraction systems and carefully crafted rules, which are often domain-specific and hard to maintain or generalize. However, such methods may not pay enough attention to the implicit information that lies underneath texts, thus often suffer from coverage issues. In this paper, we propose a novel graph based neural network method, GUpdater, to tackle these problems1. GUpdater is built upon graph neural networks (GNNs) with a text-based attention mechanism to guide the updating message passing through the KG structures. Experiments on a real-world KG updating dataset show that our model can effectively broadcast the news information to the KG structures and perform necessary link-adding or link-deleting operations to ensure the KG up-to-date according to news snippets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2632,2641,,,,,,,,,,,,,,,,WOS:000854193302073,0
C,"Vania, C; Kementchedjhieva, Y; Sogaard, A; Lopez, A",,,Assoc Computat Linguist,"Vania, Clara; Kementchedjhieva, Yova; Sogaard, Anders; Lopez, Adam",,,A systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Parsers are available for only a handful of the world's languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages-North Sami, Galician, and Kazah-We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1105,1116,,,,,,,,,,,,,,,,WOS:000854193301024,0
C,"Wang, YL; Wu, Y; Mou, LL; Li, ZJ; Chao, WH",,,Assoc Computat Linguist,"Wang, Yunli; Wu, Yu; Mou, Lili; Li, Zhoujun; Chao, Wenhan",,,Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Formality text style transfer plays an important role in various NLP applications, such as non-native speaker assistants and child education. Early studies normalize informal sentences with rules, before statistical and neural models become a prevailing method in the field. While a rule-based system is still a common preprocessing step for formality style transfer in the neural era, it could introduce noise if we use the rules in a naive way such as data preprocessing. To mitigate this problem, we study how to harness rules into a state-of-the-art neural network that is typically pretrained on massive corpora. We propose three fine-tuning methods in this paper and achieve a new state-of-the-art on benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3573,3578,,,,,,,,,,,,,,,,WOS:000854193303083,0
C,"Wu, CH; Wu, FZ; Ge, SY; Qi, T; Huang, YF; Xie, X",,,Assoc Computat Linguist,"Wu, Chuhan; Wu, Fangzhao; Ge, Suyu; Qi, Tao; Huang, Yongfeng; Xie, Xing",,,Neural News Recommendation with Multi-Head Self-Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"News recommendation can help users find interested news and alleviate information overload. Precisely modeling news and users is critical for news recommendation, and capturing the contexts of words and news is important to learn news and user representations. In this paper, we propose a neural news recommendation approach with multi-head selfattention (NRMS). The core of our approach is a news encoder and a user encoder. In the news encoder, we use multi-head self-attentions to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multi-head self-attention to capture the relatedness between the news. Besides, we apply additive attention to learn more informative news and user representations by selecting important words and news. Experiments on a realworld dataset validate the effectiveness and efficiency of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6389,6394,,,,,,,,,,,,,,,,WOS:000854193306082,0
C,"Wu, LJ; Zhu, JH; Gao, F; He, D; Qin, T; Lai, JH; Liu, TY",,,Assoc Computat Linguist,"Wu, Lijun; Zhu, Jinhua; Gao, Fei; He, Di; Qin, Tao; Lai, Jianhuang; Liu, Tie-Yan",,,Machine Translation With Weakly Paired Documents,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural machine translation, which achieves near human-level performance in some languages, strongly relies on the large amounts of parallel sentences, which hinders its applicability to low-resource language pairs. Recent works explore the possibility of unsupervised machine translation with monolingual data only, leading to much lower accuracy compared with the supervised one. Observing that weakly paired bilingual documents are much easier to collect than bilingual sentences, e.g., from Wikipedia, news websites or books, in this paper, we investigate training translation models with weakly paired bilingual documents. Our approach contains two components. 1) We provide a simple approach to mine implicitly bilingual sentence pairs from document pairs which can then be used as supervised training signals. 2) We leverage the topic consistency of two weakly paired documents and learn the sentence translation model by constraining the word distribution-level alignments. We evaluate our method on weakly paired documents from Wikipedia on six tasks, the widely used WMT16 German$English, WMT13 Spanish$English and WMT16 Romanian$English translation tasks. We obtain 24:1/30:3, 28:1/27:6 and 30:1/27:6 BLEU points separately, outperforming previous results by more than 5 BLEU points in each direction and reducing the gap between unsupervised translation and supervised translation up to 50%",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4375,4384,,,,,,,,,,,,,,,,WOS:000854193304051,0
C,"Xu, JC; Durrett, G",,,Assoc Computat Linguist,"Xu, Jiacheng; Durrett, Greg",,,Neural Extractive Text Summarization with Syntactic Compression,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the CNN/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-theshelf compression module, and human and manual evaluation shows that our model's output generally remains grammatical.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3292,3303,,,,,,,,,,,,,,,,WOS:000854193303042,0
C,"Xu, JJ; Wang, YC; Tang, DY; Duan, N; Yang, PC; Zeng, Q; Zhou, M; Sun, X",,,Assoc Computat Linguist,"Xu, Jingjing; Wang, Yuechen; Tang, Duyu; Duan, Nan; Yang, Pengcheng; Zeng, Qi; Zhou, Ming; Sun, Xu",,,Asking Clarification Questions in Knowledge-Based Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1618,1629,,,,,,,,,,,,,,,,WOS:000854193301094,0
C,"Yilmaz, ZA; Wang, SJ; Yang, W; Zhang, HT; Lin, J",,,ASSOC COMPUTAT LINGUIST,"Yilmaz, Zeynep Akkalyoncu; Wang, Shengjin; Yang, Wei; Zhang, Haotian; Lin, Jimmy",,,Applying BERT to Document Retrieval with Birch,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve stateof-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,19,24,,,,,,,,,,,,,,,,WOS:000855231500004,0
C,"Zhang, XC; Rajagopal, D; Gamon, M; Jauhar, SK; Lu, CT",,,Assoc Computat Linguist,"Zhang, Xuchao; Rajagopal, Dheeraj; Gamon, Michael; Jauhar, Sujay Kumar; Lu, Chang-Tien",,,Modeling the Relationship between User Comments and Edits in Document Revision,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Management of collaborative documents can be difficult, given the profusion of edits and comments that multiple authors make during a document's evolution. Reliably modeling the relationship between edits and comments is a crucial step towards helping the user keep track of a document in flux. A number of authoring tasks, such as categorizing and summarizing edits, detecting completed to-dos, and visually rearranging comments could benefit from such a contribution. Thus, in this paper we explore the relationship between comments and edits by defining two novel, related tasks: Comment Ranking and Edit Anchoring. We begin by collecting a dataset with more than half a million comment-edit pairs based on Wikipedia revision histories. We then propose a hierarchical multi-layer deep neural-network to model the relationship between edits and comments. Our architecture tackles both Comment Ranking and Edit Anchoring tasks by encoding specific edit actions such as additions and deletions, while also accounting for document context. In a number of evaluation settings, our experimental results show that our approach outperforms several strong baselines significantly. We are able to achieve a precision@1 of 71.0% and a precision@3 of 94.4% for Comment Ranking, while we achieve 74.4% accuracy on Edit Anchoring.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5002,5011,,,,,,,,,,,,,,,,WOS:000854193305018,0
C,"Zhao, H; Luo, ZC; Feng, C; Zheng, AQ; Liu, XP",,,Assoc Computat Linguist,"Zhao, He; Luo, Zhunchen; Feng, Chong; Zheng, Anqing; Liu, Xiaopeng",,,A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a new task of modeling the role and function for on-line resource citations in scientific literature. By categorizing the online resources and analyzing the purpose of resource citations in scientific texts, it can greatly help resource search and recommendation systems to better understand and manage the scientific resources. For this novel task, we are the first to create an annotation scheme, which models the different granularity of information from a hierarchical perspective. And we construct a dataset SciRes, which includes 3,088 manually annotated resource contexts. In this paper, we propose a possible solution by using a multi-task framework to build the scientific resource classifier (SciResCLF) for jointly recognizing the role and function types. Then we use the classification results to help a scientific resource recommendation (SciResREC) task. Experiments show that our model achieves the best results on both the classification task and the recommendation task. The SciRes dataset1 will be released for future research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5206,5215,,,,,,,,,,,,,,,,WOS:000854193305037,0
C,"Abdou, M; Kulmizev, A; Hill, F; Low, DM; Sogaard, A",,,Assoc Computat Linguist,"Abdou, Mostafa; Kulmizev, Artur; Hill, Felix; Low, Daniel M.; Sogaard, Anders",,,Higher-order Comparisons of Sentence Encoder Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Representational Similarity Analysis (RSA) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities (e.g., fMRI, electrophysiology, behavior). As a framework, RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classification: namely, it does not require large training samples, is not prone to overfitting, and it enables a more transparent comparison between the representational geometries of different models and modalities. We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely-employed pre-trained language encoders and human processing difficulty via eye-tracking data, showcasing its potential in the interpretability toolbox for neural models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5838,5845,,,,,,,,,,,,,,,,WOS:000854193306004,0
C,"Ajjour, Y; Alshomary, M; Wachsmuth, H; Stein, B",,,Assoc Computat Linguist,"Ajjour, Yamen; Alshomary, Milad; Wachsmuth, Henning; Stein, Benno",,,Modeling Frames in Argumentation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In argumentation, framing is used to emphasize a specific aspect of a controversial topic while concealing others. When discussing the legalization of drugs, for instance, its economical aspect may be emphasized. In general, we call a set of arguments that focus on the same aspect a frame. An argumentative text has to serve the right frame(s) to convince the audience to adopt the author's stance (e.g., being pro or con legalizing drugs). More specifically, an author has to choose frames that fit the audience's interests and cultural background. This paper introduces frame identification, which is the task of splitting a set of arguments into a set of non-overlapping frames. We present a fully unsupervised approach to this task, which first removes topical information from the arguments and then identifies frames using clustering. For evaluation purposes, we provide a corpus with 12 326 debate-portal arguments, organized along the frames of the debates' topics. On this corpus, our approach outperforms different strong baselines, achieving an F-1-score of 0.28.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2922,2932,,,,,,,,,,,,,,,,WOS:000854193303008,0
C,"Anastasopoulos, A; Neubig, G",,,Assoc Computat Linguist,"Anastasopoulos, Antonios; Neubig, Graham",,,Pushing the Limits of Low-Resource Morphological Inflection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"tRecent years have seen exceptional strides in the task of automatic morphological inflection generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of improvements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we investigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points.(1) Also, we identify the crucial factors for success with cross-lingual transfer for morphological inflection: typological similarity and a common representation across languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,984,996,,,,,,,,,,,,,,,,WOS:000854193301013,0
C,"Balazevic, I; Allen, C; Hospedales, TM",,,Assoc Computat Linguist,"Balazevic, Ivana; Allen, Carl; Hospedales, Timothy M.",,,TuckER: Tensor Factorization for Knowledge Graph Completion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5185,5194,,,,,,,,,,,,,,,,WOS:000854193305035,0
C,"Bi, B; Wu, C; Yan, M; Wang, W; Xia, JN; Li, CL",,,Assoc Computat Linguist,"Bi, Bin; Wu, Chen; Yan, Ming; Wang, Wei; Xia, Jiangnan; Li, Chenliang",,,Incorporating External Knowledge into Machine Reading for Generative Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2521,2530,,,,,,,,,,,,,,,,WOS:000854193302063,0
C,"Bodell, MH; Arvidsson, M; Magnusson, M",,,Assoc Computat Linguist,"Bodell, Miriam Hurtado; Arvidsson, Martin; Magnusson, Mans",,,Interpretable Word Embeddings via Informative Priors,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Word embeddings have demonstrated strong performance on NLP tasks. However, lack of interpretability and the unsupervised nature of word embeddings have limited their use within computational social science and digital humanities. We propose the use of informative priors to create interpretable and domain-informed dimensions for probabilistic word embeddings. Experimental results show that sensible priors can capture latent semantic concepts better than or on-par with the current state of the art, while retaining the simplicity and generalizability of using priors.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6323,6329,,,,,,,,,,,,,,,,WOS:000854193306072,0
C,"Bohm, F; Gao, Y; Meyer, CM; Shapira, O; Dagan, I; Gurevych, I",,,Assoc Computat Linguist,"Boehm, Florian; Gao, Yang; Meyer, Christian M.; Shapira, Ori; Dagan, Ido; Gurevych, Iryna",,,Better Rewards Yield Better Summaries: Learning to Summarise Without References,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Reinforcement Learning (RL) based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can be used to train RL-based summarisation systems without using any reference summaries. We show that our learned rewards have significantly higher correlation with human ratings than previous approaches. Human evaluation experiments show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward- no-reference.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3110,3120,,,,,,,,,,,,,,,,WOS:000854193303025,0
C,"Deng, X; Sun, H",,,Assoc Computat Linguist,"Deng, Xiang; Sun, Huan",,,Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Distant supervision (DS) has been widely used to automatically construct (noisy) labeled data for relation extraction (RE). Given two entities, distant supervision exploits sentences that directly mention them for predicting their semantic relation. We refer to this strategy as 1-hop DS, which unfortunately may not work well for long-tail entities with few supporting sentences. In this paper, we introduce a new strategy named 2-hop DS to enhance distantly supervised RE, based on the observation that there exist a large number of relational tables on theWeb which contain entity pairs that share common relations. We refer to such entity pairs as anchors for each other, and collect all sentences that mention the anchor entity pairs of a given target entity pair to help relation prediction. We develop a new neural RE method REDS2 in the multi-instance learning paradigm, which adopts a hierarchical model structure to fuse information respectively from 1-hop DS and 2-hop DS. Extensive experimental results on a benchmark dataset show that REDS2 can consistently outperform various baselines across different settings by a substantial margin.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,410,420,,,,,,,,,,,,,,,,WOS:000854193300039,0
C,"Ding, JW; Hu, W; Xu, QX; Qu, YZ",,,Assoc Computat Linguist,"Ding, Jiwei; Hu, Wei; Xu, Qixin; Qu, Yuzhong",,,Leveraging Frequent Query Substructures to Generate Formal Queries for Complex Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Formal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions. Also, it achieves promising performance with limited training data and noisy entity/relation linking results.",,,,,"Ding, Jiwei/HDO-3529-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2614,2622,,,,,,,,,,,,,,,,WOS:000854193302071,0
C,"Duan, XY; Yu, HF; Yin, MM; Zhang, M; Luo, WH; Zhang, Y",,,Assoc Computat Linguist,"Duan, Xiangyu; Yu, Hongfei; Yin, Mingming; Zhang, Min; Luo, Weihua; Zhang, Yue",,,Contrastive Attention Mechanism for Abstractive Sentence Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a contrastive attention mechanism to extend the sequence-to-sequence framework for abstractive sentence summarization task, which aims to generate a brief summary of a given source sentence. The proposed contrastive attention mechanism accommodates two categories of attention: one is the conventional attention that attends to relevant parts of the source sentence, the other is the opponent attention that attends to irrelevant or less relevant parts of the source sentence. Both attentions are trained in an opposite way so that the contribution from the conventional attention is encouraged and the contribution from the opponent attention is discouraged through a novel softmax and softmin functionality. Experiments on benchmark datasets show that, the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the stateof-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/Abstractive-Text-Summarization.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3044,3053,,,,,,,,,,,,,,,,WOS:000854193303019,0
C,"Ethayarajh, K",,,Assoc Computat Linguist,"Ethayarajh, Kawin",,,Rotate King to get Queen: Word Relationships as Orthogonal Transformations in Embedding Space,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A notable property of word embeddings is that word relationships can exist as linear substructures in the embedding space. For example, gender corresponds to woman man and queen king. This, in turn, allows word analogies to be solved arithmetically: king man + woman approximate to queen. This property is notable because it suggests that models trained on word embeddings can easily learn such relationships as geometric translations. However, there is no evidence that models exclusively represent relationships in this manner. We document an alternative way in which downstream models might learn these relationships: orthogonal and linear transformations. For example, given a translation vector for gender, we can find an orthogonal matrix R, representing a rotation and reflection, such that R(king) approximate to queen and R(man) approximate to woman. Analogical reasoning using orthogonal transformations is almost as accurate as using vector arithmetic; using linear transformations is more accurate than both. Our findings suggest that these transformations can be as good a representation of word relationships as translation vectors.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3503,3508,,,,,,,,,,,,,,,,WOS:000854193303072,0
C,"Fauceglia, NR; Gliozzo, A; Dash, S; Chowdhury, MFM; Mihindukulasooriya, N",,,ASSOC COMPUTAT LINGUIST,"Fauceglia, Nicolas Rodolfo; Gliozzo, Alfio; Dash, Sarthak; Chowdhury, Md Faisal Mahbub; Mihindukulasooriya, Nandana",,,Automatic Taxonomy Induction and Expansion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The Knowledge Graph Induction Service (KGIS) is an end-to-end knowledge induction system. One of its main capabilities is to automatically induce taxonomies1 from input documents using a hybrid approach that takes advantage of linguistic patterns, semantic web and neural networks. KGIS allows the user to semiautomatically curate and expand the induced taxonomy through a component called smart spreadsheet by exploiting distributional semantics. In this paper, we describe these taxonomy induction and expansion features of KGIS. A screencast video demonstrating the system is available in https://ibm.box. com/v/emnlp-2019- demo",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,25,30,,,,,,,,,,,,,,,,WOS:000855231500005,0
C,"Ghosal, D; Majumder, N; Poria, S; Chhaya, N; Gelbukh, A",,,Assoc Computat Linguist,"Ghosal, Deepanway; Majumder, Navonil; Poria, Soujanya; Chhaya, Niyati; Gelbukh, Alexander",,,DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.",,,,,"Poria, Soujanya/L-8361-2015",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,154,164,,,,,,,,,,,,,,,,WOS:000854193300015,0
C,"Hayashi, K; Shimbo, M",,,Assoc Computat Linguist,"Hayashi, Katsuhiko; Shimbo, Masashi",,,A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) cannot be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, relation matrices can be non-commutative, allowing composite relations to be modeled by matrix product. The model is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique is developed on the basis of the duality of the Fourier transform of circulant matrices.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2422,2430,,,,,,,,,,,,,,,,WOS:000854193302054,0
C,"Iyer, S; Cheung, A; Zettlemoyer, L",,,Assoc Computat Linguist,"Iyer, Srinivasan; Cheung, Alvin; Zettlemoyer, Luke",,,Learning Programmatic Idioms for Scalable Semantic Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5x larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5426,5435,,,,,,,,,,,,,,,,WOS:000854193305058,0
C,"Jiang, TW; Zhao, T; Qin, B; Liu, T; Chawla, NV; Jiang, M",,,Assoc Computat Linguist,"Jiang, Tianwen; Zhao, Tong; Qin, Bing; Liu, Ting; Chawla, Nitesh, V; Jiang, Meng",,,Multi-Input Multi-Output Sequence Labeling for Joint Extraction of Fact and Condition Tuples from Scientific Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Condition is essential in scientific statement. Without the conditions (e.g., equipment, environment) that were precisely specified, facts (e.g., observations) in the statements may no longer be valid. Existing ScienceIE methods, which aim at extracting factual tuples from scientific text, do not consider the conditions. In this work, we propose a new sequence labeling framework (as well as a new tag schema) to jointly extract the fact and condition tuples from statement sentences. The framework has (1) a multi-output module to generate one or multiple tuples and (2) a multi-input module to feed in multiple types of signals as sequences. It improves F1 score relatively by 4.2% on BioNLP2013 and by 6.2% on a new bio-text dataset for tuple extraction.",,,,,"liu, ting/GZM-3326-2022; Chawla, Nitesh/F-2690-2016","Chawla, Nitesh/0000-0003-3932-5956",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,302,312,,,,,,,,,,,,,,,,WOS:000854193300029,0
C,"Jin, WK; Zhao, Z; Gu, M; Xiao, J; Wei, FR; Zhuang, YT",,,Assoc Computat Linguist,"Jin, Weike; Zhao, Zhou; Gu, Mao; Xiao, Jun; Wei, Furu; Zhuang, Yueting",,,Video Dialog via Progressive Inference and Cross-Transformer,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Video dialog is a new and challenging task, which requires the agent to answer questions combining video information with dialog history. And different from single-turn video question answering, the additional dialog history is important for video dialog, which often includes contextual information for the question. Existing visual dialog methods mainly use RNN to encode the dialog history as a single vector representation, which might be rough and straightforward. Some more advanced methods utilize hierarchical structure, attention and memory mechanisms, which still lack an explicit reasoning process. In this paper, we introduce a novel progressive inference mechanism for video dialog, which progressively updates query information based on dialog history and video content until the agent think the information is sufficient and unambiguous. In order to tackle the multimodal fusion problem, we propose a crosstransformer module, which could learn more fine-grained and comprehensive interactions both inside and between the modalities. And besides answer generation, we also consider question generation, which is more challenging but significant for a complete video dialog system. We evaluate our method on two largescale datasets, and the extensive experiments show the effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2109,2118,,,,,,,,,,,,,,,,WOS:000854193302025,0
C,"Lawrence, C; Kotnis, B; Niepert, M",,,Assoc Computat Linguist,"Lawrence, Carolin; Kotnis, Bhushan; Niepert, Mathias",,,Attending to Future Tokens for Bidirectional Sequence Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1,10,,,,,,,,,,,,,,,,WOS:000854193300001,0
C,"Li, SQ; Zhao, WX; Yin, EJ; Wen, JR",,,Assoc Computat Linguist,"Li, Siqing; Zhao, Wayne Xin; Yin, Eddy Jing; Wen, Ji-Rong",,,A Neural Citation Count Prediction Model based on Peer Review Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Citation count prediction (CCP) has been an important research task for automatically estimating the future impact of a scholarly paper. Previous studies mainly focus on extracting or mining useful features from the paper itself or the associated authors. An important kind of data signals, i.e., peer review text, has not been utilized for the CCP task. In this paper, we take the initiative to utilize peer review data for the CCP task with a neural prediction model. Our focus is to learn a comprehensive semantic representation for peer review text for improving the prediction performance. To achieve this goal, we incorporate the abstract-review match mechanism and the cross-review match mechanism to learn deep features from peer review text. We also consider integrating hand-crafted features via a wide component. The deep and wide components jointly make the prediction. Extensive experiments have demonstrated the usefulness of the peer review data and the effectiveness of the proposed model. Our dataset has been released online.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4914,4924,,,,,,,,,,,,,,,,WOS:000854193305010,0
C,"Lin, ZH; Huang, XJ; Ji, F; Chen, HQ; Zhang, Y",,,Assoc Computat Linguist,"Lin, Zehao; Huang, Xinjing; Ji, Feng; Chen, Haiqing; Zhang, Yin",,,Task-Oriented Conversation Generation Using Heterogeneous Memory Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"How to incorporate external knowledge into a neural dialogue model is critically important for dialogue systems to behave like real humans. To handle this problem, memory networks are usually a great choice and a promising way. However, existing memory networks do not perform well when leveraging heterogeneous information from different sources. In this paper, we propose a novel and versatile external memory networks called Heterogeneous Memory Networks (HMNs), to simultaneously utilize user utterances, dialogue history and background knowledge tuples. In our method, historical sequential dialogues are encoded and stored into the context-aware memory enhanced by gating mechanism while grounding knowledge tuples are encoded and stored into the context-free memory. During decoding, the decoder augmented with HMNs recurrently selects each word in one response utterance from these two memories and a general vocabulary. Experimental results on multiple real-world datasets show that HMNs significantly outperform the state-of-the-art datadriven task-oriented dialogue models in most domains.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4558,4567,,,,,,,,,,,,,,,,WOS:000854193304068,0
C,"Liu, Y; Lapata, M",,,Assoc Computat Linguist,"Liu, Yang; Lapata, Mirella",,,Text Summarization with Pretrained Encoders,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Bidirectional Encoder Representations from Transformers (BERT; Devlin et al. 2019) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several intersentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves stateof-the-art results across the board in both extractive and abstractive settings.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3730,3740,,,,,,,,,,,,,,,,WOS:000854193303105,0
C,"Melamud, O; Bornea, M; Barker, K",,,Assoc Computat Linguist,"Melamud, Oren; Bornea, Mihaela; Barker, Ken",,,Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Supervised learning models often perform poorly at low-shot tasks, i.e. tasks for which little labeled data is available for training. One prominent approach for improving low-shot learning is to use unsupervised pre-trained neural models. Another approach is to obtain richer supervision by collecting annotator rationales (explanations supporting label annotations). In this work, we combine these two approaches to improve lowshot text classification with two novel methods: a simple bag-of-words embedding approach; and a more complex context-aware method, based on the BERT model. In experiments with two English text classification datasets, we demonstrate substantial performance gains from combining pre-training with rationales. Furthermore, our investigation of a range of train-set sizes reveals that the simple bag-of-words approach is the clear top performer when there are only a few dozen training instances or less, while more complex models, such as BERT or CNN, require more training data to shine.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3884,3893,,,,,,,,,,,,,,,,WOS:000854193304006,0
C,"Mesbah, S; Yang, J; Sips, RJ; Torre, MV; Lofi, C; Bozzon, A; Houben, GJ",,,Assoc Computat Linguist,"Mesbah, Sepideh; Yang, Jie; Sips, Robert-Jan; Torre, Manuel Valle; Lofi, Christoph; Bozzon, Alessandro; Houben, Geert-Jan",,,Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Social media provides a timely yet challenging data source for adverse drug reaction (ADR) detection. Existing dictionary-based, semi-supervised learning approaches are intrinsically limited by the coverage and maintainability of laymen health vocabularies. In this paper, we introduce a data augmentation approach that leverages variational autoencoders to learn high-quality data distributions from a large unlabeled dataset, and subsequently, to automatically generate a large labeled training set from a small set of labeled samples. This allows for efficient social-media ADR detection with low training and re-training costs to adapt to the changes and emergence of informal medical laymen terms. An extensive evaluation performed on Twitter and Reddit data shows that our approach matches the performance of fully-supervised approaches while requiring only 25% of training data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2349,2359,,,,,,,,,,,,,,,,WOS:000854193302047,0
C,"Mullenbach, J; Gordon, J; Peng, NY; May, J",,,Assoc Computat Linguist,"Mullenbach, James; Gordon, Jonathan; Peng, Nanyun; May, Jonathan",,,Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarine's captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question Given a whole described by an adjective, does the adjective also describe a given part? We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6052,6058,,,,,,,,,,,,,,,,WOS:000854193306036,0
C,"Oh, B; Seo, S; Shin, C; Jo, E; Lee, KH",,,Assoc Computat Linguist,"Oh, Byungkook; Seo, Seungmin; Shin, Cheolheon; Jo, Eunju; Lee, Kyong-Ho",,,Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a novel topic-guided coherence modeling (TGCM) for sentence ordering. Our attention based pointer decoder directly utilize sentence vectors in a permutation-invariant manner, without being compressed into a single fixed-length vector as the paragraph representation. Thus, TGCM can improve global dependencies among sentences and preserve relatively informative paragraph-level semantics. Moreover, to predict the next sentence, we capture topic-enhanced sentence-pair interactions between the current predicted sentence and each next-sentence candidate. With the coherent topical context matching, we promote local dependencies that help identify the tight semantic connections for sentence ordering. The experimental results show that TGCM outperforms state-of-the-art models from various perspectives.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2273,2283,,,,,,,,,,,,,,,,WOS:000854193302040,0
C,"Ringel, D; Lavee, G; Guy, I; Radinsky, K",,,Assoc Computat Linguist,"Ringel, Dor; Lavee, Gal; Guy, Ido; Radinsky, Kira",,,Cross-Cultural Transfer Learning for Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Large training datasets are required to achieve competitive performance in most natural language tasks. The acquisition process for these datasets is labor intensive, expensive, and time consuming. This process is also prone to human errors. In this work, we show that crosscultural differences can be harnessed for natural language text classification. We present a transfer-learning framework that leverages widely-available unaligned bilingual corpora for classification tasks, using no task-specific data. Our empirical evaluation on two tasks - formality classification and sarcasm detection - shows that the cross-cultural difference between German and American English, as manifested in product review text, can be applied to achieve good performance for formality classification, while the difference between Japanese and American English can be applied to achieve good performance for sarcasm detection - both without any task-specific labeled data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3873,3883,,,,,,,,,,,,,,,,WOS:000854193304005,0
C,"Sharma, S; Santra, B; Jana, A; Santosh, TYSS; Ganguly, N; Goyal, P",,,Assoc Computat Linguist,"Sharma, Soumya; Santra, Bishal; Jana, Abhik; Santosh, T. Y. S. S.; Ganguly, Niloy; Goyal, Pawan",,,Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, biomedical version of embeddings obtained from language models such as BioELMo have shown state-of-the-art results for the textual inference task in the medical domain. In this paper, we explore how to incorporate structured domain knowledge, available in the form of a knowledge graph (UMLS), for the Medical NLI task. Specifically, we experiment with fusing embeddings obtained from knowledge graph with the state-of-the-art approaches for NLI task, which mainly rely on contextual word embeddings. We also experiment with fusing the domain-specific sentiment information for the task. Experiments conducted on MedNLI dataset clearly show that this strategy improves the baseline BioELMo architecture for the Medical NLI task(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6092,6097,,,,,,,,,,,,,,,,WOS:000854193306042,0
C,"Simpson, E; Gurevych, I",,,Assoc Computat Linguist,"Simpson, Edwin; Gurevych, Iryna",,,A Bayesian Approach for Sequence Tagging with Crowds,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current methods for sequence tagging, a core task in NLP, are data hungry, which motivates the use of crowdsourcing as a cheap way to obtain labelled data. However, annotators are often unreliable and current aggregation methods cannot capture common types of span annotation errors. To address this, we propose a Bayesian method for aggregating sequence tags that reduces errors by modelling sequential dependencies between the annotations as well as the ground-truth labels. By taking a Bayesian approach, we account for uncertainty in the model due to both annotator errors and the lack of data for modelling annotators who complete few tasks. We evaluate our model on crowdsourced data for named entity recognition, information extraction and argument mining, showing that our sequential model outperforms the previous state of the art. We also find that our approach can reduce crowdsourcing costs through more effective active learning, as it better captures uncertainty in the sequence labels when there are few annotations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1093,1104,,,,,,,,,,,,,,,,WOS:000854193301023,0
C,"Stewart, M; Lie, W; Cardell-Oliver, R",,,ASSOC COMPUTAT LINGUIST,"Stewart, Michael; Lie, Wei; Cardell-Oliver, Rachel",,,Redcoat: A Collaborative Annotation Tool for Hierarchical Entity Typing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce Redcoat, a web-based annotation tool that supports collaborative hierarchical entity typing. As an annotation tool, Redcoat also facilitates knowledge elicitation by allowing the creation and continuous refinement of concept hierarchies during annotation. It aims to minimise not only annotation time but the time it takes for project creators to set up and distribute projects to annotators. Projects created using the web-based interface can be rapidly distributed to a list of email addresses. Redcoat handles the propagation of documents amongst annotators and automatically scales the annotation workload depending on the number of active annotators. In this paper we discuss these key features and outline Redcoat's system architecture. We also highlight Redcoat's unique benefits over existing annotation tools via a qualitative comparison.",,,,,,"Cardell-Oliver, Rachel/0000-0003-0590-1003; Liu, Wei/0000-0002-7409-0948",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,193,198,,,,,,,,,,,,,,,,WOS:000855231500033,0
C,"Wang, X; Tu, ZP; Wang, LY; Shi, SM",,,Assoc Computat Linguist,"Wang, Xing; Tu, Zhaopeng; Wang, Longyue; Shi, Shuming",,,Self-Attention with Structural Position Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Although self-attention networks (SANs) have advanced the state-of-the-art on various NLP tasks, one criticism of SANs is their ability of encoding positions of input words (Shaw et al., 2018). In this work, we propose to augment SANs with structural position representations to model the latent structure of the input sentence, which is complementary to the standard sequential positional representations. Specifically, we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree.Experimental results on NIST Chinese double right arrow English and WMT14 English double right arrow German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1403,1409,,,,,,,,,,,,,,,,WOS:000854193301067,0
C,"Yang, XW; Liu, YR; Xie, DL; Wang, X; Balasubramanian, N",,,Assoc Computat Linguist,"Yang, Xuewen; Liu, Yingru; Xie, Dongliang; Wang, Xin; Balasubramanian, Niranjan",,,Latent Part-of-Speech Sequences for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the codependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,780,790,,,,,,,,,,,,,,,,WOS:000854193300072,0
C,"Yilmaz, ZA; Yang, W; Zhang, HT; Lin, J",,,Assoc Computat Linguist,"Yilmaz, Zeynep Akkalyoncu; Yang, Wei; Zhang, Haotian; Lin, Jimmy",,,Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges: relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate sentence-level evidence to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3490,3496,,,,,,,,,,,,,,,,WOS:000854193303070,0
C,"Yu, T; Zhang, R; Er, HY; Li, SY; Xue, E; Pang, B; Lin, XV; Tan, YC; Shi, TZ; Li, ZH; Jiang, YX; Yasunaga, M; Shim, S; Chen, T; Fabbri, A; Li, ZF; Chen, LY; Zhang, YW; Dixit, S; Zhang, V; Xiong, CM; Socher, R; Lasecki, WS; Radev, D",,,Assoc Computat Linguist,"Yu, Tao; Zhang, Rui; Er, He Yang; Li, Suyi; Xue, Eric; Pang, Bo; Lin, Xi Victoria; Tan, Yi Chern; Shi, Tianze; Li, Zihan; Jiang, Youxuan; Yasunaga, Michihiro; Shim, Sungrok; Chen, Tao; Fabbri, Alexander; Li, Zifan; Chen, Luyao; Zhang, Yuwen; Dixit, Shreya; Zhang, Vincent; Xiong, Caiming; Socher, Richard; Lasecki, Walter S.; Radev, Dragomir",,,CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slotvalue pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1962,1979,,,,,,,,,,,,,,,,WOS:000854193302012,0
C,"Zhao, ZJ; Ma, XJ",,,Assoc Computat Linguist,"Zhao, Zhenjie; Ma, Xiaojuan",,,Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text emotion distribution learning (EDL) aims to develop models that can predict the intensity values of a sentence across a set of emotion categories. Existing methods based on supervised learning require a large amount of well-labelled training data, which is difficult to obtain due to inconsistent perception of fine-grained emotion intensity. In this paper, we propose a meta-learning approach to learn text emotion distributions from a small sample. Specifically, we propose to learn low-rank sentence embeddings by tensor decomposition to capture their contextual semantic similarity, and use K-nearest neighbors (KNNs) of each sentence in the embedding space to generate sample clusters. We then train a meta-learner that can adapt to new data with only a few training samples on the clusters, and further fit the meta-learner on KNNs of a testing sample for EDL. In this way, we effectively augment the learning ability of a model on the small sample. To demonstrate the performance, we compare the proposed approach with state-of-the-art EDL methods on a widely used EDL dataset: SemEval 2007 Task 14 (Strapparava and Mihalcea, 2007). Results show the superiority of our method on small-sample emotion distribution learning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3957,3967,,,,,,,,,,,,,,,,WOS:000854193304013,0
C,"Zheng, CM; Cai, Y; Xu, JY; Leung, HF; Xu, GD",,,Assoc Computat Linguist,"Zheng, Changmeng; Cai, Yi; Xu, Jingyun; Leung, Ho-fung; Xu, Guandong",,,A Boundary-aware Neural Model for Nested Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In natural language processing, it is common that many entities contain other entities inside them. Most existing works on named entity recognition (NER) only deal with flat entities but ignore nested ones. We propose a boundary-aware neural model for nested NER which leverages entity boundaries to predict entity categorical labels. Our model can locate entities precisely by detecting boundaries using sequence labeling models. Based on the detected boundaries, our model utilizes the boundary-relevant regions to predict entity categorical labels, which can decrease computation cost and relieve error propagation problem in layered sequence labeling model. We introduce multitask learning to capture the dependencies of entity boundaries and their categorical labels, which helps to improve the performance of identifying entities. We conduct our experiments on nested NER datasets and the experimental results demonstrate that our model outperforms other state-of-the-art methods.",,,,,,"zheng, changmeng/0000-0002-2945-8248",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,357,366,,,,,,,,,,,,,,,,WOS:000854193300034,0
C,"Barke, S; Kunkel, R; Polikarpova, N; Meinhardt, E; Bakovic, E; Bergen, L",,,Assoc Computat Linguist,"Barke, Shraddha; Kunkel, Rose; Polikarpova, Nadia; Meinhardt, Eric; Bakovic, Eric; Bergen, Leon",,,Constraint-based Learning of Phonological Processes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Phonological processes are context-dependent sound changes in natural languages. We present an unsupervised approach to learning human-readable descriptions of phonological processes from collections of related utterances. Our approach builds upon a technique from the programming languages community called constraint-based program synthesis. We contribute a novel encoding of the learning problem into Satisfiablity Modulo Theory constraints, which enables both data efficiency and fast inference. We evaluate our system on textbook phonology problems and lexical databases, and show that it achieves high accuracy at speeds two orders of magnitude faster than state-of-the-art approaches.",,,,,,"Bakovic, Eric/0000-0002-2048-5135",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6176,6186,,,,,,,,,,,,,,,,WOS:000854193306050,0
C,"Davidson, S; Yu, D; Yu, Z",,,Assoc Computat Linguist,"Davidson, Sam; Yu, Dian; Yu, Zhou",,,Dependency Parsing for Spoken Dialog Systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dependency parsing of conversational input can play an important role in language understanding for dialog systems by identifying the relationships between entities extracted from user utterances. Additionally, effective dependency parsing can elucidate differences in language structure and usage for discourse analysis of human-human versus human-machine dialogs. However, models trained on datasets based on news articles and web data do not perform well on spoken human-machine dialog, and currently available annotation schemes do not adapt well to dialog data. Therefore, we propose the Spoken Conversation Universal Dependencies (SCUD) annotation scheme that extends the Universal Dependencies (UD) (Nivre et al., 2016) guidelines to spoken human-machine dialogs. We also provide ConvBank, a conversation dataset between humans and an opendomain conversational dialog system with SCUD annotation. Finally, to demonstrate the utility of the dataset, we train a dependency parser on the ConvBank dataset. We demonstrate that by pre-training a dependency parser on a set of larger public datasets and finetuning on ConvBank data, we achieved the best result, 85.05% unlabeled and 77.82% labeled attachment accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1513,1519,,,,,,,,,,,,,,,,WOS:000854193301084,0
C,"Fan, A; Gardent, C; Braud, C; Bordes, A",,,Assoc Computat Linguist,"Fan, Angela; Gardent, Claire; Braud, Chloe; Bordes, Antoine",,,Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4186,4196,,,,,,,,,,,,,,,,WOS:000854193304033,0
C,"Fan, LS; White, M; Sharma, E; Su, RS; Choubey, PK; Huang, RH; Wang, L",,,Assoc Computat Linguist,"Fan, Lisa; White, Marshall; Sharma, Eva; Su, Ruisi; Choubey, Prafulla Kumar; Huang, Ruihong; Wang, Lu",,,In Plain Sight: Media Bias Through the Lens of Factual Reporting,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans1 and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by finetuning BERT on our labeled data, indicating the challenges of the task and future directions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6343,6349,,,,,,,,,,,,,,,,WOS:000854193306075,0
C,"Hagiwara, M; Ito, T; Kuribayashi, T; Suzuki, J; Inui, K",,,ASSOC COMPUTAT LINGUIST,"Hagiwara, Masato; Ito, Takumi; Kuribayashi, Tatsuki; Suzuki, Jun; Inui, Kentaro",,,TEASPN: Framework and Protocol for Integrated Writing Assistance Environments,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Language technologies play a key role in assisting people with their writing. Although there has been steady progress in e.g., grammatical error correction (GEC), human writers are yet to benefit from this progress due to the high development cost of integrating with writing software. We propose TEASPN1, a protocol and an open-source framework for achieving integrated writing assistance environments. The protocol standardizes the way writing software communicates with servers that implement such technologies, allowing developers and researchers to integrate the latest developments in natural language processing (NLP) with low cost. As a result, users can enjoy the integrated experience in their favorite writing software. The results from experiments with human participants show that users use a wide range of technologies and rate their writing experience favorably, allowing them to write more fluent text.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,229,234,,,,,,,,,,,,,,,,WOS:000855231500039,0
C,"Han, RJ; Ning, Q; Peng, NY",,,Assoc Computat Linguist,"Han, Rujun; Ning, Qiang; Peng, Nanyun",,,Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10% and 6.8% on two benchmark datasets respectively.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,434,444,,,,,,,,,,,,,,,,WOS:000854193300041,0
C,"Huang, BX; Carley, KM",,,Assoc Computat Linguist,"Huang, Binxuan; Carley, Kathleen M.",,,Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect level sentiment classication aims to identify the sentiment expressed towards an aspect given a context sentence. Previous neural network based methods largely ignore the syntax structure in one sentence. In this paper, we propose a novel target-dependent graph attention network (TD-GAT) for aspect level sentiment classification, which explicitly utilizes the dependency relationship among words. Using the dependency graph, it propagates sentiment features directly from the syntactic context of an aspect target. In our experiments, we show our method outperforms multiple baselines with GloVe embeddings. We also demonstrate that using BERT representations further substantially boosts the performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5469,5477,,,,,,,,,,,,,,,,WOS:000854193305062,0
C,"Kiyono, S; Suzuki, J; Mita, M; Mizumoto, T; Inui, K",,,Assoc Computat Linguist,"Kiyono, Shun; Suzuki, Jun; Mita, Masato; Mizumoto, Tomoya; Inui, Kentaro",,,An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F-0.5 = 65.0) and the official test set of the BEA-2019 shared task (F-0.5 = 70.2) without making any modifications to the model architecture.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1236,1242,,,,,,,,,,,,,,,,WOS:000854193301041,0
C,"Lai, CT; Hong, YT; Chen, HY; Lu, CJ; Lin, SD",,,Assoc Computat Linguist,"Lai, Chih-Te; Hong, Yi-Te; Chen, Hong-You; Lu, Chi-Jen; Lin, Shou-De",,,Multiple Text Style Transfer by using Word-level Conditional Generative Adversarial Network with Two-Phase Training,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The objective of non-parallel text style transfer is to alter specific attributes (e.g. sentiment, mood, tense, politeness, etc) of a given text while preserving unrelated content. Adversarial training is a popular method to ensure the transferred sentences have the desired target styles. However, previous works often suffer from content leaking problem. In this paper, we propose a new adversarial training model with a word-level conditional architecture and a two-phase training procedure. By using a style-related condition architecture before generating a word, our model is able to maintain style-unrelated words while changing the others. By separating the training procedure into reconstruction and transfer phases, our model is able to balance the reconstruction and adversarial losses. We test our model on polarity sentiment transfer and multiple-attribute transfer tasks. The empirical results show that our model achieves comparable evaluation scores in both transfer accuracy and fluency but significantly outperforms other state-of-the-art models in content compatibility on three real-world datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3579,3584,,,,,,,,,,,,,,,,WOS:000854193303084,0
C,"Lai, T; Tran, QH; Bui, T; Kihara, D",,,Assoc Computat Linguist,"Lai, Tuan; Tran, Quan Hung; Bui, Trung; Kihara, Daisuke",,,A Gated Self-attention Memory Network for Answer Selection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Answer selection is an important research problem, with applications in many areas. Previous deep learning based approaches for the task mainly adopt the Compare-Aggregate architecture that performs word-level comparison followed by aggregation. In this work, we take a departure from the popular Compare-Aggregate architecture, and instead, propose a new gated self-attention memory network for the task. Combined with a simple transfer learning technique from a large-scale online corpus, our model outperforms previous methods by a large margin, achieving new state-of-the-art results on two standard answer selection datasets: TrecQA and WikiQA.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5953,5959,,,,,,,,,,,,,,,,WOS:000854193306021,0
C,"Liu, HY; Fang, L; Liu, Q; Chen, B; Lou, JG; Li, ZJ",,,Assoc Computat Linguist,"Liu, Haoyan; Fang, Lei; Liu, Qian; Chen, Bei; Lou, Jian-Guang; Li, Zhoujun",,,Leveraging Adjective-Noun Phrasing Knowledge for Comparison Relation Prediction in Text-to-SQL,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"One key component in text-to-SQL is to predict the comparison relations between columns and their values. To the best of our knowledge, no existing models explicitly introduce external common knowledge to address this problem, thus their capabilities of predicting comparison relations are limited beyond training data. In this paper, we propose to leverage adjective-noun phrasing knowledge mined from the web to predict the comparison relations in text-to-SQL. Experimental results on both the original and the re-split Spider dataset show that our approach achieves significant improvement over state-of-the-art methods on comparison relation prediction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3515,3520,,,,,,,,,,,,,,,,WOS:000854193303074,0
C,"Min, S; Chen, DQ; Hajishirzi, H; Zettlemoyer, L",,,Assoc Computat Linguist,"Min, Sewon; Chen, Danqi; Hajishirzi, Hannaneh; Zettlemoyer, Luke",,,A Discrete Hard EM Approach for Weakly Supervised Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TRIVIAQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2-10%, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2851,2864,,,,,,,,,,,,,,,,WOS:000854193303002,0
C,"Nagata, R",,,Assoc Computat Linguist,"Nagata, Ryo",,,Toward a Task of Feedback Comment Generation for Writing Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we introduce a novel task called feedback comment generation - a task of automatically generating feedback comments such as a hint or an explanatory note for writing learning for non-native learners of English. There has been almost no work on this task nor corpus annotated with feedback comments. We have taken the first step by creating learner corpora consisting of approximately 1,900 essays where all preposition errors are manually annotated with feedback comments. We have tested three baseline methods on the dataset, showing that a simple neural retrievalbased method sets a baseline performance with an F-measure of 0.34 to 0.41. Finally, we have looked into the results to explore what modifications we need to make to achieve better performance. We also have explored problems unaddressed in this work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3206,3215,,,,,,,,,,,,,,,,WOS:000854193303034,0
C,"Suhr, A; Yan, C; Schluger, J; Yu, S; Khader, H; Mouallem, M; Zhang, I; Artzi, Y",,,Assoc Computat Linguist,"Suhr, Alane; Yan, Claudia; Schluger, Jacob; Yu, Stanley; Khader, Hadi; Mouallem, Marwa; Zhang, Iris; Artzi, Yoav",,,Executing Instructions in Situated Collaborative Interactions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We study a collaborative scenario where a user not only instructs a system to complete tasks, but also acts alongside it. This allows the user to adapt to the system abilities by changing their language or deciding to simply accomplish some tasks themselves, and requires the system to effectively recover from errors as the user strategically assigns it new goals. We build a game environment to study this scenario, and learn to map user instructions to system actions. We introduce a learning approach focused on recovery from cascading errors between instructions, and modeling methods to explicitly reason about instructions with multiple goals. We evaluate with a new evaluation protocol using recorded interactions and online games with human users, and observe how users adapt to the system abilities.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2119,2130,,,,,,,,,,,,,,,,WOS:000854193302026,0
C,"Weller, O; Seppi, K",,,Assoc Computat Linguist,"Weller, Orion; Seppi, Kevin",,,Humor Detection: A Transformer Gets the Last Laugh,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Much previous work has been done in attempting to identify humor in text. In this paper we extend that capability by proposing a new task: assessing whether or not a joke is humorous. We present a novel way of approaching this problem by building a model that learns to identify humorous jokes based on ratings gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using these ratings to determine the level of humor, we then employ a Transformer architecture for its advantages in learning from sentence context. We demonstrate the effectiveness of this approach and show results that are comparable to human performance. We further demonstrate our model's increased capabilities on humor identification problems, such as the previously created datasets for short jokes and puns. These experiments show that this method outperforms all previous work done on these tasks, with an F-measure of 93.1% for the Puns dataset and 98.6% on the Short Jokes dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3621,3625,,,,,,,,,,,,,,,,WOS:000854193303090,0
C,"Winata, GI; Lin, ZJ; Shin, J; Liu, ZH; Fung, P",,,Assoc Computat Linguist,"Winata, Genta Indra; Lin, Zhaojiang; Shin, Jamin; Liu, Zihan; Fung, Pascale",,,Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In countries that speak multiple main languages, mixing up different languages within a conversation is commonly called code-switching. Previous works addressing this challenge mainly focused on word-level aspects such as word embeddings. However, in many cases, languages share common subwords, especially for closely related languages, but also for languages that are seemingly irrelevant. Therefore, we propose Hierarchical Meta-Embeddings (HME) that learn to combine multiple monolingual word-level and subword-level embeddings to create language-agnostic lexical representations. On the task of Named Entity Recognition for English-Spanish code-switching data, our model achieves the state-of-the-art performance in the multilingual settings. We also show that, in cross-lingual settings, our model not only leverages closely related languages, but also learns from languages with different roots. Finally, we show that combining different subunits are crucial for capturing codeswitching entities.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3541,3547,,,,,,,,,,,,,,,,WOS:000854193303078,0
C,"Xu, YF; Lin, Z; Liu, YX; Liu, R; Wang, WP; Meng, D",,,Assoc Computat Linguist,"Xu, Yanfu; Lin, Zheng; Liu, Yuanxin; Liu, Rui; Wang, Weiping; Meng, Dan",,,Ranking and Sampling in Open-Domain Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Open-domain question answering (OpenQA) aims to answer questions based on a number of unlabeled paragraphs. Existing approaches always follow the distantly supervised setup where some of the paragraphs are wrong-labeled (noisy), and mainly utilize the paragraph-question relevance to denoise. However, the paragraph-paragraph relevance, which may aggregate the evidence among relevant paragraphs, can also be utilized to discover more useful paragraphs. Moreover, current approaches mainly focus on the positive paragraphs which are known to contain the answer during training. This will affect the generalization ability of the model and make it be disturbed by the similar but irrelevant (distracting) paragraphs during testing. In this paper, we first introduce a ranking model leveraging the paragraph-question and the paragraph-paragraph relevance to compute a confidence score for each paragraph. Furthermore, based on the scores, we design a modified weighted sampling strategy for training to mitigate the influence of the noisy and distracting paragraphs. Experiments on three public datasets (Quasar-T, SearchQA and TriviaQA) show that our model advances the state of the art.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2412,2421,,,,,,,,,,,,,,,,WOS:000854193302053,0
C,"Zhang, R; Yu, T; Er, HY; Shim, S; Xue, ER; Lin, XV; Shi, TZ; Xiong, CM; Socher, R; Radev, D",,,Assoc Computat Linguist,"Zhang, Rui; Yu, Tao; Er, He Yang; Shim, Sungrok; Xue, Eric; Lin, Xi Victoria; Shi, Tianze; Xiong, Caiming; Socher, Richard; Radev, Dragomir",,,Editing-Based SQL Query Generation for Cross-Domain Context-Dependent Questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to error propagation. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at https://github.com/ ryanzhumich/sparc_atis_pytorch.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5338,5349,,,,,,,,,,,,,,,,WOS:000854193305050,0
C,"Zhu, J; Li, JH; Zhu, MH; Qian, LH; Zhang, M; Zhou, GD",,,Assoc Computat Linguist,"Zhu, Jie; Li, Junhui; Zhu, Muhua; Qian, Longhua; Zhang, Min; Zhou, Guodong",,,Modeling Graph Structure in Transformer for Better AMR-to-Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent studies on AMR-to-text generation often formalize the task as a sequence-tosequence (seq2seq) learning problem by converting an Abstract Meaning Representation (AMR) graph into a word sequence. Graph structures are further modeled into the seq2seq framework in order to utilize the structural information in the AMR graphs. However, previous approaches only consider the relations between directly connected concepts while ignoring the rich structure in AMR graphs. In this paper we eliminate such a strong limitation and propose a novel structure-aware selfattention approach to better modeling the relations between indirectly connected concepts in the state-of-the-art seq2seq model, i.e., the Transformer. In particular, a few different methods are explored to learn structural representations between two concepts. Experimental results on English AMR benchmark datasets show that our approach significantly outperforms the state of the art with 29.66 and 31.82 BLEU scores on LDC2015E86 and LDC2017T10, respectively. To the best of our knowledge, these are the best results achieved so far by supervised models on the benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5459,5468,,,,,,,,,,,,,,,,WOS:000854193305061,0
C,"Aralikatte, R; Lent, H; Gonzalez, AV; Hershcovich, D; Qiu, C; Sandholm, A; Ringaard, M; Sogaard, A",,,Assoc Computat Linguist,"Aralikatte, Rahul; Lent, Heather; Gonzalez, Ana Valeria; Hershcovich, Daniel; Qiu, Chen; Sandholm, Anders; Ringaard, Michael; Sogaard, Anders",,,Rewarding Coreference Resolvers for Being Consistent with World Knowledge,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Unresolved coreference is a bottleneck for relation extraction, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1229,1235,,,,,,,,,,,,,,,,WOS:000854193301040,0
C,"Chakravarti, R; Pendus, C; Sakrajda, A; Ferritto, A; Pan, L; Glass, M; Castelli, V; Murdock, W; Florian, R; Roukos, S; Sil, A",,,ASSOC COMPUTAT LINGUIST,"Chakravarti, Rishav; Pendus, Cezar; Sakrajda, Andrzej; Ferritto, Anthony; Pan, Lin; Glass, Michael; Castelli, Vittorio; Murdock, William; Florian, Radu; Roukos, Salim; Sil, Avirup",,,CFO: A Framework for Building Production NLP Systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper introduces a novel orchestration framework, called CFO (COMPUTATION FLOW ORCHESTRATOR), for building, experimenting with, and deploying interactive NLP (Natural Language Processing) and IR (Information Retrieval) systems to production environments. We then demonstrate a question answering system built using this framework which incorporates state-of-the-art BERT based MRC (Machine Reading Comprehension) with IR components to enable end-to-end answer retrieval. Results from the demo system are shown to be high quality in both academic and industry domain specific settings. Finally, we discuss best practices when (pre-)training BERT based MRC models for production systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,31,36,,,,,,,,,,,,,,,,WOS:000855231500006,0
C,"Chan, ZM; Chen, XY; Wang, YL; Li, JT; Zhang, ZQ; Gai, K; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Chan, Zhangming; Chen, Xiuying; Wang, Yongliang; Li, Juntao; Zhang, Zhiqiang; Gai, Kun; Zhao, Dongyan; Yan, Rui",,,Stick to Facts: Towards Fidelity-oriented Product Description Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Different from other text generation tasks, in product description generation, it is of vital importance to generate faithful descriptions that stick to the product attribute information. However, little attention has been paid to this problem. To bridge this gap we propose a model named Fidelity-oriented Product Description Generator (FPDG). FPDG takes the entity label of each word into account, since the product attribute information is always conveyed by entity words. Specifically, we first propose a Recurrent Neural Network (RNN) decoder based on the Entity-label-guided Long Short-Term Memory (ELSTM) cell, taking both the embedding and the entity label of each word as input. Second, we establish a keyword memory that stores the entity labels as keys and keywords as values, and FPDG will attend to keywords through attending to their entity labels. Experiments conducted a large-scale real-world product description dataset show that our model achieves the state-of-the-art performance in terms of both traditional generation metrics as well as human evaluations. Specifically, FPDG increases the fidelity of the generated descriptions by 25%.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4959,4968,,,,,,,,,,,,,,,,WOS:000854193305014,0
C,"Chen, XC; Lyu, CC; Titov, I",,,Assoc Computat Linguist,"Chen, Xinchi; Lyu, Chunchuan; Titov, Ivan",,,Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic role labeling (SRL) involves extracting propositions (i.e. predicates and their typed arguments) from natural language sentences. State-of-the-art SRL models rely on powerful encoders (e.g., LSTMs) and do not model non-local interaction between arguments. We propose a new approach to modeling these interactions while maintaining efficient inference. Specifically, we use Capsule Networks (Sabour et al., 2017): each proposition is encoded as a tuple of capsules, one capsule per argument type (i.e. role). These tuples serve as embeddings of entire propositions. In every network layer, the capsules interact with each other and with representations of words in the sentence. Each iteration results in updated proposition embeddings and updated predictions about the SRL structure. Our model substantially outperforms the nonrefinement baseline model on all 7 CoNLL-2019 languages and achieves state-of-the-art results on 5 languages (including English) for dependency SRL. We analyze the types of mistakes corrected by the refinement procedure. For example, each role is typically (but not always) filled with at most one argument. Whereas enforcing this approximate constraint is not useful with the modern SRL system, iterative procedure corrects the mistakes by capturing this intuition in a flexible and contextsensitive way.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5415,5425,,,,,,,,,,,,,,,,WOS:000854193305057,0
C,"Correia, GM; Niculae, V; Martins, AFT",,,Assoc Computat Linguist,"Correia, Goncalo M.; Niculae, Vlad; Martins, Andre F. T.",,,Adaptively Sparse Transformers,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter - which controls the shape and sparsity of alpha-entmax - allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2174,2184,,,,,,,,,,,,,,,,WOS:000854193302031,0
C,"Feldman, J; Davison, J; Rush, AM",,,Assoc Computat Linguist,"Feldman, Joshua; Davison, Joe; Rush, Alexander M.",,,Commonsense Knowledge Mining from Pretrained Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Inferring commonsense knowledge is a key challenge in natural language processing, but due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple's validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though this method performs worse on a test set than models explicitly trained on a corresponding training set, it outperforms these methods when mining commonsense knowledge from new sources, suggesting that unsupervised techniques may generalize better than current supervised approaches.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1173,1178,,,,,,,,,,,,,,,,WOS:000854193301031,0
C,"Fisch, A; Guo, J; Barzilay, R",,,Assoc Computat Linguist,"Fisch, Adam; Guo, Jiang; Barzilay, Regina",,,Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper explores the task of leveraging typology in the context of cross-lingual dependency parsing. While this linguistic information has shown great promise in preneural parsing, results for neural architectures have been mixed. The aim of our investigation is to better understand this stateof-the-art. Our main findings are as follows: 1) The benefit of typological information is derived from coarsely grouping languages into syntactically-homogeneous clusters rather than from learning to leverage variations along individual typological dimensions in a compositional manner; 2) Typology consistent with the actual corpus statistics yields better transfer performance; 3) Typological similarity is only a rough proxy of crosslingual transferability with respect to parsing.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5714,5720,,,,,,,,,,,,,,,,WOS:000854193305087,0
C,"Gao, YB; Wang, WY; Ney, H",,,Assoc Computat Linguist,"Gao, Yingbo; Wang, Weiyue; Ney, Hermann",,,uniblock: Scoring and Filtering Corpus with Unicode Block Information,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The preprocessing pipelines in Natural Language Processing usually involve a step of removing sentences consisted of illegal characters. The definition of illegal characters and the specific removal strategy depend on the task, language, domain, etc, which often lead to tiresome and repetitive scripting of rules. In this paper, we introduce a simple statistical method, uniblock1, to overcome this problem. For each sentence, uniblock generates a fixed-size feature vector using Unicode block information of the characters. A Gaussian mixture model is then estimated on some clean corpus using variational inference. The learned model can then be used to score sentences and filter corpus. We present experimental results on Sentiment Analysis, Language-Modeling and Machine Translation, and show the simplicity and effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1324,1329,,,,,,,,,,,,,,,,WOS:000854193301055,0
C,"Geiger, A; Cases, I; Karttunen, L; Potts, C",,,Assoc Computat Linguist,"Geiger, Atticus; Cases, Ignacio; Karttunen, Lauri; Potts, Christopher",,,Posing Fair Generalization Tasks for Natural Language Inference,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Deep learning models for semantics are generally evaluated using naturalistic corpora. Adversarial methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that good performance at these naturalistic tasks can hide serious shortcomings. However, we should insist that these evaluations be fair - that the models are given data sufficient to support the requisite kinds of generalization. In this paper, we define and motivate a formal notion of fairness in this sense. We then apply these ideas to natural language inference by constructing very challenging but provably fair artificial datasets and showing that standard neural models fail to generalize in the required ways; only task-specific models that jointly compose the premise and hypothesis are able to achieve high performance, and even these models do not solve the task perfectly.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4485,4495,,,,,,,,,,,,,,,,WOS:000854193304061,0
C,"Gu, JC; Ling, ZH; Zhu, XD; Liu, Q",,,Assoc Computat Linguist,"Gu, Jia-Chen; Ling, Zhen-Hua; Zhu, Xiaodan; Liu, Quan",,,Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper proposes a dually interactive matching network (DIM) for presenting the personalities of dialogue agents in retrieval-based chatbots. This model develops from the interactive matching network (IMN) which models the matching degree between a context composed of multiple utterances and a response candidate. Compared with previous persona fusion approaches which enhance the representation of a context by calculating its similarity with a given persona, the DIM model adopts a dual matching architecture, which performs interactive matching between responses and contexts and between responses and personas respectively for ranking response candidates. Experimental results on PERSONA-CHAT dataset show that the DIM model outperforms its baseline model, i.e., IMN with persona fusion, by a margin of 14.5% and outperforms the current state-of-the-art model by a margin of 27.7% in terms of top-1 accuracy hits@1.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1845,1854,,,,,,,,,,,,,,,,WOS:000854193302001,0
C,"Hua, XY; Wang, L",,,Assoc Computat Linguist,"Hua, Xinyu; Wang, Lu",,,Sentence-Level Content Planning and Style Specification for Neural Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Building effective text generation systems requires three critical components: content selection, text planning, and surface realization, and traditionally they are tackled as separate problems. Recent all-in-one style neural generation models have made impressive progress, yet they often produce outputs that are incoherent and unfaithful to the input. To address these issues, we present an end-to-end trained two-step generation model, where a sentence-level content planner first decides on the keyphrases to cover as well as a desired language style, followed by a surface realization decoder that generates relevant and coherent text. For experiments, we consider three tasks from domains with diverse topics and varying language styles: persuasive argument construction from Reddit, paragraph generation for normal and simple versions of Wikipedia, and abstract generation for scientific articles. Automatic evaluation shows that our system can significantly outperform competitive comparisons. Human judges further rate our system generated text as more fluent and correct, compared to the generations by its variants that do not consider language style.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,591,602,,,,,,,,,,,,,,,,WOS:000854193300055,0
C,"Huang, BX; Carley, KM",,,Assoc Computat Linguist,"Huang, Binxuan; Carley, Kathleen M.",,,A Hierarchical Location Prediction Neural Network for Twitter User Geolocation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Accurate estimation of user location is important for many online services. Previous neural network based methods largely ignore the hierarchical structure among locations. In this paper, we propose a hierarchical location prediction neural network for Twitter user geolocation. Our model first predicts the home country for a user, then uses the country result to guide the city-level prediction. In addition, we employ a character-aware word embedding layer to overcome the noisy information in tweets. With the feature fusion layer, our model can accommodate various feature combinations and achieves state-of-the-art results over three commonly used benchmarks under different feature settings. It not only improves the prediction accuracy but also greatly reduces the mean error distance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4732,4742,,,,,,,,,,,,,,,,WOS:000854193304084,0
C,"Huang, ZX; Shen, YL; Li, X; Wei, YA; Cheng, G; Zhou, L; Dai, XY; Qu, YZ",,,Assoc Computat Linguist,"Huang, Zixian; Shen, Yulin; Li, Xiao; Wei, Yuang; Cheng, Gong; Zhou, Lin; Dai, Xinyu; Qu, Yuzhong",,,GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School Level,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Scenario-based question answering (SQA) has attracted increasing research attention. It typically requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by a scenario. SQA widely exists in the medical, geography, and legal domains-both in practice and in the exams. In this paper, we introduce the GeoSQA dataset. It consists of 1,981 scenarios and 4,110 multiple-choice questions in the geography domain at high school level, where diagrams (e.g., maps, charts) have been manually annotated with natural language descriptions to benefit NLP research. Benchmark results on a variety of state-of-the-art methods for question answering, textual entailment, and reading comprehension demonstrate the unique challenges presented by SQA for future research.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5866,5871,,,,,,,,,,,,,,,,WOS:000854193306008,0
C,"Jiang, L; Hu, SH; Huang, MY; Wang, ZC; Yang, JJ; Ye, XJ; Zheng, W",,,ASSOC COMPUTAT LINGUIST,"Jiang, Lan; Hu, Shuhan; Huang, Mingyu; Wang, Zhichun; Yang, Jinjian; Ye, Xiaoju; Zheng, Wei",,,MAssistant: A Personal Knowledge Assistant for MOOC Learners,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Massive Open Online Courses (MOOCs) have developed rapidly and attracted large number of learners. In this work, we present MAssistant system, a personal knowledge assistant for MOOC learners. MAssistant helps users to trace the concepts they have learned in MOOCs, and to build their own concept graphs. There are three key components in MAssistant: (i) a large-scale concept graph built from open data sources, which contains concepts in various domains and relations among them; (ii) a browser extension which interacts with learners when they are watching video lectures, and presents important concepts to them; (iii) a web application allowing users to explore their personal concept graphs, which are built based on their learning activities on MOOCs. MAssistant will facilitate the knowledge management task for MOOC learners, and make the learning on MOOCs easier.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,133,138,,,,,,,,,,,,,,,,WOS:000855231500023,0
C,"Kurisinkel, LJ; Chen, NF",,,Assoc Computat Linguist,"Kurisinkel, Litton J.; Chen, Nancy F.",,,Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present set to ordered text, a natural language generation task applied to automatically generating discharge instructions from admission ICD (International Classification of Diseases) codes. This task differs from other natural language generation tasks in the following ways: (1) The input is a set of identifiable entities (ICD codes) where the relations between individual entities are not explicitly specified. (2) The output text is not a narrative description (e.g. news articles) composed from the input. Rather, inferences are made from the input (ICD codes, which represent diagnoses and clinical procedures) to generate the output (instructions). (3) There is an optimal order in which each sentence (instruction) should appear in the output. Unlike most other tasks, neither the input (ICD codes) nor their corresponding text representations of diagnoses and clinical procedures appear in the output, so the ordering of the output instructions needs to be learned in an unsupervised fashion. We hypothesize that each instruction in the output is mapped to a subset of ICD codes specified in the input. We propose a neural architecture that jointly models (a) subset selection: choosing relevant subsets from a set of input entities; (b) content ordering: learning the order of instructions; (c) text generation: representing the instructions corresponding to the selected subsets in natural language. In addition, we penalize redundancy during beam search to improve tractability for long text generation. We formulate the problem setup and conducted experiments using the MIMIC-III dataset. Our model outperforms baseline models in both BLEU scores and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6165,6175,,,,,,,,,,,,,,,,WOS:000854193306049,0
C,"Liang, YL; Meng, FD; Zhang, JC; Xu, JA; Chen, YF; Zhou, J",,,Assoc Computat Linguist,"Liang, Yunlong; Meng, Fandong; Zhang, Jinchao; Xu, Jinan; Chen, Yufeng; Zhou, Jie",,,A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Aspect based sentiment analysis (ABSA) aims to identify the sentiment polarity towards the given aspect in a sentence, while previous models typically exploit an aspectindependent (weakly associative) encoder for sentence representation generation. In this paper, we propose a novel Aspect-Guided Deep Transition model, named AGDT, which utilizes the given aspect to guide the sentence encoding from scratch with the speciallydesigned deep transition architecture. Furthermore, an aspect-oriented objective is designed to enforce AGDT to reconstruct the given aspect with the generated sentence representation. In doing so, our AGDT can accurately generate aspect-specific sentence representation, and thus conduct more accurate sentiment predictions. Experimental results on multiple SemEval datasets demonstrate the effectiveness of our proposed approach, which significantly outperforms the best reported results with the same setting.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5569,5580,,,,,,,,,,,,,,,,WOS:000854193305072,0
C,"Lowell, D; Lipton, ZC; Wallace, BC",,,Assoc Computat Linguist,"Lowell, David; Lipton, Zachary C.; Wallace, Byron C.",,,Practical Obstacles to Deploying Active Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Active learning (AL) is a widely-used training strategy for maximizing predictive performance subject to a fixed annotation budget. In AL one iteratively selects training examples for annotation, often those for which the current model is most uncertain (by some measure). The hope is that active sampling leads to better performance than would be achieved under independent and identically distributed (i.i.d.) random samples. While AL has shown promise in retrospective evaluations, these studies often ignore practical obstacles to its use. In this paper we show that while AL may provide benefits when used with specific models and for particular domains, the benefits of current approaches do not generalize reliably across models and tasks. This is problematic because in practice one does not have the opportunity to explore and compare alternative AL strategies. Moreover, AL couples the training dataset with the model used to guide its acquisition. We find that subsequently training a successor model with an actively-acquired dataset does not consistently outperform training on i.i.d. sampled data. Our findings raise the question of whether the downsides inherent to AL are worth the modest and inconsistent performance gains it tends to afford.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,21,30,,,,,,,,,,,,,,,,WOS:000854193300003,0
C,"Mabona, A; Rimell, L; Clark, S; Vlachos, A",,,Assoc Computat Linguist,"Mabona, Amandla; Rimell, Laura; Clark, Stephen; Vlachos, Andreas",,,Neural Generative Rhetorical Structure Parsing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Rhetorical structure trees have been shown to be useful for several document-level tasks including summarization and document classification. Previous approaches to RST parsing have used discriminative models; however, these are less sample efficient than generative models, and RST parsing datasets are typically small. In this paper, we present the first generative model for RST parsing. Our model is a document-level RNN grammar (RNNG) with a bottom-up traversal order. We show that, for our parser's traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing. We develop a novel beam search algorithm that keeps track of both structureand word-generating actions without exhibiting this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2284,2295,,,,,,,,,,,,,,,,WOS:000854193302041,0
C,"Newman-Griffis, D; Fosler-Lussier, E",,,ASSOC COMPUTAT LINGUIST,"Newman-Griffis, Denis; Fosler-Lussier, Eric",,,HARE: a Flexible Highlighting Annotator for Ranking and Exploration,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Exploration and analysis of potential data sources is a significant challenge in the application of NLP techniques to novel information domains. We describe HARE, a system for highlighting relevant information in document collections to support ranking and triage, which provides tools for post-processing and qualitative analysis for model development and tuning. We apply HARE to the use case of narrative descriptions of mobility information in clinical data, and demonstrate its utility in comparing candidate embedding features. We provide a web-based interface for annotation visualization and document ranking, with a modular backend to support interoperability with existing annotation tools.",,,,,"Newman-Griffis, Denis/AAT-3760-2020; Newman-Griffis, Denis/HGU-3908-2022","Newman-Griffis, Denis/0000-0002-0473-4226; Newman-Griffis, Denis/0000-0002-0473-4226; Fosler-Lussier, Eric/0000-0001-8004-5169",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,85,90,,,,,,,,,,,,,,,,WOS:000855231500015,0
C,"Patra, B; Moniz, JRA",,,Assoc Computat Linguist,"Patra, Barun; Moniz, Joel Ruben Antony",,,Weakly Supervised Attention Networks for Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The task of entity recognition has traditionally been modelled as a sequence labelling task. However, this usually requires a large amount of fine-grained data annotated at the token level, which in turn can be expensive and cumbersome to obtain. In this work, we aim to circumvent this requirement of word-level annotated data. To achieve this, we propose a novel architecture for entity recognition from a corpus containing weak binary presence/absence labels, which are relatively easier to obtain. We show that our proposed weakly supervised model, trained solely on a multi-label classification task, performs reasonably well on the task of entity recognition, despite not having access to any token-level ground truth data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6268,6273,,,,,,,,,,,,,,,,WOS:000854193306063,0
C,"Ren, S; Wu, Y; Liu, SJ; Zhou, M; Ma, S",,,Assoc Computat Linguist,"Ren, Shuo; Wu, Yu; Liu, Shujie; Zhou, Ming; Ma, Shuai",,,Explicit Cross-lingual Pre-training for Unsupervised Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pre-training has proven to be effective in unsupervised machine translation due to its ability to model deep context information in cross-lingual scenarios. However, the cross-lingual information obtained from shared BPE spaces is inexplicit and limited. In this paper, we propose a novel cross-lingual pre-training method for unsupervised machine translation by incorporating explicit cross-lingual training signals. Specifically, we first calculate cross-lingual n-gram embeddings and infer an n-gram translation table from them. With those n-gram translation pairs, we propose a new pre-training model called Cross-lingual Masked Language Model (CMLM), which randomly chooses source n-grams in the input text stream and predicts their translation candidates at each time step. Experiments show that our method can incorporate beneficial cross-lingual information into pre-trained models. Taking pre-trained CMLM models as the encoder and decoder, we significantly improve the performance of unsupervised machine translation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,770,779,,,,,,,,,,,,,,,,WOS:000854193300071,0
C,"Shen, T; Geng, XB; Qin, T; Guo, DY; Tang, DY; Duan, N; Long, GD; Jiang, DX",,,Assoc Computat Linguist,"Shen, Tao; Geng, Xiubo; Qin, Tao; Guo, Daya; Tang, Duyu; Duan, Nan; Long, Guodong; Jiang, Daxin",,,Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues: 1) errors in earlier subtasks will be propagated and negatively affect downstream ones; and 2) each subtask cannot naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed framework thus enables shared supervisions and alleviates the effect of error propagation. Experiments on a large-scale conversational question answering dataset containing 1.6M question answering pairs over 12.8M entities show that the proposed framework improves overall F1 score from 67% to 79% compared with previous state-of-the-art work.",,,,,,"Qin, Tao/0000-0002-9095-0776",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2442,2451,,,,,,,,,,,,,,,,WOS:000854193302056,0
C,"Wang, HZ; Henderson, J; Merlo, P",,,Assoc Computat Linguist,"Wang, Haozhou; Henderson, James; Merlo, Paola",,,Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information not only in a single language but also across different languages. Current unsupervised adversarial approaches show that it is possible to build a mapping matrix that aligns two sets of monolingual word embeddings without high quality parallel data, such as a dictionary or a sentence-aligned corpus. However, without an additional step of refinement, the preliminary mapping learnt by these methods is unsatisfactory, leading to poor performance for typologically distant languages. In this paper, we propose a weakly-supervised adversarial training method to overcome this limitation, based on the intuition that mapping across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4419,4430,,,,,,,,,,,,,,,,WOS:000854193304055,0
C,"Watanabe, T; Tamura, A; Ninomiya, T; Makino, T; Iwakura, T",,,Assoc Computat Linguist,"Watanabe, Taiki; Tamura, Akihiro; Ninomiya, Takashi; Makino, Takuya; Iwakura, Tomoya",,,Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,We propose a method to improve named entity recognition (NER) for chemical compounds using multi-task learning by jointly training a chemical NER model and a chemical compound paraphrase model. Our method enables the long short-term memory (LSTM) of the NER model to capture chemical compound paraphrases by sharing the parameters of the LSTM and character embeddings between the two models. The experimental results on the BioCreative IV's CHEMDNER task show that our method improves chemical NER and achieves state-of-the-art performance (+1.43 F-score).,,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6244,6249,,,,,,,,,,,,,,,,WOS:000854193306059,0
C,"Wu, LW; Rao, Y; Jin, HL; Nazir, A; Sun, L",,,Assoc Computat Linguist,"Wu, Lianwei; Rao, Yuan; Jin, Haolin; Nazir, Ambreen; Sun, Ling",,,Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, neural networks based on multi-task learning have achieved promising performance on fake news detection, which focus on learning shared features among tasks as complementary features to serve different tasks. However, in most of the existing approaches, the shared features are completely assigned to different tasks without selection, which may lead to some useless and even adverse features integrated into specific tasks. In this paper, we design a sifted multi-task learning method with a selected sharing layer for fake news detection. The selected sharing layer adopts gate mechanism and attention mechanism to filter and select shared feature flows between tasks. Experiments on two public and widely used competition datasets, i.e. RumourEval and PHEME, demonstrate that our proposed method achieves the state-of-the-art performance and boosts the F1-score by more than 0.87%, 1.31%, respectively.",,,,,,"Wu, Lianwei/0000-0003-1451-9295",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4644,4653,,,,,,,,,,,,,,,,WOS:000854193304076,0
C,"Yu, DA; Cohn, M; Yang, YM; Chen, CY; Wen, WM; Zhang, JP; Zhou, MY; Jesse, K; Chau, A; Bhowmick, A; Iyer, S; Sreenivasulu, G; Davidson, S; Bhandare, A; Yu, Z",,,ASSOC COMPUTAT LINGUIST,"Yu, Dian; Cohn, Michelle; Yang, Yi Mang; Chen, Chun-Yen; Wen, Weiming; Zhang, Jiaping; Zhou, Mingyang; Jesse, Kevin; Chau, Austin; Bhowmick, Antara; Iyer, Shreenath; Sreenivasulu, Giritheja; Davidson, Sam; Bhandare, Ashwin; Yu, Zhou",,,Gunrock: A Social Bot for Complex and Engaging Long Conversations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazonselected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction.",,,,,,"Cohn, Michelle/0000-0002-4847-1464",,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,79,84,,,,,,,,,,,,,,,,WOS:000855231500014,0
C,"Zhang, DY; Zhang, HT; Liu, XK; Lin, HF; Xia, F",,,Assoc Computat Linguist,"Zhang, Dongyu; Zhang, Heting; Liu, Xikai; Lin, Hongfei; Xia, Feng",,,Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Humor plays important role in human communication, which makes it important problem for natural language processing. Prior work on the analysis of humor focuses on whether text is humorous or not, or the degree of funniness, but this is insufficient to explain why it is funny. We therefore create a dataset on humor with 9,123 manually annotated jokes in Chinese. We propose a novel annotation scheme to give scenarios of how humor arises in text. Specifically, our annotations of linguistic humor not only contain the degree of funniness, like previous work, but they also contain key words that trigger humor as well as character relationship, scene, and humor categories. We report reasonable agreement between annotators. We also conduct an analysis and exploration of the dataset. To the best of our knowledge, we are the first to approach humor annotation for exploring the underlying mechanism of the use of humor, which may contribute to a significantly deeper analysis of humor. We also contribute with a scarce and valuable dataset, which we will release publicly.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6402,6407,,,,,,,,,,,,,,,,WOS:000854193306084,0
C,"Zou, YY; Lu, W",,,Assoc Computat Linguist,"Zou, Yanyan; Lu, Wei",,,Text2Math: End-to-end Parsing Text into Math Expressions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose Text2Math, a model for semantically parsing text into math expressions. The model can be used to solve different math related problems including arithmetic word problems (Roy and Roth, 2017; Liang et al., 2018) and equation parsing problems (Roy et al., 2016). Unlike previous approaches, we tackle the problem from an end-to-end structured prediction perspective where our algorithm aims to predict the complete math expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5327,5337,,,,,,,,,,,,,,,,WOS:000854193305049,0
C,"Cao, YX; Hu, ZK; Chua, TS; Liu, ZY; Ji, H",,,Assoc Computat Linguist,"Cao, Yixin; Hu, Zikun; Chua, Tat-Seng; Liu, Zhiyuan; Ji, Heng",,,Low-Resource Name Tagging Learned with Weakly Labeled Data,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency(1).",,,,,"li, zhiyuan/HGD-9581-2022","Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,261,270,,,,,,,,,,,,,,,,WOS:000854193300025,0
C,"Changpinyo, S; Pang, B; Sharma, P; Soricut, R",,,Assoc Computat Linguist,"Changpinyo, Soravit; Pang, Bo; Sharma, Piyush; Soricut, Radu",,,Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for transfer learning. In this paper, we examine the effect of decoupling box proposal and featurization for down-stream tasks. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective transfer learning and improved image captioning and visual question answering models, as measured on publiclyavailable benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1468,1474,,,,,,,,,,,,,,,,WOS:000854193301077,0
C,"Chinnappa, D; Murugan, S; Blanco, E",,,Assoc Computat Linguist,"Chinnappa, Dhivya; Murugan, Srikala; Blanco, Eduardo",,,Extracting Possessions from Social Media: Images Complement Language,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper describes a new dataset and experiments to determine whether authors of tweets possess the objects they tweet about. We work with 5,000 tweets and show that both humans and neural networks benefit from images in addition to text. We also introduce a simple yet effective strategy to incorporate visual information into any neural network beyond weights from pretrained networks. Specifically, we consider the tags identified in an image as an additional textual input, and leverage pretrained word embeddings as usually done with regular text. Experimental results show this novel strategy is beneficial.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,663,672,,,,,,,,,,,,,,,,WOS:000854193300061,0
C,"Crabbe, B; Popa-Fabre, M; Pallier, C",,,Assoc Computat Linguist,"Crabbe, Benoit; Popa-Fabre, Murielle; Pallier, Christophe",,,Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,This paper describes a method of variable beam size inference for Recurrent Neural Network Grammar (RNNG) by drawing inspiration from sequential Monte-Carlo methods such as particle filtering. The paper studies the relevance of such methods for speeding up the computations of direct generative parsing for RNNG. But it also studies the potential cognitive interpretation of the underlying representations built by the search method (beam activity) through analysis of neuro-imaging signal through correlations with the neuro-imaging signal during naturalistic text listening,,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1150,1160,,,,,,,,,,,,,,,,WOS:000854193301028,0
C,"Dodge, J; Schwartz, R; Peng, H; Smith, NA",,,Assoc Computat Linguist,"Dodge, Jesse; Schwartz, Roy; Peng, Hao; Smith, Noah A.",,,RNN Architecture Learning with Sparse Regularization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural models for NLP typically use large numbers of parameters to reach state-of-theart performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finitestate automata (WFSAs). We take advantage of rational RNNs' natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We show that sparsifying such models makes them easier to visualize, and we present models that rely exclusively on as few as three WFSAs after pruning more than 90% of the weights. We publicly release our code.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1179,1184,,,,,,,,,,,,,,,,WOS:000854193301032,0
C,"Dou, ZY; Hu, JJ; Anastasopoulos, A; Neubig, G",,,Assoc Computat Linguist,"Dou, Zi-Yi; Hu, Junjie; Anastasopoulos, Antonios; Neubig, Graham",,,Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1417,1422,,,,,,,,,,,,,,,,WOS:000854193301069,0
C,"Drozdov, A; Verga, P; Chen, YP; Iyyer, M; McCallum, A",,,Assoc Computat Linguist,"Drozdov, Andrew; Verga, Pat; Chen, Yi-Pei; Iyyer, Mohit; McCallum, Andrew",,,Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoencoders,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Understanding text often requires identifying meaningful constituent spans such as noun phrases and verb phrases. In this work, we show that we can effectively recover these types of labels using the learned phrase vectors from deep inside-outside recursive autoencoders (DIORA). Specifically, we cluster span representations to induce span labels. Additionally, we improve the model's labeling accuracy by integrating latent code learning into the training procedure. We evaluate this approach empirically through unsupervised labeled constituency parsing. Our method outperforms ELMo and BERT on two versions of the Wall Street Journal (WSJ) dataset and is competitive to prior work that requires additional human annotations, improving over a previous state-of-the-art system that depends on ground-truth part-of-speech tags by 5 absolute F1 points (19% relative error reduction).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1507,1512,,,,,,,,,,,,,,,,WOS:000854193301083,0
C,"Han, H; Choi, S; Park, H; Hwang, SW",,,Assoc Computat Linguist,"Han, Hojae; Choi, Seungtaek; Park, Haeju; Hwang, Seung-won",,,MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper studies the problem of non-factoid question answering, where the answer may span over multiple sentences. Existing solutions can be categorized into representationand interaction-focused approaches. We combine their complementary strength, by a hybrid approach allowing multi-granular interactions, but represented at word level, enabling an easy integration with strong word-level signals. Specifically, we propose MICRON: Multigranular Interaction for Contextualizing RepresentatiON, a novel approach which derives contextualized uni-gram representation from n-grams. Our contributions are as follows: First, we enable multi-granular matches between question and answer n-grams. Second, by contextualizing word representation with surrounding n-grams, MICRON can naturally utilize word-based signals for query term weighting, known to be effective in information retrieval. We validate MICRON in two public non-factoid question answering datasets: WikiPassageQA and InsuranceQA, showing our model achieves the state of the art among baselines with reported performances on both datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5890,5895,,,,,,,,,,,,,,,,WOS:000854193306012,0
C,"Le, M; Boureau, YL; Nickel, M",,,Assoc Computat Linguist,"Le, Matthew; Boureau, Y-Lan; Nickel, Maximilian",,,Revisiting the Evaluation of Theory of Mind through Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Theory of mind, i.e., the ability to reason about intents and beliefs of agents is an important task in artificial intelligence and central to resolving ambiguous references in natural language dialogue. In this work, we revisit the evaluation of theory of mind through question answering. We show that current evaluation methods are flawed and that existing benchmark tasks can be solved without theory of mind due to dataset biases. Based on prior work, we propose an improved evaluation protocol and dataset in which we explicitly control for data regularities via a careful examination of the answer space. We show that state-of-the-art methods which are successful on existing benchmarks fail to solve theory-of-mind tasks in our proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5872,5877,,,,,,,,,,,,,,,,WOS:000854193306009,0
C,"Li, XJ; Li, CY; Xia, QL; Bisk, Y; Celikyilmaz, A; Gao, JF; Smith, NA; Choi, Y",,,Assoc Computat Linguist,"Li, Xiujun; Li, Chunyuan; Xia, Qiaolin; Bisk, Yonatan; Celikyilmaz, Asli; Gao, Jianfeng; Smith, Noah A.; Choi, Yejin",,,Robust Navigation with Language Pretraining and Stochastic Sampling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Core to the vision-and-language navigation (VLN) challenge is building robust instruction representations and action decoding schemes, which can generalize well to previously unseen instructions and environments. In this paper, we report two simple but highly effective methods to address these challenges and lead to a new state-of-the-art performance. First, we adapt large-scale pretrained language models to learn text representations that generalize better to previously unseen instructions. Second, we propose a stochastic sampling scheme to reduce the considerable gap between the expert actions in training and sampled actions in test, so that the agent can learn to correct its own mistakes during long sequential action decoding. Combining the two techniques, we achieve a new state of the art on the Room-toRoom benchmark with 6% absolute gain over the previous best result (47% -> 53%) on the Success Rate weighted by Path Length metric.",,,,,,"Bisk, Yonatan/0000-0002-2111-9081",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1494,1499,,,,,,,,,,,,,,,,WOS:000854193301081,0
C,"Lin, HY; Lu, YJ; Han, XP; Sun, L; Dong, B; Jiang, SS",,,Assoc Computat Linguist,"Lin, Hongyu; Lu, Yaojie; Han, Xianpei; Sun, Le; Dong, Bin; Jiang, Shanshan",,,Gazetteer-Enhanced Attentive Neural Networks for Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current region-based NER models only rely on fully-annotated training data to learn effective region encoder, which often face the training data bottleneck. To alleviate this problem, this paper proposes Gazetteer-Enhanced Attentive Neural Networks, which can enhance region-based NER by learning name knowledge of entity mentions from easily-obtainable gazetteers, rather than only from fully-annotated data. Specially, we first propose an attentive neural network (ANN), which explicitly models the mention-context association and therefore is convenient for integrating externally-learned knowledge. Then we design an auxiliary gazetteer network, which can effectively encode name regularity of mentions only using gazetteers. Finally, the learned gazetteer network is incorporated into ANN for better NER. Experiments show that our ANN can achieve the state-of-the-art performance on ACE2005 named entity recognition benchmark. Besides, incorporating gazetteer network can further improve the performance and significantly reduce the requirement of training data.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6232,6237,,,,,,,,,,,,,,,,WOS:000854193306057,0
C,"Liu, C; Osama, M; de Andrade, A",,,Assoc Computat Linguist,"Liu, Chen; Osama, Muhammad; de Andrade, Anderson",,,DENS: A Dataset for Multi-class Emotion Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a new dataset for multi-class emotion analysis from long-form narratives in English. The Dataset for Emotions of Narrative Sequences (DENS) was collected from both classic literature available on Project Gutenberg and modern online narratives available on Wattpad, annotated using Amazon Mechanical Turk. A number of statistics and baseline benchmarks are provided for the dataset. Of the tested techniques, we find that the fine-tuning of a pre-trained BERT model achieves the best results, with an average micro-F1 score of 60.4%. Our results show that the dataset provides a novel opportunity in emotion analysis that requires moving beyond existing sentence-level techniques.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6293,6298,,,,,,,,,,,,,,,,WOS:000854193306067,0
C,"Liu, ZH; Shin, JM; Xu, Y; Winata, GI; Xu, P; Madotto, A; Fung, P",,,Assoc Computat Linguist,"Liu, Zihan; Shin, Jamin; Xu, Yan; Winata, Genta Indra; Xu, Peng; Madotto, Andrea; Fung, Pascale",,,Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to lowresource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1297,1303,,,,,,,,,,,,,,,,WOS:000854193301051,0
C,"Mihaylov, T; Frank, A",,,Assoc Computat Linguist,"Mihaylov, Todor; Frank, Anette",,,Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this work, we propose to use linguistic annotations as a basis for a Discourse-Aware Semantic Self-Attention encoder that we employ for reading comprehension on narrative texts. We extract relations between discourse units, events and their arguments as well as coreferring mentions, using available annotation tools. Our empirical evaluation shows that the investigated structures improve the overall performance (up to +3.4 Rouge-L), especially intra-sentential and cross-sentential discourse relations, sentence-internal semantic role relations, and long-distance coreference relations. We show that dedicating self-attention heads to intra-sentential relations and relations connecting neighboring sentences is beneficial for finding answers to questions in longer contexts. Our findings encourage the use of discourse-semantic annotations to enhance the generalization capacity of self-attention models for reading comprehension.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2541,2552,,,,,,,,,,,,,,,,WOS:000854193302065,0
C,"Ponti, EM; Vulic, I; Glavas, G; Reichart, R; Korhonen, A",,,Assoc Computat Linguist,"Ponti, Edoardo M.; Vulic, Ivan; Glavas, Goran; Reichart, Roi; Korhonen, Anna",,,Cross-lingual Semantic Specialization via Lexical Relation Induction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of constraints. However, this technique cannot be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps: 1) Inducing noisy constraints in the target language through automatic word translation; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of distributional vectors in the target language with the refined constraints. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages: lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2206,2217,,,,,,,,,,,,,,,,WOS:000854193302034,0
C,"Prabhu, A; Dognin, C; Singh, M",,,Assoc Computat Linguist,"Prabhu, Ameya; Dognin, Charles; Singh, Maneesh",,,Sampling Bias in Deep Active Classification: An Empirical Study,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny highquality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state-of-the-art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at: https://github.com/drimpossible/Sampling-Bias-Active-Learning",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4058,4068,,,,,,,,,,,,,,,,WOS:000854193304022,0
C,"Qiu, DL; Zhang, YZ; Feng, XW; Liao, XW; Jiang, WB; Lyu, YJ; Liu, K; Zhao, J",,,Assoc Computat Linguist,"Qiu, Delai; Zhang, Yuanzhe; Feng, Xinwei; Liao, Xiangwen; Jiang, Wenbin; Lyu, Yajuan; Liu, Kang; Zhao, Jun",,,Machine Reading Comprehension Using Structural Knowledge Graph-aware Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Leveraging external knowledge is an emerging trend in machine comprehension task. Previous work usually utilizes knowledge graphs such as ConceptNet as external knowledge, and extracts triples from them to enhance the initial representation of the machine comprehension context. However, such method cannot capture the structural information in the knowledge graph. To this end, we propose a Structural Knowledge Graph-aware Network (SKG) model, constructing sub-graphs for entities in the machine comprehension context. Our method dynamically updates the representation of the knowledge according to the structural information of the constructed sub-graph. Experiments show that SKG achieves state-ofthe-art performance on the ReCoRD dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5896,5901,,,,,,,,,,,,,,,,WOS:000854193306013,0
C,"Sap, M; Rashkin, H; Chen, D; Le Bras, R; Choi, Y",,,Assoc Computat Linguist,"Sap, Maarten; Rashkin, Hannah; Chen, Derek; Le Bras, Ronan; Choi, Yejin",,,SOCIAL IQA: Commonsense Reasoning about Social Interactions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce SOCIAL IQA, the first largescale benchmark for commonsense reasoning about social situations. SOCIAL IQA contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this? A: Make sure no one else could hear). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish SOCIAL IQA as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4463,4473,,,,,,,,,,,,,,,,WOS:000854193304059,0
C,"Schuster, T; Shah, DJ; Yeo, YJS; Filizzola, D; Santus, E; Barzilay, R",,,Assoc Computat Linguist,"Schuster, Tal; Shah, Darsh J.; Yeo, Yun Jie Serene; Filizzola, Daniel; Santus, Enrico; Barzilay, Regina",,,Towards Debiasing Fact Verification Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidenceaware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3419,3425,,,,,,,,,,,,,,,,WOS:000854193303059,0
C,"Shen, YL; Zeng, XY; Jin, HX",,,Assoc Computat Linguist,"Shen, Yilin; Zeng, Xiangyu; Jin, Hongxia",,,A Progressive Model to Enable Continual Learning for Semantic Slot Filling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on pre-collected data, it is crucial to continually improve the model after deployment to learn users' new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24% and 3.03% on two benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1279,1284,,,,,,,,,,,,,,,,WOS:000854193301048,0
C,"Sun, SL; Sun, QF; Zhou, K; Lv, T",,,Assoc Computat Linguist,"Sun, Shengli; Sun, Qingfeng; Zhou, Kevin; Lv, Tengchao",,,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most of the current effective methods for text classification task are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available. In this paper, we propose a hierarchical attention prototypical networks (HAPN) for few-shot text classification. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space. We verify the effectiveness of our model on two standard benchmark fewshot text classification datasets - FewRel and CSID, and achieve the state-of-the-art performance. The visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances separately. In addition, our attention mechanism increases support set augmentability and accelerates convergence speed in the training stage.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,476,485,,,,,,,,,,,,,,,,WOS:000854193300045,0
C,"Tsarfaty, R; Seker, A; Sadde, S; Klein, S",,,ASSOC COMPUTAT LINGUIST,"Tsarfaty, Reut; Seker, Amit; Sadde, Shoval; Klein, Stay",,,What's Wrong with Hebrew NLP? And How to Make it Right,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"For languages with simple morphology, such as English, automatic annotation pipelines such as spaCy or Stanford's CoreNLP successfully serve AI/DS projects in academia and the industry. For many morphologically-rich languages (MRLs), similar pipelines show suboptimal performance that limits their applicability for text analysis in research and commerical use. The suboptimal performance is mainly due to errors in early morphological disambiguation decisions, which cannot be recovered later in the pipeline, yielding incoherent annotations on the whole. In this paper we describe the design and use of the ONLP suite, a joint morpho-syntactic parsing framework for processing Modern Hebrew texts. The joint inference over morphology and syntax substantially limits error propagation, and leads to high accuracy. ONLP provides rich and expressive output which already serves diverse academic and commercial needs. Its accompanying demo further serves educational activities, introducing Hebrew NLP intricacies to researchers and non-researchers alike.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,259,264,,,,,,,,,,,,,,,,WOS:000855231500044,0
C,"Tseng, BH; Rei, M; Budzianowski, P; Turner, RE; Byrne, B; Korhonen, A",,,Assoc Computat Linguist,"Tseng, Bo-Hsiang; Rei, Marek; Budzianowski, Pawel; Turner, Richard E.; Byrne, Bill; Korhonen, Anna",,,Semi-Supervised Bootstrapping of Dialogue State Trackers for Task-Oriented Modelling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dialogue systems benefit greatly from optimizing on detailed annotations, such as transcribed utterances, internal dialogue state representations and dialogue act labels. However, collecting these annotations is expensive and time-consuming, holding back development in the area of dialogue modelling. In this paper, we investigate semi-supervised learning methods that are able to reduce the amount of required intermediate labelling. We find that by leveraging un-annotated data instead, the amount of turn-level annotations of dialogue state can be significantly reduced when building a neural dialogue system. Our analysis on the MultiWOZ corpus, covering a range of domains and topics, finds that annotations can be reduced by up to 30% while maintaining equivalent system performance. We also describe and evaluate the first end-to-end dialogue model created for the MultiWOZ corpus.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1273,1278,,,,,,,,,,,,,,,,WOS:000854193301047,0
C,"Wadden, D; Wennberg, U; Luan, Y; Hajishirzi, H",,,Assoc Computat Linguist,"Wadden, David; Wennberg, Ulme; Luan, Yi; Hajishirzi, Hannaneh",,,"Entity, Relation, and Event Extraction with Contextualized Span Representations",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DYGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5784,5789,,,,,,,,,,,,,,,,WOS:000854193305098,0
C,"Wei, PH; Xu, N; Mao, WJ",,,Assoc Computat Linguist,"Wei, Penghui; Xu, Nan; Mao, Wenji",,,Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatically verifying rumorous information has become an important and challenging task in natural language processing and social media analytics. Previous studies reveal that people's stances towards rumorous messages can provide indicative clues for identifying the veracity of rumors, and thus determining the stances of public reactions is a crucial preceding step for rumor veracity prediction. In this paper, we propose a hierarchical multi-task learning framework for jointly predicting rumor stance and veracity on Twitter, which consists of two components. The bottom component of our framework classifies the stances of tweets in a conversation discussing a rumor via modeling the structural property based on a novel graph convolutional network. The top component predicts the rumor veracity by exploiting the temporal dynamics of stance evolution. Experimental results on two benchmark datasets show that our method outperforms previous methods in both rumor stance classification and veracity prediction.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4787,4798,,,,,,,,,,,,,,,,WOS:000854193304089,0
C,"Xu, QK; Xu, CC; Qu, LZ",,,ASSOC COMPUTAT LINGUIST,"Xu, Qiongkai; Xu, Chenchen; Qu, Lizhen",,,ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,13,18,,,,,,,,,,,,,,,,WOS:000855231500003,0
C,"Yang, Y; Zhou, DY; He, YL; Zhang, M",,,Assoc Computat Linguist,"Yang, Yang; Zhou, Deyu; He, Yulan; Zhang, Meng",,,Interpretable Relevant Emotion Ranking with Event-Driven Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multiple emotions with different intensities are often evoked by events described in documents. Oftentimes, such event information is hidden and needs to be discovered from texts. Unveiling the hidden event information can help to understand how the emotions are evoked and provide explainable results. However, existing studies often ignore the latent event information. In this paper, we proposed a novel interpretable relevant emotion ranking model with the event information incorporated into a deep learning architecture using the event-driven attentions. Moreover, corpuslevel event embeddings and document-level event distributions are introduced respectively to consider the global events in corpus and the document-specific events simultaneously. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label approaches. Moreover, interpretable results can be obtained to shed light on the events which trigger certain emotions.",,,,,,"SONG, Won Wook/0000-0002-8530-2184",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,177,187,,,,,,,,,,,,,,,,WOS:000854193300017,0
C,"Zhao, ZJ; Zhu, S; Yu, K",,,Assoc Computat Linguist,"Zhao, Zijian; Zhu, Su; Yu, Kai",,,Data Augmentation with Atomic Templates for Spoken Language Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Spoken Language Understanding (SLU) converts user utterances into structured semantic representations. Data sparsity is one of the main obstacles of SLU due to the high cost of human annotation, especially when domain changes or a new domain comes. In this work, we propose a data augmentation method with atomic templates for SLU, which involves minimum human efforts. The atomic templates produce exemplars for fine-grained constituents of semantic representations. We propose an encoder-decoder model to generate the whole utterance from atomic exemplars. Moreover, the generator could be transferred from source domains to help a new domain which has little data. Experimental results show that our method achieves significant improvements on DSTC 2&3 dataset which is a domain adaptation setting of SLU.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3637,3643,,,,,,,,,,,,,,,,WOS:000854193303093,0
C,"Zheng, BG; Zheng, RJ; Ma, MB; Huang, L",,,Assoc Computat Linguist,"Zheng, Baigong; Zheng, Renjie; Ma, Mingbo; Huang, Liang",,,Simpler and Faster Learning of Adaptive Policies for Simultaneous Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Simultaneous translation is widely useful but remains challenging. Previous work falls into two main categories: (a) fixed-latency policies such as Ma et al. (2019) and (b) adaptive policies such as Gu et al. (2017). The former are simple and effective, but have to aggressively predict future content due to diverging source-target word order; the latter do not anticipate, but suffer from unstable and inefficient training. To combine the merits of both approaches, we propose a simple supervised-learning framework to learn an adaptive policy from oracle READ/WRITE sequences generated from parallel text. At each step, such an oracle sequence chooses to WRITE the next target word if the available source sentence context provides enough information to do so, otherwise READ the next source word. Experiments on German>English show that our method, without retraining the underlying NMT model, can learn flexible policies with better BLEU scores and similar latencies compared to previous work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1349,1354,,,,,,,,,,,,,,,,WOS:000854193301059,0
C,"Zlatkova, D; Nakov, P; Koychev, I",,,Assoc Computat Linguist,"Zlatkova, Dimitrina; Nakov, Preslav; Koychev, Ivan",,,Fact-Checking Meets Fauxtography: Verifying Claims About Images,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The recent explosion of false claims in social media and on the Web in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about images. This is despite the fact that visual imagery is more influential than text and naturally appears alongside fake news. Here we aim at bridging this gap. In particular, we create a new dataset for this problem, and we explore a variety of features modeling the claim, the image, and the relationship between the claim and the image. The evaluation results show sizable improvements over the baseline. We release our dataset, hoping to enable further research on fact-checking claims about images.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2099,2108,,,,,,,,,,,,,,,,WOS:000854193302024,0
C,"Bapna, A; Firat, O",,,Assoc Computat Linguist,"Bapna, Ankur; Firat, Orhan",,,"Simple, Scalable Adaptation for Neural Machine Translation",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1538,1548,,,,,,,,,,,,,,,,WOS:000854193301087,0
C,"Chaudhary, A; Xie, JT; Sheikh, Z; Neubig, G; Carbonell, JG",,,Assoc Computat Linguist,"Chaudhary, Aditi; Xie, Jiateng; Sheikh, Zaid; Neubig, Graham; Carbonell, Jaime G.",,,A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now several proposed approaches involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. This paper poses the question: given this recent progress, and limited human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we find a dual-strategy approach best, starting with a crosslingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5164,5174,,,,,,,,,,,,,,,,WOS:000854193305033,0
C,"Chauhan, DS; Akhtar, MS; Ekbal, A; Bhattacharyya, P",,,Assoc Computat Linguist,"Chauhan, Dushyant Singh; Akhtar, Md Shad; Ekbal, Asif; Bhattacharyya, Pushpak",,,Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In recent times, multi-modal analysis has been an emerging and highly sought-after field at the intersection of natural language processing, computer vision, and speech processing. The prime objective of such studies is to leverage the diversified information, (e.g., textual, acoustic and visual), for learning a model. The effective interaction among these modalities often leads to a better system in terms of performance. In this paper, we introduce a recurrent neural network based approach for the multi-modal sentiment and emotion analysis. The proposed model learns the inter-modal interaction among the participating modalities through an auto-encoder mechanism. We employ a context-aware attention module to exploit the correspondence among the neighboring utterances. We evaluate our proposed approach for five standard multi-modal affect analysis datasets. Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state-of-the-art systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5647,5657,,,,,,,,,,,,,,,,WOS:000854193305079,0
C,"Dasigi, P; Liu, NF; Marasovic, A; Smith, NA; Gardner, M",,,Assoc Computat Linguist,"Dasigi, Pradeep; Liu, Nelson F.; Marasovic, Ana; Smith, Noah A.; Gardner, Matt",,,QUOREF: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark-the best model performance is 70.5 F-1, while the estimated human performance is 93.4 F-1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5925,5932,,,,,,,,,,,,,,,,WOS:000854193306017,0
C,"Dodge, J; Gururangan, S; Card, D; Schwartz, R; Smith, NA",,,Assoc Computat Linguist,"Dodge, Jesse; Gururangan, Suchin; Card, Dallas; Schwartz, Roy; Smith, Noah A.",,,Show Your Work: Improved Reporting of Experimental Results,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparison, and provide code to allow researchers to use our technique.(1)",,,,,,"Card, Dallas/0000-0001-5573-8836",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2185,2194,,,,,,,,,,,,,,,,WOS:000854193302032,0
C,"Dong, Z; Sun, SZ; Liu, HZ; Lou, JG; Zhang, DM",,,Assoc Computat Linguist,"Dong, Zhen; Sun, Shizhao; Liu, Hongzhi; Lou, Jian-Guang; Zhang, Dongmei",,,Data-Anonymous Encoding for Text-to-SQL Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"On text-to-SQL generation, the input utterance usually contains lots of tokens that are related to column names or cells in the table, called table-related tokens. These table-related tokens are troublesome for the downstream neural semantic parser because it brings complex semantics and hinders the sharing across the training examples. However, existing approaches either ignore handling these tokens before the semantic parser or simply use deterministic approaches based on string-match or word embedding similarity. In this work, we propose a more efficient approach to handle table-related tokens before the semantic parser. First, we formulate it as a sequential tagging problem and propose a two-stage anonymization model to learn the semantic relationship between tables and input utterances. Then, we leverage the implicit supervision from SQL queries by policy gradient to guide the training. Experiments demonstrate that our approach consistently improves performances of different neural semantic parsers and significantly outperforms deterministic approaches.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5405,5414,,,,,,,,,,,,,,,,WOS:000854193305056,0
C,"Galle, M",,,Assoc Computat Linguist,"Galle, Matthias",,,Investigating the Effectiveness of BPE: The Power of Shorter Sequences,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Byte-Pair Encoding (BPE) is an unsupervised sub-word tokenization technique, commonly used in neural machine translation and other NLP tasks. Its effectiveness makes it a de facto standard, but the reasons for this are not well understood. We link BPE to the broader family of dictionary-based compression algorithms and compare it with other members of this family. Our experiments across datasets, language pairs, translation models, and vocabulary size show that - given a fixed vocabulary size budget - the fewer tokens an algorithm needs to cover the test set, the better the translation (as measured by BLEU).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1375,1381,,,,,,,,,,,,,,,,WOS:000854193301063,0
C,"Garg, S; Galstyan, A; Ver Steeg, G; Cecchi, G",,,Assoc Computat Linguist,"Garg, Sahil; Galstyan, Aram; Ver Steeg, Greg; Cecchi, Guillermo",,,Nearly-Unsupervised Hashcode Representations for Biomedical Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, kernelized locality sensitive hash-codes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifier following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",,,,,,"Galstyan, Aram/0000-0003-4215-0886",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4026,4036,,,,,,,,,,,,,,,,WOS:000854193304019,0
C,"Kruk, J; Lubin, J; Sikka, K; Lin, X; Jurafsky, D; Divakaran, A",,,Assoc Computat Linguist,"Kruk, Julia; Lubin, Jonah; Sikka, Karan; Lin, Xiao; Jurafsky, Dan; Divakaran, Ajay",,,Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Computing author intent from multimodal data like Instagram posts requires modeling a complex relationship between text and image. For example, a caption might evoke an ironic contrast with the image, so neither caption nor image is a mere transcript of the other. Instead they combine-via what has been called meaning multiplication Bateman (2014)-to create a new meaning that has a more complex relation to the literal meanings of text and image. Here we introduce a multimodal dataset of 1299 Instagram posts labeled for three orthogonal taxonomies: the authorial intent behind the image-caption pair, the contextual relationship between the literal meanings of the image and caption, and the semiotic relationship between the signified meanings of the image and caption. We build a baseline deep multimodal classifier to validate the taxonomy, showing that employing both text and image improves intent detection by 9:6% compared to using only the image modality, demonstrating the commonality of non-intersective meaning multiplication. The gain with multimodality is greatest when the image and caption diverge semiotically. Our dataset offers a new resource for the study of the rich meanings that result from pairing text and image.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4622,4632,,,,,,,,,,,,,,,,WOS:000854193304074,0
C,"Lee, D",,,Assoc Computat Linguist,"Lee, Dongjun",,,Clause-Wise and Recursive Decoding for Complex and Cross-Domain Text-to-SQL Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries over a single table. We focus on the Spider dataset, a complex and crossdomain text-to-SQL task, which includes complex queries over multiple tables. In this paper, we propose a SQL clause-wise decoding neural architecture with a self-attention based database schema encoder to address the Spider task. Each of the clause-specific decoders consists of a set of sub-modules, which is defined by the syntax of each clause. Additionally, our model works recursively to support nested queries. When evaluated on the Spider dataset, our approach achieves 4.6% and 9.8% accuracy gain in the test and dev sets, respectively. In addition, we show that our model is significantly more effective at predicting complex and nested queries than previous work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6045,6051,,,,,,,,,,,,,,,,WOS:000854193306035,0
C,"Li, B; He, JX; Neubig, G; Berg-Kirkpatrick, T; Yang, YM",,,Assoc Computat Linguist,"Li, Bohan; He, Junxian; Neubig, Graham; Berg-Kirkpatrick, Taylor; Yang, Yiming",,,A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling. (1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3603,3614,,,,,,,,,,,,,,,,WOS:000854193303088,0
C,"Liu, YJ; Meng, FD; Zhang, JC; Zhou, J; Chen, YF; Xu, JA",,,Assoc Computat Linguist,"Liu, Yijin; Meng, Fandong; Zhang, Jinchao; Zhou, Jie; Chen, Yufeng; Xu, Jinan",,,CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize cooccurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1051,1060,,,,,,,,,,,,,,,,WOS:000854193301019,0
C,"Luo, L; Ao, X; Song, Y; Pan, FY; Yang, M; He, Q",,,Assoc Computat Linguist,"Luo, Ling; Ao, Xiang; Song, Yan; Pan, Feiyang; Yang, Min; He, Qing",,,Reading Like HER: Human Reading Inspired Extractive Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this work, we re-examine the problem of extractive text summarization for long documents. We observe that the process of extracting summarization of human can be divided into two stages: 1) a rough reading stage to look for sketched information, and 2) a subsequent careful reading stage to select key sentences to form the summary. By simulating such a two-stage process, we propose a novel approach for extractive summarization. We formulate the problem as a contextualbandit problem and solve it with policy gradient. We adopt a convolutional neural network to encode gist of paragraphs for rough reading, and a decision making policy with an adapted termination mechanism for careful reading. Experiments on the CNN and Daily-Mail datasets show that our proposed method can provide high-quality summaries with varied length, and significantly outperform the state-of-the-art extractive methods in terms of ROUGE metrics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3033,3043,,,,,,,,,,,,,,,,WOS:000854193303018,0
C,"Majumder, BP; Li, SY; Ni, JM; McAuley, J",,,Assoc Computat Linguist,"Majumder, Bodhisattwa Prasad; Li, Shuyang; Ni, Jianmo; McAuley, Julian",,,Generating Personalized Recipes from Historical User Preferences,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing approaches to recipe generation are unable to create recipes for users with culinary preferences but incomplete knowledge of ingredients in specific dishes. We propose a new task of personalized recipe generation to help these users: expanding a name and incomplete ingredient details into complete natural-text instructions aligned with the user's historical preferences. We attend on technique- and recipe-level representations of a user's previously consumed recipes, fusing these `useraware' representations in an attention fusion layer to control recipe text generation. Experiments on a new dataset of 180K recipes and 700K interactions show our model's ability to generate plausible and personalized recipes compared to non-personalized baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5976,5982,,,,,,,,,,,,,,,,WOS:000854193306024,0
C,"Mao, YN; Tian, JJ; Han, JW; Rena, X",,,Assoc Computat Linguist,"Mao, Yuning; Tian, Jingjing; Han, Jiawei; Rena, Xiang",,,Hierarchical Text Classification with Reinforced Label Assignment,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While existing hierarchical text classification (HTC) methods attempt to capture label hierarchies for model training, they either make local decisions regarding each label or completely ignore the hierarchy information during inference. To solve the mismatch between training and inference as well as modeling label dependencies in a more principled way, we formulate HTC as a Markov decision process and propose to learn a Label Assignment Policy via deep reinforcement learning to determine where to place an object and when to stop the assignment process. The proposed method, HiLAP, explores the hierarchy during both training and inference time in a consistent manner and makes inter-dependent decisions. As a general framework, HiLAP can incorporate different neural encoders as base models for end-to-end training. Experiments on five public datasets and four base models show that HiLAP yields an average improvement of 33.4% in Macro-F1 over flat classifiers and outperforms state-of-the-art HTC methods by a large margin.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,445,455,,,,,,,,,,,,,,,,WOS:000854193300042,0
C,"Murahari, V; Chattopadhyay, P; Batra, D; Parikh, D; Das, A",,,Assoc Computat Linguist,"Murahari, Vishvak; Chattopadhyay, Prithvijit; Batra, Dhruv; Parikh, Devi; Das, Abhishek",,,Improving Generative Visual Dialog by Answering Diverse Questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Prior work on training generative Visual Dialog models with reinforcement learning (Das et al., 2017b) has explored a Q- BOT-A-BOT image-guessing game and shown that this `self-talk' approach can lead to improved performance at the downstream dialogconditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of interaction, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between Q- BOT and A-BOT during self-talk, which are not informative with respect to the image. To improve this, we devise a simple auxiliary objective that incentivizes Q-BOT to ask diverse questions, thus reducing repetitions and in turn enabling A-BOT to explore a larger state space during RL i.e. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of automatic metrics and human studies, and demonstrate that it leads to better dialog, i.e. dialog that is more diverse (i.e. less repetitive), consistent (i.e. has fewer conflicting exchanges), fluent (i.e. more humanlike), and detailed, while still being comparably image-relevant as prior work and ablations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1449,1454,,,,,,,,,,,,,,,,WOS:000854193301074,0
C,"Papasarantopoulos, N; Frermann, L; Lapata, M; Cohen, SB",,,Assoc Computat Linguist,"Papasarantopoulos, Nikos; Frermann, Lea; Lapata, Mirella; Cohen, Shay B.",,,Partners in Crime: Multi-view Sequential Inference for Movie Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multi-view learning algorithms are powerful representation learning tools, often exploited in the context of multimodal problems. However, for problems requiring inference at the token-level of a sequence (that is, a separate prediction must be made for every time step), it is often the case that single-view systems are used, or that more than one views are fused in a simple manner. We describe an incremental neural architecture paired with a novel training objective for incremental inference. The network operates on multi-view data. We demonstrate the effectiveness of our approach on the problem of predicting perpetrators in crime drama series, for which our model significantly outperforms previous work and strong baselines. Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2057,2067,,,,,,,,,,,,,,,,WOS:000854193302020,0
C,"Rabinovich, E; Sultani, M; Stevenson, S",,,Assoc Computat Linguist,"Rabinovich, Ella; Sultani, Masih; Stevenson, Suzanne",,,CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In contrast to many decades of research on oral code-switching, the study of written multilingual productions has only recently enjoyed a surge of interest. Many open questions remain regarding the sociolinguistic underpinnings of written code-switching, and progress has been limited by a lack of suitable resources. We introduce a novel, large, and diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far. We investigate whether findings in oral code-switching concerning content and style, as well as speaker proficiency, are carried over into written code-switching in discussion forums. The released dataset can further facilitate a range of research and practical activities.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4776,4786,,,,,,,,,,,,,,,,WOS:000854193304088,0
C,"Scialom, T; Lamprier, S; Piwowarski, B; Staiano, J",,,Assoc Computat Linguist,"Scialom, Thomas; Lamprier, Sylvain; Piwowarski, Benjamin; Staiano, Jacopo",,,Answers Unite! Unsupervised Metrics for Reinforced Summarization Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A Abstractive summarization approaches based on Reinforcement Learning (RL) have recently been proposed to overcome classical likelihood maximization. RL enables to consider complex, possibly non-differentiable, metrics that globally assess the quality and relevance of the generated outputs. ROUGE, the most used summarization metric, is known to suffer from bias towards lexical similarity as well as from suboptimal accounting for fluency and readability of the generated abstracts. We thus explore and propose alternative evaluation measures: the reported human-evaluation analysis shows that the proposed metrics, based on Question Answering, favorably compares to ROUGE - with the additional property of not requiring reference summaries. Training a RL-based model on these metrics leads to improvements (both in terms of human or automated metrics) over current approaches that use ROUGE as a reward.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3246,3256,,,,,,,,,,,,,,,,WOS:000854193303038,0
C,"Soto, CX; Yoo, S",,,Assoc Computat Linguist,"Soto, Carlos X.; Yoo, Shinjae",,,Visual Detection with Context for Document Layout Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present 1) a work in progress method to visually segment key regions of scientific articles using an object detection technique augmented with contextual features, and 2) a novel dataset of region-labeled articles. A continuing challenge in scientific literature mining is the difficulty of consistently extracting high-quality text from formatted PDFs. To address this, we adapt the object-detection technique Faster R-CNN for document layout detection, incorporating contextual information that leverages the inherently localized nature of article contents to improve the region detection performance. Due to the limited availability of high-quality region-labels for scientific articles, we also contribute a novel dataset of region annotations, the first version of which covers 9 region classes and 822 article pages. Initial experimental results demonstrate a 23.9% absolute improvement in mean average precision over the baseline model by incorporating contextual features, and a processing speed 14x faster than a text-based technique. Ongoing work on further improvements is also discussed.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3464,3470,,,,,,,,,,,,,,,,WOS:000854193303066,0
C,"Strzyz, M; Vilares, D; Gomez-Rodriguez, C",,,Assoc Computat Linguist,"Strzyz, Michalina; Vilares, David; Gomez-Rodriguez, Carlos",,,Towards Making a Dependency Parser See,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We explore whether it is possible to leverage eye-tracking data in an RNN dependency parser (for English) when such information is only available during training - i.e. no aggregated or token-level gaze features are used at inference time. To do so, we train a multitask learning model that parses sentences as sequence labeling and leverages gaze features as auxiliary tasks. Our method also learns to train from disjoint datasets, i.e. it can be used to test whether already collected gaze features are useful to improve the performance on new non-gazed annotated treebanks. Accuracy gains are modest but positive, showing the feasibility of the approach. It can serve as a first step towards architectures that can better leverage eye-tracking data or other complementary information available only for training sentences, possibly leading to improvements in syntactic parsing.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1500,1506,,,,,,,,,,,,,,,,WOS:000854193301082,0
C,"Tan, X; Chen, JL; He, D; Xia, YC; Qin, T; Liu, TY",,,Assoc Computat Linguist,"Tan, Xu; Chen, Jiale; He, Di; Xia, Yingce; Qin, Tao; Liu, Tie-Yan",,,Multilingual Neural Machine Translation with Language Clustering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,963,973,,,,,,,,,,,,,,,,WOS:000854193301011,0
C,"Urbanek, J; Fan, A; Karamcheti, S; Jain, S; Humeau, S; Dinan, E; Rocktaschel, T; Kiela, D; Szlam, A; Weston, J",,,Assoc Computat Linguist,"Urbanek, Jack; Fan, Angela; Karamcheti, Siddharth; Jain, Saachi; Humeau, Samuel; Dinan, Emily; Rocktaschel, Tim; Kiela, Douwe; Szlam, Arthur; Weston, Jason",,,Learning to Speak and Act in a Fantasy Text Adventure Game,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to e ffectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their a ffordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,673,683,,,,,,,,,,,,,,,,WOS:000854193300062,0
C,"Wallace, E; Tuyls, J; Wang, JL; Subramanian, S; Gardner, M; Singh, S",,,ASSOC COMPUTAT LINGUIST,"Wallace, Eric; Tuyls, Jens; Wang, Junlin; Subramanian, Sanjay; Gardner, Matt; Singh, Sameer",,,AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural NLP models are increasingly accurate but are imperfect and opaque-they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit's flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp. org/interpret.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,7,12,,,,,,,,,,,,,,,,WOS:000855231500002,0
C,"Wang, YS; Lee, HY; Chen, YN",,,Assoc Computat Linguist,"Wang, Yau-Shian; Lee, Hung-Yi; Chen, Yun-Nung",,,Tree Transformer: Integrating Tree Structures into Self-Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed Constituent Attention module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1061,1070,,,,,,,,,,,,,,,,WOS:000854193301020,0
C,"Wang, ZG; Ng, P; Ma, XF; Nallapati, R; Xiang, B",,,Assoc Computat Linguist,"Wang, Zhiguo; Ng, Patrick; Ma, Xiaofei; Nallapati, Ramesh; Xiang, Bing",,,Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F-1 over all non-BERT models, and 5.8% EM and 6.5% F-1 over BERT-based models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5878,5882,,,,,,,,,,,,,,,,WOS:000854193306010,0
C,"Wu, SJ; Dredze, M",,,Assoc Computat Linguist,"Wu, Shijie; Dredze, Mark",,,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2019) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero-shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,833,844,,,,,,,,,,,,,,,,WOS:000854193300077,0
C,"Xenouleas, S; Malakasiotis, P; Apidianaki, M; Androutsopoulos, I",,,Assoc Computat Linguist,"Xenouleas, Stratos; Malakasiotis, Prodromos; Apidianaki, Marianna; Androutsopoulos, Ion",,,SUM-QE: a BERT-based Summary Quality Estimation Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose SUM-QE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SUM-QE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SUM- QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6005,6011,,,,,,,,,,,,,,,,WOS:000854193306029,0
C,"Xiong, L; Hu, CA; Xiong, CY; Campos, D; Overwijk, A",,,Assoc Computat Linguist,"Xiong, Lee; Hu, Chuan; Xiong, Chenyan; Campos, Daniel; Overwijk, Arnold",,,Open DomainWeb Keyphrase Extraction Beyond Language Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLINGKPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5175,5184,,,,,,,,,,,,,,,,WOS:000854193305034,0
C,"Zhang, Y; Wang, R; Si, L",,,Assoc Computat Linguist,"Zhang, Yue; Wang, Rui; Si, Luo",,,Syntax-Enhanced Self-Attention-Based Semantic Role Labeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of encoding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we conduct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,616,626,,,,,,,,,,,,,,,,WOS:000854193300057,0
C,"Akimoto, K; Hiraoka, T; Sadamasa, K; Niepert, M",,,Assoc Computat Linguist,"Akimoto, Kosuke; Hiraoka, Takuya; Sadamasa, Kunihiko; Niepert, Mathias",,,Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal Schemas,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Most existing relation extraction approaches exclusively target binary relations, and n-ary relation extraction is relatively unexplored. Current state-of-the-art n-ary relation extraction method is based on a supervised learning approach and, therefore, may suffer from the lack of sufficient relation labels. In this paper, we propose a novel approach to cross-sentence n-ary relation extraction based on universal schemas. To alleviate the sparsity problem and to leverage inherent decomposability of n-ary relations, we propose to learn relation representations of lower-arity facts that result from decomposing higher-arity facts. The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower-arity facts. We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n-ary relation extraction performance compared to previous methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6225,6231,,,,,,,,,,,,,,,,WOS:000854193306056,0
C,"Almarwani, N; Aldarmaki, H; Diab, M",,,Assoc Computat Linguist,"Almarwani, Nada; Aldarmaki, Hanan; Diab, Mona",,,Efficient Sentence Embedding using Discrete Cosine Transform,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for syntactic structure. While more complex sequential or convolutional networks potentially yield superior classification performance, the improvements in classification accuracy are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3672,3678,,,,,,,,,,,,,,,,WOS:000854193303098,0
C,"Barrett, M; Kementchedjhieva, Y; Elazar, Y; Elliott, D; Sogaard, A",,,Assoc Computat Linguist,"Barrett, Maria; Kementchedjhieva, Yova; Elazar, Yanai; Elliott, Desmond; Sogaard, Anders",,,Adversarial Removal of Demographic Attributes Revisited,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a heldout subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.",,,,,,"Elliott, Desmond/0000-0003-3112-7904",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6330,6335,,,,,,,,,,,,,,,,WOS:000854193306073,0
C,"Fan, C; Yan, HY; Du, JC; Gui, L; Bing, LD; Yang, M; Xu, RF; Mao, RB",,,Assoc Computat Linguist,"Fan, Chuang; Yan, Hongyu; Du, Jiachen; Gui, Lin; Bing, Lidong; Yang, Min; Xu, Ruifeng; Mao, Ruibin",,,A Knowledge Regularized Hierarchical Approach for Emotion Cause Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Emotion cause analysis, which aims to identify the reasons behind emotions, is a key topic in sentiment analysis. A variety of neural network models have been proposed recently, however, these previous models mostly focus on the learning architecture with local textual information, ignoring the discourse and prior knowledge, which play crucial roles in human text comprehension. In this paper, we propose a new method to extract emotion cause with a hierarchical neural model and knowledge-based regularizations, which aims to incorporate discourse context information and restrain the parameters by sentiment lexicon and common knowledge. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance on two public datasets in different languages (Chinese and English), outperforming a number of competitive baselines by at least 2.08% in F-measure.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5614,5624,,,,,,,,,,,,,,,,WOS:000854193305076,0
C,"Grenander, M; Dong, Y; Cheung, JCK; Louis, A",,,Assoc Computat Linguist,"Grenander, Matt; Dong, Yue; Cheung, Jackie C. K.; Louis, Annie",,,Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Sentence position is a strong feature for news summarization, since the lead often (but not always) summarizes the key points of the article. In this paper, we show that recent neural systems excessively exploit this trend, which although powerful for many inputs, is also detrimental when summarizing documents where important content should be extracted from later parts of the article. We propose two techniques to make systems sensitive to the importance of content in different parts of the article. The first technique employs `unbiased' data; i.e., randomly shuffled sentences of the source document, to pretrain the model. The second technique uses an auxiliary ROUGE-based loss that encourages the model to distribute importance scores throughout a document by mimicking sentence-level ROUGE scores on the training data. We show that these techniques significantly improve the performance of a competitive reinforcement learning based extractive system, with the auxiliary loss being more powerful than pretraining.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6019,6024,,,,,,,,,,,,,,,,WOS:000854193306031,0
C,"Gui, T; Zou, YC; Zhang, Q; Peng, ML; Fu, JL; Wei, ZY; Huang, XJ",,,Assoc Computat Linguist,"Gui, Tao; Zou, Yicheng; Zhang, Qi; Peng, Minlong; Fu, Jinlan; Wei, Zhongyu; Huang, Xuanjing",,,A Lexicon-Based Graph Neural Network for Chinese NER,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recurrent neural networks (RNN) used for Chinese named entity recognition (NER) that sequentially track character and word information have achieved great success. However, the characteristic of chain structure and the lack of global semantics determine that RNN-based models are vulnerable to word ambiguities. In this work, we try to alleviate this problem by introducing a lexicon-based graph neural network with global semantics, in which lexicon knowledge is used to connect characters to capture the local composition, while a global relay node can capture global sentence semantics and longrange dependency. Based on the multiple graph-based interactions among characters, potential words, and the whole-sentence semantics, word ambiguities can be effectively tackled. Experiments on four NER datasets show that the proposed model achieves significant improvements against other baseline models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1040,1050,,,,,,,,,,,,,,,,WOS:000854193301018,0
C,"Gupta, A; Durrett, G",,,Assoc Computat Linguist,"Gupta, Aditya; Durrett, Greg",,,Effective Use of Transformer Networks for Entity Tracking,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Tracking entities in procedural language requires understanding the transformations arising from actions on entities as well as those entities' interactions. While self-attention-based pre-trained language encoders like GPT and BERT have been successfully applied across a range of natural language understanding tasks, their ability to handle the nuances of procedural texts is still untested. In this paper, we explore the use of pre-trained transformer networks for entity tracking tasks in procedural text. First, we test standard lightweight approaches for prediction with pre-trained transformers, and find that these approaches underperform even simple baselines. We show that much stronger results can be attained by restructuring the input to guide the transformer model to focus on a particular entity. Second, we assess the degree to which transformer networks capture the process dynamics, investigating such factors as merged entities and oblique entity references. On two different tasks, ingredient detection in recipes and QA over scientific processes, we achieve state-ofthe-art results, but our models still largely attend to shallow context clues and do not form complex representations of intermediate entity or process state.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,759,769,,,,,,,,,,,,,,,,WOS:000854193300070,0
C,"Huang, HY; Liang, YB; Duan, N; Gong, M; Shou, LJ; Jiang, DX; Zhou, M",,,Assoc Computat Linguist,"Huang, Haoyang; Liang, Yaobo; Duan, Nan; Gong, Ming; Shou, Linjun; Jiang, Daxin; Zhou, Ming",,,Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019), three new crosslingual pre-training tasks are proposed, including cross-lingual word recovery, crosslingual paraphrase classification and crosslingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on French and German) is obtained.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2485,2494,,,,,,,,,,,,,,,,WOS:000854193302060,0
C,"Jiang, Y; Han, WJ; Tu, KW",,,Assoc Computat Linguist,"Jiang, Yong; Han, Wenjuan; Tu, Kewei",,,A Regularization-based Framework for Bilingual Grammar Induction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Grammar induction aims to discover syntactic structures from unannotated sentences. In this paper, we propose a framework in which the learning process of the grammar model of one language is influenced by knowledge from the model of another language. Unlike previous work on multilingual grammar induction, our approach does not rely on any external resources, such as parallel corpora, word alignments or linguistic phylogenetic trees. We propose three regularization methods that encourage similarity between model parameters, dependency edge scores, and parse trees respectively. We deploy our methods on a state-of-the-art unsupervised discriminative parser and evaluate it on both transfer grammar induction and bilingual grammar induction. Empirical results on multiple languages show that our methods outperform strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1423,1428,,,,,,,,,,,,,,,,WOS:000854193301070,0
C,"Jin, HL; Hou, L; Li, JZ; Dong, TS",,,Assoc Computat Linguist,"Jin, Hailong; Hou, Lei; Li, Juanzi; Dong, Tiansi",,,Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper addresses the problem of inferring the fine-grained type of an entity from a knowledge base. We convert this problem into the task of graph-based semi-supervised classification, and propose Hierarchical Multi Graph Convolutional Network (HMGCN), a novel Deep Learning architecture to tackle this problem. We construct three kinds of connectivity matrices to capture different kinds of semantic correlations between entities. A recursive regularization is proposed to model the subClassOf relations between types in given type hierarchy. Extensive experiments with two large-scale public datasets show that our proposed method significantly outperforms four state-of-the-art methods.",,,,,,"Hou, Lei/0000-0002-8907-3526",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4969,4978,,,,,,,,,,,,,,,,WOS:000854193305015,0
C,"Jin, T; Huang, SY; Li, YM; Zhang, ZF",,,Assoc Computat Linguist,"Jin, Tao; Huang, Siyu; Li, Yingming; Zhang, Zhongfei",,,Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"This paper addresses the challenging task of video captioning which aims to generate descriptions for video data. Recently, the attention-based encoder-decoder structures have been widely used in video captioning. In existing literature, the attention weights are often built from the information of an individual modality, while, the association relationships between multiple modalities are neglected. Motivated by this observation, we propose a video captioning model with HighOrder Cross-Modal Attention (HOCA) where the attention weights are calculated based on the high-order correlation tensor to capture the frame-level cross-modal interaction of different modalities sufficiently. Furthermore, we novelly introduce Low-Rank HOCA which adopts tensor decomposition to reduce the extremely large space requirement of HOCA, leading to a practical and efficient implementation in real-world applications. Experimental results on two benchmark datasets, MSVD and MSR-VTT, show that Low-rank HOCA establishes a new state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2001,2011,,,,,,,,,,,,,,,,WOS:000854193302015,0
C,"Kumar, H; Agarwal, A; Joshi, S",,,Assoc Computat Linguist,"Kumar, Harshit; Agarwal, Arvind; Joshi, Sachindra",,,A Practical Dialogue-Act-Driven Conversation Model for Multi-Turn Response Selection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dialogue Acts play an important role in conversation modeling. Research has shown the utility of dialogue acts for the response selection task, however, the underlying assumption is that the dialogue acts are readily available, which is impractical, as dialogue acts are rarely available for new conversations. This paper proposes an end-to-end multi-task model for conversation modeling, which is optimized for two tasks, dialogue act prediction and response selection, with the latter being the task of interest. It proposes a novel way of combining the predicted dialogue acts of context and response with the context (previous utterances) and response (follow-up utterance) in a crossway fashion, such that, it achieves at par performance for the response selection task compared to the model that uses actual dialogue acts. Through experiments on two well known datasets, we demonstrate that the multi-task model not only improves the accuracy of the dialogue act prediction task but also improves the MRR for the response selection task. Also, the cross-stitching of dialogue acts of context and response with the context and response is better than using either one of them individually.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1980,1989,,,,,,,,,,,,,,,,WOS:000854193302013,0
C,"Lee, J; Tang, R; Lin, J",,,ASSOC COMPUTAT LINGUIST,"Lee, Jaejun; Tang, Raphael; Lin, Jimmy",,,Honkling: In-Browser Personalization for Ubiquitous Keyword Spotting,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Used for simple commands recognition on devices from smart speakers to mobile phones, keyword spotting systems are everywhere. Ubiquitous as well are web applications, which have grown in popularity and complexity over the last decade. However, despite their obvious advantages in natural language interaction, voice-enabled web applications are still few and far between. We attempt to bridge this gap with Honkling, a novel, JavaScript-based keyword spotting system. Purely client-side and cross-device compatible, Honkling can be deployed directly on user devices. Our inbrowser implementation enables seamless personalization, which can greatly improve model quality; in the presence of underrepresented, non-American user accents, we can achieve up to an absolute 10% increase in accuracy in the personalized model with only a few examples.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,91,96,,,,,,,,,,,,,,,,WOS:000855231500016,0
C,"Li, CJ; Cao, YX; Hou, L; Shi, JX; Li, JZ; Chua, TS",,,Assoc Computat Linguist,"Li, Chengjiang; Cao, Yixin; Hou, Lei; Shi, Jiaxin; Li, Juanzi; Chua, Tat-Seng",,,Semi-supervised Entity Alignment via Joint Knowledge Embedding Model and Cross-graph Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Entity alignment aims at integrating complementary knowledge graphs (KGs) from different sources or languages, which may benefit many knowledge-driven applications. It is challenging due to the heterogeneity of KGs and limited seed alignments. In this paper, we propose a semi-supervised entity alignment method by joint Knowledge Embedding model and Cross-Graph model (KECG). It can make better use of seed alignments to propagate over the entire graphs with KG-based constraints. Specifically, as for the knowledge embedding model, we utilize TransE to implicitly complete two KGs towards consistency and learn relational constraints between entities. As for the crossgraph model, we extend Graph Attention Network (GAT) with projection constraint to robustly encode graphs, and two KGs share the same GAT to transfer structural knowledge as well as to ignore unimportant neighbors for alignment via attention mechanism. Results on publicly available datasets as well as further analysis demonstrate the effectiveness of KECG. Our codes can be found in https: //github.com/THU-KEG/KECG.",,,,,,"Hou, Lei/0000-0002-8907-3526",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2723,2732,,,,,,,,,,,,,,,,WOS:000854193302082,0
C,"Lin, BY; Chen, XY; Chen, J; Ren, X",,,Assoc Computat Linguist,"Lin, Bill Yuchen; Chen, Xinyue; Chen, Jamin; Ren, Xiang",,,KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KAGNE T, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for BERT-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning. We open-source our code1 to the community for future research in knowledge-aware commonsense reasoning.",,,,,"chen, xinyue/HHR-9308-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2829,2839,,,,,,,,,,,,,,,,WOS:000854193302090,0
C,"Lu, CJ; Chen, L; Tan, CL; Li, XL; Xiao, J",,,Assoc Computat Linguist,"Lu, Chujie; Chen, Long; Tan, Chilie; Li, Xiaolin; Xiao, Jun",,,DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we focus on natural language video localization: localizing (i.e., grounding) a natural language description in a long and untrimmed video sequence. All currently published models for addressing this problem can be categorized into two types: (i) top-down approach: it does classification and regression for a set of pre-cut video segment candidates; (ii) bottom-up approach: it directly predicts probabilities for each video frame as the temporal boundaries (i.e., start and end time point). However, both two approaches suffer several limitations: the former is computation-intensive for densely placed candidates, while the latter has trailed the performance of the top-down counterpart thus far. To this end, we propose a novel dense bottom-up framework: DEnse Bottom-Up Grounding (DEBUG). DEBUG regards all frames falling in the ground truth segment as foreground, and each foreground frame regresses the unique distances from its location to bi-directional ground truth boundaries. Extensive experiments on three challenging benchmarks (TACoS, CharadesSTA, and ActivityNet Captions) show that DEBUG is able to match the speed of bottom-up models while surpassing the performance of the state-of-the-art top-down models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5144,5153,,,,,,,,,,,,,,,,WOS:000854193305031,0
C,"Madasu, A; Rao, VA",,,Assoc Computat Linguist,"Madasu, Avinash; Rao, Vijjini Anvesh",,,Sequential Learning of Convolutional Features for Effective Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text classification has been one of the major problems in natural language processing. With the advent of deep learning, convolutional neural network (CNN) has been a popular solution to this task. However, CNNs which were first proposed for images, face many crucial challenges in the context of text processing, namely in their elementary blocks: convolution filters and max pooling. These challenges have largely been overlooked by the most existing CNN models proposed for text classification. In this paper, we present an experimental study on the fundamental blocks of CNNs in text categorization. Based on this critique, we propose Sequential Convolutional Attentive Recurrent Network (SCARN). The proposed SCARN model utilizes both the advantages of recurrent and convolutional structures efficiently in comparison to previously proposed recurrent convolutional models. We test our model on different text classification datasets across tasks like sentiment analysis and question classification. Extensive experiments establish that SCARN outperforms other recurrent convolutional architectures with significantly less parameters. Furthermore, SCARN achieves better performance compared to equally large various deep CNN and LSTM architectures.",,,,,"Madasu, Avinash/HGU-0417-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5658,5667,,,,,,,,,,,,,,,,WOS:000854193305080,0
C,"Mao, HR; Lee, HY",,,Assoc Computat Linguist,"Mao, Hong-Ren; Lee, Hung-Yi",,,PollyWant a Cracker: Analyzing Performance of Parroting on Paraphrase Generation Datasets,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-theart models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5960,5968,,,,,,,,,,,,,,,,WOS:000854193306022,0
C,"Min, BN; Chan, YS; Qiu, HL; Fasching, J",,,Assoc Computat Linguist,"Min, Bonan; Chan, Yee Seng; Qiu, Haoling; Fasching, Joshua",,,Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Solving long-lasting problems such as food insecurity requires a comprehensive understanding of interventions applied by governments and international humanitarian assistance organizations, and their results and consequences. Towards achieving this grand goal, a crucial first step is to extract past interventions and when and where they have been applied, from hundreds of thousands of reports automatically. In this paper, we developed a corpus annotated with interventions to foster research, and developed an information extraction system for extracting interventions and their location and time from text. We demonstrate early, very encouraging results on extracting interventions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6444,6448,,,,,,,,,,,,,,,,WOS:000854193306091,0
C,"Raghuvanshi, A; Emhar, V; Ramakrishnan, V; Carroll, L; Raghunathan, K",,,ASSOC COMPUTAT LINGUIST,"Raghuvanshi, Arushi; Emhar, Varsha; Ramakrishnan, Vijay; Carroll, Lucien; Raghunathan, Karthik",,,Entity resolution for noisy ASR transcripts,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Large vocabulary domain-agnostic Automatic Speech Recognition (ASR) systems often mistranscribe domain-specific words and phrases. Since these generic ASR systems are the first component of most voice assistants in production, building Natural Language Understanding (NLU) systems that are robust to these errors can be a challenging task. In this paper, we focus on handling ASR errors in named entities, specifically person names, for a voicebased collaboration assistant. We demonstrate an effective method for resolving person names that are mistranscribed by black-box ASR systems, using character and phonemebased information retrieval techniques and contextual information, which improves accuracy by 40.8% on our production system. We provide a live interactive demo to further illustrate the nuances of this problem and the effectiveness of our solution.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,61,66,,,,,,,,,,,,,,,,WOS:000855231500011,0
C,"Ross, A; Pavlick, E",,,Assoc Computat Linguist,"Ross, Alexis; Pavlick, Ellie",,,How well do NLI models capture verb veridicality?,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical-a bias which is amplified in BERT. We further show that, encouragingly, BERT's inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2230,2240,,,,,,,,,,,,,,,,WOS:000854193302036,0
C,"Shao, B; Gong, YY; Qi, WZ; Duan, N; Lin, XL",,,Assoc Computat Linguist,"Shao, Bo; Gong, Yeyun; Qi, Weizhen; Duan, Nan; Lin, Xiaola",,,Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this work, we propose an aggregation method to combine the Bidirectional Encoder Representations from Transformer (BERT) with a MatchLSTM layer for Sequence Matching. Given a sentence pair, we extract the output representations of it from BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of the sentence pair for sequence matching tasks. Taking natural language inference as an example, we split BERT output into two parts, which is from premise sentence and hypothesis sentence. At each position of the hypothesis sentence, both the weighted representation of the premise sentence and the representation of the current token are fed into LSTM. We jointly train the aggregation layer and pre-trained layer for sequence matching. We conduct an experiment on two publicly available datasets, WikiQA and SNLI. Experiments show that our model achieves significant improvement compared with state-of-the-art methods on both datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6059,6063,,,,,,,,,,,,,,,,WOS:000854193306037,0
C,"Shu, L; Xu, H; Liu, B; Molino, P",,,Assoc Computat Linguist,"Shu, Lei; Xu, Hu; Liu, Bing; Molino, Piero",,,Modeling Multi-Action Policy for Task-Oriented Dialogues,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dialogue management (DM) plays a key role in the quality of the interaction with the user in a task-oriented dialogue system. In most existing approaches, the agent predicts only one DM policy action per turn. This significantly limits the expressive power of the conversational agent and introduces unwanted turns of interactions that may challenge users' patience. Longer conversations also lead to more errors and the system needs to be more robust to handle them. In this paper, we compare the performance of several models on the task of predicting multiple acts for each turn. A novel policy model is proposed based on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the limitations of the existing models. Experimental results show that gCAS1 outperforms other approaches.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1304,1310,,,,,,,,,,,,,,,,WOS:000854193301052,0
C,"Tan, M; Wang, DK; Gao, YP; Wang, HY; Potdar, S; Guo, XX; Chang, SY; Yu, M",,,Assoc Computat Linguist,"Tan, Ming; Wang, Dakuo; Gao, Yupeng; Wang, Haoyu; Potdar, Saloni; Guo, Xiaoxiao; Chang, Shiyu; Yu, Mo",,,Context-Aware Conversation Thread Detection in Multi-Party Chat,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In multi-party chat, it is common for multiple conversations to occur concurrently, leading to intermingled conversation threads in chat logs. In this work, we propose a novel Context-Aware Thread Detection (CATD) model that automatically disentangles these conversation threads. We evaluate our model on three real-world datasets and demonstrate an overall improvement in thread detection accuracy over state-of-the-art benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6456,6461,,,,,,,,,,,,,,,,WOS:000854193306093,0
C,"Thorne, J; Vlachos, A; Christodoulopoulos, C; Mittal, A",,,Assoc Computat Linguist,"Thorne, James; Vlachos, Andreas; Christodoulopoulos, Christos; Mittal, Arpit",,,Evaluating adversarial attacks against multiple fact verification systems,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2944,2953,,,,,,,,,,,,,,,,WOS:000854193303010,0
C,"Voita, E; Sennrich, R; Titov, I",,,Assoc Computat Linguist,"Voita, Elena; Sennrich, Rico; Titov, Ivan",,,The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.",,,,,,"Sennrich, Rico/0000-0002-1438-4741",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4396,4406,,,,,,,,,,,,,,,,WOS:000854193304053,0
C,"Wang, PZ; Wang, WY",,,Assoc Computat Linguist,"Wang, Prince Zizhuang; Wang, William Yang",,,Neural Gaussian Copula for Variational Autoencoder,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.",,,,,,"Wang, William S-Y./0000-0001-6153-8240",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4333,4343,,,,,,,,,,,,,,,,WOS:000854193304047,0
C,"Wang, R; Zhou, DY; He, YL",,,Assoc Computat Linguist,"Wang, Rui; Zhou, Deyu; He, Yulan",,,Open Event Extraction from Online Text using a Generative Adversarial Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"To extract the structured representations of open-domain events, Bayesian graphical models have made some progress. However, these approaches typically assume that all words in a document are generated from a single event. While this may be true for short text such as tweets, such an assumption does not generally hold for long text such as news articles. Moreover, Bayesian graphical models often rely on Gibbs sampling for parameter inference which may take long time to converge. To address these limitations, we propose an event extraction model based on Generative Adversarial Nets, called Adversarial-neural Event Model (AEM). AEM models an event with a Dirichlet prior and uses a generator network to capture the patterns underlying latent events. A discriminator is used to distinguish documents reconstructed from the latent events and the original documents. A byproduct of the discriminator is that the features generated by the learned discriminator network allow the visualization of the extracted events. Our model has been evaluated on two Twitter datasets and a news article dataset. Experimental results show that our model outperforms the baseline approaches on all the datasets, with more significant improvements observed on the news article dataset where an increase of 15% is observed in F-measure.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,282,291,,,,,,,,,,,,,,,,WOS:000854193300027,0
C,"Wiegreffe, S; Pinter, Y",,,Assoc Computat Linguist,"Wiegreffe, Sarah; Pinter, Yuval",,,Attention is not not Explanation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.",,,,,,"Pinter, Yuval/0000-0003-3174-1621",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,11,20,,,,,,,,,,,,,,,,WOS:000854193300002,0
C,"Zeng, JL; Liu, Y; Su, JS; Ge, YB; Lu, YJ; Yin, YJ; Luo, JB",,,Assoc Computat Linguist,"Zeng, Jiali; Liu, Yang; Su, Jinsong; Ge, Yubin; Lu, Yaojie; Yin, Yongjing; Luo, Jiebo",,,Iterative Dual Domain Adaptation for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous studies on the domain adaptation for neural machine translation (NMT) mainly focus on the one-pass transferring out-of-domain translation knowledge to in-domain NMT model. In this paper, we argue that such a strategy fails to fully extract the domain-shared translation knowledge, and repeatedly utilizing corpora of different domains can lead to better distillation of domain-shared translation knowledge. To this end, we propose an iterative dual domain adaptation framework for NMT. Specifically, we first pretrain in-domain and out-of-domain NMT models using their own training corpora respectively, and then iteratively perform bidirectional translation knowledge transfer (from in-domain to out-of-domain and then vice versa) based on knowledge distillation until the in-domain NMT model convergences. Furthermore, we extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the above-mentioned transfer is performed sequentially between the in-domain and each out-of-domain NMT models in the ascending order of their domain similarities. Empirical results on Chinese-English and English-German translation tasks demonstrate the effectiveness of our framework.",,,,,,"Luo, Jiebo/0000-0002-4516-9729",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,845,855,,,,,,,,,,,,,,,,WOS:000854193300078,0
C,"Zhong, PX; Wang, D; Miao, CY",,,Assoc Computat Linguist,"Zhong, Peixiang; Wang, Di; Miao, Chunyan",,,Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Messages in human conversations inherently convey emotions. The task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks. However, enabling machines to analyze emotions in conversations is challenging, partly because humans often rely on the context and commonsense knowledge to express emotions. In this paper, we address these challenges by proposing a Knowledge-Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a context-aware affective graph attention mechanism. Experiments on multiple textual conversation datasets demonstrate that both context and commonsense knowledge are consistently beneficial to the emotion detection performance. In addition, the experimental results show that our KET model outperforms the state-of-the-art models on most of the tested datasets in F1 score.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,165,176,,,,,,,,,,,,,,,,WOS:000854193300016,0
C,"Zhou, WJ; Zhang, MH; Wu, YF",,,Assoc Computat Linguist,"Zhou, Wenjie; Zhang, Minghua; Wu, Yunfang",,,Question-type Driven Question Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Question generation is a challenging task which aims to ask a question based on an answer and relevant context. The existing works suffer from the mismatching between question type and answer, i.e. generating a question with type how while the answer is a personal name. We propose to automatically predict the question type based on the input answer and context. Then, the question type is fused into a seq2seq model to guide the question generation, so as to deal with the mismatching problem. We achieve significant improvement on the accuracy of question type prediction and finally obtain state-of-the-art results for question generation on both SQuAD and MARCO datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6032,6037,,,,,,,,,,,,,,,,WOS:000854193306033,0
C,"Baevski, A; Edunov, S; Liu, YH; Zettlemoyer, L; Auli, M",,,Assoc Computat Linguist,"Baevski, Alexei; Edunov, Sergey; Liu, Yinhan; Zettlemoyer, Luke; Auli, Michael",,,Cloze-driven Pretraining of Self-attention Networks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5360,5369,,,,,,,,,,,,,,,,WOS:000854193305052,0
C,"Chen, Q; Lin, XZ; He, XH; Tou, HX; Chen, T; Wei, ZY",,,Assoc Computat Linguist,"Chen, Qin; Lin, Xinzhu; He, Xiahui; Tou, Huaixiao; Chen, Ting; Wei, Zhongyu",,,Enhancing Dialogue Symptom Diagnosis with Global Attention and Symptom Graph,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5033,5042,,,,,,,,,,,,,,,,WOS:000854193305021,0
C,"Desai, S; Sinno, B; Rosenfeld, A; Li, JYJ",,,Assoc Computat Linguist,"Desai, Shrey; Sinno, Barea; Rosenfeld, Alex; Li, Junyi Jessy",,,Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Insightful findings in political science often require researchers to analyze documents of a certain subject or type, yet these documents are usually contained in large corpora that do not distinguish between pertinent and non-pertinent documents. In contrast, we can find corpora that label relevant documents but have limitations (e.g., from a single source or era), preventing their use for political science research. To bridge this gap, we present adaptive ensembling, an unsupervised domain adaptation framework, equipped with a novel text classification model and time-aware training to ensure our methods work well with diachronic corpora. Experiments on an expert-annotated dataset show that our framework outperforms strong benchmarks. Further analysis indicates that our methods are more stable, learn better representations, and extract cleaner corpora for fine-grained analysis.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4718,4730,,,,,,,,,,,,,,,,WOS:000854193304083,0
C,"Escolano, C; Costa-jussa, MR; Lacroux, E; Vazquez, PP",,,ASSOC COMPUTAT LINGUIST,"Escolano, Carlos; Costa-jussa, Marta R.; Lacroux, Elora; Vazquez, Pere-Pau",,,"Multilingual, Multi-scale and Multi-layer Visualization of Sequence-based Intermediate Representations",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The main alternatives nowadays to deal with sequences are Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) architectures and the Transformer. In this context, RNN's, CNN's and Transformer have most commonly been used as an encoderdecoder architecture with multiple layers in each module. Far beyond this, these architectures are the basis for the contextual word embeddings which are revolutionizing most natural language downstream applications. However, intermediate layer representations in sequence-based architectures can be difficult to interpret. To make each layer representation within these architectures more accessible and meaningful, we introduce a web-based tool that visualizes them both at the sentence and token level. We present three use cases. The first analyses gender issues in contextual word embeddings. The second and third are showing multilingual intermediate representations for sentences and tokens and the evolution of these intermediate representations along the multiple layers of the decoder and in the context of multilingual machine translation.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,151,156,,,,,,,,,,,,,,,,WOS:000855231500026,0
C,"Geng, RY; Li, BH; Li, YB; Zhu, XD; Jian, P; Sun, J",,,Assoc Computat Linguist,"Geng, Ruiying; Li, Binhua; Li, Yongbin; Zhu, Xiaodan; Jian, Ping; Sun, Jian",,,Induction Networks for Few-Shot Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3904,3913,,,,,,,,,,,,,,,,WOS:000854193304008,0
C,"Hu, MT; Wu, YK; Zhao, SW; Guo, HL; Cheng, RH; Su, Z",,,Assoc Computat Linguist,"Hu, Mengting; Wu, Yike; Zhao, Shiwan; Guo, Honglei; Cheng, Renhong; Su, Zhong",,,Domain-Invariant Feature Distillation for Cross-Domain Sentiment Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Cross-domain sentiment classification has drawn much attention in recent years. Most existing approaches focus on learning domain-invariant representations in both the source and target domains, while few of them pay attention to the domain-specific information. Despite the non-transferability of the domain-specific information, simultaneously learning domain-dependent representations can facilitate the learning of domain-invariant representations. In this paper, we focus on aspectlevel cross-domain sentiment classification, and propose to distill the domain-invariant sentiment features with the help of an orthogonal domain-dependent task, i.e. aspect detection, which is built on the aspects varying widely in different domains. We conduct extensive experiments on three public datasets and the experimental results demonstrate the effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5559,5568,,,,,,,,,,,,,,,,WOS:000854193305071,0
C,"Karmakharm, T; Aletras, N; Bontcheva, K",,,ASSOC COMPUTAT LINGUIST,"Karmakharm, Twin; Aletras, Nikolaos; Bontcheva, Kalina",,,Journalist-in-the-Loop: Continuous Learning as a Service for Rumour Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,Automatically identifying rumours in social media and assessing their veracity is an important task with downstream applications in journalism. A significant challenge is how to keep rumour analysis tools up-to-date as new information becomes available for particular rumours that spread in a social network. This paper presents a novel open-source web-based rumour analysis tool that can continuous learn from journalists. The system features a rumour annotation service that allows journalists to easily provide feedback for a given social media post through a web-based interface. The feedback allows the system to improve an underlying state-of-the-art neural networkbased rumour classification model. The system can be easily integrated as a service into existing tools and platforms used by journalists using a REST API.,,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,115,120,,,,,,,,,,,,,,,,WOS:000855231500020,0
C,"Kim, Y; Petrov, P; Petrushkov, P; Khadivi, S; Ney, H",,,Assoc Computat Linguist,"Kim, Yunsu; Petrov, Petre; Petrushkov, Pavel; Khadivi, Shahram; Ney, Hermann",,,Pivot-based Transfer Learning for Neural Machine Translation between Non-English Languages,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present effective pre-training strategies for neural machine translation (NMT) using parallel corpora involving a pivot language, i.e., source-pivot and pivot-target, leading to a significant improvement in source -> target translation. We propose three methods to increase the relation among source, pivot, and target languages in the pre-training: 1) step-wise training of a single model for different language pairs, 2) additional adapter component to smoothly connect pre-trained encoder and decoder, and 3) cross-lingual encoder training via autoencoding of the pivot language. Our methods greatly outperform multilingual models up to +2.6% BLEU in WMT 2019 French -> German and German -> Czech tasks. We show that our improvements are valid also in zero-shot/zero-resource scenarios.",,,,,"Pakhlov, Pavel N/K-2158-2013","Pakhlov, Pavel N/0000-0001-7426-4824",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,866,876,,,,,,,,,,,,,,,,WOS:000854193301002,0
C,"Kovaleva, O; Romanov, A; Rogers, A; Rumshisky, A",,,Assoc Computat Linguist,"Kovaleva, Olga; Romanov, Alexey; Rogers, Anna; Rumshisky, Anna",,,Revealing the Dark Secrets of BERT,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"BERT-based architectures currently give stateof-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of selfattention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT's heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4365,4374,,,,,,,,,,,,,,,,WOS:000854193304050,0
C,"Larson, S; Mahendran, A; Peper, JJ; Clarke, C; Lee, A; Hill, P; Kummerfeld, JK; Leach, K; Laurenzano, MA; Tang, LJ; Mars, J",,,Assoc Computat Linguist,"Larson, Stefan; Mahendran, Anish; Peper, Joseph J.; Clarke, Christopher; Lee, Andrew; Hill, Parker; Kummerfeld, Jonathan K.; Leach, Kevin; Laurenzano, Michael A.; Tang, Lingjia; Mars, Jason",,,An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Task-oriented dialog systems need to know when a query falls outside their range of supported intents, but current text classification corpora only define label sets that cover every example. We introduce a new dataset that includes queries that are out-of-scopei.e., queries that do not fall into any of the system's supported intents. This poses a new challenge because models cannot assume that every query at inference time belongs to a system-supported intent class. Our dataset also covers 150 intent classes over 10 domains, capturing the breadth that a production taskoriented agent must handle. We evaluate a range of benchmark classifiers on our dataset along with several different out-of-scope identification schemes. We find that while the classifiers perform well on in-scope intent classification, they struggle to identify out-of-scope queries. Our dataset and evaluation fill an important gap in the field, offering a way of more rigorously and realistically benchmarking text classification in task-driven dialog systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1311,1316,,,,,,,,,,,,,,,,WOS:000854193301053,0
C,"Ma, S; Wang, G; Feng, YS; Huai, JP",,,Assoc Computat Linguist,"Ma, Shuai; Wang, Gang; Feng, Yansong; Huai, Jinpeng",,,Easy First Relation Extraction with Information Redundancy,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Many existing relation extraction (RE) models make decisions globally using integer linear programming (ILP). However, it is nontrivial to make use of integer linear programming as a blackbox solver for RE. Its cost of time and memory may become unacceptable with the increase of data scale, and redundant information needs to be encoded cautiously for ILP. In this paper, we propose an easy first approach for relation extraction with information redundancies, embedded in the results produced by local sentence level extractors, during which conflict decisions are resolved with domain and uniqueness constraints. Information redundancies are leveraged to support both easy first collective inference for easy decisions in the first stage and ILP for hard decisions in a subsequent stage. Experimental study shows that our approach improves the efficiency and accuracy of RE, and outperforms both ILP and neural network-based methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3851,3861,,,,,,,,,,,,,,,,WOS:000854193304003,0
C,"Ma, Y; Zong, L; Yang, YK; Su, JL",,,Assoc Computat Linguist,"Ma, Ye; Zong, Lu; Yang, Yikang; Su, Jionglong",,,News2vec: News Network Embedding with Subnode Information,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"With the development of NLP technologies, news can be automatically categorized and labeled according to a variety of characteristics, at the same time be represented as low dimensional embeddings. However, it lacks a systematic approach that effectively integrates the inherited features and inter-textual knowledge of news to represent the collective information with a dense vector. With the aim of filling this gap, the News2vec model is proposed to allow the distributed representation of news taking into account its associated features. To describe the cross-document linkages between news, a network consisting of news and its attributes is constructed. Moreover, the News2vec model treats the news node as a bag of features by developing the Subnode model. Based on the biased random walk and the skip-gram model, each news feature is mapped to a vector, and the news is thus represented as the sum of its features. This approach offers an easy solution to create embeddings for unseen news nodes based on its attributes. To evaluate our model, dimension reduction plots and correlation heat-maps are created to visualize the news vectors, together with the application of two downstream tasks, the stock movement prediction and news recommendation. By comparing with other established text/sentence embedding models, we show that News2vec achieves state-of-the-art performance on these news-related tasks.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4843,4852,,,,,,,,,,,,,,,,WOS:000854193305003,0
C,"Ma, YK; Chen, PH; Hsieh, CJ",,,Assoc Computat Linguist,"Ma, Yukun; Chen, Pei-Hung (Patrick); Hsieh, Cho-Jui",,,MulCode: A Multiplicative Multi-way Model for Compressing Neural Language Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"It is challenging to deploy deep neural nets on memory-constrained devices due to the explosion of numbers of parameters. Especially, the input embedding layer and Softmax layer usually dominate the memory usage in an RNNbased language model. For example, input embedding and Softmax matrices in IWSLT2014 German-to-English data set account for more than 80% of the total model parameters. To compress these embedding layers, we propose MulCode, a novel multi-way multiplicative neural compressor. MulCode learns an adaptively created matrix and its multiplicative compositions. Together with a prior weighted loss, MulCode is more effective than the state-of-the-art compression methods. On the IWSLT-2014 machine translation data set, MulCode achieved 17 times compression rate for the embedding and Softmax matrices, and when combined with quantization technique, our method can achieve 41.38 times compression rate with very little loss in performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5257,5266,,,,,,,,,,,,,,,,WOS:000854193305042,0
C,"Miyazaki, T; Makino, K; Takei, Y; Okamoto, H; Goto, J",,,Assoc Computat Linguist,"Miyazaki, Taro; Makino, Kiminobu; Takei, Yuka; Okamoto, Hiroki; Goto, Jun",,,Label Embedding using Hierarchical Structure of Labels for Twitter Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Twitter is used for various applications such as disaster monitoring and news material gathering. In these applications, each Tweet is classified into pre-defined classes. These classes have a semantic relationship with each other and can be classified into a hierarchical structure, which is regarded as important information. Label texts of pre-defined classes themselves also include important clues for classification. Therefore, we propose a method that can consider the hierarchical structure of labels and label texts themselves. We conducted evaluation over the Text REtrieval Conference (TREC) 2018 Incident Streams (IS) track dataset, and we found that our method outperformed the methods of the conference participants.",,,,,"Miyazaki, Taro/HIK-3257-2022; Makino, Kiminobu/AAH-9945-2020","Makino, Kiminobu/0000-0002-2482-7964",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6317,6322,,,,,,,,,,,,,,,,WOS:000854193306071,0
C,"Ni, J; Florian, R",,,Assoc Computat Linguist,"Ni, Jian; Florian, Radu",,,Neural Cross-Lingual Relation Extraction Based on BilingualWord Embedding Mapping,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a welltrained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,399,409,,,,,,,,,,,,,,,,WOS:000854193300038,0
C,"Niu, MY; Cai, J",,,Assoc Computat Linguist,"Niu, Muyao; Cai, Jie",,,A Label Informative Wide & Deep Classifier for Patents and Papers,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we provide a simple and effective baseline for classifying both patents and papers to the well-established Cooperative Patent Classification (CPC). We propose a labelinformative classifier based on the Wide & Deep structure, where the Wide part encodes string-level similarities between texts and labels, and the Deep part captures semanticlevel similarities via non-linear transformations. Our model trains on millions of patents, and transfers to papers by developing distant-supervised training set and domain-specific features. Extensive experiments show that our model achieves comparable performance to the state-of-the-art model used in industry on both patents and papers. The output of this work should facilitate the searching, granting and filing of innovative ideas for patent examiners, attorneys and researchers.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3438,3443,,,,,,,,,,,,,,,,WOS:000854193303062,0
C,"Pan, ZF; Bai, K; Wang, Y; Zhou, LQ; Liu, XJ",,,Assoc Computat Linguist,"Pan, Zhufeng; Bai, Kun; Wang, Yan; Zhou, Lianqiang; Liu, Xiaojiang",,,Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In multi-turn dialogue, utterances do not always take the full form of sentences. These incomplete utterances will greatly reduce the performance of open-domain dialogue systems. Restoring more incomplete utterances from context could potentially help the systems generate more relevant responses. To facilitate the study of incomplete utterance restoration for open-domain dialogue systems, a large-scale multi-turn dataset Restoration-200K(1) is collected and manually labeled with the explicit relation between an utterance and its context. We also propose a pick-andcombine model to restore the incomplete utterance from its context. Experimental results demonstrate that the annotated dataset and the proposed approach significantly boost the response quality of both single-turn and multi-turn dialogue systems.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1824,1833,,,,,,,,,,,,,,,,WOS:000854193301113,0
C,"Poerner, N; Schutze, H",,,Assoc Computat Linguist,"Poerner, Nina; Schuetze, Hinrich",,,Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We address the problem of Duplicate Question Detection (DQD) in low-resource domain-specific Community Question Answering forums. Our multi-view framework MV-DASE combines an ensemble of sentence encoders via Generalized Canonical Correlation Analysis, using unlabeled data only. In our experiments, the ensemble includes generic and domain-specific averaged word embeddings, domain-finetuned BERT and the Universal Sentence Encoder. We evaluate MV-DASE on the CQADupStack corpus and on additional low-resource Stack Exchange forums. Combining the strengths of different encoders, we significantly outperform BM25, all single-view systems as well as a recent supervised domain-adversarial DQD method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1630,1641,,,,,,,,,,,,,,,,WOS:000854193301095,0
C,"Qian, D; Cheung, WK",,,Assoc Computat Linguist,"Qian, Dong; Cheung, William K.",,,Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4047,4057,,,,,,,,,,,,,,,,WOS:000854193304021,0
C,"Qian, J; Bethke, A; Liu, YY; Belding, E; Wang, WY",,,Assoc Computat Linguist,"Qian, Jing; Bethke, Anna; Liu, Yinyin; Belding, Elizabeth; Wang, William Yang",,,A Benchmark Dataset for Learning to Intervene in Online Hate Speech,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Countering online hate speech is a critical yet challenging task, but one which can be aided by the use of Natural Language Processing (NLP) techniques. Previous research has primarily focused on the development of NLP methods to automatically and effectively detect online hate speech while disregarding further action needed to calm and discourage individuals from using hate speech in the future. In addition, most existing hate speech datasets treat each post as an isolated instance, ignoring the conversational context. In this paper, we propose a novel task of generative hate speech intervention, where the goal is to automatically generate responses to intervene during online conversations that contain hate speech. As a part of this work, we introduce two fully-labeled large-scale hate speech intervention datasets(1) collected from Gab(2) and Reddit(3). These datasets provide conversation segments, hate speech labels, as well as intervention responses written by Mechanical Turk(4) Workers. In this paper, we also analyze the datasets to understand the common intervention strategies and explore the performance of common automatic response generation methods on these new datasets to provide a benchmark for future research.",,,,,,"Wang, William S-Y./0000-0001-6153-8240",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4755,4764,,,,,,,,,,,,,,,,WOS:000854193304086,0
C,"Ran, Q; Lin, YK; Li, P; Zhou, J; Liu, ZY",,,Assoc Computat Linguist,"Ran, Qiu; Lin, Yankai; Li, Peng; Zhou, Jie; Liu, Zhiyuan",,,NumNet: Machine Reading Comprehension with Numerical Reasoning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.",,,,,,"Liu, Zhiyuan/0000-0002-7709-2543; Li, Peng/0000-0003-1374-5979",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2474,2484,,,,,,,,,,,,,,,,WOS:000854193302059,0
C,"Sabir, A; Moreno-Noguer, F; Padro, L",,,Assoc Computat Linguist,"Sabir, Ahmed; Moreno-Noguer, Francesc; Padro, Lluis",,,Semantic Relatedness Based Re-ranker for Text Spotting,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Applications such as textual entailment, plagiarism detection or document clustering rely on the notion of semantic similarity, and are usually approached with dimension reduction techniques like LDA or with embedding-based neural approaches. We present a scenario where semantic similarity is not enough, and we devise a neural approach to learn semantic relatedness. The scenario is text spotting in the wild, where a text in an image (e.g. street sign, advertisement or bus destination) must be identified and recognized. Our goal is to improve the performance of vision systems by leveraging semantic information. Our rationale is that the text to be spotted is often related to the image context in which it appears (word pairs such as Delta-airplane, or quarters-parking are not similar, but are clearly related). We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points, outperforming other measures in a benchmark dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3451,3457,,,,,,,,,,,,,,,,WOS:000854193303064,0
C,"Sheng, E; Chang, KW; Natarajan, P; Peng, NY",,,Assoc Computat Linguist,"Sheng, Emily; Chang, Kai-Wei; Natarajan, Premkumar; Peng, Nanyun",,,The Woman Worked as a Babysitter: On Biases in Language Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3407,3412,,,,,,,,,,,,,,,,WOS:000854193303057,0
C,"Shoemark, P; Liza, FF; Nguyen, D; Hale, SA; McGillivray, B",,,Assoc Computat Linguist,"Shoemark, Philippa; Liza, Farhana Ferdousi; Nguyen, Dong; Hale, Scott A.; McGillivray, Barbara",,,Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Word embeddings are increasingly used for the automatic detection of semantic change; yet, a robust evaluation and systematic comparison of the choices involved has been lacking. We propose a new evaluation framework for semantic change detection and find that (i) using the whole time series is preferable over only comparing between the first and last time points; (ii) independently trained and aligned embeddings perform better than continuously trained embeddings for long time periods; and (iii) that the reference point for comparison matters. We also present an analysis of the changes detected on a large Twitter dataset spanning 5.5 years.",,,,,,"McGillivray, Barbara/0000-0003-3426-8200; Hale, Scott/0000-0002-6894-4951",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,66,76,,,,,,,,,,,,,,,,WOS:000854193300007,0
C,"Wang, H; Li, SY; Pan, R; Mao, MZ",,,Assoc Computat Linguist,"Wang, Heng; Li, Shuangyin; Pan, Rong; Mao, Mingzhi",,,Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Knowledge Graph (KG) reasoning aims at finding reasoning paths for relations, in order to solve the problem of incompleteness in KG. Many previous path-based methods like PRA and DeepPath suffer from lacking memory components, or stuck in training. Therefore, their performances always rely on wellpretraining. In this paper, we present a deep reinforcement learning based model named by AttnPath, which incorporates LSTM and Graph Attention Mechanism as the memory components. We define two metrics, Mean Selection Rate (MSR) and Mean Replacement Rate (MRR), to quantitatively measure how difficult it is to learn the query relations, and take advantages of them to fine-tune the model under the framework of reinforcement learning. Meanwhile, a novel mechanism of reinforcement learning is proposed by forcing an agent to walk forward every step to avoid the agent stalling at the same entity node constantly. Based on this operation, the proposed model not only can get rid of the pretraining process, but also achieves state-of-the-art performance comparing with the other models. We test our model on FB15K-237 and NELL-995 datasets with different tasks. Extensive experiments show that our model is effective and competitive with many current state-of-the-art methods, and also performs well in practice.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2623,2631,,,,,,,,,,,,,,,,WOS:000854193302072,0
C,"Wang, JJ; Sun, CL; Li, SS; Wang, JC; Si, L; Zhang, M; Liu, XZ; Zhou, GD",,,Assoc Computat Linguist,"Wang, Jingjing; Sun, Changlong; Li, Shoushan; Wang, Jiancheng; Si, Luo; Zhang, Min; Liu, Xiaozhong; Zhou, Guodong",,,Human-Like Decision Making: Document-level Aspect Sentiment Classification via Hierarchical Reinforcement Learning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, neural networks have shown promising results on Document-level Aspect Sentiment Classification (DASC). However, these approaches often offer little transparency w.r.t. their inner working mechanisms and lack interpretability. In this paper, to simulating the steps of analyzing aspect sentiment in a document by human beings, we propose a new Hierarchical Reinforcement Learning (HRL) approach to DASC. This approach incorporates clause selection and word selection strategies to tackle the data noise problem in the task of DASC. First, a high-level policy is proposed to select aspect-relevant clauses and discard noisy clauses. Then, a low-level policy is proposed to select sentiment-relevant words and discard noisy words inside the selected clauses. Finally, a sentiment rating predictor is designed to provide reward signals to guide both clause and word selection. Experimental results demonstrate the impressive effectiveness of the proposed approach to DASC over the state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5581,5590,,,,,,,,,,,,,,,,WOS:000854193305073,0
C,"Wang, XZ; Wang, ZQ; Han, X; Liu, ZY; Li, JZ; Li, P; Sun, MS; Zhou, J; Ren, X",,,Assoc Computat Linguist,"Wang, Xiaozhi; Wang, Ziqi; Han, Xu; Liu, Zhiyuan; Li, Juanzi; Li, Peng; Sun, Maosong; Zhou, Jie; Ren, Xiang",,,HMEAE: Hierarchical Modular Event Argument Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.",,,,,,"Wang, Xiaozhi/0000-0002-5727-143X; Li, Peng/0000-0003-1374-5979",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5777,5783,,,,,,,,,,,,,,,,WOS:000854193305097,0
C,"Xie, JY; Pinto, RF; Hirst, G; Xu, Y",,,Assoc Computat Linguist,"Xie, Jing Yi; Pinto, Renato Ferreira, Jr.; Hirst, Graeme; Xu, Yang",,,Text-based inference of moral sentiment change,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a text-based framework for investigating moral sentiment change of the public via longitudinal corpora. Our framework is based on the premise that language use can inform people's moral perception toward right or wrong, and we build our methodology by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as slavery and democracy over centuries at three incremental levels: moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying natural language processing toward characterizing moral sentiment change in society.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4654,4663,,,,,,,,,,,,,,,,WOS:000854193304077,0
C,"Ye, QY; Liu, LY; Zhang, MS; Ren, X",,,Assoc Computat Linguist,"Ye, Qinyuan; Liu, Liyuan; Zhang, Maosen; Ren, Xiang",,,Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23% relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/ shifted-label- distribution.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3841,3850,,,,,,,,,,,,,,,,WOS:000854193304002,0
C,"Zheng, ZX; Huang, SJ; Tu, ZP; Dai, XY; Chen, JJ",,,Assoc Computat Linguist,"Zheng, Zaixiang; Huang, Shujian; Tu, Zhaopeng; Dai, Xin-Yu; Chen, Jiajun",,,Dynamic Past and Future for Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Previous studies have shown that neural machine translation (NMT) models can benefit from explicitly modeling translated (PAST) and untranslated (FUTURE) source contents as recurrent states (Zheng et al., 2018). However, this less interpretable recurrent process hinders its power to model the dynamic updating of PAST and FUTURE contents during decoding. In this paper, we propose to model the dynamic principles by explicitly separating source words into groups of translated and untranslated contents through parts-to-wholes assignment. The assignment is learned through a novel variant of routing-by-agreement mechanism (Sabour et al., 2017), namely Guided Dynamic Routing, where the translating status at each decoding step guides the routing process to assign each source word to its associated group (i.e., translated or untranslated content) represented by a capsule, enabling translation to be made from holistic context. Experiments show that our approach achieves substantial improvements over both RNMT and Transformer by producing more adequate translations. Extensive analysis demonstrates that our method is highly interpretable, which is able to recognize the translated and untranslated contents as expected.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,931,941,,,,,,,,,,,,,,,,WOS:000854193301008,0
C,"Zhou, B; Khashabi, D; Ning, Q; Roth, D",,,Assoc Computat Linguist,"Zhou, Ben; Khashabi, Daniel; Ning, Qiang; Roth, Dan",,,Going on a vacation takes longer than Going for a walk: A Study of Temporal Commonsense Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3363,3369,,,,,,,,,,,,,,,,WOS:000854193303050,0
C,"Atkinson, D; Srinivasan, KB; Tan, CH",,,Assoc Computat Linguist,"Atkinson, David; Srinivasan, Kumar Bhargav; Tan, Chenhao",,,What Gets Echoed? Understanding the Pointers in Explanations of Persuasive Arguments,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations.",,,,,,"Tan, Chenhao/0000-0002-3981-2116",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2911,2921,,,,,,,,,,,,,,,,WOS:000854193303007,0
C,"Bolt, TJ; Flynt, JH; Chaudhuri, P; Dexter, JP",,,ASSOC COMPUTAT LINGUIST,"Bolt, Thomas J.; Flynt, Jeffrey H.; Chaudhuri, Pramit; Dexter, Joseph P.",,,A Stylometry Toolkit for Latin Literature,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Computational stylometry has become an increasingly important aspect of literary criticism, but many humanists lack the technical expertise or language-specific NLP resources required to exploit computational methods. We demonstrate a stylometry toolkit for analysis of Latin literary texts, which is freely available at www.qcrit.org/stylometry. Our toolkit generates data for a diverse range of literary features and has an intuitive pointand-click interface. The features included have proven effective for multiple literary studies and are calculated using custom heuristics without the need for syntactic parsing. As such, the toolkit models one approach to the user-friendly generation of stylometric data, which could be extended to other premodern and non-English languages underserved by standard NLP resources.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,205,210,,,,,,,,,,,,,,,,WOS:000855231500035,0
C,"Du, WY; Ji, YF",,,Assoc Computat Linguist,"Du, Wanyu; Ji, Yangfeng",,,An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model(1). Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6012,6018,,,,,,,,,,,,,,,,WOS:000854193306030,0
C,"Eisenschlos, J; Ruder, S; Czapla, P; Kardas, M; Gugger, S; Howard, J",,,Assoc Computat Linguist,"Eisenschlos, Julian; Ruder, Sebastian; Czapla, Piotr; Kardas, Marcin; Gugger, Sylvain; Howard, Jeremy",,,MultiFiT: Efficient Multi-lingual Language Model Fine-tuning,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5702,5707,,,,,,,,,,,,,,,,WOS:000854193305085,0
C,"Espinosa, K; Miwa, M; Ananiadou, S",,,Assoc Computat Linguist,"Espinosa, Kurt; Miwa, Makoto; Ananiadou, Sophia",,,A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3679,3686,,,,,,,,,,,,,,,,WOS:000854193303099,0
C,"Forbes, M; Kaeser-Chen, C; Sharma, P; Belongie, S",,,Assoc Computat Linguist,"Forbes, Maxwell; Kaeser-Chen, Christine; Sharma, Piyush; Belongie, Serge",,,Neural Naturalist: Generating Fine-Grained Image Comparisons,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We introduce the new Birds-to-Words dataset of 41k sentences describing fine-grained differences between photographs of birds. The language collected is highly detailed, while remaining understandable to the everyday observer (e.g., heart-shaped face, squat body). Paragraph-length descriptions naturally adapt to varying levels of taxonomic and visual distance-drawn from a novel stratified sampling approach-with the appropriate level of detail. We propose a new model called Neural Naturalist that uses a joint image encoding and comparative module to generate comparative language, and evaluate the results with humans who must use the descriptions to distinguish real images. Our results indicate promising potential for neural models to explain differences in visual embedding space using natural language, as well as a concrete path for machine learning to aid citizen scientists in their effort to preserve biodiversity.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,708,717,,,,,,,,,,,,,,,,WOS:000854193300065,0
C,"Gao, S; Chen, XY; Li, PJ; Chan, ZM; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Gao, Shen; Chen, Xiuying; Li, Piji; Chan, Zhangming; Zhao, Dongyan; Yan, Rui",,,How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Under special circumstances, summaries should conform to a particular style with patterns, such as court judgments and abstracts in academic papers. To this end, the prototype document-summary pairs can be utilized to generate better summaries. There are two main challenges in this task: (1) the model needs to incorporate learned patterns from the prototype, but (2) should avoid copying contents other than the patternized words-such as irrelevant facts-into the generated summaries. To tackle these challenges, we design a model named Prototype Editing based Summary Generator (PESG). PESG first learns summary patterns and prototype facts by analyzing the correlation between a prototype document and its summary. Prototype facts are then utilized to help extract facts from the input document. Next, an editing generator generates new summary based on the summary pattern or extracted facts. Finally, to address the second challenge, a fact checker is used to estimate mutual information between the input document and generated summary, providing an additional signal for the generator. Extensive experiments conducted on a large-scale real-world text summarization dataset1 show that PESG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3741,3751,,,,,,,,,,,,,,,,WOS:000854193303106,0
C,"Kaliamoorthi, P; Ravi, S; Kozareva, Z",,,Assoc Computat Linguist,"Kaliamoorthi, Prabhu; Ravi, Sujith; Kozareva, Zornitsa",,,PRADO: Projection Attention Networks for Document Classification On-Device,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recently, there has been a great interest in the development of small and accurate neural networks that run entirely on devices such as mobile phones, smart watches and IoT. This enables user privacy, consistent user experience and low latency. Although a wide range of applications have been targeted from wake word detection to short text classification, yet there are no on-device networks for long text classification. We propose a novel projection attention neural network PRADO that combines trainable projections with attention and convolutions. We evaluate our approach on multiple large document text classification tasks. Our results show the effectiveness of the trainable projection model in finding semantically similar phrases and reaching high performance while maintaining compact size. Using this approach, we train tiny neural networks just 200 Kilobytes in size that improve over prior CNN and LSTM models and achieve near state of the art performance on multiple long document classification tasks. We also apply our model for transfer learning, show its robustness and ability to further improve the performance in limited data scenarios.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5012,5021,,,,,,,,,,,,,,,,WOS:000854193305019,0
C,"Mao, HH; Majumder, BP; McAuley, J; Cottrell, GW",,,Assoc Computat Linguist,"Mao, Huanru Henry; Majumder, Bodhisattwa Prasad; McAuley, Julian; Cottrell, Garrison W.",,,Improving Neural Story Generation by Targeted Common Sense Grounding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Stories generated with neural language models have shown promise in grammatical and stylistic consistency. However, the generated stories are still lacking in common sense reasoning, e.g., they often contain sentences deprived of world knowledge. We propose a simple multi-task learning scheme to achieve quantitatively better common sense reasoning in language models by leveraging auxiliary training signals from datasets designed to provide common sense grounding. When combined with our two-stage fine-tuning pipeline, our method achieves improved common sense reasoning and state-of-the-art perplexity on the Writing-Prompts (Fan et al., 2018) story generation dataset.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5988,5993,,,,,,,,,,,,,,,,WOS:000854193306026,0
C,"Niu, GC; Xu, HR; He, BL; Xiao, XY; Wu, H; Gao, S",,,Assoc Computat Linguist,"Niu, Guocheng; Xu, Hengru; He, Bolei; Xiao, Xinyan; Wu, Hua; Gao, Sheng",,,Enhancing Local Feature Extraction with Global Representation for Neural Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"For text classification, traditional local feature driven models learn long dependency by deeply stacking or hybrid modeling. This paper proposes a novel Encoder1-Encoder2 architecture, where global information is incorporated into the procedure of local feature extraction from scratch. In particular, Encoder1 serves as a global information provider, while Encoder2 performs as a local feature extractor and is directly fed into the classifier. Meanwhile, two modes are also designed for their interactions. Thanks to the awareness of global information, our method is able to learn better instance specific local features and thus avoids complicated upper operations. Experiments conducted on eight benchmark datasets demonstrate that our proposed architecture promotes local feature driven models by a substantial margin and outperforms the previous best models in the fully-supervised setting.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,496,506,,,,,,,,,,,,,,,,WOS:000854193300047,0
C,"Ousidhoum, N; Lin, ZZ; Zhang, HM; Song, YQ; Yeung, DY",,,Assoc Computat Linguist,"Ousidhoum, Nedjma; Lin, Zizheng; Zhang, Hongming; Song, Yangqiu; Yeung, Dit-Yan",,,Multilingual and Multi-Aspect Hate Speech Analysis,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current research on hate speech analysis is typically oriented towards monolingual and single classification tasks. In this paper, we present a new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches. We evaluate our dataset in various classification settings, then we discuss how to leverage our annotations in order to improve hate speech detection and classification in general.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4675,4684,,,,,,,,,,,,,,,,WOS:000854193304079,0
C,"Qin, LH; Bosselut, A; Holtzman, A; Bhagavatula, C; Clark, E; Choi, Y",,,Assoc Computat Linguist,"Qin, Lianhui; Bosselut, Antoine; Holtzman, Ari; Bhagavatula, Chandra; Clark, Elizabeth; Choi, Yejin",,,Counterfactual Story Reasoning and Generation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present TIMETRAVEL, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 81,407 counterfactual branches without a rewritten storyline to support future work on semior un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5043,5053,,,,,,,,,,,,,,,,WOS:000854193305022,0
C,"Stepanjans, A; Freitas, A",,,Assoc Computat Linguist,"Stepanjans, Armins; Freitas, Andre",,,Identifying and Explaining Discriminative Attributes,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs: (i) knowledge graphs built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the task (F1-score = 0.69), while delivering full model transparency and explainability.",,,,,,"Freitas, Andre/0000-0002-4430-4837",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4313,4322,,,,,,,,,,,,,,,,WOS:000854193304045,0
C,"Wang, S; Durrett, G; Erk, K",,,Assoc Computat Linguist,"Wang, Su; Durrett, Greg; Erk, Katrin",,,Query-Focused Scenario Construction,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The news coverage of events often contains not one but multiple incompatible accounts of what happened. We develop a query-based system that extracts compatible sets of events (scenarios) from such data, formulated as one-class clustering. Our system incrementally evaluates each event's compatibility with already selected events, taking order into account. We use synthetic data consisting of article mixtures for scalable training and evaluate our model on a new human-curated dataset of scenarios about real-world news topics. Stronger neural network models and harder synthetic training settings are both important to achieve high performance, and our final scenario construction system substantially outperforms baselines based on prior work.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2712,2722,,,,,,,,,,,,,,,,WOS:000854193302081,0
C,"Wang, Y",,,Assoc Computat Linguist,"Wang, Yu",,,Single Training Dimension Selection for Word Embedding with PCA,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we present a fast and reliable method based on PCA to select the number of dimensions for word embeddings. First, we train one embedding with a generous upper bound (e.g. 1,000) of dimensions. Then we transform the embeddings using PCA and incrementally remove the lesser dimensions one at a time while recording the embeddings' performance on language tasks. Lastly, we select the number of dimensions while balancing model size and accuracy. Experiments using various datasets and language tasks demonstrate that we are able to train 10 times fewer sets of embeddings while retaining optimal performance. Researchers interested in training the best-performing embeddings for downstream tasks, such as sentiment analysis, question answering and hypernym extraction, as well as those interested in embedding compression should find the method helpful.",,,,,,/0000-0001-5666-7881,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3597,3602,,,,,,,,,,,,,,,,WOS:000854193303087,0
C,"Wu, CH; Wu, FZ; Qi, T; Ge, SY; Huang, YF; Xie, X",,,Assoc Computat Linguist,"Wu, Chuhan; Wu, Fangzhao; Qi, Tao; Ge, Suyu; Huang, Yongfeng; Xie, Xing",,,Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"User and item representation learning is critical for recommendation. Many of existing recommendation methods learn representations of users and items based on their ratings and reviews. However, the user-user and item-item relatedness are usually not considered in these methods, which may be insufficient. In this paper, we propose a neural recommendation approach which can utilize useful information from both review content and user-item graphs. Since reviews and graphs have different characteristics, we propose to use a multi-view learning framework to incorporate them as different views. In the review content-view, we propose to use a hierarchical model to first learn sentence representations from words, then learn review representations from sentences, and finally learn user/item representations from reviews. In addition, we propose to incorporate a threelevel attention network into this view to select important words, sentences and reviews for learning informative user and item representations. In the graph-view, we propose a hierarchical graph neural network to jointly model the user-item, user-user and item-item relatedness by capturing the first- and secondorder interactions between users and items in the user-item graph. In addition, we apply attention mechanism to model the importance of these interactions to learn informative user and item representations. Extensive experiments on four benchmark datasets validate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4884,4893,,,,,,,,,,,,,,,,WOS:000854193305007,0
C,"Xiao, W; Carenini, G",,,Assoc Computat Linguist,"Xiao, Wen; Carenini, Giuseppe",,,Extractive Summarization of Long Documents by Combining Global and Local Context,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In this paper, we propose a novel neural single-document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers, Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3011,3021,,,,,,,,,,,,,,,,WOS:000854193303016,0
C,"Yang, Z; Xu, C; Wu, W; Li, ZJ",,,Assoc Computat Linguist,"Yang, Ze; Xu, Can; Wu, Wei; Li, Zhoujun",,,"Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatic news comment generation is a new testbed for techniques of natural language generation. In this paper, we propose a readattend-comment procedure for news comment generation and formalize the procedure with a reading network and a generation network. The reading network comprehends a news article and distills some important points from it, then the generation network creates a comment by attending to the extracted discrete points and the news title. We optimize the model in an end-to-end manner by maximizing a variational lower bound of the true objective using the back-propagation algorithm. Experimental results on two datasets indicate that our model can significantly outperform existing methods in terms of both automatic evaluation and human judgment.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5077,5089,,,,,,,,,,,,,,,,WOS:000854193305025,0
C,"Zheng, X; Sun, AX; Li, J; Muthuswamy, K",,,Assoc Computat Linguist,"Zheng, Xin; Sun, Aixin; Li, Jing; Muthuswamy, Karthik",,,Subtopic-driven Multi-Document Summarization,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"In multi-document summarization, a set of documents to be summarized is assumed to be on the same topic, known as the underlying topic in this paper. That is, the underlying topic can be collectively represented by all the documents in the set. Meanwhile, different documents may cover various different subtopics and the same subtopic can be across several documents. Inspired by topic model, the underlying topic of a document set can also be viewed as a collection of different subtopics of different importance. In this paper, we propose a summarization model called STDS. The model generates the underlying topic representation from both document view and subtopic view in parallel. The learning objective is to minimize the distance between the representations learned from the two views. The contextual information is encoded through a hierarchical RNN architecture. Sentence salience is estimated in a hierarchical way with subtopic salience and relative sentence salience, by considering the contextual information. Top ranked sentences are then extracted as a summary. Note that the notion of subtopic enables us to bring in additional information (e.g., comments to news articles) that is helpful for document summarization. Experimental results show that the proposed solution outperforms state-of-the-art methods on benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3153,3162,,,,,,,,,,,,,,,,WOS:000854193303029,0
C,"Zhou, K; Zhang, K; Wu, Y; Liu, SJ; Yu, JS",,,Assoc Computat Linguist,"Zhou, Kun; Zhang, Kai; Wu, Yu; Liu, Shujie; Yu, Jingsong",,,Unsupervised Context Rewriting for Open Domain Conversation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Context modeling has a pivotal role in open domain conversation. Existing works either use heuristic methods or jointly learn context modeling and response generation with an encoder-decoder framework. This paper proposes an explicit context rewriting method, which rewrites the last utterance by considering context history. We leverage pseudo-parallel data and elaborate a context rewriting network, which is built upon the CopyNet with the reinforcement learning method. The rewritten utterance is beneficial to candidate retrieval, explainable context modeling, as well as enabling to employ a single-turn framework to the multi-turn scenario. The empirical results show that our model outperforms baselines in terms of the rewriting quality, the multi-turn response generation, and the end-to-end retrieval-based chatbots.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1834,1844,,,,,,,,,,,,,,,,WOS:000854193301114,0
C,"Chang, TY; Chen, YN",,,Assoc Computat Linguist,"Chang, Ting-Yun; Chen, Yun-Nung",,,What Does This Word Mean? Explaining Contextualized Embeddings with Natural Language Definition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextualized word embeddings have boosted many NLP tasks compared with traditional static word embeddings. However, the word with a specific sense may have different contextualized embeddings due to its various contexts. To further investigate what contextualized word embeddings capture, this paper analyzes whether they can indicate the corresponding sense definitions and proposes a general framework that is capable of explaining word meanings given contextualized word embeddings for better interpretation. The experiments show that both ELMo and BERT embeddings can be well interpreted via a readable textual form, and the findings may benefit the research community for a better understanding of what the embeddings capture(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6064,6070,,,,,,,,,,,,,,,,WOS:000854193306038,0
C,"Ding, N; Li, ZR; Liu, ZY; Zheng, HT; Lin, ZB",,,Assoc Computat Linguist,"Ding, Ning; Li, Ziran; Liu, Zhiyuan; Zheng, Hai-Tao; Lin, Zibo",,,Event Detection with Trigger-Aware Lattice Neural Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Event detection (ED) aims to locate trigger words in raw text and then classify them into correct event types. In this task, neural network based models became mainstream in recent years. However, two problems arise when it comes to languages without natural delimiters, such as Chinese. First, word-based models severely suffer from the problem of wordtrigger mismatch, limiting the performance of the methods. In addition, even if trigger words could be accurately located, the ambiguity of polysemy of triggers could still affect the trigger classification stage. To address the two issues simultaneously, we propose the Trigger-aware Lattice Neural Network (TLNN). (1) The framework dynamically incorporates word and character information so that the trigger-word mismatch issue can be avoided. (2) Moreover, for polysemous characters and words, we model all senses of them with the help of an external linguistic knowledge base, so as to alleviate the problem of ambiguous triggers. Experiments on two benchmark datasets show that our model could effectively tackle the two issues and outperforms previous state-of-the-art methods significantly, giving the best results. The source code of this paper can be obtained from https://github.com/thunlp/TLNN.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,347,356,,,,,,,,,,,,,,,,WOS:000854193300033,0
C,"Ding, XA; Gimpel, K",,,Assoc Computat Linguist,"Ding, Xiaoan; Gimpel, Kevin",,,Latent-Variable Generative Models for Data-Efficient Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generative classifiers offer potential advantages over their discriminative counterparts, namely in the areas of data efficiency, robustness to data shift and adversarial examples, and zero-shot learning (Ng and Jordan, 2002; Yogatama et al., 2017; Lewis and Fan, 2019). In this paper, we improve generative text classifiers by introducing discrete latent variables into the generative story, and explore several graphical model configurations. We parameterize the distributions using standard neural architectures used in conditional language modeling and perform learning by directly maximizing the log marginal likelihood via gradient-based optimization, which avoids the need to do expectation-maximization. We empirically characterize the performance of our models on six text classification datasets. The choice of where to include the latent variable has a significant impact on performance, with the strongest results obtained when using the latent variable as an auxiliary conditioning variable in the generation of the textual input. This model consistently outperforms both the generative and discriminative classifiers in small-data settings. We analyze our model by using it for controlled generation, finding that the latent variable captures interpretable properties of the data, even with very small training sets.",,,,,"Ding, Xiaoan/HGE-5442-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,507,517,,,,,,,,,,,,,,,,WOS:000854193300048,0
C,"Eichler, M; Sahin, GG; Gurevych, I",,,ASSOC COMPUTAT LINGUIST,"Eichler, Max; Sahin, Gozde Gul; Gurevych, Iryna",,,LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present LINSPECTOR WEB, an open source multilingual inspector to analyze word representations. Our system provides researchers working in low-resource settings with an easily accessible web based probing tool to gain quick insights into their word embeddings especially outside of the English language. To do this we employ 16 simple linguistic probing tasks such as gender, case marking, and tense for a diverse set of 28 languages. We support probing of static word embeddings along with pretrained AllenNLP models that are commonly used for NLP downstream tasks such as named entity recognition, natural language inference and dependency parsing. The results are visualized in a polar chart and also provided as a table. LINSPECTOR WEB is available as an offline tool or at https://linspector. ukp.informatik.tu-darmstadt.de.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,127,132,,,,,,,,,,,,,,,,WOS:000855231500022,0
C,"Handler, A; Brendan, O",,,Assoc Computat Linguist,"Handler, Abram; Brendan, O'Connor",,,Query-focused Sentence Compression in Linear Time,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Search applications often display shortened sentences which must contain certain query terms and must fit within the space constraints of a user interface. This work introduces a new transition-based sentence compression technique developed for such settings. Our query-focused method constructs length and lexically constrained compressions in linear time, by growing a subgraph in the dependency parse of a sentence. This theoretically efficient approach achieves an 11x empirical speedup over baseline ILP methods, while better reconstructing gold constrained shortenings. Such speedups help query-focused applications, because users are measurably hindered by interface lags. Additionally, our technique does not require an ILP solver or a GPU.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5969,5975,,,,,,,,,,,,,,,,WOS:000854193306023,0
C,"Jie, ZM; Lu, W",,,Assoc Computat Linguist,"Jie, Zhanming; Lu, Wei",,,Dependency-Guided LSTM-CRF for Named Entity Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Dependency tree structures capture long-distance and syntactic relationships between words in a sentence. The syntactic relations (e.g., nominal subject, object) can potentially infer the existence of certain named entities. In addition, the performance of a named entity recognizer could benefit from the longdistance dependencies between the words in dependency trees. In this work, we propose a simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency trees and capture the above properties for the task of named entity recognition (NER). The data statistics show strong correlations between the entity types and dependency relations. We conduct extensive experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving NER and achieving state-of-theart performance. Our analysis reveals that the significant improvements mainly result from the dependency relations and long-distance interactions provided by dependency trees.",,,,,"Lu, Wei/AHA-5606-2022","Lu, Wei/0000-0003-0827-0382",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3862,3872,,,,,,,,,,,,,,,,WOS:000854193304004,0
C,"Nguyen, K; Daume, H",,,Assoc Computat Linguist,"Khanh Nguyen; Daume, Hal, III",,,"Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Mobile agents that can leverage help from humans can potentially accomplish more complex tasks than they could entirely on their own. We develop Help, Anna! (HANNA), an interactive photo-realistic simulator in which an agent fulfills object-finding tasks by requesting and interpreting natural languageand-vision assistance. An agent solving tasks in a HANNA environment can leverage simulated human assistants, called ANNA (Automatic Natural Navigation Assistants), which, upon request, provide natural language and visual instructions to direct the agent towards the goals. To address the HANNA problem, we develop a memory-augmented neural agent that hierarchically models multiple levels of decision-making, and an imitation learning algorithm that teaches the agent to avoid repeating past mistakes while simultaneously predicting its own chances of making future progress. Empirically, our approach is able to ask for help more effectively than competitive baselines and, thus, attains higher task success rate on both previously seen and previously unseen environments. We publicly release code and data at https://github. com/khanhptnk/hanna",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,684,695,,,,,,,,,,,,,,,,WOS:000854193300063,0
C,"Kim, G",,,Assoc Computat Linguist,"Kim, Gyuwan",,,Subword Language Model for Query Auto-Completion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Current neural query auto-completion (QAC) systems rely on character-level language models, but they slow down when queries are long. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with subwords shorten a decoding length significantly. To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization. As a result, our model achieves up to 2.5 times faster while maintaining a similar quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the model could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank-based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component(1).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5022,5032,,,,,,,,,,,,,,,,WOS:000854193305020,0
C,"Kobayashi, N; Hirao, T; Nakamura, K; Kamigaito, H; Okumura, M; Nagata, M",,,Assoc Computat Linguist,"Kobayashi, Naoki; Hirao, Tsutomu; Nakamura, Kengo; Kamigaito, Hidetaka; Okumura, Manabu; Nagata, Masaaki",,,Split or Merge: Which is Better for Unsupervised RST Parsing?,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on dynamic programming. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F-1 score, which is close to the scores of the previous supervised parsers.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5797,5802,,,,,,,,,,,,,,,,WOS:000854193305100,0
C,"Liu, JC; Hockenmaier, J",,,Assoc Computat Linguist,"Liu, Jiacheng; Hockenmaier, Julia",,,Phrase Grounding by Soft-Label Chain Conditional Random Field,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The phrase grounding task aims to ground each entity mention in a given caption of an image to a corresponding region in that image. Although there are clear dependencies between how different mentions of the same caption should be grounded, previous structured prediction methods that aim to capture such dependencies need to resort to approximate inference or non-differentiable losses. In this paper, we formulate phrase grounding as a sequence labeling task where we treat candidate regions as potential labels, and use neural chain Conditional Random Fields (CRFs) to model dependencies among regions for adjacent mentions. In contrast to standard sequence labeling tasks, the phrase grounding task is defined such that there may be multiple correct candidate regions. To address this multiplicity of gold labels, we define socalled Soft-Label Chain CRFs, and present an algorithm that enables convenient end-toend training. Our method establishes a new state-of-the-art on phrase grounding on the Flickr30k Entities dataset. Analysis shows that our model benefits both from the entity dependencies captured by the CRF and from the soft-label training regime. Our code is available at github.com/liujch1998/ SoftLabelCCRF",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5112,5122,,,,,,,,,,,,,,,,WOS:000854193305028,0
C,"Oren, Y; Sagawa, S; Hashimoto, T; Liang, P",,,Assoc Computat Linguist,"Oren, Yonatan; Sagawa, Shiori; Hashimoto, Tatsunori; Liang, Percy",,,Distributionally Robust Language Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4227,4237,,,,,,,,,,,,,,,,WOS:000854193304037,0
C,"Pirtoaca, GS; Rebedea, T; Ruseti, S",,,Assoc Computat Linguist,"Pirtoaca, George-Sebastian; Rebedea, Traian; Ruseti, Stefan",,,Answering questions by learning to rank - Learning to rank by answering questions,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Answering multiple-choice questions in a setting in which no supporting documents are explicitly provided continues to stand as a core problem in natural language processing. The contribution of this article is two-fold. First, it describes a method which can be used to semantically rank documents extracted from Wikipedia or similar natural language corpora. Second, we propose a model employing the semantic ranking that holds the first place in two of the most popular leaderboards for answering multiple-choice questions: ARC Easy and Challenge. To achieve this, we introduce a self-attention based neural network that latently learns to rank documents by their importance related to a given question, whilst optimizing the objective of predicting the correct answer. These documents are considered relevant contexts for the underlying question. We have published the ranked documents so that they can be used off-the-shelf to improve downstream decision models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2531,2540,,,,,,,,,,,,,,,,WOS:000854193302064,0
C,"Shi, W; Demberg, V",,,Assoc Computat Linguist,"Shi, Wei; Demberg, Vera",,,Next Sentence Prediction helps Implicit Discourse Relation Classification within and across Domains,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Implicit discourse relation classification is one of the most difficult tasks in discourse parsing. Previous studies have generally focused on extracting better representations of the relational arguments. In order to solve the task, it is however additionally necessary to capture what events are expected to cause or follow each other. Current discourse relation classifiers fall short in this respect. We here show that this shortcoming can be effectively addressed by using the bidirectional encoder representation from transformers (BERT) proposed by Devlin et al. (2019), which were trained on a next sentence prediction task, and thus encode a representation of likely next sentences. The BERT-based model outperforms the current state of the art in 11-way classification by 8% points on the standard PDTB dataset. Our experiments also demonstrate that the model can be successfully ported to other domains: on the BioDRB dataset, the model outperforms the state of the art system around 15% points.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5790,5796,,,,,,,,,,,,,,,,WOS:000854193305099,0
C,"Sung, C; Dhamecha, TI; Saha, S; Ma, TF; Reddy, V; Arora, R",,,Assoc Computat Linguist,"Sung, Chul; Dhamecha, Tejas Indulal; Saha, Swarnadeep; Ma, Tengfei; Reddy, Vinay; Arora, Rishi",,,Pre-Training BERT on Domain Resources for Short Answer Grading,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Pre-trained BERT contextualized representations have achieved state-of-the-art results on multiple downstream NLP tasks by fine-tuning with task-specific data. While there has been a lot of focus on task-specific fine-tuning, there has been limited work on improving the pretrained representations. In this paper, we explore ways of improving the pre-trained contextual representations for the task of automatic short answer grading, a critical component of intelligent tutoring systems. We show that the pre-trained BERT model can be improved by augmenting data from the domainspecific resources like textbooks. We also present a new approach to use labeled short answering grading data for further enhancement of the language model. Empirical evaluation on multi-domain datasets shows that task-specific fine-tuning on the enhanced pretrained language model achieves superior performance for short answer grading.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6071,6075,,,,,,,,,,,,,,,,WOS:000854193306039,0
C,"Tang, GB; Sennrich, R; Nivre, J",,,Assoc Computat Linguist,"Tang, Gongbo; Sennrich, Rico; Nivre, Joakim",,,Encoders Help You Disambiguate Word Senses in Neural Machine Translation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Neural machine translation (NMT) has achieved new state-of-the-art performance in translating ambiguous words. However, it is still unclear which component dominates the process of disambiguation. In this paper, we explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the distributions of self-attention. We train a classifier to predict whether a translation is correct given the representation of an ambiguous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context.",,,,,,"Sennrich, Rico/0000-0002-1438-4741",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1429,1435,,,,,,,,,,,,,,,,WOS:000854193301071,0
C,"Wang, WC; Feng, S; Wang, DL; Zhang, YF",,,Assoc Computat Linguist,"Wang, Weichao; Feng, Shi; Wang, Daling; Zhang, Yifei",,,Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Generating intriguing question is a key step towards building human-like open-domain chatbots. Although some recent works have focused on this task, compared with questions raised by humans, significant gaps remain in maintaining semantic coherence with the posts, which may result in generating dull or deviated questions. We observe that the answer has strong semantic coherence to its question and post, which can be used to guide question generation. Thus, we devise two methods to further enhance semantic coherence between post and question under the guidance of answer. First, the coherence score between generated question and answer is used as the reward function in a reinforcement learning framework, to encourage the cases that are consistent with the answer in semantic. Second, we incorporate adversarial training to explicitly control question generation in the direction of question-answer coherence. The extensive experiments show that our two methods outperform state-of-the-art baseline algorithms with large margins in raising semantic coherent questions.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5066,5076,,,,,,,,,,,,,,,,WOS:000854193305024,0
C,"Xia, C; Zhang, HX; Moghtader, J; Wu, A; Chang, KW",,,ASSOC COMPUTAT LINGUIST,"Xia, Chen; Zhang, Haoxiang; Moghtader, Jacob; Wu, Allen; Chang, Kai-Wei",,,Visualizing Trends of Key Roles in News Articles,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"There are tons of news articles generated every day reflecting the activities of key roles such as people, organizations and political parties. Analyzing these key roles allows us to understand the trends in news. In this paper, we present a demonstration system that visualizes the trend of key roles in news articles based on natural language processing techniques. Specifically, we apply a semantic role labeler and the dynamic word embedding technique to understand relationships between key roles in the news across different time periods and visualize the trends of key role and news topics change over time.",,,,,,,,,,,,,,,,,,,,,978-1-950737-92-5,,,,2019,,,,,,,247,252,,,,,,,,,,,,,,,,WOS:000855231500042,0
C,"Xing, LZ; Paul, MJ; Carenini, G",,,Assoc Computat Linguist,"Xing, Linzi; Paul, Michael J.; Carenini, Giuseppe",,,Evaluating Topic Quality with Posterior Variability,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Probabilistic topic models such as latent Dirichlet allocation (LDA) are popularly used with Bayesian inference methods such as Gibbs sampling to learn posterior distributions over topic model parameters. We derive a novel measure of LDA topic quality using the variability of the posterior distributions. Compared to several existing baselines for automatic topic evaluation, the proposed metric achieves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora.1 We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3471,3477,,,,,,,,,,,,,,,,WOS:000854193303067,0
C,"Yao, ZY; Su, Y; Sun, H; Yih, WT",,,Assoc Computat Linguist,"Yao, Ziyu; Su, Yu; Sun, Huan; Yih, Wen-tau",,,Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5447,5458,,,,,,,,,,,,,,,,WOS:000854193305060,0
C,"Yu, M; Chang, SY; Zhang, Y; Jaakkola, TS",,,Assoc Computat Linguist,"Yu, Mo; Chang, Shiyu; Zhang, Yang; Jaakkola, Tommi S.",,,Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Selective rationalization has become a common mechanism to ensure that predictive models reveal how they use any available features. The selection may be soft or hard, and identifies a subset of input features relevant for prediction. The setup can be viewed as a cooperate game between the selector (aka rationale generator) and the predictor making use of only the selected features. The co-operative setting may, however, be compromised for two reasons. First, the generator typically has no direct access to the outcome it aims to justify, resulting in poor performance. Second, there's typically no control exerted on the information left outside the selection. We revise the overall co-operative framework to address these challenges. We introduce an introspective model which explicitly predicts and incorporates the outcome into the selection process. Moreover, we explicitly control the rationale complement via an adversary so as not to leave any useful information out of the selection. We show that the two complementary mechanisms maintain both high predictive accuracy and lead to comprehensive rationales.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4094,4103,,,,,,,,,,,,,,,,WOS:000854193304025,0
C,"Yu, XT; Zhang, HM; Song, YQ; Song, Y; Zhang, CS",,,Assoc Computat Linguist,"Yu, Xintong; Zhang, Hongming; Song, Yangqiu; Song, Yan; Zhang, Changshui",,,What You See is What You Get: Visual Pronoun Coreference Resolution in Dialogues,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Grounding a pronoun to a visual object it refers to requires complex reasoning from various information sources, especially in conversational scenarios. For example, when people in a conversation talk about something all speakers can see, they often directly use pronouns (e.g., it) to refer to it without previous introduction. This fact brings a huge challenge for modern natural language understanding systems, particularly conventional contextbased pronoun coreference models. To tackle this challenge, in this paper, we formally define the task of visual-aware pronoun coreference resolution (PCR) and introduce VisPro, a large-scale dialogue PCR dataset, to investigate whether and how the visual information can help resolve pronouns in dialogues. We then propose a novel visual-aware PCR model, VisCoref, for this task and conduct comprehensive experiments and case studies on our dataset. Results demonstrate the importance of the visual information in this PCR case and show the effectiveness of the proposed model.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5123,5132,,,,,,,,,,,,,,,,WOS:000854193305029,0
C,"Zhao, ZJ; Cattle, A; Papalexakis, EE; Ma, XJ",,,Assoc Computat Linguist,"Zhao, Zhenjie; Cattle, Andrew; Papalexakis, Evangelos E.; Ma, Xiaojuan",,,Embedding Lexical Features via Tensor Decomposition for Small Sample Humor Recognition,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We propose a novel tensor embedding method that can effectively extract lexical features for humor recognition. Specifically, we use word-word co-occurrence to encode the contextual content of documents, and then decompose the tensor to get corresponding vector representations. We show that this simple method can capture features of lexical humor effectively for continuous humor recognition. In particular, we achieve a distance of 0.887 on a global humor ranking task, comparable to the top performing systems from SemEval 2017 Task 6B (Potash et al., 2017) but without the need for any external training corpus. In addition, we further show that this approach is also beneficial for small sample humor recognition tasks through a semi-supervised label propagation procedure, which achieves about 0.7 accuracy on the 16000 One-Liners (Mihalcea and Strapparava, 2005) and Pun of the Day (Yang et al., 2015) humour classification datasets using only 10% of known labels.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6376,6381,,,,,,,,,,,,,,,,WOS:000854193306080,0
C,"Andrews, N; Bishop, M",,,Assoc Computat Linguist,"Andrews, Nicholas; Bishop, Marcus",,,Learning Invariant Representations of Social Media Users,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The evolution of social media users' behavior over time complicates user-level comparison tasks such as verification, classification, clustering, and ranking. As a result, naive approaches may fail to generalize to new users or even to future observations of previously known users. In this paper, we propose a novel procedure to learn a mapping from short episodes of user activity on social media to a vector space in which the distance between points captures the similarity of the corresponding users' invariant features. We fit the model by optimizing a surrogate metric learning objective over a large corpus of unlabeled social media content. Once learned, the mapping may be applied to users not seen at training time and enables efficient comparisons of users in the resulting vector space. We present a comprehensive evaluation to validate the benefits of the proposed approach using data from Reddit, Twitter, and Wikipedia.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1684,1695,,,,,,,,,,,,,,,,WOS:000854193301100,0
C,"Chen, HJ; Cai, D; Dai, W; Dai, ZH; Ding, YD",,,Assoc Computat Linguist,"Chen, Huajie; Cai, Deng; Dai, Wei; Dai, Zehui; Ding, Yadong",,,Charge-Based Prison Term Prediction with Deep Gating Network,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Judgment prediction for legal cases has attracted much research efforts for its practice use, of which the ultimate goal is prison term prediction. While existing work merely predicts the total prison term, in reality a defendant is often charged with multiple crimes. In this paper, we argue that charge-based prison term prediction (CPTP) not only better fits realistic needs, but also makes the total prison term prediction more accurate and interpretable. We collect the first large-scale structured data for CPTP and evaluate several competitive baselines. Based on the observation that fine-grained feature selection is the key to achieving good performance, we propose the Deep Gating Network (DGN) for charge-specific feature selection and aggregation. Experiments show that DGN achieves the state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6362,6367,,,,,,,,,,,,,,,,WOS:000854193306078,0
C,"Dong, X; de Melo, G",,,Assoc Computat Linguist,"Dong, Xin; de Melo, Gerard",,,A Robust Self-Learning Framework for Cross-Lingual Text Classification,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Based on massive amounts of data, recent pretrained contextual representation models have made significant strides in advancing a number of different English NLP tasks. However, for other languages, relevant training data may be lacking, while state-of-the-art deep learning methods are known to be data-hungry. In this paper, we present an elegantly simple robust self-learning framework to include unlabeled non-English samples in the fine-tuning process of pretrained multilingual representation models. We leverage a multilingual model's own predictions on unlabeled nonEnglish data in order to obtain additional information that can be used during further finetuning. Compared with original multilingual models and other cross-lingual classification models, we observe significant gains in effectiveness on document and sentiment classification for a range of diverse languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6306,6310,,,,,,,,,,,,,,,,WOS:000854193306069,0
C,"Gong, H; Feng, XC; Qin, B; Liu, T",,,Assoc Computat Linguist,"Gong, Heng; Feng, Xiaocheng; Qin, Bing; Liu, Ting",,,"Table-to-Text Generation with Effective Hierarchical Encoder on Three Dimensions (Row, Column and Time)",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Although Seq2Seq models for table-to-text generation have achieved remarkable progress, modeling table representation in one dimension is inadequate. This is because (1) the table consists of multiple rows and columns, which means that encoding a table should not depend only on one dimensional sequence or set of records and (2) most of the tables are time series data (e.g. NBA game data, stock market data), which means that the description of the current table may be affected by its historical data. To address aforementioned problems, not only do we model each table cell considering other records in the same row, we also enrich table's representation by modeling each table cell in context of other cells in the same column or with historical (time dimension) data respectively. In addition, we develop a table cell fusion gate to combine representations from row, column and time dimension into one dense vector according to the saliency of each dimension's representation. We evaluated our methods on ROTOWIRE, a benchmark dataset of NBA basketball games. Both automatic and human evaluation results demonstrate the effectiveness of our model with improvement of 2.66 in BLEU over the strong baseline and outperformance of state-of-the-art model.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3143,3152,,,,,,,,,,,,,,,,WOS:000854193303028,0
C,"Hadiwinoto, C; Ng, HT; Gan, WC",,,Assoc Computat Linguist,"Hadiwinoto, Christian; Ng, Hwee Tou; Gan, Wee Chung",,,Improved Word Sense Disambiguation Using Pre-Trained ContextualizedWord Representations,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Contextualized word representations are able to give different representations for the same word in different contexts, and they have been shown to be effective in downstream natural language processing tasks, such as question answering, named entity recognition, and sentiment analysis. However, evaluation on word sense disambiguation (WSD) in prior work shows that using contextualized word representations does not outperform the state-of-the-art approach that makes use of noncontextualized word embeddings. In this paper, we explore different strategies of integrating pre-trained contextualized word representations and our best strategy achieves accuracies exceeding the best prior published accuracies by significant margins on multiple benchmark WSD datasets.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5297,5306,,,,,,,,,,,,,,,,WOS:000854193305046,0
C,"Hessel, J; Lee, L; Mimno, D",,,Assoc Computat Linguist,"Hessel, Jack; Lee, Lillian; Mimno, David",,,"Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents",2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2034,2045,,,,,,,,,,,,,,,,WOS:000854193302018,0
C,"Kang, GC; Lim, J; Zhang, BT",,,Assoc Computat Linguist,"Kang, Gi-Cheon; Lim, Jaeseo; Zhang, Byoung-Tak",,,Dual Attention Networks for Visual Reference Resolution in Visual Dialog,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Visual dialog (VisDial) is a task which requires a dialog agent to answer a series of questions grounded in an image. Unlike in visual question answering (VQA), the series of questions should be able to capture a temporal context from a dialog history and utilizes visually-grounded information. Visual reference resolution is a problem that addresses these challenges, requiring the agent to resolve ambiguous references in a given question and to find the references in a given image. In this paper, we propose Dual Attention Networks (DAN) for visual reference resolution in VisDial. DAN consists of two kinds of attention modules, REFER and FIND. Specifically, REFER module learns latent relationships between a given question and a dialog history by employing a multi-head attention mechanism. FIND module takes image features and reference-aware representations (i.e., the output of REFER module) as input, and performs visual grounding via bottom-up attention mechanism. We qualitatively and quantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art model by a significant margin.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2024,2033,,,,,,,,,,,,,,,,WOS:000854193302017,0
C,"Lei, WQ; Xu, WW; Aw, AT; Xiang, YX; Chua, TS",,,Assoc Computat Linguist,"Lei, Wenqiang; Xu, Weiwen; Aw, Ai Ti; Xiang, Yuanxin; Chua, Tat-Seng",,,Revisit Automatic Error Detection for Wrong and Missing Translation - A Supervised Approach,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While achieving great fluency, current machine translation (MT) techniques are bottle-necked by adequacy issues. To have a closer study of these issues and accelerate model development, we propose automatic detecting adequacy errors in MT hypothesis for MT model evaluation. To do that, we annotate missing and wrong translations, the two most prevalent issues for current neural machine translation model, in 15000 Chinese-English translation pairs. We build a supervised alignment model for translation error detection (AlignDet) based on a simple Alignment Triangle strategy to set the benchmark for automatic error detection task. We also discuss the difficulties of this task and the benefits of this task for existing evaluation metrics.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,942,952,,,,,,,,,,,,,,,,WOS:000854193301009,0
C,"Muller, T; Piccinno, F; Nicosia, M; Shaw, P; Altun, Y",,,Assoc Computat Linguist,"Mueller, Thomas; Piccinno, Francesco; Nicosia, Massimo; Shaw, Peter; Altun, Yasemin",,,Answering Conversational Questions on Structured Data without Logical Forms,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present a novel approach to answering sequential questions based on structured objects such as knowledge bases or tables without using a logical form as an intermediate representation. We encode tables as graphs using a graph neural network model based on the Transformer architecture. The answers are then selected from the encoded graph using a pointer network. This model is appropriate for processing conversations around structured data, where the attention mechanism that selects the answers to a question can also be used to resolve conversational references. We demonstrate the validity of this approach with competitive results on the Sequential Question Answering (SQA) task (Iyyer et al., 2017).",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5902,5910,,,,,,,,,,,,,,,,WOS:000854193306014,0
C,"Nie, YX; Wang, SH; Bansal, M",,,Assoc Computat Linguist,"Nie, Yixin; Wang, Songhe; Bansal, Mohit",,,Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Machine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by reading information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2553,2566,,,,,,,,,,,,,,,,WOS:000854193302066,0
C,"Nishino, M; Takase, S; Hirao, T; Nagata, M",,,Assoc Computat Linguist,"Nishino, Masaaki; Takase, Sho; Hirao, Tsutomu; Nagata, Masaaki",,,Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"An anagram is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, Trims cash is an anagram of Christmas. Existing automatic anagram generation methods can find possible combinations of words form an anagram. However, they do not pay much attention to the naturalness of the generated anagrams. In this paper, we show that simple depth-first search can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed method can generate significantly more natural anagrams than baseline methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,6408,6412,,,,,,,,,,,,,,,,WOS:000854193306085,0
C,"Niu, T; Bansal, M",,,Assoc Computat Linguist,"Niu, Tong; Bansal, Mohit",,,Automatically Learning Data Augmentation Policies for Dialogue Tasks,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,1317,1323,,,,,,,,,,,,,,,,WOS:000854193301054,0
C,"Peng, H; Schwartz, R; Smith, NA",,,Assoc Computat Linguist,"Peng, Hao; Schwartz, Roy; Smith, Noah A.",,,PaLM: A Hybrid Parser and Language Model,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"We present PaLM, a hybrid parser and neural language model. Building on an RNN language model, PaLM adds an attention layer over text spans in the left context. An unsupervised constituency parser can be derived from its attention weights, using a greedy decoding algorithm. We evaluate PaLM on language modeling, and empirically show that it outperforms strong baselines. If syntactic annotations are available, the attention component can be trained in a supervised manner, providing syntactically-informed representations of the context, and further improving language modeling performance.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3644,3651,,,,,,,,,,,,,,,,WOS:000854193303094,0
C,"Qin, LB; Che, WX; Li, YM; Wen, HY; Liu, T",,,Assoc Computat Linguist,"Qin, Libo; Che, Wanxiang; Li, Yangming; Wen, Haoyang; Liu, Ting",,,A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guides the slot filling. In our framework, we adopt a joint model with StackPropagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,2078,2087,,,,,,,,,,,,,,,,WOS:000854193302022,0
C,"Rao, JF; Liu, LQ; Tay, Y; Yang, W; Shi, P; Lin, J",,,Assoc Computat Linguist,"Rao, Jinfeng; Liu, Linqing; Tay, Yi; Yang, Wei; Shi, Peng; Lin, Jimmy",,,Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user's query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5370,5381,,,,,,,,,,,,,,,,WOS:000854193305053,0
C,"Shimorina, A; Gardent, C",,,Assoc Computat Linguist,"Shimorina, Anastasia; Gardent, Claire",,,Surface Realisation Using Full Delexicalisation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Surface realisation (SR) maps a meaning representation to a sentence and can be viewed as consisting of three subtasks: word ordering, morphological inflection and contraction generation (e.g., clitic attachment in Portuguese or elision in French). We propose a modular approach to surface realisation which models each of these components separately, and evaluate our approach on the 10 languages covered by the SR'18 Surface Realisation Shared Task shallow track. We provide a detailed evaluation of how word order, morphological realisation and contractions are handled by the model and an analysis of the differences in word ordering performance across languages.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3086,3096,,,,,,,,,,,,,,,,WOS:000854193303023,0
C,"Trichelair, P; Emami, A; Trischler, A; Suleman, K; Cheung, JCK",,,Assoc Computat Linguist,"Trichelair, Paul; Emami, Ali; Trischler, Adam; Suleman, Kaheer; Cheung, Jackie Chi Kit",,,How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge (WSC) and SWAG. The question we ask in this paper is whether improved performance on these benchmarks represents genuine progress towards common-sense-enabled systems. We make case studies of both benchmarks and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,3382,3387,,,,,,,,,,,,,,,,WOS:000854193303053,0
C,"Vulic, I; Glavas, G; Reichart, R; Korhonen, A",,,Assoc Computat Linguist,"Vulic, Ivan; Glavas, Goran; Reichart, Roi; Korhonen, Anna",,,Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Recent efforts in cross-lingual word embedding (CLWE) learning have predominantly focused on fully unsupervised approaches that project monolingual embeddings into a shared cross-lingual space without any cross-lingual signal. The lack of any supervision makes such approaches conceptually attractive. Yet, their only core difference from (weakly) supervised projection-based CLWE methods is in the way they obtain a seed dictionary used to initialize an iterative self-learning procedure. The fully unsupervised methods have arguably become more robust, and their primary use case is CLWE induction for pairs of resourcepoor and distant languages. In this paper, we question the ability of even the most robust unsupervised CLWE approaches to induce meaningful CLWEs in these more challenging settings. A series of bilingual lexicon induction (BLI) experiments with 15 diverse languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4407,4418,,,,,,,,,,,,,,,,WOS:000854193304054,0
C,"Wang, S; Liu, Y; Wang, C; Luan, HB; Sun, MS",,,Assoc Computat Linguist,"Wang, Shuo; Liu, Yang; Wang, Chao; Luan, Huanbo; Sun, Maosong",,,Improving Back-Translation with Uncertainty-based Confidence Estimation,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"While back-translation is simple and effective in exploiting abundant monolingual corpora to improve low-resource neural machine translation (NMT), the synthetic bilingual corpora generated by NMT models trained on limited authentic bilingual data are inevitably noisy. In this work, we propose to quantify the confidence of NMT model predictions based on model uncertainty. With word- and sentence-level confidence measures based on uncertainty, it is possible for back-translation to better cope with noise in synthetic bilingual corpora. Experiments on Chinese-English and English-German translation tasks show that uncertainty-based confidence estimation significantly improves the performance of back-translation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,791,802,,,,,,,,,,,,,,,,WOS:000854193300073,0
C,"Wu, CH; Wu, FZ; An, MX; Qi, T; Huang, JQ; Huang, YF; Xie, X",,,Assoc Computat Linguist,"Wu, Chuhan; Wu, Fangzhao; An, Mingxiao; Qi, Tao; Huang, Jianqiang; Huang, Yongfeng; Xie, Xing",,,Neural News Recommendation with Heterogeneous User Behavior,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"News recommendation is important for online news platforms to help users find interested news and alleviate information overload. Existing news recommendation methods usually rely on the news click history to model user interest. However, these methods may suffer from the data sparsity problem, since the news click behaviors of many users in online news platforms are usually very limited. Fortunately, some other kinds of user behaviors such as webpage browsing and search queries can also provide useful clues of users' news reading interest. In this paper, we propose a neural news recommendation approach which can exploit heterogeneous user behaviors. Our approach contains two major modules, i.e., news representation and user representation. In the news representation module, we learn representations of news from their titles via CNN networks, and apply attention networks to select important words. In the user representation module, we propose an attentive multi-view learning framework to learn unified representations of users from their heterogeneous behaviors such as search queries, clicked news and browsed webpages. In addition, we use word- and record-level attentions to select informative words and behavior records. Experiments on a real-world dataset validate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,4874,4883,,,,,,,,,,,,,,,,WOS:000854193305006,0
C,"Yan, LY; Han, XP; Sun, L; He, B",,,Assoc Computat Linguist,"Yan, Lingyong; Han, Xianpei; Sun, Le; He, Ben",,,Learning to Bootstrap for Entity Set Expansion,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"Bootstrapping for Entity Set Expansion (ESE) aims at iteratively acquiring new instances of a specific target category. Traditional bootstrapping methods often suffer from two problems: 1) delayed feedback, i.e., the pattern evaluation relies on both its direct extraction quality and the extraction quality in later iterations. 2) sparse supervision, i.e., only few seed entities are used as the supervision. To address the above two problems, we propose a novel bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network, which can efficiently estimate delayed feedback for pattern evaluation and adaptively score entities given sparse supervision signals. Experimental results confirm the effectiveness of the proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,292,301,,,,,,,,,,,,,,,,WOS:000854193300028,0
C,"Zhang, BA; Titov, I; Sennrich, R",,,Assoc Computat Linguist,"Zhang, Biao; Titov, Ivan; Sennrich, Rico",,,Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connections and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt.(1)",,,,,,"Sennrich, Rico/0000-0002-1438-4741",,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,898,909,,,,,,,,,,,,,,,,WOS:000854193301005,0
C,"Zhou, XB; Wang, ZQ; Li, SS; Zhou, GD; Zhang, M",,,Assoc Computat Linguist,"Zhou, Xiabing; Wang, Zhongqing; Li, Shoushan; Zhou, Guodong; Zhang, Min",,,Emotion Detection with Neural Personal Discrimination,2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF THE CONFERENCE,,,,,Conference on Empirical Methods in Natural Language Processing / 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"NOV 03-07, 2019","Hong Kong, HONG KONG","Google,Facebook,Apple,ASAPP,Salesforce,Huawei,Baidu,Deepmind,Amazon,PolyAI,Naver,ByteDance,Megagon Labs,Zhuiyi,Verisk,MI",,,,"There have been a recent line of works to automatically predict the emotions of posts in social media. Existing approaches consider the posts individually and predict their emotions independently. Different from previous researches, we explore the dependence among relevant posts via the authors' backgrounds, since the authors with similar backgrounds, e.g., gender, location, tend to express similar emotions. However, such personal attributes are not easy to obtain in most social media websites, and it is hard to capture attributesaware words to connect similar people. Accordingly, we propose a Neural Personal Discrimination (NPD) approach to address above challenges by determining personal attributes from posts, and connecting relevant posts with similar attributes to jointly learn their emotions. In particular, we employ adversarial discriminators to determine the personal attributes, with attention mechanisms to aggregate attributes-aware words. In this way, social correlationship among different posts can be better addressed. Experimental results show the usefulness of personal attributes, and the effectiveness of our proposed NPD approach in capturing such personal attributes with significant gains over the state-of-the-art models.",,,,,,,,,,,,,,,,,,,,,978-1-950737-90-1,,,,2019,,,,,,,5499,5507,,,,,,,,,,,,,,,,WOS:000854193305065,0
C,"Kallmeyer, L",,"Schutze, H; Su, KY",,"Kallmeyer, L",,,A query tool for syntactically annotated corpora,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper presents a query tool for syntactically annotated corpora. The query tool is developed to search the Verbmobil treebanks annotated at the University of Tubingen. However, in principle it also can be adapted to other corpora such as the Negra Corpus, the Penn Treebank or the French treebank developed in Paris. The tool uses a query language that allows to search for tokens, syntactic categories, grammatical functions and binary relations of (immediate) dominance and linear precedence between nodes. The overall idea is to extract in an initializing phase the relevant information from the corpus and store it in a relational database. An incoming query is then translated into a corresponding SQL query that is evaluated on the database.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,190,198,,,,,,,,,,,,,,,,WOS:000223092900024,0
C,"Xu, JX; Weischedel, R",,"Schutze, H; Su, KY",,"Xu, JX; Weischedel, R",,,Cross-lingual information retrieval using hidden Markov models,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4). Since our interest is in languages where resources may be minimal, we use an integrated probabilistic model that requires only a bilingual dictionary as a resource. We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity. In addition, we estimate an upper bound on performance, if translation ambiguity were a solved problem. We also measure performance as a function of bilingual dictionary size.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,95,103,,,,,,,,,,,,,,,,WOS:000223092900012,0
C,"Kudo, T; Matsumoto, Y",,"Schutze, H; Su, KY",,"Kudo, T; Matsumoto, Y",,,Japanese dependency structure analysis based on support vector machines,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper presents a method of Japanese dependency structure analysis based on Support Vector Machines (SVMs). Conventional parsing techniques based on Machine Learning framework, such as Decision Trees and Maximum Entropy Models, have difficulty in selecting useful features as well as finding appropriate combination of selected features. On the other hand, it is well-known that SVMs achieve high generalization performance even with input data of very high dimensional feature space. Further-more, by introducing the Kernel principle, SVMs can carry out the training in high-dimensional spaces with a smaller computational cost independent of their dimensionality. We apply SVMs to Japanese dependency structure identification problem. Experimental results on Kyoto University corpus show that our system achieves the accuracy of 89.09% even with small training data (7958 sentences).",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,18,25,,,,,,,,,,,,,,,,WOS:000223092900003,0
C,"Korhonen, A",,"Schutze, H; Su, KY",,"Korhonen, A",,,Using semantically motivated estimates to help subcategorization acquisition,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Research into the automatic acquisition of subcategorization frames from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One source of error lies in the lack of accurate back-off estimates for subcategorization frames, delimiting the performance of statistical techniques frequently employed in verbal acquisition. In this paper, we propose a method of obtaining more accurate, semantically motivated back-off estimates, demonstrate how these estimates can be used to improve the learning of subcategorization frames, and discuss using the method to benefit large-scale lexical acquisition.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,216,223,,,,,,,,,,,,,,,,WOS:000223092900027,0
C,"Zhou, JF; Liu, WQ",,"Schutze, H; Su, KY",,"Zhou, JF; Liu, WQ",,,A real-time integration of concept-based search and summarization on Chinese websites,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper introduces an intuitive search environment for casual and novice Chinese users over Internet. The system consists of four components, a concept network, a query reformulation model, a standard search engine, and an automatic summarizer. When the user enters one or more fairly general and vague terms, the search engine returns an initial answer set, and at the same time pipes the query to the concept network that connects thousands of conceptual nodes, each referring to a specific concept for a given domain and pointing to a number of associated conceptual terms. If the concept is located in the network, the related conceptual terms are displayed. The user has the option of using one or more of these specific terms to reformulate the next round of searches. Such search iterations continue until the user's ultimate information seeking goal is reached. For each search iteration, auto summarizer presents the main theme of the document retrieved and an optional text-to-speech engine can read out the output summary if the user prefers.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,148,154,,,,,,,,,,,,,,,,WOS:000223092900019,0
C,"Brill, E",,"Schutze, H; Su, KY",,"Brill, E",,,Pattern-based disambiguation for natural language processing,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site. In this paper we demonstrate that classification accuracy can be improved by invoking a more descriptive feature set than what is typically used. We present a technique that disambiguates by learning regular expressions describing the string contexts in which the ambiguity sites appear.,,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,1,8,,,,,,,,,,,,,,,,WOS:000223092900001,0
C,"Le, S; Youbing, J; Du, L; Wang, SF",,"Schutze, H; Su, KY",,"Le, S; Youbing, J; Du, L; Wang, SF",,,Word alignment of English-Chinese bilingual corpus based on chunks,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed. The chunks of English sentences are identified firstly. Then the chunk boundaries of Chinese sentences are predicted by the translations of English chunks and heuristic information. The ambiguities of Chinese chunk boundaries are resolved by the coterminous words in English chunks. With the chunk aligned bilingual corpus, a translation relation probability is proposed to align words. Finally, we evaluate our system by real corpus and present the experiment results.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,110,116,,,,,,,,,,,,,,,,WOS:000223092900014,0
C,"Xia, F; Palmer, M; Joshi, A",,"Schutze, H; Su, KY",,"Xia, F; Palmer, M; Joshi, A",,,A uniform method of grammar extraction and its applications,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of hand-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,53,62,,,,,,,,,,,,,,,,WOS:000223092900007,0
C,"Hwa, R",,"Schutze, H; Su, KY",,"Hwa, R",,,Sample selection for statistical grammar induction,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Corpus-based grammar induction relies on using many hand-parsed sentences as training examples. However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task. We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators. This paper shows that the amount of annotated training data can be reduced by 36% without degrading the quality of the induced grammars.",,,,,,"Hwa, Rebecca/0000-0003-1158-7014",,,,,,,,,,,,,,,,,,,2000,,,,,,,45,52,,,,,,,,,,,,,,,,WOS:000223092900006,0
C,"Zhang, YB; Sun, L; Du, L; Sun, Y",,"Schutze, H; Su, KY",,"Zhang, YB; Sun, L; Du, L; Sun, Y",,,Query translation in Chinese-English cross-language information retrieval,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora. Instead of looking up a bilingual phrase dictionary, the compositional phrase (the translation of phrase can be derived from the translation of its components) in the query can be indirectly translated via a general-purpose Chinese-English dictionary look-up procedure. A novel selection method for translations of query terms is also presented in detail. Our query translation method ultimately constructs an English query in which each query term has a weight. The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple word-by-word translation way.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,104,109,,,,,,,,,,,,,,,,WOS:000223092900013,0
C,"Korhonen, A; Gorrell, G; McCarthy, D",,"Schutze, H; Su, KY",,"Korhonen, A; Gorrell, G; McCarthy, D",,,Statistical filtering and subcategorization frame acquisition,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Research into the automatic acquisition of subcategorization frames (SCFS) from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One significant source of error lies in the statistical filtering used by some researchers to remove noise from automatically acquired sub categorization frames. In this paper, we compare three different approaches to filtering out spurious hypotheses. Two hypothesis tests perform poorly, compared to filtering frames on the basis of relative frequency. We discuss reasons for this and consider directions for future research.",,,,,,"Gorrell, Genevieve/0000-0002-8324-606X",,,,,,,,,,,,,,,,,,,2000,,,,,,,199,206,,,,,,,,,,,,,,,,WOS:000223092900025,0
C,"Escudero, G; Marquez, L; Rigau, G",,"Schutze, H; Su, KY",,"Escudero, G; Marquez, L; Rigau, G",,,An empirical study of the domain dependence of supervised word sense disambiguation systems,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambiguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a different corpus from that they were trained oil; exploring their ability to tune to new domains, and demonstrating empirically that the Lazy-Boosting algorithm outperforms state-of-the-art supervised WSD algorithms in both previous situations.",,,,,"Rigau, German/H-7235-2015","Rigau, German/0000-0003-1119-0930",,,,,,,,,,,,,,,,,,,2000,,,,,,,172,180,,,,,,,,,,,,,,,,WOS:000223092900022,0
C,"Umemura, K; Church, KW",,"Schutze, H; Su, KY",,"Umemura, K; Church, KW",,,Empirical term weighting and expansion frequency,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"We propose an empirical method for estimating term weights directly from relevance judgements, avoiding various standard but potentially troublesome assumptions. It is common to assume, for example, that weights vary with term frequency (tf) and inverse document frequency (idf) in a particular way, e.g., tf (.) idf, but the fact that there axe so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions. Our method is similar to the Berkeley regression method where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc. Training methods not only improve performance, but also extend naturally to include additional factors such as burstiness and query expansion. The proposed histogram-based training method provides a simple way to model complicated interactions among factors such as tf, idf, burstiness and expansion frequency (a generalization of query expansion) The correct handling of expanded term is realized based on statistical information. Expansion frequency dramatically improves performance from a level comparable to BKJJBIDS, Berkeley's entry in the Japanese NACSIS NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation. JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry. We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance.",,,,,"Church, Kenneth/AAV-9667-2021; Church, Kenneth/G-3167-2010; Church, Kenneth/GYR-1624-2022","Church, Kenneth/0000-0001-8378-6069; ",,,,,,,,,,,,,,,,,,,2000,,,,,,,117,123,,,,,,,,,,,,,,,,WOS:000223092900015,0
C,"Martinez, D; Agirre, E",,"Schutze, H; Su, KY",,"Martinez, D; Agirre, E",,,One sense per collocation and genre/topic variations,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora. We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities). We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to the other, following genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models.",,,,,"AGIRRE, ENEKO/H-7323-2015","AGIRRE, ENEKO/0000-0002-0195-4899; agirre, eneko/0000-0003-0775-6057",,,,,,,,,,,,,,,,,,,2000,,,,,,,207,215,,,,,,,,,,,,,,,,WOS:000223092900026,0
C,"Lee, C; Lee, G; Jung Yun, S",,"Schutze, H; Su, KY",,"Lee, C; Lee, G; Jung Yun, S",,,Automatic WordNet mapping using word sense disambiguation,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,This paper presents the automatic construction of a Korean WordNet from pre-existing lexical resources. A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets. We will show how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns.,,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,142,147,,,,,,,,,,,,,,,,WOS:000223092900018,0
C,"Tang, LR; Mooney, RJ",,"Schutze, H; Su, KY",,"Tang, LR; Mooney, RJ",,,Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,The development of natural language interfaces (NLI's) for databases has been a challenging problem in natural language processing (NLP) since the 1970's. The need for NLI's has become more pronounced due to the widespread access to complex databases now available through the Internet. A challenging problem for empirical NLP is the automated acquisition of NLI's from training examples. We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches. Experimental results from three different domains suggest that such an approach is more robust than a previous purely logic-based approach.,,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,133,141,,,,,,,,,,,,,,,,WOS:000223092900017,0
C,"Jacquemin, C; Bush, C",,"Schutze, H; Su, KY",,"Jacquemin, C; Bush, C",,,Combining lexical and formatting cues for named entity aquisition from the web,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Because of their constant renewal, it is necessary to acquire fresh named entities (NEs) from recent text sources. We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructions (HTML tags) for analyzing enumerations and associated initializers.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,181,189,,,,,,,,,,,,,,,,WOS:000223092900023,0
C,"Kim, SD; Zhang, BT; Kim, YT",,"Schutze, H; Su, KY",,"Kim, SD; Zhang, BT; Kim, YT",,,Reducing parsing complexity by intra-sentence segmentation based on maximum entropy model,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Long sentence analysis has been a critical problem because of high complexity. This paper addresses the reduction of parsing complexity by intra-sentence segmentation, and presents maximum entropy model for determining segmentation positions. The model features lexical contexts of segmentation positions, giving a probability to each potential position. Segmentation coverage and accurarcy of the proposed method are 96% and 88% respectively. The parsing efficiency is improved by 77% in time and 71% in space.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,164,171,,,,,,,,,,,,,,,,WOS:000223092900021,0
C,"Ng, HT; Teo, LH; Kwan, JLP",,"Schutze, H; Su, KY",,"Ng, HT; Teo, LH; Kwan, JLP",,,A machine learning approach to answering questions for reading comprehension tests,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"In this paper, we report results on answering questions for the reading comprehension task, using a machine learning approach. We evaluated our approach on the Remedia data set, a common data set used in several recent papers on the reading comprehension task. Our learning approach achieves accuracy competitive to previous approaches that rely on hand-crafted, deterministic rules and algorithms. To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,124,132,,,,,,,,,,,,,,,,WOS:000223092900016,0
C,"Li, H; Yamanishi, K",,"Schutze, H; Su, KY",,"Li, H; Yamanishi, K",,,Topic analysis using a finite mixture model,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"We address the issue of 'topic analysis,' by which is determined a text's topic structure, which indicates what topics are included in a text, and how topics change within the text. We propose a novel approach to this issue, one based on statistical modeling and learning. We represent topics by means of word clusters, and employ a finite mixture model to represent a word distribution within a text. Our experimental results indicate that our method significantly outperforms a method that combines existing techniques.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,35,44,,,,,,,,,,,,,,,,WOS:000223092900005,0
C,"Teufel, S; Moens, M",,"Schutze, H; Su, KY",,"Teufel, S; Moens, M",,,What's yours and what's mine: Determining intellectual attribution in scientific text,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"We believe that identifying the structure of scientific argumentation in articles can help in tasks such as automatic summarization or the automated construction of citation indexes. One particularly important aspect of this structure is the question of who a given scientific statement is attributed to: other researchers, the field in general, or the authors themselves. We present the algorithm and a systematic evaluation of a system which can recognize the most salient textual properties that contribute to the global argumentative structure of a text. In this paper we concentrate on two particular features, namely the occurrences of prototypical agents and their actions in scientific text.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,9,17,,,,,,,,,,,,,,,,WOS:000223092900002,0
C,"Bikel, DM",,"Schutze, H; Su, KY",,"Bikel, DM",,,A statistical model for parsing and word-sense disambiguation,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,155,163,,,,,,,,,,,,,,,,WOS:000223092900020,0
C,"Florian, R; Henderson, JC; Ngai, G",,"Schutze, H; Su, KY",,"Florian, R; Henderson, JC; Ngai, G",,,Coaxing confidences from an old friend: Probabilistic classifications from transformation rule lists,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"Transformation-based learning has been successfully employed to solve many natural language processing problems. It has many positive features, but one drawback is that it does not provide estimates of class membership probabilities. In this paper, we present a novel method for obtaining class membership probabilities from a transformation-based rule fist classifier. Three experiments are presented which measure the modeling accuracy and cross-entropy of the probabilistic classifier on unseen data and the degree to which the output probabilities from the classifier can be used to estimate confidences in its classification decisions. The results of these experiments show that, for the task of text chunking(1), the estimates produced by this technique axe more informative than those generated by a state-of-the-art decision tree.",,,,,"Ngai, Grace/A-1846-2014; Florian, Radu/AAF-1434-2019","Ngai, Grace/0000-0002-2027-168X; ",,,,,,,,,,,,,,,,,,,2000,,,,,,,26,34,,,,,,,,,,,,,,,,WOS:000223092900004,0
C,"Kurohashi, S; Ori, M",,"Schutze, H; Su, KY",,"Kurohashi, S; Ori, M",,,Nonlocal language modeling based on context co-occurrence vectors,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper presents a novel nonlocal language model which utilizes contextual information. A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors. The sum of word co-occurrence vectors represents the context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the long-distance lexical dependencies. Experiments on the Mainichi Newspaper corpus show significant improvement in perplexity (5.0% overall and 27.2% on target vocabulary).",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,80,86,,,,,,,,,,,,,,,,WOS:000223092900010,0
C,"Zhou, GD; Su, R",,"Schutze, H; Su, KY",,"Zhou, GD; Su, R",,,Error-driven HMM-based chunk tagger with context-dependent lexicon,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries. Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types, 93.60% and 94.64% for noun phrases, and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types, 96.49% and 96.99% for noun phrases, and 97.13% and 97.36% for verb phrases.",,,,,,,,,,,,,,,,,,,,,,,,,2000,,,,,,,71,79,,,,,,,,,,,,,,,,WOS:000223092900009,0
C,"Toutanova, K; Manning, CD",,"Schutze, H; Su, KY",,"Toutanova, K; Manning, CD",,,Enriching the knowledge sources used in a maximum entropy part-of-speech tagger,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"This paper presents results for a maximum-entropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging. In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs. The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.",,,,,"Manning, Christopher/AAM-9535-2020","Manning, Christopher/0000-0001-6155-649X",,,,,,,,,,,,,,,,,,,2000,,,,,,,63,70,,10.3115/1117794.1117802,0.0,,,,,,,,,,,,,WOS:000223092900008,0
C,"Hung, KY; Luk, RWP; Yeung, D; Chung, KFL; Shu, W",,"Schutze, H; Su, KY",,"Hung, KY; Luk, RWP; Yeung, D; Chung, KFL; Shu, W",,,Detection of language (model) errors,PROCEEDINGS OF THE 2000 JOINT SIGDAT CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND VERY LARGE CORPORA,,,,,Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora held in Conjunction with the 38th Annual Meeting of the Association-for-Computational-Linguistics,"OCT 07-08, 2000","Hong Kong Univ Sci & Technol, Hong Kong, PEOPLES R CHINA","Behavior Design Corp,GroupFire Inc,Intel China Res Ctr,LEXIS-NEXIS",Hong Kong Univ Sci & Technol,,,"The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages. However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation. We apply pattern recognition techniques (i.e. Bayesian, decision tree and neural network classifiers) to discover language model errors. We have examined 2 general types of features: model-based and language-specific features. In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%). Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%). The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%).",,,,,"Luk, Robert/B-9382-2015","Luk, Robert/0000-0002-9310-8867; CHUNG, Fu Lai Korris/0000-0001-5294-8168",,,,,,,,,,,,,,,,,,,2000,,,,,,,87,94,,,,,,,,,,,,,,,,WOS:000223092900011,0
J,"Beeferman, D; Berger, A; Lafferty, J",,,,"Beeferman, D; Berger, A; Lafferty, J",,,Statistical models for text segmentation,MACHINE LEARNING,,,,,1997 Conference on Empirical Methods in Natural Language Processing,1997,"PROVIDENCE, RHODE ISLAND",,,,,"This paper introduces a new statistical approach to automatically partitioning text into coherent segments. The approach is based on a technique that incrementally builds an exponential model to extract features that are correlated with the presence of boundaries in labeled training text. The models use two classes of features: topicality features that use adaptive language models in a novel way to detect broad changes of topic, and cue-word features that detect occurrences of specific words, which may he domain-specific, that tend to be used near segment boundaries, Assessment of our approach on quantitative and qualitative grounds demonstrates its effectiveness in two very different domains, Wall Street Journal news articles and television broadcast news story transcripts. Quantitative results on these domains are presented using a new probabilistically motivated error metric, which combines precision and recall in a natural and flexible way. This metric is used to make a quantitative assessment of the relative contributions of the different feature types, as well as a comparison with decision trees and previously proposed text segmentation algorithms.",,,,,,,,,,,,,,,,,,,0885-6125,,,,,FEB,1999,34.0,1-3,,,,,177,210,,10.1023/A:1007506220214,0.0,,,,,,,,,,,,,WOS:000079753100008,0
C,"Altun, Y; Johnson, M; Hofmann, T",,"Collins, M; Steedman, M",,"Altun, Y; Johnson, M; Hofmann, T",,,Investigating loss functions and optimization methods for discriminative learning of label sequences,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"Discriminative models have been of interest in the NLP community in recent years. Previous research has shown that they are advantageous over generative models. In this paper, we investigate how different objective functions and optimization methods affect the performance of the classifiers in the discriminative learning framework. We focus on the sequence labelling problem, particularly POS tagging and NER tasks. Our experiments show that changing the objective function is not as effective as changing the features included in the model.",,,,,,"Johnson, Mark/0000-0003-4809-8441",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,145,152,,,,,,,,,,,,,,,,WOS:000223080400019,0
C,"Jing, HY; Florian, R; Luo, XQ; Zhang, T; Ittycheriah, A",,"Collins, M; Steedman, M",,"Jing, HY; Florian, R; Luo, XQ; Zhang, T; Ittycheriah, A",,,HowtogetaChineseName(Entity): Segmentation and combination issues,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"When building a Chinese named entity recognition system, one must deal with certain language-specific issues such as! whether the model should be based on characters or words. While there is no unique answer to this question, we discuss in detail advantages and disadvantages of each model, identify problems in segmentation and suggest possible solutions, presenting our observations, analysis, and experimental results. The second topic of this paper is classifier combination. We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs. The results demonstrate that classifier combination is an effective technique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error.",,,,,"Florian, Radu/AAF-1434-2019",,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,200,207,,,,,,,,,,,,,,,,WOS:000223080400026,0
C,"Sassano, M",,"Collins, M; Steedman, M",,"Sassano, M",,,Virtual examples for text classification with support vector machines,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We explore how virtual examples (artificially created examples) improve performance of text. classification with Support Vector Machines (SVMs). We propose techniques to create virtual examples for text classification based on the assumption that the category of a document is unchanged even if a small number of words are Added or deleted. We evaluate the proposed methods by Reuters-21758 test set collection. Experimental results show virtual examples improve the performance of text classification with SVMs, especially for small training sets.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,208,215,,,,,,,,,,,,,,,,WOS:000223080400027,0
C,"Tillmann, C",,"Collins, M; Steedman, M",,"Tillmann, C",,,A projection extension algorithm for statistical machine translation,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying high-precision word alignment. The system performance is significantly increased by applying a novel block extension algorithm using an additional high-recall word alignment. The blocks are further filtered using unigram-count selection criteria. The system has been successfully test on a Chinese-English and an Arabic-English translation task.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,1,8,,,,,,,,,,,,,,,,WOS:000223080400001,0
C,"Baldwin, T; Bond, F",,"Collins, M; Steedman, M",,"Baldwin, T; Bond, F",,,A plethora of methods for learning English countability,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"This paper compares a range of methods for classifying words based on linguistic diagnostics, focusing on the task of learning countabilities for English nouns. We propose two basic approaches to feature representation: distribution-based representation, which simply looks at the distribution of features in the corpus data, and agreement-based representation which analyses the level of token-wise agreement between multiple preprocessor systems. We additionally compare a single multiclass classifier architecture with a suite of binary classifiers, and combine analyses from multiple preprocessors. Finally, we present and evaluate a feature selection method.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,73,80,,,,,,,,,,,,,,,,WOS:000223080400010,0
C,"Fleischman, M; Kwon, N; Hovy, E",,"Collins, M; Steedman, M",,"Fleischman, M; Kwon, N; Hovy, E",,,Maximum-entropy models for FrameNet classification,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p<0.01) of over 6%.",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,49,56,,,,,,,,,,,,,,,,WOS:000223080400007,0
C,"Barzilay, R; Elhadad, N",,"Collins, M; Steedman, M",,"Barzilay, R; Elhadad, N",,,Sentence alignment for monolingual comparable corpora,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We address the problem of sentence alignment for monolingual corpora, a phenomenon distinct from alignment in parallel corpora. Aligning large comparable corpora automatically would provide a valuable resource for learning of text-to-text rewriting rules. We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs. Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,25,32,,,,,,,,,,,,,,,,WOS:000223080400004,0
C,"Clark, S; Curran, JR",,"Collins, M; Steedman, M",,"Clark, S; Curran, JR",,,Log-linear models for wide-coverage CCG parsing,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG). Log-linear models can easily encode the long-range dependencies inherent in coordination and extraction phenomena, which CCG was designed to handle. Log-linear models have previously been applied to statistical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,97,104,,,,,,,,,,,,,,,,WOS:000223080400013,0
C,"Genzel, D; Charniak, E",,"Collins, M; Steedman, M",,"Genzel, D; Charniak, E",,,Variation of entropy and parse trees of sentences as a function of the sentence number,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"In this paper we explore the variation of sentences as a function of the sentence number. We demonstrate that while the entropy of the sentence increases with the sentence number, it decreases at the paragraph boundaries in accordance with the Entropy Rate Constancy principle (introduced in related work). We also demonstrate that the principle holds for different genres and languages and explore the role of genre informativeness. We investigate potential causes of entropy variation by looking at the tree depth, the branching factor, the size of constituents, and the occurrence of gapping.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,65,72,,,,,,,,,,,,,,,,WOS:000223080400009,0
C,"Duboue, PA; McKeown, KR",,"Collins, M; Steedman, M",,"Duboue, PA; McKeown, KR",,,Statistical acquisition of content selection rules for natural language generation,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"A Natural Language Generation system produces text using as input semantic data. One of its very first tasks is to decide which pieces of information to convey in the output. This task, called Content Selection, is quite domain dependent, requiring considerable re-engineering to transport the system from one scenario to another. In this paper, we present a method to acquire content selection rules automatically from a corpus of text and associated semantics. Our proposed technique was evaluated by comparing its output with information selected by human authors in unseen texts, where we were able to filter half the input data set without loss of recall.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,121,128,,,,,,,,,,,,,,,,WOS:000223080400016,0
C,"Schafer, C; Yarowsky, D",,"Collins, M; Steedman, M",,"Schafer, C; Yarowsky, D",,,Statistical machine translation using coercive two-level syntactic transduction,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We define, implement and evaluate a novel model for statistical machine translation, which is based on shallow syntactic analysis (part-of-speech tagging and phrase chunking) in both the source and target languages. It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language. We also examine aspects of lexical transfer, suggesting and exploring a concept of translation coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabilities, which holds promise for improving machine translation of low-density languages. Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques. Performance is automatically evaluated via the Bleu score metric.",,,,,"Yarowsky, David/A-3270-2010",,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,9,16,,,,,,,,,,,,,,,,WOS:000223080400002,0
C,"Modjeska, NN; Markert, K; Nissim, M",,"Collins, M; Steedman, M",,"Modjeska, NN; Markert, K; Nissim, M",,,Using the Web in machine learning for other-anaphora resolution,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We present a machine learning framework for resolving other-anaphora. Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web. We search the Web via lexico-syntactic patterns that are specific to other-anaphors. Incorporating this innovative feature leads, to an 11.4 percentage point improvement in the classifier's F-measure (25% improvement relative to results without this feature).",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,176,183,,,,,,,,,,,,,,,,WOS:000223080400023,0
C,"Gildea, D; Hockenmaier, J",,"Collins, M; Steedman, M",,"Gildea, D; Hockenmaier, J",,,Identifying semantic roles using combinatory categorial grammar,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar. This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,57,64,,,,,,,,,,,,,,,,WOS:000223080400008,0
C,"Shen, LB; Sarkar, A; Joshi, AK",,"Collins, M; Steedman, M",,"Shen, LB; Sarkar, A; Joshi, AK",,,Using LTAG based features in parse reranking,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser. In this paper, we extend the notion of a tree kernel over arbitrary sub-trees of the parse to the derivation trees and derived trees provided by the LTAG formalism, and in addition, we extend the original definition of the tree kernel, making it more lexicalized and more compact. We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ section 23 of Penn Treebank for sentences of length < 100 words. Our results show that the use of LTAG based tree kernel gives rise to a 17% relative difference in f-score improvement over the use of a linear kernel without LTAG based features.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,89,96,,,,,,,,,,,,,,,,WOS:000223080400012,0
C,"Ng, V; Cardie, C",,"Collins, M; Steedman, M",,"Ng, V; Cardie, C",,,Bootstrapping coreference classifiers with multiple machine learning algorithms,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"Successful application of multi-view co-training algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated. This can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split. To bootstrap coreference classifiers, we propose and evaluate a single-view weakly supervised algorithm that relies on two different learning algorithms in lieu of the two different views required by co-training. In addition, we investigate a method for ranking unlabeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping.",,,,,,"Cardie, Claire/0000-0002-2061-6094",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,113,120,,,,,,,,,,,,,,,,WOS:000223080400015,0
C,"Isozaki, H; Hirao, T",,"Collins, M; Steedman, M",,"Isozaki, H; Hirao, T",,,Japanese zero pronoun resolution based on ranking rules and machine learning,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"Anaphora resolution is one of the most important research topics in Natural Language Processing. In English, overt pronouns such as she and definite noun,phrases such as the company are anaphors that refer to preceding entities (antecedents). In Japanese, anaphors are often omitted, and these omissions are called zero pronouns. There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach. Since we have to take various factors into consideration, it is difficult to find a good combination of heuristic rules. Therefore, the machine learning approach is attractive, but it requires a large amount of training data. In this paper, we propose a method that combines ranking rules and machine learning. The ranking rules are simple and effective, while machine learning can take more factors into account. From the results of our experiments, this combination gives better performance than either of the two previous approaches.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,184,191,,,,,,,,,,,,,,,,WOS:000223080400024,0
C,"Xu, P; Emami, A; Jelinek, F",,"Collins, M; Steedman, M",,"Xu, P; Emami, A; Jelinek, F",,,Training Connectionist models for the structured language model,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We investigate the performance of the Structured Language, Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models. The connectionist models use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connectionist model in fighting the data sparseness problem, but also because of the sublinear growth in the model size when the context length is increased. The connectionist models can be further trained by an EM procedure, similar to the previously used procedure for training the SLM. Our experiments show that the connectionist models can significantly improve the PPL over the interpolated and back-off models on the UPENN Treebank corpora, after interpolating with a baseline trigram language model. The EM training procedure can improve the connectionist models further, by using hidden events obtained by the SLM parser.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,160,167,,,,,,,,,,,,,,,,WOS:000223080400021,0
C,"Hulth, A",,"Collins, M; Steedman, M",,"Hulth, A",,,Improved automatic keyword extraction given more linguistic knowledge,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the PoS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.",,,,,"Hulth, Anette/E-2162-2017","Hulth, Anette/0000-0002-3132-808X",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,216,223,,,,,,,,,,,,,,,,WOS:000223080400028,0
C,"Weeds, J; Weir, D",,"Collins, M; Steedman, M",,"Weeds, J; Weir, D",,,General framework for distributional similarity,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"dWe present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.",,,,,,"Weeds, Julie/0000-0002-3831-4019",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,81,88,,,,,,,,,,,,,,,,WOS:000223080400011,0
C,"Yu, H; Hatzivassiloglou, V",,"Collins, M; Steedman, M",,"Yu, H; Hatzivassiloglou, V",,,Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,129,136,,,,,,,,,,,,,,,,WOS:000223080400017,0
C,"Ciaramita, M; Johnson, M",,"Collins, M; Steedman, M",,"Ciaramita, M; Johnson, M",,,Supersense tagging of unknown nouns in WordNet,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We present a new framework for classifying common nouns that extends named-entity classification. We used a fixed set of 26 semantic labels, which we called supersenses. These are the labels used by lexicographers developing WordNet. This framework has a number of practical advantages. We show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns. We also define a more realistic evaluation procedure than cross-validation.",,,,,,"Johnson, Mark/0000-0003-4809-8441",,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,168,175,,,,,,,,,,,,,,,,WOS:000223080400022,0
C,"Dienes, P; Dubey, A",,"Collins, M; Steedman, M",,"Dienes, P; Dubey, A",,,Antecedent recovery: Experiments with a trace tagger,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"This paper explores the problem of finding non-local dependencies. First, we isolate a set of features useful for this task. Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds non-local dependencies while parsing. We find that the former outperforms the latter because it makes better use of the features we isolate.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,33,40,,,,,,,,,,,,,,,,WOS:000223080400005,0
C,"Kim, W; Khudanpur, S",,"Collins, M; Steedman, M",,"Kim, W; Khudanpur, S",,,Cross-lingual lexical triggers in statistical language modeling,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages. We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood-based adaptation scheme for combining a trigger model with an N-gram model. We describe the application of such language models for automatic speech recognition. By exploiting a side-corpus of contemporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors. We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross-lingual information retrieval and machine translation, proposed elsewhere.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,17,24,,,,,,,,,,,,,,,,WOS:000223080400003,0
C,"Kazama, J; Tsujii, J",,"Collins, M; Steedman, M",,"Kazama, J; Tsujii, J",,,Evaluation and extension of maximum entropy models with inequality constraints,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"A maximum entropy (ME) model is usually estimated so that it conforms to equality constraints on feature expectations. However, the equality constraint is inappropriate for sparse and therefore unreliable features. This study explores an ME model with box-type inequality constraints, where the equality can be violated to reflect this unreliability. We evaluate the inequality ME model using text categorization datasets. We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation. Experimental results demonstrate the advantage of the inequality models and the proposed extension.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,137,144,,,,,,,,,,,,,,,,WOS:000223080400018,0
C,"Luo, XQ",,"Collins, M; Steedman, M",,"Luo, XQ",,,A maximum entropy Chinese character-based parser,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"The paper presents a maximum entropy Chinese character-based parser trained on the Chinese Treebank (CTB henceforth). Word-based parse trees in CTB are first converted into character-based trees, where word-level part-of-speech (POS) tags become constituent labels and character-level tags are derived from word-level POS tags. A maximum entropy parser is then trained on the character-based corpus. The parser does word-segmentation, POS-tagging and parsing in a unified framework. An average label F-measure 81.4% and word-segmentation F-measure 96.0% are achieved by the parser. Our results show that word-level POS tags can improve significantly word-segmentation, but higher-level syntactic strutures are of little use to word segmentation in the maximum entropy parser. A word-dictionary helps to improve both word-segmentation and parsing accuracy.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,192,199,,,,,,,,,,,,,,,,WOS:000223080400025,0
C,"Zhou, YQ; Weng, FL; Wu, L; Schmidt, H",,"Collins, M; Steedman, M",,"Zhou, YQ; Weng, FL; Wu, L; Schmidt, H",,,A fast algorithm for feature selection in conditional maximum entropy modeling,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"This paper describes a fast algorithm that selects features for conditional maximum entropy modeling. Berger et al. (1996) presents an incremental feature selection (IFS) algorithm, which computes the approximate gains for all candidate features at each selection stage, and is very time-consuming for any problems with large feature spaces. In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages. Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selection process while maintaining the same quality of selected features. One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features. The new algorithm is easy to implement, and given a feature space of size F, it only uses O(F) more space than the original IFS algorithm.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,153,159,,,,,,,,,,,,,,,,WOS:000223080400020,0
C,"Chen, J; Rambow, O",,"Collins, M; Steedman, M",,"Chen, J; Rambow, O",,,Use of deep linguistic features for the recognition and labeling of semantic arguments,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features. We also show that predicting labels from a lightweight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,41,48,,,,,,,,,,,,,,,,WOS:000223080400006,0
C,"Riloff, E; Wiebe, J",,"Collins, M; Steedman, M",,"Riloff, E; Wiebe, J",,,Learning extraction patterns for subjective expressions,PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 11-12, 2003","Sapporo, JAPAN",,,,,"This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision.",,,,,,,,,,,,,,,,,,,,,1-932432-13-2,,,,2003,,,,,,,105,112,,,,,,,,,,,,,,,,WOS:000223080400014,0
C,"Boratko, M; Padigela, H; Mikkilineni, D; Yuvraj, P; Das, R; McCallum, A; Chang, M; Fokoue, A; Kapanipathi, P; Mattei, N; Musa, R; Talamadupula, K; Witbrock, M",,,Assoc Computat Linguist,"Boratko, Michael; Padigela, Harshit; Mikkilineni, Divyendra; Yuvraj, Pritish; Das, Rajarshi; McCallum, Andrew; Chang, Maria; Fokoue, Achille; Kapanipathi, Pavan; Mattei, Nicholas; Musa, Ryan; Talamadupula, Kartik; Witbrock, Michael",,,An Interface for Annotating Science Questions,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Recent work introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into an Easy Set and a Challenge Set. That work includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them. However, it does not include clear definitions of these types, nor does it offer information about the quality of the labels or the annotation process used. In this paper, we introduce a novel interface for human annotation of science question-answer pairs with their respective knowledge and reasoning types, in order that the classification of new questions may be improved. We build on the classification schema proposed by prior work on the ARC dataset, and evaluate the effectiveness of our interface with a preliminary study involving 10 participants.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,102,107,,,,,,,,,,,,,,,,WOS:000855233900018,0
C,"Lee, YK; Ng, HT",,"Hajic, J; Matsumoto, Y",,"Lee, YK; Ng, HT",,,An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,41,48,,,,,,,,,,,,,,,,WOS:000223079900006,0
C,"Tanveer, MI; Ture, F",,,Assoc Computat Linguist,"Tanveer, Md Iftekhar; Ture, Ferhan",,,SyntaViz: Visualizing Voice Queries through a Syntax-Driven Hierarchical Ontology,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"This paper describes SYNTAVIZ, a visualization interface specifically designed for analyzing natural-language queries that were created by users of a voice-enabled product. SYNTAVIZ provides a platform for browsing the ontology of user queries from a syntax-driven perspective, providing quick access to highimpact failure points of the existing intent understanding system and evidence for datadriven decisions in the development cycle. A case study on Xfinity X1 (a voice-enabled entertainment platform from Comcast) reveals that SYNTAVIZ helps developers identify multiple action items in a short amount of time without any special training. SYNTAVIZ has been open-sourced for the benefit of the community.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,1,6,,,,,,,,,,,,,,,,WOS:000855233900001,0
C,"Gao, JF; Suzuki, H; Wen, Y",,"Hajic, J; Matsumoto, Y",,"Gao, JF; Suzuki, H; Wen, Y",,,Exploiting headword dependency and predictive clustering for language modeling,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper presents several practical ways of incorporating linguistic structure into language models. A headword detector is first applied to detect the headword of each phrase in a sentence. A permuted headword trigram model (PHTM) is then generated from the annotated corpus. Finally, PHTM is extended to a cluster PHTM (C-PHTM) by defining clusters for similar words in the corpus. We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion. Experiments show that C-PHTM achieves 15% error rate reduction over the word trigram model. This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,248,256,,,,,,,,,,,,,,,,WOS:000223079900032,0
C,"Kudo, T; Richardson, J",,,Assoc Computat Linguist,"Kudo, Taku; Richardson, John",,,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,66,71,,,,,,,,,,,,,,,,WOS:000855233900012,0
C,"Ning, Q; Zhou, B; Feng, ZL; Peng, HR; Roth, D",,,Assoc Computat Linguist,"Ning, Qiang; Zhou, Ben; Feng, Zhili; Peng, Haoruo; Roth, Dan",,,CogCompTime: A Tool for Understanding Time in Natural Language,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Automatic extraction of temporal information is important for natural language understanding. It involves two basic tasks: (1) Understanding time expressions that are mentioned explicitly in text (e.g., February 27, 1998 or tomorrow), and (2) Understanding temporal information that is conveyed implicitly via relations. This paper introduces CogCompTime, a system that has these two important functionalities. It incorporates the most recent progress, achieves state-of-the-art performance, and is publicly available.(1) We believe that this demo will provide valuable insight for temporal understanding and be useful for multiple time-aware applications.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,72,77,,,,,,,,,,,,,,,,WOS:000855233900013,0
C,"Barzilay, R; Lee, L",,"Hajic, J; Matsumoto, Y",,"Barzilay, R; Lee, L",,,Bootstrapping lexical choice via multiple-sequence alignment,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"An important component of any generation system is the mapping dictionary, a lexicon of elementary semantic expressions and corresponding natural language realizations. Typically, labor-intensive knowledge-based methods are used to construct the dictionary. We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics. Crucially, our method leverages latent information contained in multi-parallel corpora - datasets that supply several verbalizations of the corresponding semantics rather than just one. We used our techniques to generate natural language versions of computer-generated mathematical proofs, with good results on both a per-component and overall-output basis. For example, in evaluations involving a dozen human judges, our system produced output whose readability and faithfulness to the semantic input rivaled that of a traditional generation system.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,164,171,,,,,,,,,,,,,,,,WOS:000223079900022,0
C,"Brew, C; Walde, SSI",,"Hajic, J; Matsumoto, Y",,"Brew, C; Walde, SSI",,,Spectral clustering for German verbs,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs. Our previous work has shown that standard clustering techniques succeed in inducing Levin-style semantic classes from verb subcategorisation information. But clustering in the very high dimensional spaces that we use is fraught with technical and conceptual difficulties. Spectral clustering performs a dimensionality reduction on the verb frame patterns, and provides a robustness and efficiency that standard clustering methods do not display in direct use. The clustering results axe evaluated according to the alignment (Christianini et al., 2002) between the Gram matrix defined by the cluster output and the corresponding matrix defined by a gold standard.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,117,124,,,,,,,,,,,,,,,,WOS:000223079900016,0
C,"Federico, M; Bertoldi, N; Sandrini, V",,"Hajic, J; Matsumoto, Y",,"Federico, M; Bertoldi, N; Sandrini, V",,,Bootstrapping named entity recognition for Italian broadcast news,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper presents the development of a Named Entity (NE) recognition system for the Italian broadcast news domain. A statistical model is introduced based on a trigram language model defined on words and NE classes. The estimation of the NE model is carried out with a very little list of 2,360 manually tagged NEs and a large untagged newspaper corpus. An iterative training procedure is applied which goes through the estimation of simpler models, whose parameters axe used to initialize the complete NE model. In the end, NE recognition experiments are reported, on broadcast news transcripts generated by a speech recognition system.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,296,303,,,,,,,,,,,,,,,,WOS:000223079900038,0
C,"Munteanu, DS; Marcu, D",,"Hajic, J; Matsumoto, Y",,"Munteanu, DS; Marcu, D",,,Processing comparable corpora with bilingual suffix trees,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We introduce Bilingual Suffix Trees (BST), a data structure that is suitable for exploiting comparable corpora. We discuss algorithms that use BSTs in order to create parallel corpora and learn translations of unseen words from comparable corpora. Starting with a small bilingual dictionary that was derived automatically from a corpus of 5.000 parallel sentences, we have automatically extracted a corpus of 33.926 parallel phrases of size greater than 3, and learned 9 new word translations from a comparable corpus of 1.3M words (100.000 sentences).",,,,,"Munteanu, Dragos V/AAB-7656-2020",,,,,,,,,,,,,,,,,,,,2002,,,,,,,289,295,,,,,,,,,,,,,,,,WOS:000223079900037,0
C,"Pang, B; Lee, L; Vaithyanathan, S",,"Hajic, J; Matsumoto, Y",,"Pang, B; Lee, L; Vaithyanathan, S",,,Thumbs up? Sentiment classification using machine learning techniques,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,79,86,,10.3115/1118693.1118704,0.0,,,,,,,,,,,,,WOS:000223079900011,0
C,"Wang, XL; Utiyama, M; Sumita, E",,,Assoc Computat Linguist,"Wang, Xiaolin; Utiyama, Masao; Sumita, Eiichiro",,,CytonMT: an Efficient Neural Machine Translation Open-source Toolkit Implemented in C plus,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"This paper presents an open-source neural machine translation toolkit named CytonMT(1). The toolkit is built from scratch only using C++ and NVIDIA's GPU-accelerated libraries. The toolkit features training efficiency, code simplicity and translation quality. Benchmarks show that CytonMT accelerates the training speed by 64.5% to 110.8% on neural networks of various sizes, and achieves competitive translation quality.",,,,,"wang, xiao/HGB-7081-2022","wang, xiao/0000-0002-4088-3341",,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,133,138,,,,,,,,,,,,,,,,WOS:000855233900023,0
C,"Fox, HJ",,"Hajic, J; Matsumoto, Y",,"Fox, HJ",,,Phrasal cohesion and statistical machine translation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"There has been much interest in using phrasal movement to improve statistical machine translation. We explore how well phrases cohere across two languages, specifically English and French, and examine the particular conditions under which they do not. We demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system. We also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,304,311,,,,,,,,,,,,,,,,WOS:000223079900039,0
C,"Han, X; Cao, SL; Lv, X; Lin, YK; Liu, ZY; Sun, MS; Li, JZ",,,Assoc Computat Linguist,"Han, Xu; Cao, Shulin; Lv, Xin; Lin, Yankai; Liu, Zhiyuan; Sun, Maosong; Li, Juanzi",,,OpenKE: An Open Toolkit for Knowledge Embedding,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on http://openke.thunlp.org/.",,,,,,"Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,139,144,,,,,,,,,,,,,,,,WOS:000855233900024,0
C,"Zhang, N; Zhang, TT; Bhattacharya, I; Ji, H; Radke, R",,,Assoc Computat Linguist,"Zhang, Ni; Zhang, Tongtao; Bhattacharya, Indrani; Ji, Heng; Radke, Rich",,,Visualizing Group Dynamics based on Multiparty Meeting Understanding,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Group discussions are usually aimed at sharing opinions, reaching consensus and making good decisions based on group knowledge. During a discussion, participants might adjust their own opinions as well as tune their attitudes towards others' opinions, based on the unfolding interactions. In this paper, we demonstrate a framework to visualize such dynamics; at each instant of a conversation, the participants' opinions and potential influence on their counterparts is easily visualized. We use multi-party meeting opinion mining based on bipartite graphs to extract opinions and calculate mutual influential factors, using the Lunar Survival Task as a study case.",,,,,"Radke, Richard/I-3289-2013","Radke, Richard/0000-0001-5064-7775",,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,96,101,,,,,,,,,,,,,,,,WOS:000855233900017,0
C,"Patel, A; Sands, A; Callison-Burch, C; Apidianaki, M",,,Assoc Computat Linguist,"Patel, Ajay; Sands, Alexander; Callison-Burch, Chris; Apidianaki, Marianna",,,"Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package",CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Vector space embedding models like word2vec, GloVe, fastText, and ELMo are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings. Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,120,126,,,,,,,,,,,,,,,,WOS:000855233900021,0
C,"Peter, JT; Beck, E; Ney, H",,,Assoc Computat Linguist,"Peter, Jan-Thorsten; Beck, Eugen; Ney, Hermann",,,"Sisyphus, a Workflow Manager Designed for Machine Translation and Automatic Speech Recognition",CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Training and testing many possible parameters or model architectures of state-of-the-art machine translation or automatic speech recognition system is a cumbersome task. They usually require a long pipeline of commands reaching from pre-processing the training data to post-processing and evaluating the output. This paper introduces Sisyphus, a tool that aims at managing scientific experiments in an efficient way. After defining the workflow for a given task, Sisyphus runs all required steps and ensures that all commands finish successfully. It avoids unnecessary computations by reusing tasks that are needed for multiple parts of the workflow and saves the user time by determining the order in which the tasks are to be performed. Since the program and workflow are written in Python they can be easily extended to contain arbitrary code. This makes it possible to use the rich collection of Python tools for editing, debugging, and documentation. It only has few requirements on the underlying server or cluster, and has been successfully tested in many large scale setups and can handle thousands of tasks inside the work-flow.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,84,89,,,,,,,,,,,,,,,,WOS:000855233900015,0
C,"Raux, A; Ma, Y; Yang, P; Wong, F",,,Assoc Computat Linguist,"Raux, Antoine; Ma, Yi; Yang, Paul; Wong, Felicia",,,PizzaPal: Conversational Pizza Ordering using a High-Density Conversational AI Platform,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"This paper describes PizzaPal, a voice-only agent for ordering pizza, as well as the Conversational AI architecture built at b4.ai. Based on the principles of high-density conversational AI, it supports natural and flexible interactions through neural conversational language understanding, robust dialog state tracking, and hierarchical task decomposition.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,151,156,,,,,,,,,,,,,,,,WOS:000855233900026,0
C,"Brill, E; Dumais, S; Banko, N",,"Hajic, J; Matsumoto, Y",,"Brill, E; Dumais, S; Banko, N",,,An analysis of the AskMSR question-answering system,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,257,264,,,,,,,,,,,,,,,,WOS:000223079900033,0
C,"Foster, G; Langlais, P; Lapalme, G",,"Hajic, J; Matsumoto, Y",,"Foster, G; Langlais, P; Lapalme, G",,,User-friendly text prediction for translators,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"Text prediction is a form of interactive machine translation that is well suited to skilled translators. In principle it can assist in the production of a target text with minimal disruption to a translator's normal routine. However, recent evaluations of a prototype prediction system showed that it significantly decreased the productivity of most translators who used it. In this paper, we analyze the reasons for this and propose a solution which consists in seeking predictions that maximize the expected benefit to the translator, rather than just trying to anticipate some amount of upcoming text. Using a model of a typical translator constructed from data collected in the evaluations of the prediction prototype, we show that this approach has the potential to turn text prediction into a help rather than a hindrance to a translator.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,148,155,,,,,,,,,,,,,,,,WOS:000223079900020,0
C,"Cucerzan, S; Yarowsky, D",,"Hajic, J; Matsumoto, Y",,"Cucerzan, S; Yarowsky, D",,,Augmented mixture models for lexical disambiguation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classification tasks. We present a new classification correction technique that successfully addresses the problem of under-estimation of infrequent classes in the training data. We show that the mixture models are boosting-friendly and that both Adaboost and our original correction technique can improve the results of the raw model significantly, achieving state-of-the-art performance on several standard test sets in four languages. With substantially different output to Naive Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination.",,,,,"Yarowsky, David/A-3270-2010",,,,,,,,,,,,,,,,,,,,2002,,,,,,,33,40,,,,,,,,,,,,,,,,WOS:000223079900005,0
C,"Engel, D; Charniak, E; Johnson, M",,"Hajic, J; Matsumoto, Y",,"Engel, D; Charniak, E; Johnson, M",,,Parsing and disfluency placement,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"It has been suggested that some forms of speech disfluencies, most notable interjections and parentheticals, tend to occur disproportionally at major clause boundaxies [6] and thus might serve to aid parsers in establishing these boundaries. We have tested a current statistical parser [1] on Switchboard text with and without interjections and parentheticals and found that the parser performed better when not faced with these extra phenomena. This suggest that for current parsers, at least, interjection and parenthetical placement does not help in the parsing process.",,,,,,"Johnson, Mark/0000-0003-4809-8441; Engel, Don/0000-0003-2838-0140",,,,,,,,,,,,,,,,,,,2002,,,,,,,49,54,,,,,,,,,,,,,,,,WOS:000223079900007,0
C,"Klein, D; Manning, CD",,"Hajic, J; Matsumoto, Y",,"Klein, D; Manning, CD",,,Conditional structure versus conditional estimation in NLP models,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper separates conditional parameter estimation, which consistently raises test set accuracy on statistical NLP tasks, from conditional model structures, such as the conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other ways in which the independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.",,,,,"Manning, Christopher/AAM-9535-2020","Manning, Christopher/0000-0001-6155-649X",,,,,,,,,,,,,,,,,,,2002,,,,,,,9,16,,,,,,,,,,,,,,,,WOS:000223079900002,0
C,"Sable, C; McKeown, K; Church, KW",,"Hajic, J; Matsumoto, Y",,"Sable, C; McKeown, K; Church, KW",,,NLP found helpful (at least for one text categorization task),PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"Attempts to use natural language processing (NLP) for text categorization and information retrieval (IR) have had mixed results. Nevertheless, there is a strong intuition that NLP is important, at least for some tasks. In this paper, we discuss a task involving captioned images for which the subject and the predicate are critical. The usefulness of NLP for this task is established in two ways. In addition to the standard method of introducing a new system and comparing its performance with others in the literature, we also present evidence from experiments with human subjects showing that NLP generally improves speed and accuracy.",,,,,"Church, Kenneth/G-3167-2010; Church, Kenneth/GYR-1624-2022; Church, Kenneth/AAV-9667-2021","Church, Kenneth/0000-0001-8378-6069",,,,,,,,,,,,,,,,,,,2002,,,,,,,172,179,,,,,,,,,,,,,,,,WOS:000223079900023,0
C,"Toutanova, K; Ilhan, HT; Manning, CD",,"Hajic, J; Matsumoto, Y",,"Toutanova, K; Ilhan, HT; Manning, CD",,,Extensions to HMM-based statistical word alignment models,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper describes improved HMM-based word level alignment models for statistical machine translation. We present a method for using part of speech tag information to improve alignment accuracy, and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model. We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4. The results show up to 16% alignment error reduction.",,,,,"Manning, Christopher/AAM-9535-2020","Manning, Christopher/0000-0001-6155-649X",,,,,,,,,,,,,,,,,,,2002,,,,,,,87,94,,,,,,,,,,,,,,,,WOS:000223079900012,0
C,"Cer, D; Yang, YF; Kong, SY; Hua, N; Limtiaco, N; St John, R; Constant, N; Guajardo-Cespedes, M; Yuan, S; Tar, C; Sung, YH; Strope, B; Kurzweil, R",,,Assoc Computat Linguist,"Cer, Daniel; Yang, Yinfei; Kong, Sheng-yi; Hua, Nan; Limtiaco, Nicole; St John, Rhomni; Constant, Noah; Guajardo-Cespedes, Mario; Yuan, Steve; Tar, Chris; Sung, Yun-Hsuan; Strope, Brian; Kurzweil, Ray",,,Universal Sentence Encoder for English,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,169,174,,,,,,,,,,,,,,,,WOS:000855233900029,0
C,"Fujii, A; Itou, K; Ishikawa, T",,"Hajic, J; Matsumoto, Y",,"Fujii, A; Itou, K; Ishikawa, T",,,A method for open-vocabulary speech-driven text retrieval,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"While recent retrieval techniques do not limit the number of index terms, out-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at retrieving information with spoken queries, we fill the gap between speech recognition and text retrieval in terms of the vocabulary size. Given a spoken query, we generate a transcription and detect OOV words through speech recognition. We then correspond detected OOV words to terms indexed in a target collection to complete the transcription, and search the collection for documents relevant to the completed transcription. We show the effectiveness of our method by way of experiments.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,188,195,,,,,,,,,,,,,,,,WOS:000223079900025,0
C,"Dou, LX; Qin, GH; Wang, JP; Yao, JG; Lin, CY",,,Assoc Computat Linguist,"Dou, Longxu; Qin, Guanghui; Wang, Jinpeng; Yao, Jin-Ge; Lin, Chin-Yew",,,Data2Text Studio: Automated Text Generation from Structured Data,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Data2Text Studio is a platform for automated text generation from structured data. It is equipped with a Semi-HMMs model to extract high-quality templates and corresponding trigger conditions from parallel data automatically, which improves the interactivity and interpretability of the generated text. In addition, several easy-to-use tools are provided for developers to edit templates of pre-trained models, and APIs are released for developers to call the pre-trained model to generate texts in third-party applications. We conduct experiments on ROTOWIRE datasets for template extraction and text generation. The results show that our model achieves improvements on both tasks.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,13,18,,,,,,,,,,,,,,,,WOS:000855233900003,0
C,"Collins, M",,"Hajic, J; Matsumoto, Y",,"Collins, M",,,Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,1,8,,,,,,,,,,,,,,,,WOS:000223079900001,0
C,"Smith, NA",,"Hajic, J; Matsumoto, Y",,"Smith, NA",,,From words to corpora: Recognizing translation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper presents a technique for discovering translationally equivalent texts. It is comprised of the application of a matching algorithm at two different levels of analysis and a well-founded similarity score. This approach can be applied to any multilingual corpus using any kind of translation lexicon; it is therefore adaptable to varying levels of multilingual resource availability. Experimental results are shown on two tasks: a search for matching thirty-word segments in a corpus where some segments are mutual translations, and classification of candidate pairs of web pages that may or may not be translations of each other. The latter results compare competitively with previous, document-structure-based approaches to the same problem.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,95,102,,,,,,,,,,,,,,,,WOS:000223079900013,0
C,"Mamou, J; Pereg, O; Wasserblat, M; Eirew, A; Green, Y; Guskin, S; Izsak, P; Korat, D",,,Assoc Computat Linguist,"Mamou, Jonathan; Pereg, Oren; Wasserblat, Moshe; Eirew, Alon; Green, Yael; Guskin, Shira; Izsak, Peter; Korat, Daniel",,,Term Set Expansion based NLP Architect by Intel AI Lab,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We present SetExpander, the term set expansion system based NLP Architect by Intel AI Lab. SetExpander is a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. It implements an iterative endto-end workflow and enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used successfully in real-life use cases including integration into an automated recruitment system and an issues and defects resolution system.(1)",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,19,24,,,,,,,,,,,,,,,,WOS:000855233900004,0
C,"Wiedemann, G; Yimam, SM; Biemann, C",,,Assoc Computat Linguist,"Wiedemann, Gregor; Yimam, Seid Muhie; Biemann, Chris",,,A Multilingual Information Extraction Pipeline for Investigative Journalism,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We introduce an advanced information extraction pipeline to automatically process very large collections of unstructured textual data for the purpose of investigative journalism. The pipeline serves as a new input processor for the upcoming major release of our New/s/leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks. Our software prepares a visually-aided exploration of the collection to quickly learn about potential stories contained in the data. It is based on the automatic extraction of entities and their co-occurrence in documents. In contrast to comparable projects, we focus on the following three major requirements particularly serving the use case of investigative journalism in cross-border collaborations: 1) composition of multiple state-of-the-art NLP tools for entity extraction, 2) support of multi-lingual document sets up to 40 languages, 3) fast and easy-to-use extraction of full-text, metadata and entities from various file formats.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,78,83,,,,,,,,,,,,,,,,WOS:000855233900014,0
C,"Florian, R; Yarowsky, D",,"Hajic, J; Matsumoto, Y",,"Florian, R; Yarowsky, D",,,Modeling consensus: Classifier combination for word sense disambiguation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper demonstrates the substantial empirical success of classifier combination for the word sense disambiguation task. It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Naive Bayes, cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mixture models). The paper also includes in-depth performance analysis sensitive to properties of the feature space and component classifiers. When evaluated on the standard SENSEVAL 1 and 2 data sets on 4 languages (English, Spanish, Basque, and Swedish), classifier combination performance exceeds the best published results on these data sets.",,,,,"Florian, Radu/AAF-1434-2019; Yarowsky, David/A-3270-2010",,,,,,,,,,,,,,,,,,,,2002,,,,,,,25,32,,,,,,,,,,,,,,,,WOS:000223079900004,0
C,"Marcu, D; Wong, W",,"Hajic, J; Matsumoto, Y",,"Marcu, D; Wong, W",,,"A phrase-based, joint probability model for statistical machine translation",PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,133,139,,,,,,,,,,,,,,,,WOS:000223079900018,0
C,"Liu, SS; Li, T; Li, ZM; Srikumar, V; Pascucci, V; Bremer, PT",,,Assoc Computat Linguist,"Liu, Shusen; Li, Tao; Li, Zhimin; Srikumar, Vivek; Pascucci, Valerio; Bremer, Peer-Timo",,,Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Neural networks models have gained unprecedented popularity in natural language processing due to their state-of-the-art performance and the flexible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and refinement of the models. In this work, we present a flexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the model internals (i.e., attention), and the output predictions, which in turn shed light on the model decision-making process.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,36,41,,,,,,,,,,,,,,,,WOS:000855233900007,0
C,"Eisner, J",,"Hajic, J; Matsumoto, Y",,"Eisner, J",,,Transformational priors over grammars,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs. To estimate the parameters is to discover a notion of relatedness among context-free rules such that related rules tend to have related probabilities. The prior favors grammars in which the relationships are simple to describe and have few major exceptions. A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank (20% reduction of rule perplexity over the best previous method).,,,,,"Eisner, Jason/D-3606-2009",,,,,,,,,,,,,,,,,,,,2002,,,,,,,63,70,,,,,,,,,,,,,,,,WOS:000223079900009,0
C,"Prolo, CA",,"Hajic, J; Matsumoto, Y",,"Prolo, CA",,,Fast LR parsing using rich (tree adjoining) grammars,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We describe an LR parser of parts-of-speech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking. We evaluate the parser using the Penn Treebank showing that the method yield very fast parsers with at least reasonable accuracy, confirming the intuition that LR parsing benefits from the use of rich grammars.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,103,110,,,,,,,,,,,,,,,,WOS:000223079900014,0
C,"Ueffing, N; Och, FJ; Ney, H",,"Hajic, J; Matsumoto, Y",,"Ueffing, N; Och, FJ; Ney, H",,,Generation of word graphs in statistical machine translation,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,Statistical machine translation systems usually compute the single sentence that has the highest probability according to the models that are trained on data. We describe a method for constructing a word graph to represent alternative hypotheses in an efficient way. The advantage is that these hypotheses can be rescored using a refined language or translation model. Results are presented on the German-English Verbmobil corpus.,,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,156,163,,,,,,,,,,,,,,,,WOS:000223079900021,0
C,"Lakomkin, E; Magg, S; Weber, C; Wermter, S",,,Assoc Computat Linguist,"Lakomkin, Egor; Magg, Sven; Weber, Cornelius; Wermter, Stefan",,,KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition from YouTube Videos,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"In this paper, we describe KT-Speech-Crawler: an approach for automatic dataset construction for speech recognition by crawling YouTube videos. We outline several filtering and post-processing steps, which extract samples that can be used for training end-to-end neural speech recognition systems. In our experiments, we demonstrate that a single-core version of die crawler can obtain around 150 hours of transcribed speech within a day, containing an estimated 3.5% word error rate in the transcriptions. Automatically collected samples contain reading and spontaneous speech recorded in various conditions including background noise and music, distant microphone recordings, and a variety of accents and reverberation. When training a deep neural network on speech recognition, we observed around 40% word error rate reduction on the Wall Street Journal dataset by integrating 200 hours of the collected samples into the training set. The demo' and the crawler code 2 are publicly available.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,90,95,,,,,,,,,,,,,,,,WOS:000855233900016,0
C,"Sorokin, D; Gurevych, I",,,Assoc Computat Linguist,"Sorokin, Daniil; Gurevych, Iryna",,,Interactive Instance-based Evaluation of Knowledge Base Question Answering,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Most approaches to Knowledge Base Question Answering are based on semantic parsing. In this paper, we present a tool that aids in debugging of question answering systems that construct a structured semantic representation for the input question. Previous work has largely focused on building question answering interfaces or evaluation frameworks that unify multiple data sets. The primary objective of our system is to enable interactive debugging of model predictions on individual instances (questions) and to simplify manual error analysis. Our interactive interface helps researchers to understand the shortcomings of a particular model, qualitatively analyze the complete pipeline and compare different models. A set of sit-by sessions was used to validate our interface design.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,114,119,,,,,,,,,,,,,,,,WOS:000855233900020,0
C,"Phillips, W; Riloff, E",,"Hajic, J; Matsumoto, Y",,"Phillips, W; Riloff, E",,,Exploiting strong syntactic heuristics and co-training to learn semantic lexicons,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We present a bootstrapping method that uses strong syntactic heuristics to learn semantic lexicons. The three sources of information are appositives, compound nouns, and ISA clauses. We apply heuristics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training. Results on WSJ articles and a pharmaceutical corpus show that this method obtains high precision and finds a large number of terms.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,125,132,,,,,,,,,,,,,,,,WOS:000223079900017,0
C,"Ajjour, Y; Wachsmuth, H; Kiesel, D; Riehmann, P; Fan, F; Castiglia, G; Adejoh, R; Frohlich, B; Stein, B",,,Assoc Computat Linguist,"Ajjour, Yamen; Wachsmuth, Henning; Kiesel, Dora; Riehmann, Patrick; Fan, Fan; Castiglia, Giuliano; Adejoh, Rosemary; Froehlich, Bernd; Stein, Benno",,,Visualization of the Topic Space of Argument Search Results in args.me,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"In times of fake news and alternative facts, pro and con arguments on controversial topics are of increasing importance. Recently, we presented args.me as the first search engine for arguments on the web. In its initial version, args.me ranked arguments solely by their relevance to a topic queried for, making it hard to learn about the diverse topical aspects covered by the search results. To tackle this shortcoming, we integrated a visualization interface for result exploration in args.me that provides an instant overview of the main aspects in a barycentric coordinate system. This topic space is generated ad-hoc from controversial issues on Wikipedia and argument-specific LDA models. In two case studies, we demonstrate how individual arguments can be found easily through interactions with the visualization, such as highlighting and filtering.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,60,65,,,,,,,,,,,,,,,,WOS:000855233900011,0
C,"Raghuvanshi, A; Carroll, L; Raghunathan, K",,,Assoc Computat Linguist,"Raghuvanshi, Arushi; Carroll, Lucien; Raghunathan, Karthik",,,Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We demonstrate an end-to-end approach for building conversational interfaces from prototype to production that has proven to work well for a number of applications across diverse verticals. Our architecture improves on the standard domain-intent-entity classification hierarchy and dialogue management architecture by leveraging shallow semantic parsing. We observe that NLU systems for industry applications often require more structured representations of entity relations than provided by the standard hierarchy, yet without requiring full semantic parses which are often inaccurate on real-world conversational data. We distinguish two kinds of semantic properties that can be provided through shallow semantic parsing: entity groups and entity roles. We also provide live demos of conversational apps built for two different use cases: food ordering and meeting control.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,157,162,,,,,,,,,,,,,,,,WOS:000855233900027,0
C,"Chu-Carroll, J; Prager, J; Ravin, Y; Cesar, C",,"Hajic, J; Matsumoto, Y",,"Chu-Carroll, J; Prager, J; Ravin, Y; Cesar, C",,,A hybrid approach to natural language Web search,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We describe a hybrid approach to improving search performance by providing a natural language front end to a traditional keyword-based search engine. The key component of the system is iterative query formulation and retrieval, in which one or more queries are automatically formulated from the user's question, issued to the search engine, and the results accumulated to form the hit list. New queries are generated by relaxing previously-issued queries using transformation rules, applied in an order obtained by reinforcement learning. This statistical component is augmented by a knowledge-driven hub-page identifier that retrieves a hub-page for the most salient noun phrase in the question, if possible. Evaluation on an unseen test set over the www.ibm.com public website with 1.3 million webpages shows that both components make substantial contribution to improving search performance, achieving a combined 137% relative improvement in the number of questions correctly answered, compared to a baseline of keyword queries consisting of two noun phrases.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,180,187,,,,,,,,,,,,,,,,WOS:000223079900024,0
C,"Fukumoto, F; Suzuki, Y",,"Hajic, J; Matsumoto, Y",,"Fukumoto, F; Suzuki, Y",,,Manipulating large corpora for text classification,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,196,203,,,,,,,,,,,,,,,,WOS:000223079900026,0
C,"Boullosa, B; de Castilho, RE; Kumar, N; Klie, JC; Gurevych, I",,,Assoc Computat Linguist,"Boullosa, Beto; de Castilho, Richard Eckart; Kumar, Naveen; Klie, Jan-Christoph; Gurevych, Iryna",,,Integrating Knowledge-Supported Search into the INCEpTION Annotation Platform,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Annotating entity mentions and linking them to a knowledge resource are essential tasks in many domains. It disambiguates mentions, introduces cross-document coreferences, and the resources contribute extra information, e.g. taxonomic relations. Such tasks benefit from text annotation tools that integrate a search which covers the text, the annotations, as well as the knowledge resource. However, to the best of our knowledge, no current tools integrate knowledge-supported search as well as entity linking support. We address this gap by introducing knowledge-supported search functionality into the INCEpTION text annotation platform. In our approach, cross-document references are created by linking entity mentions to a knowledge base in the form of a structured hierarchical vocabulary. The resulting annotations are then indexed to enable fast and yet complex queries taking into account the text, the annotations, and the vocabulary structure.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,127,132,,,,,,,,,,,,,,,,WOS:000855233900022,0
C,"Utsuro, T; Sassano, M; Uchimoto, K",,"Hajic, J; Matsumoto, Y",,"Utsuro, T; Sassano, M; Uchimoto, K",,,Combining outputs of multiple Japanese named entity chunkers by stacking,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"In this paper, we propose a method for learning a classifier which combines outputs of more than one Japanese named entity extractors. The proposed combination method belongs to the family of stacked generalizers, which is in principle a technique of combining outputs of several classifiers at the first stage by learning a second stage classifier to combine those outputs at the first stage. Individual models to be combined are based on maximum entropy models, one of which always considers surrounding contexts of a fixed length, while the other considers those of variable lengths according to the number of constituent morphemes of named entities. As an algorithm for learning the second stage classifier, we employ a decision list learning method. Experimental evaluation shows that the proposed method achieves improvement over the best known results with Japanese named entity extractors based on maximum entropy models.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,281,288,,,,,,,,,,,,,,,,WOS:000223079900036,0
C,"Goodman, J",,"Hajic, J; Matsumoto, Y",,"Goodman, J",,,An incremental decision list learner,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We demonstrate a problem with the standard technique for learning probabilistic decision lists. We describe a simple, incremental algorithm that avoids this problem, and show how to implement it efficiently. We also show a variation that adds thresholding to the standard sorting algorithm for decision lists, leading to similar improvements. Experimental results show that the new algorithm produces substantially lower error rates and entropy, while simultaneously learning lists that are over an order of magnitude smaller than those produced by the standard algorithm.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,17,24,,,,,,,,,,,,,,,,WOS:000223079900003,0
C,"Loginova, E; Neumann, G",,,Assoc Computat Linguist,"Loginova, Ekaterina; Neumann, Guenter",,,An Interactive Web-Interface for Visualizing the Inner Workings of the Question Answering LSTM,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,Deep learning models for NLP are potent but not readily interpretable. It prevents researchers from improving a model's performance efficiently and users from applying it for a task which requires a high level of trust in the system. We present a visualisation tool which aims to illuminate the inner workings of a specific LSTM model for question answering. It plots heatmaps of neurons' firings and allows a user to check the dependency between neurons and manual features. The system possesses an interactive web-interface and can be adapted to other models and domains.,,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,30,35,,,,,,,,,,,,,,,,WOS:000855233900006,0
C,"Czuba, K; Prager, J; Chu-Carroll, J",,"Hajic, J; Matsumoto, Y",,"Czuba, K; Prager, J; Chu-Carroll, J",,,A machine learning approach to introspection in a question answering system,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"The ability to evaluate intermediate results in a Question Answering (QA) system, which we call introspection, is necessary in architectures based on planning or on processing loops. In particular, it is needed to determine if an earlier phase must be retried, or if the response No Answer must be offered. We look at an introspection task of performing a cursory evaluation of the search engine output in a QA system. We define this task as a concept-learning problem and evaluate two classifiers that use features based on score progression in the ranked list returned by the search engine and candidate answer types. Our experiments showed promising results, achieving 25% relative improvement over a majority class baseline on unseen data.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,265,272,,,,,,,,,,,,,,,,WOS:000223079900034,0
C,"Jansche, M; Abney, SP",,"Hajic, J; Matsumoto, Y",,"Jansche, M; Abney, SP",,,Information extraction from voicemail transcripts,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,Voicemail is not like email. Even such basic information as the name of the caller/ sender or a phone number for returning calls is not represented explicitly and must be obtained from message transcripts or other sources. We discuss techniques for doing this and the challenges these tasks present.,,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,320,327,,,,,,,,,,,,,,,,WOS:000223079900041,0
C,"Kumar, S; Byrne, W",,"Hajic, J; Matsumoto, Y",,"Kumar, S; Byrne, W",,,Minimum Bayes-Risk word alignments of bilingual texts,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We present Minimum Bayes-Risk word alignment for machine translation. This statistical, model-based approach attempts to minimize the expected risk of alignment errors under loss functions that measure alignment quality. We describe various loss functions, including some that incorporate linguistic: analysis as can be obtained from parse trees, and show that these approaches can improve alignments of the English-French Hansards.",,,,,"Hajic, Jan/D-3429-2017","Hajic, Jan/0000-0002-3503-7730",,,,,,,,,,,,,,,,,,,2002,,,,,,,140,147,,,,,,,,,,,,,,,,WOS:000223079900019,0
C,"Ng, V; Cardie, C",,"Hajic, J; Matsumoto, Y",,"Ng, V; Cardie, C",,,Combining sample selection and error-driven pruning for machine learning of coreference rules,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"Most machine learning solutions to noun phrase coreference resolution recast the problem as a classification task. We examine three potential problems with this reformulation, namely, skewed class distributions, the inclusion of hard training instances, and the loss of transitivity inherent in the original coreference relation. We show how these problems can be handled via intelligent sample selection and error-driven pruning of classification rule-sets. The resulting system achieves an F-measure of 69.5 and 63.4 on the MUC-6 and MUC-7 coreference resolution data sets, respectively, surpassing the performance of the best MUC-6 and MUC-7 coreference systems. In particular, the system outperforms the best-performing learning-based coreference system to date.",,,,,,"Cardie, Claire/0000-0002-2061-6094",,,,,,,,,,,,,,,,,,,2002,,,,,,,55,62,,,,,,,,,,,,,,,,WOS:000223079900008,0
C,"Labutov, I; Srivastava, S; Mitchell, T",,,Assoc Computat Linguist,"Labutov, Igor; Srivastava, Shashank; Mitchell, Tom",,,LIA: A Natural Language Programmable Personal Assistant,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We present LIA, an intelligent personal assistant that can be programmed using natural language. Our system demonstrates multiple competencies towards learning from human-like interactions. These include (i) the ability to be taught reusable conditional procedures, (ii) ability to be taught new knowledge about the world (concepts in an ontology) and (iii) the ability to be taught how to ground that knowledge in a set of sensors and effectors. Building such a system highlights design questions regarding the overall architecture that such an agent should have, as well as questions about parsing and grounding language in situational contexts. We outline key properties of this architecture, and demonstrate a prototype that embodies them in the form of a personal assistant on an Android device.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,145,150,,,,,,,,,,,,,,,,WOS:000855233900025,0
C,"Gupta, NK; Bangalore, S",,"Hajic, J; Matsumoto, Y",,"Gupta, NK; Bangalore, S",,,Extracting clauses for spoken language understanding in conversational systems,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"Spontaneous human utterances in the context of human-human and human-machine dialogs are rampant with dysfluencies, and speech repairs. Furthermore, when recognized using a speech recognizer, these utterances produce a sequence of words with no identification of clausal units. Such long strings of words combined with speech errors pose a difficult problem for spoken language parsing and understanding. In this paper, we address the issue of editing speech repairs as well as segmenting user utterances into clause units with a view of parsing and understanding spoken language utterances. We present generative and discriminative models for this task and present evaluation results on the human-human conversations obtained from the Switchboard corpus.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,273,280,,,,,,,,,,,,,,,,WOS:000223079900035,0
C,"Markert, K; Nissim, M",,"Hajic, J; Matsumoto, Y",,"Markert, K; Nissim, M",,,Metonymy resolution as a classification task,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We reformulate metonymy resolution as a classification task. This is motivated by the regularity of metonymic readings and makes general classification and word sense disambiguation methods available for metonymy resolution. We then present a case study for location names, presenting both a corpus of location names annotated for metonymy as well as experiments with a supervised classification algorithm on this corpus. We especially explore the contribution of features used in word sense disambiguation to metonymy resolution.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,204,213,,,,,,,,,,,,,,,,WOS:000223079900027,0
C,"Thelen, M; Riloff, E",,"Hajic, J; Matsumoto, Y",,"Thelen, M; Riloff, E",,,A bootstrapping method for learning semantic lexicons using extraction pattern contexts,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,214,221,,,,,,,,,,,,,,,,WOS:000223079900028,0
C,"Yimam, SM; Biemann, C",,,Assoc Computat Linguist,"Yimam, Seid Muhie; Biemann, Chris",,,Demonstrating PAR4SEM-A SemanticWriting Aid with Adaptive Paraphrasing,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"In this paper, we present PAR4SEM, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, PAR4SEM is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. PAR4SEM is a tool, which supports an adaptive, iterative, and interactive process where the underlying machine learning models are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate PAR4SEM by adopting it to a text simplification task through mere usage.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,48,53,,,,,,,,,,,,,,,,WOS:000855233900009,0
C,"Wang, W; Harper, MP",,"Hajic, J; Matsumoto, Y",,"Wang, W; Harper, MP",,,The SuperARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper. Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon. The SuperARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models. The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources. We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints. Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,238,247,,,,,,,,,,,,,,,,WOS:000223079900031,0
C,"Zelenko, D; Aone, C; Richardella, A",,"Hajic, J; Matsumoto, Y",,"Zelenko, D; Aone, C; Richardella, A",,,Kernel methods for relation extraction,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,71,78,,,,,,,,,,,,,,,,WOS:000223079900010,0
C,"Tolmachev, A; Kawahara, D; Kurohashi, S",,,Assoc Computat Linguist,"Tolmachev, Arseny; Kawahara, Daisuke; Kurohashi, Sadao",,,Juman plus plus : A Morphological Analysis Toolkit for Scriptio Continua,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,We present a three-part toolkit for developing morphological analyzers for languages without natural word boundaries. The first part is a lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained model and a partial annotation tool. Our morphological analyzer for Japanese achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis of using our toolkit.,,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,54,59,,,,,,,,,,,,,,,,WOS:000855233900010,0
C,"Blaheta, D",,"Hajic, J; Matsumoto, Y",,"Blaheta, D",,,Handling noisy training and testing data,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"In the field of empirical natural language processing, researchers constantly deal with large amounts of maxked-up data; whether the markup is done by the researcher or someone else, human nature dictates that it will have errors in it. This paper will more fully characterise the problem and discuss whether and when (and how) to correct the errors. The discussion is illustrated with specific examples involving function tagging in the Penn treebank.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,111,116,,,,,,,,,,,,,,,,WOS:000223079900015,0
C,"Nghiem, MQ; Ananiadou, S",,,Assoc Computat Linguist,"Minh-Quoc Nghiem; Ananiadou, Sophia",,,APLenty: annotation tool for creating high-quality datasets using active and proactive learning,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"In this paper, we present APLenty, an annotation tool for creating high-quality sequence labeling datasets using active and proactive learning. A major innovation of our tool is the integration of automatic annotation with active learning and proactive learning. This makes the task of creating labeled datasets easier, less time-consuming and requiring less human effort. APLenty is highly flexible and can be adapted to various other tasks.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,108,113,,,,,,,,,,,,,,,,WOS:000855233900019,0
C,"Vadapalli, R; Syed, B; Prabhu, N; Srinivasan, BV; Varma, V",,,Assoc Computat Linguist,"Vadapalli, Raghuram; Syed, Bakhtiyar; Prabhu, Nishant; Srinivasan, Balaji Vasan; Varma, Vasudeva",,,When science journalism meets artificial intelligence : An interactive demonstration,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We present an online interactive tool(1) that generates titles of blog titles and thus take the first step toward automating science journalism. Science journalism aims to transform jargon-laden scientific articles into a form that the common reader can comprehend while ensuring that the underlying meaning of the article is retained. In this work, we present a tool, which, given the title and abstract of a research paper will generate a blog title by mimicking a human science journalist. The tool makes use of a model trained on a corpus of 87, 328 pairs of research papers and their corresponding blogs, built from two science news aggregators. The architecture of the model is a two-stage mechanism which generates blog titles. Evaluation using standard metrics indicate the viability of the proposed system.",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,163,168,,,,,,,,,,,,,,,,WOS:000855233900028,0
C,"Strube, N; Rapp, S; Muller, C",,"Hajic, J; Matsumoto, Y",,"Strube, N; Rapp, S; Muller, C",,,The influence of minimum edit distance on reference resolution,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"We report on experiments in reference resolution using a decision tree approach. We started with a standard feature set used in previous work, which led to moderate results. A closer examination of the performance of the features for different forms of anaphoric expressions showed good results for pronouns, moderate results for proper names, and poor results for definite noun phrases. We then included a cheap, language and domain independent feature based on the minimum edit distance between strings. This feature yielded a significant improvement for data sets consisting of definite noun phrases and proper names, respectively. When applied to the whole data set the feature produced a smaller but still significant improvement.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,312,319,,,,,,,,,,,,,,,,WOS:000223079900040,0
C,"Cohen, J; Cohen, R; Toledo, E; Buys, J",,,Assoc Computat Linguist,"Cohen, Jaron; Cohen, Roy; Toledo, Edan; Buys, Jan",,,RepGraph: Visualising and Analysing Meaning Representation Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present RepGraph, an open source visualisation and analysis tool for meaning representation graphs. Graph-based meaning representations provide rich semantic annotations, but visualising them clearly is more challenging than for fully lexicalized representations. Our application provides a seamless, unifying interface with which to visualise, manipulate and analyse semantically parsed graph data represented in a JSON-based serialisation format. RepGraph visualises graphs in multiple formats, with an emphasis on showing the relation between nodes and their corresponding token spans, whilst keeping the representation compact. Additionally, the web-based tool provides NLP researchers with a clear, visually intuitive way of interacting with these graphs, and includes a number of graph analysis features. The tool currently supports the DMRS, EDS, PTG, UCCA, and AMR semantic frameworks. A live demo is available at https://repgraph.vercel.app/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,79,86,,,,,,,,,,,,,,,,WOS:000855241500010,0
C,"De Cao, N; Aziz, W; Titov, I",,,Assoc Computat Linguist,"De Cao, Nicola; Aziz, Wilker; Titov, Ivan",,,Editing Factual Knowledge in Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The factual knowledge acquired during pretraining and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KNOWLEDGEEDITOR, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive retraining or fine-tuning. Besides being computationally efficient, KNOWLEDGEEDITOR does not require any modifications in LM pretraining (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KNOWLEDGEEDITOR's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6491,6506,,,,,,,,,,,,,,,,WOS:000860727000037,0
C,"ElSherief, M; Ziems, C; Muchlinski, D; Anupindi, V; Seybolt, J; De Choudhury, M; Yang, DY",,,Assoc Computat Linguist,"ElSherief, Mai; Ziems, Caleb; Muchlinski, David; Anupindi, Vaishnavi; Seybolt, Jordyn; De Choudhury, Munmun; Yang, Diyi",,,Latent Hatred: A Benchmark for Understanding Implicit Hate Speech,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue. To download the data, see https://github.com/ GT-SALT/implicit-hate",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,345,363,,,,,,,,,,,,,,,,WOS:000855966300029,0
C,"Gordon, M; Duh, K; Kaplan, J",,,Assoc Computat Linguist,"Gordon, Mitchell; Duh, Kevin; Kaplan, Jared",,,Data and Parameter Scaling Laws for Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5915,5922,,,,,,,,,,,,,,,,WOS:000855966306008,0
C,"Hayati, SA; Kang, D; Ungar, L",,,Assoc Computat Linguist,"Hayati, Shirley Anugrah; Kang, Dongyeop; Ungar, Lyle",,,Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, HUMMINGBIRD, on top of bench-marking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) humanand machine-identified words share significant overlap for some styles.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6323,6331,,,,,,,,,,,,,,,,WOS:000860727000025,0
C,"Ke, ZX; Liu, B; Xu, H; Shu, L",,,Assoc Computat Linguist,"Ke, Zixuan; Liu, Bing; Xu, Hu; Shu, Lei",,,CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system needs not know the task/domain to which the test data belongs. To our knowledge, this setting has not been studied before for ASC. This paper proposes a novel model called CLASSIC. The key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing. Experimental results show the high effectiveness of CLASSIC.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6871,6883,,,,,,,,,,,,,,,,WOS:000860727000065,0
C,"Li, ZY; Qin, YL; Liu, ZH; Wang, W",,,Assoc Computat Linguist,"Li, Zeyu; Qin, Yilong; Liu, Zihan; Wang, Wei",,,Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. High-quality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, non-deep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset present a significant improvement on the F1 scores over the best existing CPC approaches.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6818,6830,,,,,,,,,,,,,,,,WOS:000860727000061,0
C,"Liang, ZW; Yang, JN; Liu, H; Huang, KJ",,,Assoc Computat Linguist,"Liang, Zongwei; Yang, Junan; Liu, Hui; Huang, Keju",,,A Semantic Filter Based on Relations for Knowledge Graph Completion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better representational models to fit symmetry/antisymmetry and combination relationships. The current embedding models are more inclined to utilize the identical vector for the same entity in various triples to measure the matching performance. The observation that measuring the rationality of specific triples means comparing the matching degree of the specific attributes associated with the relations is well-known. Inspired by this fact, this paper designs Semantic Filter Based on Relations(SFBR) to extract the required attributes of the entities. Then the rationality of triples is compared under these extracted attributes through the traditional embedding models. The semantic filter module can be added to most geometric and tensor decomposition models with minimal additional memory. Experiments on the benchmark datasets show that the semantic filter based on relations can suppress the impact of other attribute dimensions and improve link prediction performance. The tensor decomposition models with SFBR have achieved state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7920,7929,,,,,,,,,,,,,,,,WOS:000860727002003,0
C,"Lyu, XL; Li, JH; Gong, ZX; Zhang, M",,,Assoc Computat Linguist,"Lyu, Xinglin; Li, Junhui; Gong, Zhengxian; Zhang, Min",,,Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply one translation per discourse in NMT, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears at. Then we encourage the translations of those words within a link to be consistent in two ways. On the one hand, when encoding sentences within a document we properly exchange context information of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translations should be consistent. Experimental results on Chinese <-> English and English -> French translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical translation consistency.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3265,3277,,,,,,,,,,,,,,,,WOS:000855966303034,0
C,"Madan, V; Khetan, A; Karnin, Z",,,Assoc Computat Linguist,"Madan, Vivek; Khetan, Ashish; Karnin, Zohar",,,TADPOLE: Task ADapted Pre-training via anOmaLy dEtection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus itself when it is large enough, or on a manually curated unlabeled corpus of a similar domain. In this paper, we address the problem for the case when the downstream corpus is too small for additional pre-training. We propose TADPOLE, a task adapted pre-training framework based on data selection techniques adapted from Domain Adaptation. We formulate the data selection as an anomaly detection problem that unlike existing methods works well when the downstream corpus is limited in size. It results in a scalable and efficient unsupervised technique that eliminates the need for any manual data curation. We evaluate our framework on eight tasks across four different domains: Biomedical, Computer Science, News, and Movie reviews, and compare its performance against competitive baseline techniques from the area of Domain Adaptation. Our framework outperforms all the baseline methods. On small datasets with less than 5K training examples, we get a gain of 1:82% in performance with additional pre-training for only 5% steps. It also compliments some of the other techniques such as data augmentation known for boosting performance when downstream corpus is small; highest performance is achieved when data augmentation is combined with task adapted pre-training.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5732,5746,,,,,,,,,,,,,,,,WOS:000855966305067,0
C,"Marz, L; Asgari, E; Braune, F; Zimmermann, F; Roth, B",,,Assoc Computat Linguist,"Maerz, Luisa; Asgari, Ehsaneddin; Braune, Fabienne; Zimmermann, Franziska; Roth, Benjamin",,,KnowMAN: Weakly Supervised Multinomial Adversarial Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals or to generalize well. We propose KnowMAN, an adversarial scheme that enables to control influence of signals associated with specific labeling functions. KnowMAN forces the network to learn representations that are invariant to those signals and to pick up other signals that are more generally associated with an output label. KnowMAN strongly improves results compared to direct weakly supervised learning with a pre-trained transformer language model and a feature-based baseline.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9549,9557,,,,,,,,,,,,,,,,WOS:000860727003052,0
C,"Saeidi, M; Yazdani, M; Vlachos, A",,,Assoc Computat Linguist,"Saeidi, Marzieh; Yazdani, Majid; Vlachos, Andreas",,,Cross-Policy Compliance Detection via Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results in poor accuracy due to the complexity of the policies. In this paper we propose to address policy compliance detection via decomposing it into question answering, where questions check whether the conditions stated in the policy apply to the scenario, and an expression tree combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better accuracy, especially in the cross-policy setup where the policies during testing are unseen in training. In addition, it allows us to use existing question answering models pre-trained on existing large datasets. Finally, it explicitly identifies the information missing from a scenario in case policy compliance cannot be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8622,8632,,,,,,,,,,,,,,,,WOS:000860727002056,0
C,"Tam, D; Menon, RR; Bansal, M; Srivastava, S; Raffel, C",,,Assoc Computat Linguist,"Tam, Derek; Menon, Rakesh R.; Bansal, Mohit; Srivastava, Shashank; Raffel, Colin",,,Improving and Simplifying Pattern Exploiting Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few shot learning without any unlabeled data and introduce ADAPET, which modifies PET's objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4980,4991,,,,,,,,,,,,,,,,WOS:000855966305011,0
C,"Tomanek, K; Zayats, V; Padfield, D; Vaillancourt, K; Biadsy, F",,,Assoc Computat Linguist,"Tomanek, Katrin; Zayats, Vicky; Padfield, Dirk; Vaillancourt, Kara; Biadsy, Fadi",,,Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Automatic Speech Recognition (ASR) systems are often optimized to work best for speakers with canonical speech patterns. Unfortunately, these systems perform poorly when tested on atypical speech and heavily accented speech. It has previously been shown that personalization through model fine-tuning substantially improves performance. However, maintaining such large models per speaker is costly and difficult to scale. We show that by adding a relatively small number of extra parameters to the encoder layers via socalled residual adapter, we can achieve similar adaptation gains compared to model finetuning, while only updating a tiny fraction (less than 0.5%) of the model parameters. We demonstrate this on two speech adaptation tasks (atypical and accented speech) and for two state-of-the-art ASR architectures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6751,6760,,,,,,,,,,,,,,,,WOS:000860727000056,0
C,"Wang, QL; Wen, ZY; Zhao, Q; Yang, M; Xu, RF",,,Assoc Computat Linguist,"Wang, Qianlong; Wen, Zhiyuan; Zhao, Qin; Yang, Min; Xu, Ruifeng",,,Progressive Self-Training with Discriminator for Aspect Term Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce noise. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the subset become harder and more numerous as the iteration proceeds. The other is that we use a discriminator to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-ofthe-art performance.",,,,,,"Wang, Qianlong/0000-0002-3011-0580",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,257,268,,,,,,,,,,,,,,,,WOS:000855966300023,0
C,"Zhao, J; Gui, T; Zhang, Q; Zhou, YQ",,,Assoc Computat Linguist,"Zhao, Jun; Gui, Tao; Zhang, Qi; Zhou, Yaqian",,,A Relation-Oriented Clustering Method for Open Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the model to learn to cluster relational data, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our method reduces the error rate by 29.2% and 15.7%, on two datasets respectively, compared with current SOTA methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9707,9718,,,,,,,,,,,,,,,,WOS:000860727003066,0
C,"Ozenc, B; Ehsani, R; Solak, E",,,Assoc Computat Linguist,"Ozenc, Berke; Ehsani, Razieh; Solak, Ercan",,,MorAz: an Open-source Morphological Analyzer for Azerbaijani Turkish,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,MorAz is an open-source morphological analyzer for Azerbaijani Turkish. The analyzer is available through both as a website for interactive exploration and as a RESTful web service for integration into a natural language processing pipeline. MorAz implements the morphology of Azerbaijani Turkish following a two-level approach using Helsinki finite-state transducer and wraps the analyzer with python scripts in a Django instance.,,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,25,29,,,,,,,,,,,,,,,,WOS:000855233900005,0
C,"Curran, JR",,"Hajic, J; Matsumoto, Y",,"Curran, JR",,,Ensemble methods for automatic thesaurus extraction,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplicity of their evaluation task and individual classifiers. Our work explores ensemble efficacy for the more complex task of automatic thesaurus extraction on up to 300 million words. We examine our conflicting results in terms of the constraints on, and complexity of, different contextual representations, which contribute to the sparseness-and noise-induced bias behaviour of NLP systems on very large corpora.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,222,229,,,,,,,,,,,,,,,,WOS:000223079900029,0
C,"Adel, H; Bostan, L; Papay, S; Pado, S; Klinger, R",,,Assoc Computat Linguist,"Adel, Heike; Bostan, Laura; Papay, Sean; Pado, Sebastian; Klinger, Roman",,,DERE: A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"Most machine learning systems for natural language processing are tailored to specific tasks. As a result, comparability of models across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present DERE, a novel framework for declarative specification and compilation of templatebased information extraction. It uses a generic specification language for the task and for data annotations in terms of spans and frames. This formalism enables the representation of a large variety of natural language processing challenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and/or domains as well as the assessment of model generalizability. DERE is available as open-source software.",,,,,,"Pado, Sebastian/0000-0002-7529-6825",,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,42,47,,,,,,,,,,,,,,,,WOS:000855233900008,0
C,"Yin, PC; Neubig, G",,,Assoc Computat Linguist,"Yin, Pengcheng; Neubig, Graham",,,TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation,CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"OCT 31-NOV 04, 2018","Brussels, BELGIUM",,,,,"We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.(1)",,,,,,,,,,,,,,,,,,,,,978-1-948087-85-8,,,,2018,,,,,,,7,12,,,,,,,,,,,,,,,,WOS:000855233900002,0
C,"Keller, F; Lapata, M; Ourioupina, O",,"Hajic, J; Matsumoto, Y",,"Keller, F; Lapata, M; Ourioupina, O",,,Using the Web to overcome data sparseness,PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUL 06-07, 2002","Philadelphia, PA","Justsystem Corp,CLAIRVOYANCE Corp",,,,"This paper shows that the web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the web by querying a search engine. We evaluate this method by demonstrating that web frequencies and correlate with frequencies obtained from a carefully edited, balanced corpus. We also perform a task-based evaluation, showing that web frequencies can reliably predict human plausibility judgments.",,,,,,,,,,,,,,,,,,,,,,,,,2002,,,,,,,230,237,,,,,,,,,,,,,,,,WOS:000223079900030,0
C,"Cho, SC; Dernoncourt, F; Ganter, T; Bui, T; Lipka, N; Chang, W; Jin, HL; Brandt, J; Foroosh, H; Liu, F",,,Assoc Computat Linguist,"Cho, Sangwoo; Dernoncourt, Franck; Ganter, Tim; Bui, Trung; Lipka, Nedim; Chang, Walter; Jin, Hailin; Brandt, Jonathan; Foroosh, Hassan; Liu, Fei",,,StreamHover: Livestream Transcript Summarization and Annotation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6457,6474,,,,,,,,,,,,,,,,WOS:000860727000035,0
C,"Dong, MQ; Pan, CG; Luo, ZP",,,Assoc Computat Linguist,"Dong, Manqing; Pan, Chunguang; Luo, Zhipeng",,,MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a framework considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and fine-tuning can significantly improve the model performance on low-resource relation extraction tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2694,2704,,,,,,,,,,,,,,,,WOS:000855966302071,0
C,"Edwards, C; Zhai, CX; Ji, H",,,Assoc Computat Linguist,"Edwards, Carl; Zhai, ChengXiang; Ji, Heng",,,Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structurebased retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,595,607,,,,,,,,,,,,,,,,WOS:000855966300047,0
C,"Gao, TY; Yao, XC; Chen, DQ",,,Assoc Computat Linguist,"Gao, Tianyu; Yao, Xingcheng; Chen, Danqi",,,SimCSE: Simple Contrastive Learning of Sentence Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using entailment pairs as positives and contradiction pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERTbase achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show-both theoretically and empirically-that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6894,6910,,,,,,,,,,,,,,,,WOS:000860727000067,0
C,"Guo, YM; Shou, LJ; Pei, J; Gong, M; Xu, MX; Wu, ZY; Jiang, DX",,,Assoc Computat Linguist,"Guo, Yingmei; Shou, Linjun; Pei, Jian; Gong, Ming; Xu, Mingxing; Wu, Zhiyong; Jiang, Daxin",,,Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating noise in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various augmented methods. Those models provide supervision signals to each other. The experimental results show that our method outperforms the existing state of the art by 3.05 and 4.24 percentage points on two benchmark datasets, respectively. The code will be made open sourced on github.",,,,,,"Pei, Jian/0000-0002-2200-8711",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3226,3237,,,,,,,,,,,,,,,,WOS:000855966303031,0
C,"Habernal, I",,,Assoc Computat Linguist,"Habernal, Ivan",,,When differential privacy meets NLP: The devil is in the detail,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users' original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al., 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder's dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1522,1528,,,,,,,,,,,,,,,,WOS:000855966301047,0
C,"Kwon, J; Kobayashi, N; Kamigaito, H; Okumura, M",,,Assoc Computat Linguist,"Kwon, Jingun; Kobayashi, Naoki; Kamigaito, Hidetaka; Okumura, Manabu",,,Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4039,4044,,,,,,,,,,,,,,,,WOS:000855966304017,0
C,"Li, LY; Song, DM; Li, XN; Zeng, JH; Ma, RT; Qiu, XP",,,Assoc Computat Linguist,"Li, Linyang; Song, Demin; Li, Xiaonan; Zeng, Jiehang; Ma, Ruotian; Qiu, Xipeng",,,Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-Trained Models have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3023,3032,,,,,,,,,,,,,,,,WOS:000855966303013,0
C,"Li, ML; Ma, TF; Yu, M; Wu, LF; Gao, T; Ji, H; McKeown, K",,,Assoc Computat Linguist,"Li, Manling; Ma, Tengfei; Yu, Mo; Wu, Lingfei; Gao, Tian; Ji, Heng; McKeown, Kathleen",,,Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events' intra-structures (arguments) and inter-structures (event-event connections). Following a different route, we propose to represent the news articles as an event-graph, thus the summarization task becomes compressing the whole graph to its salient sub-graph. The key hypothesis is that the events connected through shared arguments and temporal order depict the skeleton of a timeline, containing events that are semantically related, structurally salient, and temporally coherent in the global event graph. A time-aware optimal transport distance is then introduced for learning the compression model in an unsupervised manner. We show that our approach significantly improves the state of the art on three real-world datasets, including two public standard benchmarks and our newly collected Timeline 100 dataset.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6443,6456,,,,,,,,,,,,,,,,WOS:000860727000034,0
C,"Lian, YC; Bisazza, A; Verhoef, T",,,Assoc Computat Linguist,"Lian, Yuchen; Bisazza, Arianna; Verhoef, Tessa",,,The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during learning, instead of developing a more efficient or systematic language.",,,,,,"Verhoef, Tessa/0000-0002-1219-3730",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10121,10129,,,,,,,,,,,,,,,,WOS:000860727004018,0
C,"Narang, S; Chung, HW; Tay, Y; Fedus, W; Fevry, T; Matena, M; Malkan, K; Fiedel, N; Shazeer, N; Lan, ZZ; Zhou, YQ; Li, W; Ding, N; Marcus, J; Roberts, A; Raffel, C",,,Assoc Computat Linguist,"Narang, Sharan; Chung, Hyung Won; Tay, Yi; Fedus, William; Fevry, Thibault; Matena, Michael; Malkan, Karishma; Fiedel, Noah; Shazeer, Noam; Lan, Zhenzhong; Zhou, Yanqi; Li, Wei; Ding, Nan; Marcus, Jake; Roberts, Adam; Raffel, Colin",,,Do Transformer Modifications Transfer Across Implementations and Applications?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5758,5773,,,,,,,,,,,,,,,,WOS:000855966305069,0
C,"Niu, T; Yavuz, S; Zhou, YB; Keskar, NS; Wang, H; Xiong, CM",,,Assoc Computat Linguist,"Niu, Tong; Yavuz, Semih; Zhou, Yingbo; Keskar, Nitish Shirish; Wang, Huan; Xiong, Caiming",,,Unsupervised Paraphrasing with Pretrained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5136,5150,,,,,,,,,,,,,,,,WOS:000855966305021,0
C,"Qu, FY; Jia, X; Wu, YF",,,Assoc Computat Linguist,"Qu, Fanyi; Jia, Xin; Wu, Yunfang",,,Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our model on the strong generative pre-training model. Experimental results show that our model makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our model, suggesting new directions for this challenging task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2583,2593,,,,,,,,,,,,,,,,WOS:000855966302061,0
C,"Sai, AB; Dixit, T; Sheth, DY; Mohan, S; Khapra, MM",,,Assoc Computat Linguist,"Sai, Ananya B.; Dixit, Tanay; Sheth, Dev Yashpal; Mohan, Sreyas; Khapra, Mitesh M.",,,Perturbation CheckLists for Evaluating NLG Evaluation Metrics,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7219,7234,,,,,,,,,,,,,,,,WOS:000860727001023,0
C,"Shi, JH; Ding, X; Du, L; Liu, T; Qin, B",,,Assoc Computat Linguist,"Shi, Jihao; Ding, Xiao; Du, Li; Liu, Ting; Qin, Bing",,,Neural Natural Logic Inference for Interpretable Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The proposed model gradually bridges a hypothesis and candidate premises following natural logic inference steps to build proof paths. Entailment scores between the acquired intermediate hypotheses and candidate premises are measured to determine if a premise entails the hypothesis. As the natural logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and premises in a Hyperbolic space rather than Euclidean space to acquire more precise representations. Empirically, our method outperforms prior work on answering multiple-choice science questions, achieving the best results on two publicly available datasets. The natural logic inference process inherently provides evidence to help explain the prediction process.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3673,3684,,,,,,,,,,,,,,,,WOS:000855966303070,0
C,"Tiyajamorn, N; Kajiwara, T; Arase, Y; Onizuka, M",,,Assoc Computat Linguist,"Tiyajamorn, Nattapong; Kajiwara, Tomoyuki; Arase, Yuki; Onizuka, Makoto",,,Language-Agnostic Representation from Multilingual Sentence Encoders for Cross-Lingual Similarity Estimation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a method to distil language-agnostic meaning embedding using a multilingual sentence encoder. By removing languagespecific information from the original embedding, we retrieve an embedding that fully represents the meaning of the sentence. The proposed method relies only on parallel corpora without any human annotations. Our meaning embedding allows for efficient cross-lingual sentence similarity estimation using a simple cosine similarity calculation. Experimental results of both the quality estimation of machine translation and cross-lingual semantic textual similarity tasks reveal that our method consistently outperforms the strong baselines using the original multilingual embeddings. The method also consistently improves the performance of any pre-trained multilingual sentence encoder, even in low-resource language pairs, where only tens of thousands of parallel sentence pairs are available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7764,7774,,,,,,,,,,,,,,,,WOS:000860727001060,0
C,"Wang, JL; Liu, Y; Wang, XE",,,Assoc Computat Linguist,"Wang, Jialu; Liu, Yang; Wang, Xin Eric",,,Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO (Lin et al., 2014) and Flickr3OK (Young et al., 2014) benchmarks show that our methods significantly reduce the gender bias in image search models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1995,2008,,,,,,,,,,,,,,,,WOS:000855966302010,0
C,"Wang, Y; Wang, WS; Joty, S; Hoi, SCH",,,Assoc Computat Linguist,"Wang, Yue; Wang, Weishi; Joty, Shafiq; Hoi, Steven C. H.",,,CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8696,8708,,,,,,,,,,,,,,,,WOS:000860727002063,0
C,"Wegmann, A; Nguyen, D",,,Assoc Computat Linguist,"Wegmann, Anna; Dong Nguyen",,,"Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac'tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7109,7130,,,,,,,,,,,,,,,,WOS:000860727001017,0
C,"Yuan, LP; Zheng, XQ; Zhou, Y; Hsieh, CJ; Chang, KW",,,Assoc Computat Linguist,"Yuan, Liping; Zheng, Xiaoqing; Zhou, Yi; Hsieh, Cho-Jui; Chang, Kai-Wei",,,On the Transferability of Adversarial Attacks against Neural Text Classifier,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a genetic algorithm to find an ensemble of models that can be used to induce adversarial examples to fool almost all existing models. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1612,1625,,,,,,,,,,,,,,,,WOS:000855966301054,0
C,"Zhang, TF; Huang, HY; Feng, C; Cao, LB",,,Assoc Computat Linguist,"Zhang, Tianfu; Huang, Heyan; Feng, Chong; Cao, Longbing",,,Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on WMT14 and WMT16 English -> German and English -> Czech language machine translation validate the RHE effectiveness.",,,,,"zhang, tian/GZK-6001-2022","cao, longbing/0000-0003-1562-9429",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3238,3248,,,,,,,,,,,,,,,,WOS:000855966303032,0
C,"Zhang, WX; Deng, Y; Li, X; Yuan, YF; Bing, LD; Lam, W",,,Assoc Computat Linguist,"Zhang, Wenxuan; Deng, Yang; Li, Xin; Yuan, Yifei; Bing, Lidong; Lam, Wai",,,Aspect Sentiment Quad Prediction as Paraphrase Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel PARAPHRASE modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of crosstask transfer with the proposed unified PARAPHRASE modeling framework.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9209,9219,,,,,,,,,,,,,,,,WOS:000860727003028,0
C,"Zharmagambetov, A; Gabidolla, M; Carreira-Perpinan, MA",,,Assoc Computat Linguist,"Zharmagambetov, Arman; Gabidolla, Magzhan; Carreira-Perpinan, Miguel A.",,,"Softmax Tree: An Accurate, Fast Classifier When the Number of Classes Is Large",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Classification problems having thousands or more classes naturally occur in NLP, for example language models or document classification. A softmax or one-vs-all classifier naturally handles many classes, but it is very slow at inference time, because every class score must be calculated to find the top class. We propose the softmax tree, consisting of a binary tree having sparse hyperplanes at the decision nodes (which make hard, not soft, decisions) and small softmax classifiers at the leaves. This is much faster at inference because the input instance follows a single path to a leaf (whose length is logarithmic on the number of leaves) and the softmax classifier at each leaf operates on a small subset of the classes. Although learning accurate tree-based models has proven difficult in the past, we are able to overcome this by using a variation of a recent algorithm, tree alternating optimization (TAO). Compared to a softmax and other classifiers, the resulting softmax trees are both more accurate in prediction and faster in inference, as shown in NLP problems having from one thousand to one hundred thousand classes.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10730,10745,,,,,,,,,,,,,,,,WOS:000860727004062,0
C,"Zhou, WX; Liu, FY; Chen, MH",,,Assoc Computat Linguist,"Zhou, Wenxuan; Liu, Fangyu; Chen, Muhao",,,Contrastive Out-of-Distribution Detection for Pretrained Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the indistribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model's penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1100,1111,,,,,,,,,,,,,,,,WOS:000855966301017,0
C,"Amplayo, RK; Angelidis, S; Lapata, M",,,Assoc Computat Linguist,"Amplayo, Reinald Kim; Angelidis, Stefanos; Lapata, Mirella",,,Aspect-Controllable Opinion Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room of a hotel). Using a review corpus, we create a synthetic training dataset of (review, summary) pairs enriched with aspect controllers which are induced by a multi-instance learning model that predicts the aspects of a document at different levels of granularity. We fine-tune a pretrained model using our synthetic dataset and generate aspect-specific summaries by modifying the aspect controllers. Experiments on two benchmarks show that our model outperforms the previous state of the art and generates personalized summaries by controlling the number of aspects discussed in them.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6578,6593,,,,,,,,,,,,,,,,WOS:000860727000043,0
C,"Brazinskas, A; Lapata, M; Titov, I",,,Assoc Computat Linguist,"Brazinskas, Arthur; Lapata, Mirella; Titov, Ivan",,,Learning Opinion Summarizers by Selecting Informative Reviews,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Opinion summarization has been traditionally approached with unsupervised, weakly-supervised and few-shot learning techniques. In this work, we collect a large dataset of summaries paired with user reviews for over 31,000 products, enabling supervised training. However, the number of reviews per product is large (320 on average), making summarization - and especially training a summarizer - impractical. Moreover, the content of many reviews is not reflected in the human-written summaries, and, thus, the summarizer trained on random review subsets hallucinates. In order to deal with both of these challenges, we formulate the task as jointly learning to select informative subsets of reviews and summarizing the opinions expressed in these subsets. The choice of the review subset is treated as a latent variable, predicted by a small and simple selector. The subset is then fed into a more powerful summarizer. For joint training, we use amortized variational inference and policy gradient methods. Our experiments demonstrate the importance of selecting informative reviews resulting in improved quality of summaries and reduced hallucinations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9424,9442,,,,,,,,,,,,,,,,WOS:000860727003044,0
C,"Conia, S; Orlando, R; Brignone, F; Cecconi, F; Navigli, R",,,Assoc Computat Linguist,"Conia, Simone; Orlando, Riccardo; Brignone, Fabrizio; Cecconi, Francesco; Navigli, Roberto",,,InVeRo-XL: Making Cross-Lingual Semantic Role Labeling Accessible with Intelligible Verbs and Roles,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Notwithstanding the growing interest in cross-lingual techniques for Natural Language Processing, there has been a surprisingly small number of efforts aimed at the development of easy-to-use tools for cross-lingual Semantic Role Labeling. In this paper, we fill this gap and present InVeRo-XL, an off-the-shelf state-of-the-art system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that our system - with its easy-to-use RESTful API and Web interface - will become a valuable tool for the research community, encouraging the integration of sentence-level semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http: //nlp.uniromal.it/invero.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,319,328,,,,,,,,,,,,,,,,WOS:000855241500036,0
C,"Gardner, M; Merrill, W; Dodge, J; Peters, ME; Ross, A; Singh, S; Smith, NA",,,Assoc Computat Linguist,"Gardner, Matt; Merrill, William; Dodge, Jesse; Peters, Matthew E.; Ross, Alexis; Singh, Sameer; Smith, Noah A.",,,Competency Problems: On Finding and Removing Artifacts in Language Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have spurious instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word amazing on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1801,1813,,,,,,,,,,,,,,,,WOS:000855966301068,0
C,"Garg, S; Moschitti, A",,,Assoc Computat Linguist,"Garg, Siddhant; Moschitti, Alessandro",,,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision/Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower Recall, e.g., reducing computation by similar to 60%, while only losing similar to 3 of Recall.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7329,7346,,,,,,,,,,,,,,,,WOS:000860727001031,0
C,"Hsu, CC; Tan, CH",,,Assoc Computat Linguist,"Hsu, Chao-Chun; Tan, Chenhao",,,Decision-Focused Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Relevance in summarization is typically defined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to provide valuable insights on how a decision can be inferred from text. To build a summary, we then select representative sentences that lead to similar model decisions as using the full text while accounting for textual non-redundancy. To evaluate our method (DecSum), we build a testbed where the task is to summarize the first ten reviews of a restaurant in support of predicting its future rating on Yelp. DecSum substantially outperforms text-only summarization methods and model-based explanation methods in decision faithfulness and representativeness. We further demonstrate that DecSum is the only method that enables humans to outperform random chance in predicting which restaurant will be better rated in the future.",,,,,,"Tan, Chenhao/0000-0002-3981-2116",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,117,132,,,,,,,,,,,,,,,,WOS:000855966300010,0
C,"Lagunas, F; Charlaix, E; Sanh, V; Rush, AM",,,Assoc Computat Linguist,"Lagunas, Francois; Charlaix, Ella; Sanh, Victor; Rush, Alexander M.",,,Block Pruning For Faster Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10619,10629,,,,,,,,,,,,,,,,WOS:000860727004053,0
C,"Lester, B; Al-Rfou, R; Constant, N",,,Assoc Computat Linguist,"Lester, Brian; Al-Rfou, Rami; Constant, Noah",,,The Power of Scale for Parameter-Efficient Prompt Tuning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we explore prompt tuning, a simple yet effective mechanism for learning soft prompts to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through back-propagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method closes the gap and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed prefix tuning of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient prompt ensembling. We release code and model checkpoints to reproduce our experiments.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3045,3059,,,,,,,,,,,,,,,,WOS:000855966303015,0
C,"Liu, QB; Cao, PF; Liu, C; Chen, JS; Cai, XL; Yang, F; He, SZ; Liu, K; Zhao, J",,,Assoc Computat Linguist,"Liu, Qingbin; Cao, Pengfei; Liu, Cao; Chen, Jiansong; Cai, Xunliang; Yang, Fan; He, Shizhu; Liu, Kang; Zhao, Jun",,,Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2301,2311,,,,,,,,,,,,,,,,WOS:000855966302035,0
C,"Liu, YH; Guan, RC; Giunchiglia, F; Liang, YC; Feng, XY",,,Assoc Computat Linguist,"Liu, Yonghao; Guan, Renchu; Giunchiglia, Fausto; Liang, Yanchun; Feng, Xiaoyue",,,Deep Attention Diffusion Graph Neural Networks for Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text classification is a fundamental task with broad applications in natural language processing. Recently, graph neural networks (GNNs) have attracted much attention due to their powerful representation ability. However, most existing methods for text classification based on GNNs consider only one-hop neighborhoods and low-frequency information within texts, which cannot fully utilize the rich context information of documents. Moreover, these models suffer from over-smoothing issues if many graph layers are stacked. In this paper, a Deep Attention Diffusion Graph Neural Network (DADGNN) model is proposed to learn text representations, bridging the chasm of interaction difficulties between a word and its distant neighbors. Experimental results on various standard benchmark datasets demonstrate the superior performance of the present approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8142,8152,,,,,,,,,,,,,,,,WOS:000860727002020,0
C,"Luo, M; Zeng, YK; Banerjee, P; Baral, C",,,Assoc Computat Linguist,"Luo, Man; Zeng, Yankai; Banerjee, Pratyay; Baral, Chitta",,,Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models' performance. To address this issue, we collect a natural language knowledge base that can be used for any VQA system. Moreover, we propose a Visual Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever aims to retrieve relevant knowledge, and the visual reader seeks to predict answers based on given knowledge. We introduce various ways to retrieve knowledge using text and images and two reader styles: classification and extraction. Both the retriever and reader are trained with weak supervision. Our experimental results show that a good retriever can significantly improve the reader's performance on the OK-VQA challenge. The code and corpus are provided in this link.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6417,6431,,,,,,,,,,,,,,,,WOS:000860727000032,0
C,"Meister, C; Pimentel, T; Haller, P; Jager, L; Cotterell, R; Levy, R",,,Assoc Computat Linguist,"Meister, Clara; Pimentel, Tiago; Haller, Patrick; Jaeger, Lena; Cotterell, Ryan; Levy, Roger",,,Revisiting the Uniform Information Density Hypothesis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal-or lack thereof-should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document-a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,963,980,,,,,,,,,,,,,,,,WOS:000855966301007,0
C,"Mi, F; Zhou, WH; Cai, FY; Kong, LJ; Huang, ML; Faltings, B",,,Assoc Computat Linguist,"Mi, Fei; Zhou, Wanhao; Cai, Fengyu; Kong, Lingjing; Huang, Minlie; Faltings, Boi",,,Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1887,1898,,,,,,,,,,,,,,,,WOS:000855966302001,0
C,"Orlando, R; Conia, S; Brignone, F; Cecconi, F; Navigli, R",,,Assoc Computat Linguist,"Orlando, Riccardo; Conia, Simone; Brignone, Fabrizio; Cecconi, Francesco; Navigli, Roberto",,,AMuSE-WSD: An All-in-one Multilingual System for Easy Word Sense Disambiguation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Over the past few years, Word Sense Disam-biguation (WSD) has received renewed interest: recently proposed systems have shown the remarkable effectiveness of deep learning techniques in this task, especially when aided by modern pretrained language models. Unfortunately, such systems are still not available as ready-to-use end-to-end packages, making it difficult for researchers to take advantage of their performance. The only alternative for a user interested in applying WSD to downstream tasks is to use currently available end-to-end WSD systems, which, however, still rely on graph-based heuristics or non-neural machine learning algorithms. In this paper, we fill this gap and propose AMuSE-WSD, the first end-to-end system to offer high-quality sense information in 40 languages through a state-of-the-art neural model for WSD. We hope that AMuSE-WSD will provide a stepping stone for the integration of meaning into real-world applications and encourage further studies in lexical semantics.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,298,307,,,,,,,,,,,,,,,,WOS:000855241500034,0
C,"Rebuffel, C; Scialom, T; Soulier, L; Piwowarski, B; Lamprier, S; Staiano, J; Scoutheeten, G; Gallinari, P",,,Assoc Computat Linguist,"Rebuffel, Clement; Scialom, Thomas; Soulier, Laure; Piwowarski, Benjamin; Lamprier, Sylvain; Staiano, Jacopo; Scoutheeten, Geoffrey; Gallinari, Patrick",,,Data-QuestEval: A Reference-less Metric for Data-to-Text Semantic Evaluation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"QUESTEVAL is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the WebNLG and WikiBio benchmarks. We make data-QUESTEVAL's code and models available for reproducibility purpose, as part of the QUESTEVAL project.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8029,8036,,,,,,,,,,,,,,,,WOS:000860727002011,0
C,"Sun, J; Ma, XZ; Peng, NY",,,Assoc Computat Linguist,"Sun, Jiao; Ma, Xuezhe; Peng, Nanyun",,,AESOP: Paraphrase Generation with Adaptive Syntactic Control,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose to control paraphrase generation with carefully chosen target syntactic structures to generate more proper and higher quality paraphrases. Our model, AESOP, leverages a pretrained language model and purposefully selected syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models' robustness to syntactic perturbation by data augmentation on two GLUE tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5176,5189,,,,,,,,,,,,,,,,WOS:000855966305024,0
C,"Xu, WJ; Carpuat, M",,,Assoc Computat Linguist,"Xu, Weijia; Carpuat, Marine",,,Rule-based Morphological Inflection Improves Neural Terminology Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate lemma constraints more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5902,5914,,,,,,,,,,,,,,,,WOS:000855966306007,0
C,"Yan, LY; Han, XP; Sun, L",,,Assoc Computat Linguist,"Yan, Lingyong; Han, Xianpei; Sun, Le",,,Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for bootstrapping which jointly models the bootstrapping process and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above adversarial learning, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9673,9682,,,,,,,,,,,,,,,,WOS:000860727003063,0
C,"Ye, CC; Zhang, LH; Zhou, DY; He, YL; Jie, W",,,Assoc Computat Linguist,"Ye, Chenchen; Zhang, Linhai; Zhou, Deyu; He, Yulan; Jie, Wu",,,Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels' hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.",,,,,,"He, Yulan/0000-0003-3948-5845",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3162,3171,,,,,,,,,,,,,,,,WOS:000855966303025,0
C,"Yu, TZ; Dai, WL; Liu, ZH; Fung, P",,,Assoc Computat Linguist,"Yu, Tiezheng; Dai, Wenliang; Liu, Zihan; Fung, Pascale",,,Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS modals cannot leverage GPLMs' powerful generation ability. To 611 this research gap, we aim to study two research questions: 1) how to inject visual information into GPLMs without hurting their generation ability: and 2) where is the optimal place in GPLMs to inject the visual information? In this paper, we present a simple yet effective, method to construct vision guided (VG) GPLMs for the MAS task using attention-based add-on layers to incorporatc, information while, maintaining their original text generation ability. Results show that our best model significantly surpasses the prior state-of-the-art model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the Howl dataset (Sanabria et al., 2018), and our visual guidance method contributes 83.6% of the overall improvement. Furthermore, we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3995,4007,,,,,,,,,,,,,,,,WOS:000855966304013,0
C,"Zheng, B; Dong, L; Huang, SH; Singhal, S; Che, WX; Liu, T; Song, X; Wei, FR",,,Assoc Computat Linguist,"Zheng, Bo; Dong, Li; Huang, Shaohan; Singhal, Saksham; Che, Wanxiang; Liu, Ting; Song, Xia; Wei, Furu",,,Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCAP to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VoCAP benefits cross-lingual language model pre-training Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github.com/bozheng-hit/VoCapXLM.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3203,3215,,,,,,,,,,,,,,,,WOS:000855966303029,0
C,"Zuo, SM; Liang, C; Jiang, HM; Liu, XD; He, PC; Gao, JF; Chen, WZ; Zhao, T",,,Assoc Computat Linguist,"Zuo, Simiao; Liang, Chen; Jiang, Haoming; Liu, Xiaodong; He, Pengcheng; Gao, Jianfeng; Chen, Weizhu; Zhao, Tuo",,,Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. This formulation induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. Different from conventional approaches, in SALT, the leader is in an advantageous position. When the leader moves, it recognizes the strategy of the follower and takes the anticipated follower's outcomes into consideration. Such a leader's advantage enables us to improve the model fitting to the unperturbed data. The leader's strategic information is captured by the Stackelberg gradient, which is obtained using an unrolling algorithm. Our experimental results on a set of machine translation and natural language understanding tasks show that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is publicly available.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6562,6577,,,,,,,,,,,,,,,,WOS:000860727000042,0
C,"Cheng, JY; Fostiropoulos, I; Boehm, B; Soleymani, M",,,Assoc Computat Linguist,"Cheng, Junyan; Fostiropoulos, Iordanis; Boehm, Barry; Soleymani, Mohammad",,,Multimodal Phased Transformer for Sentiment Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our model with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90% reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2447,2458,,,,,,,,,,,,,,,,WOS:000855966302048,0
C,"Choubey, PK; Currey, A; Mathur, P; Dinu, G",,,Assoc Computat Linguist,"Choubey, Prafulla Kumar; Currey, Anna; Mathur, Prashant; Dinu, Georgiana",,,GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1640,1654,,,,,,,,,,,,,,,,WOS:000855966301056,0
C,"Han, JL; Cheng, B; Lu, W",,,Assoc Computat Linguist,"Han, Jiale; Cheng, Bo; Lu, Wei",,,Exploring Task Difficulty for Few-Shot Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method.",,,,,"Lu, Wei/AHA-5606-2022","Lu, Wei/0000-0003-0827-0382",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2605,2616,,,,,,,,,,,,,,,,WOS:000855966302063,0
C,"Hu, Z; Fu, ZH; Yin, Y; de Melo, G",,,Assoc Computat Linguist,"Hu, Zhe; Fu, Zuohui; Yin, Yu; de Melo, Gerard",,,Context-Aware Interaction Network for Question Matching,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information. We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship. Specifically, each interaction block includes (1) a context-aware cross-attention mechanism to effectively integrate contextual information when aligning two sequences, and (2) a gate fusion layer to flexibly interpolate aligned representations. We apply multiple stacked interaction blocks to produce alignments at different levels and gradually refine the attention results. Experiments on two question matching datasets and detailed analyses demonstrate the effectiveness of our model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3846,3853,,,,,,,,,,,,,,,,WOS:000855966303084,0
C,"Juraska, J; Bowden, KK; Reed, L; Harrison, V; Cui, W; Patil, O; Rajasekaran, R; Ramirez, A; Li, C; Zamora, E; Lee, P; Bheemanpally, J; Pandey, R; Ratnaparkhi, A; Walker, M",,,Assoc Computat Linguist,"Juraska, Juraj; Bowden, Kevin K.; Reed, Lena; Harrison, Vrindavan; Cui, Wen; Patil, Omkar; Rajasekaran, Rishi; Ramirez, Angela; Li, Cecilia; Zamora, Eduardo; Lee, Phillip; Bheemanpally, Jeshwanth; Pandey, Rohan; Ratnaparkhi, Adwait; Walker, Marilyn",,,Athena 2.0: Contextualized Dialogue Management for an Alexa Prize SocialBot,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena's success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe Athena's system design and performance in the Alexa Prize during the 20/21 competition. A live demo of Athena as well as video recordings will provoke discussion on the state of the art in conversational AI.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,124,133,,,,,,,,,,,,,,,,WOS:000855241500015,0
C,"Kaster, M; Zhao, W; Eger, S",,,Assoc Computat Linguist,"Kaster, Marvin; Zhao, Wei; Eger, Steffen",,,Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including semantics, syntax, morphology, and lexical overlap. We show that the different metrics capture all aspects to some degree, but that they are all substantially sensitive to lexical overlap, just like BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we also highlight in an adversarial test scenario.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8912,8925,,,,,,,,,,,,,,,,WOS:000860727003003,0
C,"Korner, E; Wiedemann, G; Hakimi, AD; Heyer, G; Potthast, M",,,Assoc Computat Linguist,"Koerner, Erik; Wiedemann, Gregor; Hakimi, Ahmad Dawar; Heyer, Gerhard; Potthast, Martin",,,On Classifying whether Two Texts are on the Same Side of an Argument,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed. In contrast to actual stance classification, which requires a substantial amount of domain knowledge to identify whether an argument is in favor or against a certain issue, it is argued that, for S3C, only argument similarity within stances needs to be learned to successfully solve the task. We evaluate several transformer-based approaches on the dataset of the recent S3C shared task, followed by an in-depth evaluation and error analysis of our model and the task's hypothesis. We show that, although we achieve state-of-the-art results, our model fails to generalize both within as well as across topics and domains when adjusting the sampling strategy of the training and test set to a more adversarial scenario. Our evaluation shows that current state-of-the-art approaches cannot determine same side stance by considering only domain-independent linguistic similarity features, but appear to require domain knowledge and semantic inference, too.",,,,,,"Korner, Erik/0000-0002-5639-6177",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10130,10138,,,,,,,,,,,,,,,,WOS:000860727004019,0
C,"Lange, L; Adel, H; Strogen, J; Klakow, D",,,Assoc Computat Linguist,"Lange, Lukas; Adel, Heike; Stroegen, Jannik; Klakow, Dietrich",,,FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of embeddings of different types and dimensions is challenging. As an alternative to attention-based meta-embeddings, we propose feature-based adversarial meta-embeddings (FAME) with an attention function that is guided by features reflecting word-specific properties, such as shape and frequency, and show that this is beneficial to handle subword-based embeddings. In addition, FAME uses adversarial training to optimize the mappings of differently-sized embeddings to the same space. We demonstrate that FAME works effectively across languages and domains for sequence labeling and sentence classification, in particular in low-resource settings. FAME sets the new state of the art for POS tagging in 27 languages, various NER settings and question classification in different domains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8382,8395,,,,,,,,,,,,,,,,WOS:000860727002038,0
C,"Lee, CH; Cheng, H; Ostendorf, M",,,Assoc Computat Linguist,"Lee, Chia-Hsuan; Cheng, Hao; Ostendorf, Mari",,,Dialogue State Tracking with a Language Model using Schema-Driven Prompting,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Task-oriented conversational systems often use dialogue state tracking to represent the user's intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pre-trained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4937,4949,,,,,,,,,,,,,,,,WOS:000855966305008,0
C,"Leonardelli, E; Menini, S; Aprosio, AP; Guerini, M; Tonelli, S",,,Assoc Computat Linguist,"Leonardelli, Elisa; Menini, Stefano; Aprosio, Alessio Palmero; Guerini, Marco; Tonelli, Sara",,,Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators' agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, particularly in test sets, to better account for the different points of view expressed online.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10528,10539,,,,,,,,,,,,,,,,WOS:000860727004046,0
C,"Liu, DZ; Qu, XY; Dong, JF; Zhou, P",,,Assoc Computat Linguist,"Liu, Daizong; Qu, Xiaoye; Dong, Jianfeng; Zhou, Pan",,,Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with predefined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is inferior to the top-down counterpart as it fails to exploit the segmentlevel interaction. In this paper, we propose an Adaptive Proposal Generation Network (APGN) to maintain the segment-level interaction while speeding up the efficiency. Specifically, we first perform a foregroundbackground classification upon the video and regress on the foreground frames to adaptively generate proposals. In this way, the handcrafted proposal design is discarded and the redundant proposals are decreased. Then, a proposal consolidation module is further developed to enhance the semantic of the generated proposals. Finally, we locate the target moments with these generated proposals following the top-down framework. Extensive experiments on three challenging benchmarks show that our proposed APGN significantly outperforms previous state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9292,9301,,,,,,,,,,,,,,,,WOS:000860727003034,0
C,"Malkiel, I; Wolf, L",,,Assoc Computat Linguist,"Malkiel, Itzik; Wolf, Lior",,,MTAdam: Automatic Balancing of Multiple Training Loss Terms,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the derivative of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the gradients across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10713,10729,,,,,,,,,,,,,,,,WOS:000860727004061,0
C,"O'Gorman, T; Jensen, Z; Mysore, S; Mahbub, R; Huang, K; Olivetti, E; McCallum, A",,,Assoc Computat Linguist,"O'Gorman, Tim; Jensen, Zach; Mysore, Sheshera; Mahbub, Rubayyat; Huang, Kevin; Olivetti, Elsa; McCallum, Andrew",,,MS-MENTIONS: Consistently Annotating Entity Mentions in Materials Science Procedural Text,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 Material Science synthesis procedural texts (157,488 tokens), which greatly expands the training data available for the Named Entity Recognition task. We outline a new label inventory designed to provide consistent annotations and a new annotation approach intended to maximize the consistency and annotation speed of domain experts. Inter-annotator agreement studies and baseline models trained upon the data suggest that the corpus provides high-quality annotations of these mention types. This corpus helps lay a foundation for future high-quality modeling of synthesis procedures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1337,1352,,,,,,,,,,,,,,,,WOS:000855966301034,0
C,"Qin, H; Chen, GM; Tian, YH; Song, Y",,,Assoc Computat Linguist,"Qin, Han; Chen, Guimin; Tian, Yuanhe; Song, Yan",,,Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained model is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful categorical information for localized predictions. Experimental results on a simulated environment for FL with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed FL frameworks)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3942,3954,,,,,,,,,,,,,,,,WOS:000855966304008,0
C,"Roy, S; Pacheco, ML; Goldwasser, D",,,Assoc Computat Linguist,"Roy, Shamik; Pacheco, Maria Leonor; Goldwasser, Dan",,,Identifying Morality Frames in Political Tweets using Relational Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different entities, and come up with a novel and high-quality annotated dataset of tweets written by US politicians. Then, we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly. We do qualitative and quantitative evaluations, showing that moral sentiment towards entities differs highly across political ideologies.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9939,9958,,,,,,,,,,,,,,,,WOS:000860727004007,0
C,"Saeed, M; Ahmadi, N; Nakov, P; Papotti, P",,,Assoc Computat Linguist,"Saeed, Mohammed; Ahmadi, Naser; Nakov, Preslav; Papotti, Paolo",,,RULEBERT: Teaching Soft Rules to Pre-Trained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1460,1476,,,,,,,,,,,,,,,,WOS:000855966301043,0
C,"Salesky, E; Etter, D; Post, M",,,Assoc Computat Linguist,"Salesky, Elizabeth; Etter, David; Post, Matt",,,Robust Open-Vocabulary Translation from Visual Text Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an 'open vocabulary.' This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German-English task where subword models degrade to 1.9.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7235,7252,,,,,,,,,,,,,,,,WOS:000860727001024,0
C,"Sen, P; Saffari, A; Oliya, A",,,Assoc Computat Linguist,"Sen, Priyanka; Saffari, Amir; Oliya, Armin",,,Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al., 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of entities. We find that introducing intersection improves performance over a baseline model on two datasets, WebQuestionsSP (69.6% to 73.3% Hits@1) and ComplexWebQuestions (39.8% to 48.7% Hits@1), and in particular, improves performance on questions with multiple entities by over 14% on WebQuestionsSP and by 19% on ComplexWebQuestions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8805,8812,,,,,,,,,,,,,,,,WOS:000860727002072,0
C,"Strobelt, H; Hoover, B; Satyanarayan, A; Gehrmann, S",,,Assoc Computat Linguist,"Strobelt, Hendrik; Hoover, Benjamin; Satyanarayan, Arvind; Gehrmann, Sebastian",,,LMDIFF: A Visual Diff Tool to Compare Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMDIFF, a tool that visually compares probability distributions of two models that differ, e.g., through finetuning, distillation, or simply training with different parameter sizes. LMDIFF allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMDIFF for hypothesis generation across multiple case studies. A demo is available at http://lmdiff.net.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,96,105,,,,,,,,,,,,,,,,WOS:000855241500012,0
C,"Sun, SM; Krishna, K; Mattarella-Micke, A; Iyyer, M",,,Assoc Computat Linguist,"Sun, Simeng; Krishna, Kalpesh; Mattarella-Micke, Andrew; Iyyer, Mohit",,,Do Long-Range Language Models Actually Use Long-Range Context?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,807,822,,,,,,,,,,,,,,,,WOS:000855966300062,0
C,"Testoni, A; Bernardi, R",,,Assoc Computat Linguist,"Testoni, Alberto; Bernardi, Raffaella",,,Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Generating goal-oriented questions in Visual Dialogue tasks is a challenging and long-standing problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model's conjecture about the referent. We take the Guess What?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9330,9338,,,,,,,,,,,,,,,,WOS:000860727003038,0
C,"Voita, E; Sennrich, R; Titov, I",,,Assoc Computat Linguist,"Voita, Elena; Sennrich, Rico; Titov, Ivan",,,"Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8478,8491,,,,,,,,,,,,,,,,WOS:000860727002045,0
C,"Wang, HM; Wong, KF",,,Assoc Computat Linguist,"Wang, Huimin; Wong, Kam-Fai",,,A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Most reinforcement learning methods for dialog policy learning train a centralized agent that selects a predefined joint action concatenating domain name, intent type, and slot name. The centralized dialog agent suffers from a great many user-agent interaction requirements due to the large action space. Besides, designing the concatenated actions is laborious to engineers and maybe struggled with edge cases. To solve these problems, we model the dialog policy learning problem with a novel multi-agent framework, in which each part of the action is led by a different agent. The framework reduces labor costs for action templates and decreases the size of the action space for each agent. Furthermore, we relieve the non-stationary problem caused by the changing dynamics of the environment as evolving of agents' policies by introducing a joint optimization process that makes agents can exchange their policy information. Concurrently, an independent experience replay buffer mechanism is integrated to reduce the dependence between gradients of samples to improve training efficiency. The effectiveness of the proposed framework is demonstrated in a multi-domain environment with both user simulator evaluation and human evaluation.",,,,,"wang, huimin/HDM-8421-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7882,7889,,,,,,,,,,,,,,,,WOS:000860727001069,0
C,"Yoshida, R; Noji, H; Oseki, Y",,,Assoc Computat Linguist,"Yoshida, Ryo; Noji, Hiroshi; Oseki, Yohei",,,Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2964,2973,,,,,,,,,,,,,,,,WOS:000855966303007,0
C,"Yue, ZR; Kratzwald, B; Feuerriegel, S",,,Assoc Computat Linguist,"Yue, Zhenrui; Kratzwald, Bernhard; Feuerriegel, Stefan",,,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Y Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question answer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when the text corpora is of limited size. This is problematic as many niche domains rely on small text corpora, which naturally restricts the amount of synthetic data that can be generated. In this paper, we propose a novel framework for domain adaptation called contrastive domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from question generation and domain-invariant learning to answer out-of-domain questions in settings with limited text corpora. Here, we train a QA system on both source data and generated data from the target domain with a contrastive adaptation loss that is incorporated in the training objective. By combining techniques from question generation and domain-invariant learning, our model achieved considerable improvements compared to state-of-the-art baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9575,9593,,,,,,,,,,,,,,,,WOS:000860727003055,0
C,"Zhang, MJQ; Choi, E",,,Assoc Computat Linguist,"Zhang, Michael J. Q.; Choi, Eunsol",,,SITUATEDQA: Incorporating Extra-Linguistic Contexts into QA,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SITUATEDQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SITUATEDQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https: //situatedqa.github.io/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7371,7387,,,,,,,,,,,,,,,,WOS:000860727001034,0
C,"Akula, AR; Gella, S; Wang, KZ; Zhu, SC; Reddy, S",,,Assoc Computat Linguist,"Akula, Arjun R.; Gella, Spandana; Wang, Keze; Zhu, Song-Chun; Reddy, Siva",,,Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. dark cube on the left vs. black cube on the left). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of modules in NMN by up to 75% without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module's capacity in exploiting the visiolinguistic associations. Our model outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improvement in accuracy on the single-referent test set and +4.3% on the full test set. Additionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our contextualization by constructing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute accuracy on CC-Ref+, illustrating the generalization skills of our approach. Our dataset is publicly available at https://github.com/McGill-NLP/contextual-nmn.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6398,6416,,,,,,,,,,,,,,,,WOS:000860727000031,0
C,"Araujo, V; Villa, A; Mendoza, M; Moens, MF; Soto, A",,,Assoc Computat Linguist,"Araujo, Vladimir; Villa, Andres; Mendoza, Marcelo; Moens, Marie-Francine; Soto, Alvaro",,,Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current language models are usually trained using a self-supervised scheme, where the main focus is learning representations at the word or sentence level. However, there has been limited progress in generating useful discourse-level representations. In this work, we propose to use ideas from predictive coding theory to augment BERT-style language models with a mechanism that allows them to learn suitable discourse-level representations. As a result, our proposed approach is able to predict future sentences using explicit top-down connections that operate at the intermediate layers of the network. By experimenting with benchmarks designed to evaluate discourse-related knowledge using pre-trained sentence representations, we demonstrate that our approach improves performance in 6 out of 11 tasks by excelling in discourse relationship detection.",,,,,"Mendoza, Marcelo/D-2312-2014","Mendoza, Marcelo/0000-0002-7969-6041",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3016,3022,,,,,,,,,,,,,,,,WOS:000855966303012,0
C,"Bao, JZ; Liang, B; Sun, JY; Zhang, YC; Yang, M; Xu, RF",,,Assoc Computat Linguist,"Bao, Jianzhu; Liang, Bin; Sun, Jingyi; Zhang, Yice; Yang, Min; Xu, Ruifeng",,,Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the interrelations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current state-of-the-art model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3923,3934,,,,,,,,,,,,,,,,WOS:000855966304006,0
C,"Chaudhary, A; Yin, KY; Anastasopoulos, A; Neubig, G",,,Assoc Computat Linguist,"Chaudhary, Aditi; Yin, Kayo; Anastasopoulos, Antonios; Neubig, Graham",,,When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun wall has different lexical manifestations in Spanish - pared refers to an indoor wall while muro refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting concise descriptions explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted descriptions in a language learning setup for two languages, Spanish and Greek, where we use them to teach non-native speakers when to translate a given ambiguous word into its different possible translations. Code and data are publicly released here.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6911,6929,,,,,,,,,,,,,,,,WOS:000860727001001,0
C,"Dalvi, B; Jansen, P; Tafjord, O; Xie, ZN; Smith, H; Pipatanangkura, L; Clark, P",,,Assoc Computat Linguist,"Dalvi, Bhavana; Jansen, Peter; Tafjord, Oyvind; Xie, Zhengnan; Smith, Hannah; Pipatanangkura, Leighanna; Clark, Peter",,,Explaining Answers with Entailment Trees,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a rationale). If this could be done, new opportunities for understanding and debugging the system's reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK 1, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7358,7370,,,,,,,,,,,,,,,,WOS:000860727001033,0
C,"Guo, J; Kok, S",,,Assoc Computat Linguist,"Guo, Jia; Kok, Stanley",,,BiQUE: Biquaternionic Embeddings of Knowledge Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In contrast, hyperbolic models are effective at modeling hierarchical relations, but do not perform as well on patterns on which circular rotation excels. It is crucial for KGE models to unify multiple geometric transformations so as to fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a novel model that employs biquaternions to integrate multiple geometric transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic rotation. BiQUE makes the best trade-offs among geometric operators during training, picking the best one (or their best combination) for each relation. Experiments on five datasets show BiQUE's effectiveness.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8338,8351,,,,,,,,,,,,,,,,WOS:000860727002035,0
C,"Gupta, H; Del Corro, L; Broscheit, S; Hoffart, J; Brenner, E",,,Assoc Computat Linguist,"Gupta, Harsh; Del Corro, Luciano; Broscheit, Samuel; Hoffart, Johannes; Brenner, Eliot",,,Unsupervised Multi-View Post-OCR Error Correction With Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents.,,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8647,8652,,,,,,,,,,,,,,,,WOS:000860727002058,0
C,"Hamalainen, M; Alnajjar, K; Partanen, N; Rueter, J",,,Assoc Computat Linguist,"Hamalainen, Mika; Alnajjar, Khalid; Partanen, Niko; Rueter, Jack",,,Finnish Dialect Identification: The Effect of Audio and Text,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is received by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8777,8783,,,,,,,,,,,,,,,,WOS:000860727002070,0
C,"Iyer, V; Agarwal, A; Kumar, H",,,Assoc Computat Linguist,"Iyer, Vivek; Agarwal, Arvind; Kumar, Harshit",,,VeeAlign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domainspecific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in ontologies, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our model on four different datasets from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10780,10792,,,,,,,,,,,,,,,,WOS:000860727004066,0
C,"Lai, SP; Wang, AT; Meng, FD; Zhou, J; Ge, YB; Zeng, JL; Yao, JF; Huang, DG; Su, JS",,,Assoc Computat Linguist,"Lai, Shaopeng; Wang, Ante; Meng, Fandong; Zhou, Jie; Ge, Yubin; Zeng, Jiali; Yao, Junfeng; Huang, Degen; Su, Jinsong",,,Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al., 2019, 2021). Specially, given an initial sentence-entity graph, we first introduce a graph-based classifier to predict pairwise orderings between linked sentences. Then, in an iterative manner, based on the graph updated by previously predicted high-confident pairwise orderings, another classifier is used to predict the remaining uncertain pairwise orderings. At last, we adapt a GRN-based sentence ordering model (Yin et al., 2019, 2021) on the basis of final graph. Experiments on five commonly-used datasets demonstrate the effectiveness and generality of our model. Particularly, when equipped with BERT (Devlin et al., 2019) and FHDecoder (Yin et al., 2020), our model achieves state-of-the-art performance. Our code is available at https://github.com/DeepLearnXMU/IRSEG.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2407,2417,,,,,,,,,,,,,,,,WOS:000855966302045,0
C,"Li, XM; Li, Q; Wu, WS; Yin, QJ",,,Assoc Computat Linguist,"Li, Xinmeng; Li, Qian; Wu, Wansen; Yin, Quanjun",,,Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes more severe. Current models are not satisfactory for addressing the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantics of the ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we address the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2241,2249,,,,,,,,,,,,,,,,WOS:000855966302030,0
C,"Li, Z; Zhang, DQ; Cau, TY; Wei, Y; Song, YW; Yin, B",,,Assoc Computat Linguist,"Li, Zheng; Zhang, Danqing; Cau, Tianyu; Wei, Ying; Song, Yiwei; Yin, Bing",,,MetaTS: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for multiple languages. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student's feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3183,3196,,,,,,,,,,,,,,,,WOS:000855966303027,0
C,"Liang, YL; Zhou, CL; Meng, FD; Xu, JA; Chen, YF; Su, JS; Zhou, J",,,Assoc Computat Linguist,"Liang, Yunlong; Zhou, Chulun; Meng, Fandong; Xu, Jinan; Chen, Yufeng; Su, Jinsong; Zhou, Jie",,,Towards Making the Most of Dialogue Characteristics for Neural Chat Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English,German and English,Chinese) verify the effectiveness and superiority of the proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,67,79,,,,,,,,,,,,,,,,WOS:000855966300006,0
C,"Liu, FY; Vulic, I; Korhonen, A; Collier, N",,,Assoc Computat Linguist,"Liu, Fangyu; Vulic, Ivan; Korhonen, Anna; Collier, Nigel",,,"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1442,1459,,,,,,,,,,,,,,,,WOS:000855966301042,0
C,"Liu, ZH; Xia, R; Yu, JF",,,Assoc Computat Linguist,"Liu, Ziheng; Xia, Rui; Yu, Jianfei",,,Comparative Opinion Quintuple Extraction from Product Reviews,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As an important task in opinion mining, comparative opinion mining aims to identify comparative sentences from product reviews, extract the comparative elements, and obtain the corresponding comparative opinion tuples. However, most previous studies simply regarded comparative tuple extraction as comparative element extraction, which ignored the fact that many comparative sentences may contain multiple comparisons. The comparative opinion tuples defined in these studies also failed to explicitly provide comparative preferences. To address these issues, in this work we first introduce a new Comparative Opinion Quintuple Extraction (COQE) task, to identify comparative sentences from product reviews and extract all comparative opinion quintuples (Subject, Object, Comparative Aspect, Comparative Opinion, Comparative Preference). Secondly, based on the existing comparative opinion mining corpora, we make supplementary annotations and construct three datasets for the COQE task. Finally, we benchmark the COQE task by proposing a new multi-stage neural network approach which significantly outperforms the baseline systems extended from previous comparative opinion mining methods. The datasets and source code are publicly released at https://github.corn/NUSTM/COQE.",,,,,"Yu, Jianfei/GYD-3660-2022","Yu, Jianfei/0000-0001-8380-0609",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3955,3965,,,,,,,,,,,,,,,,WOS:000855966304009,0
C,"McCarthy, AD; Yancey, KP; LaFlair, GT; Egbert, J; Liao, MQ; Settles, B",,,Assoc Computat Linguist,"McCarthy, Arya D.; Yancey, Kevin P.; LaFlair, Geoffrey T.; Egbert, Jesse; Liao, Manqian; Settles, Burr",,,Jump-Starting Item Parameters for Adaptive Language Tests,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed 'cold start' estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (approximate to 6 each) from a large item bank (approximate to 4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,883,899,,,,,,,,,,,,,,,,WOS:000855966300067,0
C,"Mehnaz, L; Mahata, D; Gunturi, US; Kumar, A; Gosangi, R; Jain, R; Gupta, G; Lee, I; Acharya, A; Shah, RR",,,Assoc Computat Linguist,"Mehnaz, Laiba; Mahata, Debanjan; Gunturi, Uma Sushmitha; Kumar, Amardeep; Gosangi, Rakesh; Jain, Riya; Gupta, Gauri; Lee, Isabelle; Acharya, Anish; Shah, Rajiv Ratn",,,GupShup: Summarizing Open-Domain Code-Switched Conversations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of Hindi-English (Hi-En) code-switched conversations. We also develop the first code-switched conversation summarization dataset - GupShup, which contains over 6,800 Hi-En conversations and their corresponding human-annotated summaries in English (En) and Hi-En. We present a detailed account of the entire data collection and annotation process. We analyze the dataset using various code-switching statistics. We train state-of-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on this new dataset. We also conduct an extensive qualitative analysis to provide insight into the models and some of their shortcomings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6177,6192,,,,,,,,,,,,,,,,WOS:000860727000014,0
C,"Papi, S; Gaido, M; Negri, M; Turchi, M",,,Assoc Computat Linguist,"Papi, Sara; Gaido, Marco; Negri, Matteo; Turchi, Marco",,,Speechformer: Reducing Information Loss in Direct Speech Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer's quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to a reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en!de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1698,1706,,,,,,,,,,,,,,,,WOS:000855966301060,0
C,"Park, SH; Kim, KM; Cho, S; Park, JH; Park, H; Kim, H; Chung, S; Lee, S",,,Assoc Computat Linguist,"Park, San-Hee; Kim, Kang-Min; Cho, Seonhee; Park, Jun-Hyung; Park, Hyuntae; Kim, Hyuna; Chung, Seongwon; Lee, SangKeun",,,KOAS: Korean Text Offensiveness Analysis System,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Warning: This manuscript contains a certain level of offensive expression. As communication through social media platforms has grown immensely, the increasing prevalence of offensive language online has become a critical problem. Notably in Korea, one of the countries with the highest Internet usage, automatic detection of offensive expressions has recently been brought to attention. However, morphological richness and complex syntax of Korean causes difficulties in neural model training. Furthermore, most of previous studies mainly focus on the detection of abusive language, disregarding implicit offensiveness and underestimating a different degree of intensity. To tackle these problems, we present KOAS, a system that fully exploits both contextual and linguistic features and estimates an offensiveness score for a text. We carefully designed KOAS with a multi-task learning framework and constructed a Korean dataset for offensive analysis from various domains. Refer for a detailed demonstration. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,72,78,,,,,,,,,,,,,,,,WOS:000855241500009,0
C,"Qin, LB; Xie, TB; Huang, SJ; Chen, QG; Xu, X; Che, WX",,,Assoc Computat Linguist,"Qin, Libo; Xie, Tianbao; Huang, Shijue; Chen, Qiguang; Xu, Xiao; Che, Wanxiang",,,Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this paper, we argue that consistency problem is more urgent in task-oriented domain. To facilitate the research, we introduce CI-ToD, a novel dataset for Consistency Identification in Task-oriented Dialog system. In addition, we not only annotate the single label to enable the model to judge whether the system response is contradictory, but also provide more fine-grained labels (i.e., Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base Inconsistency) to encourage model to know what inconsistent sources lead to it. Empirical results show that state-of-the-art methods only achieve 51.3%, which is far behind the human performance of 93.2%, indicating that there is ample room for improving consistency identification ability. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide guidance for future directions. All datasets and models are publicly available at https://github.com/yizhen20133868/CI-ToD.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2357,2367,,,,,,,,,,,,,,,,WOS:000855966302041,0
C,"Ramnath, S; Johnson, M; Gupta, A; Raghuveer, A",,,Assoc Computat Linguist,"Ramnath, Sahana; Johnson, Melvin; Gupta, Abhirut; Raghuveer, Aravindan",,,HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT-a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don't filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: {Hindi,Gujarati,Tamil}!English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1717,1733,,,,,,,,,,,,,,,,WOS:000855966301062,0
C,"Ren, SH; Zhang, JC; Li, L; Sun, X; Zhou, J",,,Assoc Computat Linguist,"Ren, Shuhuai; Zhang, Jinchao; Li, Lei; Sun, Xu; Zhou, Jie",,,Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9029,9043,,,,,,,,,,,,,,,,WOS:000860727003013,0
C,"Simoncini, W; Spanakis, G",,,Assoc Computat Linguist,"Simoncini, Walter; Spanakis, Gerasimos",,,SeqAttack: On Adversarial Attacks for Named Entity Recognition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Named Entity Recognition is a fundamental task in information extraction and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to named entity recognition and the ability of adversarial training to counteract these attacks. We find that character-level and word-level attacks are the most effective, but adversarial training can grant significant protection at little to no expense of standard performance. Alongside our results, we also release SeqAttack, a framework to conduct adversarial attacks against token classification models (used in this work for named entity recognition) and a companion web application to inspect and cherry pick adversarial examples.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,308,318,,,,,,,,,,,,,,,,WOS:000855241500035,0
C,"Xiao, F; Pang, L; Lan, YY; Wang, Y; Shen, HW; Cheng, XQ",,,Assoc Computat Linguist,"Xiao, Fei; Pang, Liang; Lan, Yanyan; Wang, Yan; Shen, Huawei; Cheng, Xueqi",,,Transductive Learning for Unsupervised Text Style Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of parallel corpus hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like the salad is rude. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves topK relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our method outperforms several strong baselines. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2510,2521,,,,,,,,,,,,,,,,WOS:000855966302054,0
C,"Xiao, ZG; Wu, JR; Chen, QL; Deng, CJ",,,Assoc Computat Linguist,"Xiao, Zeguan; Wu, Jiarun; Chen, Qingliang; Deng, Congjian",,,BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pretraining language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of PLMs. In this paper, we propose a novel model, BERT4GCN, which integrates the grammatical sequential features from the PLM of BERT, and the syntactic knowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate layers of BERT and positional information between words to augment GCN (Graph Convolutional Network) to better encode the dependency graphs for the downstream classification. Experimental results demonstrate that the proposed BERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting GCN with the grammatical features from intermediate layers of BERT can significantly empower ABSC models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9193,9200,,,,,,,,,,,,,,,,WOS:000860727003026,0
C,"Zhang, JG; Bui, T; Yoon, S; Chen, X; Liu, ZW; Xia, CY; Tran, QH; Chang, W; Yu, P",,,Assoc Computat Linguist,"Zhang, Jian-Guo; Bui, Trung; Yoon, Seunghyun; Chen, Xiang; Liu, Zhiwei; Xia, Congying; Tran, Quan Hung; Chang, Walter; Yu, Philip",,,Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings.",,,,,,"Liu, Zhiwei/0000-0003-1525-1067",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1906,1912,,,,,,,,,,,,,,,,WOS:000855966302003,0
C,"Zhang, YH; Albarghouthi, A; D'Antoni, L",,,Assoc Computat Linguist,"Zhang, Yuhao; Albarghouthi, Aws; D'Antoni, Loris",,,Certified Robustness to Programmable Transformations in LSTMs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Deep neural networks for natural language processing are fragile in the face of adversarial examples-small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.",,,,,"Zhang, Yuhao/HHN-6356-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1068,1083,,,,,,,,,,,,,,,,WOS:000855966301015,0
C,"Zhou, DY; Wang, JA; Zhang, LH; He, YL",,,Assoc Computat Linguist,"Zhou, Deyu; Wang, Jianan; Zhang, Linhai; He, Yulan",,,Implicit Sentiment Analysis with Event-Centered Text Representation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Implicit sentiment analysis, aiming at detecting the sentiment of a sentence without sentiment words, has become an attractive research topic in recent years. In this paper, we focus on event-centric implicit sentiment analysis that utilizes the sentiment-aware event contained in a sentence to infer its sentiment polarity. Most existing methods in implicit sentiment analysis simply view noun phrases or entities in text as events or indirectly model events with sophisticated models. Since events often trigger sentiments in sentences, we argue that this task would benefit from explicit modeling of events and event representation learning. To this end, we represent an event as the combination of its event type and the event triplet <subject, predicate, object>. Based on such event representation, we further propose a novel model with hierarchical tensor-based composition mechanism to detect sentiment in text. In addition, we present a dataset(1) for event-centric implicit sentiment analysis where each sentence is labeled with the event representation described above. Experimental results on our constructed dataset and an existing benchmark dataset show the effectiveness of the proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6884,6893,,,,,,,,,,,,,,,,WOS:000860727000066,0
C,"Bari, MS; Haider, B; Mansotn, S",,,Assoc Computat Linguist,"Bari, M. Saiful; Haider, Batool; Mansotn, Saab",,,Nearest Neighbour Few-Shot Learning for Cross-lingual Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have led to significant performance gains on a wide range of cross-lingual NLP tasks, success on many downstream tasks still relies on the availability of sufficient annotated data. Traditional fine-tuning of pre-trained models using only a few target samples can cause over-fitting. This can be quite limiting as most languages in the world are under-resourced. In this work, we investigate cross-lingual adaptation using a simple nearest neighbor few-shot (< 15 samples) inference technique for classification tasks. We experiment using a total of 16 distinct languages across two NLP tasks-XNLI and PAWS-X. Our approach consistently improves traditional fine-tuning using only a handful of labeled samples in target locales. We also demonstrate its generalization capability across tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1745,1753,,,,,,,,,,,,,,,,WOS:000855966301064,0
C,"Bhat, MM; Sordoni, A; Mukherjee, S",,,Assoc Computat Linguist,"Bhat, Meghana Moorthy; Sordoni, Alessandro; Mukherjee, Subhabrata",,,Self-training with Few-shot Rationalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While pre-trained language models have obtained state-of-the-art performance for several natural language understanding tasks, they are quite opaque in terms of their decision-making process. While some recent works focus on rationalizing neural predictions by highlighting salient concepts in text as justifications or rationales, they rely on thousands of labeled training examples for both task labels as well as annotated rationales for every instance. Such extensive large-scale annotations are infeasible to obtain for many tasks. To this end, we develop a multi-task teacher-student framework based on self-training language models with limited task-specific labels and rationales, and judicious sample selection to learn from informative pseudo-labeled examples1. We study several characteristics of what constitutes a good rationale and demonstrate that the neural model performance can be significantly improved by making it aware of its rationalized predictions particularly in low-resource settings. Extensive experiments in several benchmark datasets demonstrate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10702,10712,,,,,,,,,,,,,,,,WOS:000860727004060,0
C,"Chapuis, E; Colombo, P; Labeau, M; Clavel, C",,,Assoc Computat Linguist,"Chapuis, Emile; Colombo, Pierre; Labeau, Matthieu; Clavel, Chloe",,,Code-switched inspired losses for generic spoken dialog representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Spoken dialog systems need to be able to handle both multiple languages and multilinguality inside a conversation (e.g in case of code-switching). In this work, we introduce new pretraining losses tailored to learn multilingual spoken dialog representations. The goal of these losses is to expose the model to code-switched language. To scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French, Italian, English, German and Spanish) from OpenSubtitles, a huge multilingual corpus composed of 24.3G tokens. We test the generic representations on MIAM, a new benchmark composed of five dialog act corpora on the same aforementioned languages as well as on two novel multilingual downstream tasks (i.e multilingual mask utterance retrieval and multilingual inconsistency identification). Our experiments show that our new code switched-inspired losses achieve a better performance in both monolingual and multilingual settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8320,8337,,,,,,,,,,,,,,,,WOS:000860727002034,0
C,"Csordas, R; Irie, K; Schmidhuber, J",,,Assoc Computat Linguist,"Csordas, Robert; Irie, Kazuki; Schmidhuber, Juergen",,,The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,619,634,,,,,,,,,,,,,,,,WOS:000855966300049,0
C,"Ding, KZ; Li, DC; Li, AHB; Fan, X; Guo, CL; Liu, Y; Liu, H",,,Assoc Computat Linguist,"Ding, Kaize; Li, Dingcheng; Li, Alexander Hanbo; Fan, Xing; Guo, Chenlei; Liu, Yang; Liu, Huan",,,Learning to Selectively Learn for Weakly-supervised Paraphrase Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Paraphrase generation is a longstanding NLP task that has diverse applications for downstream NLP tasks. However, the effectiveness of existing efforts predominantly relies on large amounts of golden labeled data. Though unsupervised endeavors have been proposed to address this issue, they may fail to generate meaningful paraphrases due to the lack of supervision signals. In this work, we go beyond the existing paradigms and propose a novel approach to generate high-quality paraphrases with weak supervision data. Specifically, we tackle the weakly-supervised paraphrase generation problem by: (1) obtaining abundant weakly-labeled parallel sentences via retrieval-based pseudo paraphrase expansion; and (2) developing a meta-learning framework to progressively select valuable samples for fine-tuning a pre-trained language model, i.e., BART, on the sentential paraphrasing task. We demonstrate that our approach achieves significant improvements over existing unsupervised approaches, and is even comparable in performance with supervised state-of-the-arts.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5930,5940,,,,,,,,,,,,,,,,WOS:000855966306010,0
C,"Dua, D; Dasigi, P; Singh, S; Gardner, M",,,Assoc Computat Linguist,"Dua, Dheeru; Dasigi, Pradeep; Singh, Sameer; Gardner, Matt",,,Learning with Instance Bundles for Reading Comprehension,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"When training most modern reading comprehension models, all the questions associated with a context are treated as being independent from each other. However, closely related questions and their corresponding answers are not independent, and leveraging these relationships could provide a strong supervision signal to a model. Drawing on ideas from contrastive estimation, we introduce several new supervision losses that compare question-answer scores across multiple related instances. Specifically, we normalize these scores across various neighborhoods of closely contrasting questions and/or answers, adding a cross entropy loss term in addition to traditional maximum likelihood estimation. Our techniques require bundles of related question-answer pairs, which we either mine from within existing data or create using automated heuristics. We empirically demonstrate the e ffectiveness of training with instance bundles on two datasetsHotpotQA and ROPES-showing up to 9% absolute gains in accuracy.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7347,7357,,,,,,,,,,,,,,,,WOS:000860727001032,0
C,"Gaido, M; Rodriguez, S; Negri, M; Bentivogli, L; Turchi, M",,,Assoc Computat Linguist,"Gaido, Marco; Rodriguez, Susana; Negri, Matteo; Bentivogli, Luisa; Turchi, Marco",,,Is moby dick a Whale or a Bird? Named Entities and Terminology in Speech Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en -> es/fr/it) show that ST systems correctly translate 75-80% of terms and 65-70% of NEs, with very low performance (37-40%) on person names.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1707,1716,,,,,,,,,,,,,,,,WOS:000855966301061,0
C,"Guo, XB; Kong, A; Zhou, H; Wang, XF; Wang, M",,,Assoc Computat Linguist,"Guo, Xiaobao; Kong, Adams; Zhou, Huan; Wang, Xianfeng; Wang, Min",,,Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Effective unimodal representation and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve unimodal representations, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a cross-modal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9143,9153,,,,,,,,,,,,,,,,WOS:000860727003022,0
C,"Gupta, A; Berant, J",,,Assoc Computat Linguist,"Gupta, Ankit; Berant, Jonathan",,,Value-aware Approximate Attention,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the value vectors to the quality of approximation. In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors. We propose a value-aware objective, and show theoretically and empirically that an optimal approximation of a value-aware objective substantially outperforms an optimal approximation that ignores values, in the context of language modeling. Moreover, we show that the choice of kernel function for computing attention similarity can substantially affect the quality of sparse approximations, where kernel functions that are less skewed are more affected by the value vectors.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9567,9574,,,,,,,,,,,,,,,,WOS:000860727003054,0
C,"Han, Z; Zhang, GY; Ma, YP; Tresp, V",,,Assoc Computat Linguist,"Han, Zhen; Zhang, Gengyuan; Ma, Yunpu; Tresp, Volker",,,Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different score functions and training strategies, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 4000 experiments and 19000 GPU hours. We classify the temporal embeddings into two classes: (1) time stamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that time stamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (Leblay and Chekol, 2018), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8104,8118,,,,,,,,,,,,,,,,WOS:000860727002017,0
C,"Han, Z; Ding, ZF; Ma, YP; Gu, YJ; Tresp, V",,,Assoc Computat Linguist,"Han, Zhen; Ding, Zifeng; Ma, Yunpu; Gu, Yujia; Tresp, Volker",,,Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the dynamic graph, i.e., edge formation and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our models superior performance on the future link forecasting task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8352,8364,,,,,,,,,,,,,,,,WOS:000860727002036,0
C,"Islam, MA; Magnani, E",,,Assoc Computat Linguist,"Islam, Md Asadul; Magnani, Enrico",,,Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"It is difficult to rank and evaluate the performance of grammatical error correction (GEC) systems, as a sentence can be rewritten in numerous correct ways. A number of GEC metrics have been used to evaluate proposed GEC systems; however, each system relies on either a comparison with one or more reference texts-in what is known as the gold standard for reference-based metrics-or a separate annotated dataset to fine-tune the reference-less metric. Reference-based systems have a low correlation with human judgement, cannot capture all the ways in which a sentence can be corrected, and require substantial work to develop a test dataset. We propose a reference-less GEC evaluation system that is strongly correlated with human judgement, solves the issues related to the use of a reference, and does not need another annotated dataset for fine-tuning. The proposed system relies solely on commonly available tools. Additionally, currently available reference-less metrics do not work properly when part of a sentence is repeated as opposed to reference-based metrics. In our proposed system, we look to address issues inherent in reference-less metrics and reference-based metrics.",,,,,"Magnani, Enrico/GYU-7755-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3009,3015,,,,,,,,,,,,,,,,WOS:000855966303011,0
C,"Kocijan, V; Lukasiewicz, T",,,Assoc Computat Linguist,"Kocijan, Vid; Lukasiewicz, Thomas",,,Knowledge Base Completion Meets Transfer Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The aim of knowledge base completion is to predict unseen facts from existing facts in knowledge bases. In this work, we introduce the first approach for transfer of knowledge from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. Such knowledge bases are a natural output of automated information extraction tools that extract structured data from unstructured text. Our main contribution is a method that can make use of a large-scale pre-training on facts, collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20K, where we obtained 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6521,6533,,,,,,,,,,,,,,,,WOS:000860727000039,0
C,"Lin, ZX; Ma, QL; Yan, JY; Chen, JY",,,Assoc Computat Linguist,"Lin, Zhenxi; Ma, Qianli; Yan, Jiangyue; Chen, Jieyu",,,CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CAFE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained model to obtain a contextual representation of target words and employs a contrastive objective to promote an increased distance between target words' literal and metaphorical senses based on linguistic theories. Furthermore, we propose a simple strategy to collect large-scale candidate instances from the general corpus and generalize the model via self-training. Extensive experiments show that CATE achieves better performance against state-of-the-art baselines on several benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3888,3898,,,,,,,,,,,,,,,,WOS:000855966304003,0
C,"Ma, WC; Lou, RZ; Zhang, K; Wang, LL; Vosoughi, S",,,Assoc Computat Linguist,"Ma, Weicheng; Lou, Renze; Zhang, Kai; Wang, Lili; Vosoughi, Soroush",,,GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5621,5632,,,,,,,,,,,,,,,,WOS:000855966305059,0
C,"Maheshwary, R; Maheshwary, S; Pudi, V",,,Assoc Computat Linguist,"Maheshwary, Rishabh; Maheshwary, Saket; Pudi, Vikram",,,A Strong Baseline for Query Efficient Attacks in a Black Box Setting,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different search methods. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages attention mechanism and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four baselines across three different search spaces. Further, we benchmark our results across the same search space used in prior attacks. In comparison to attacks proposed, on an average, we are able to reduce the query count by 75% across all datasets and target models. We also demonstrate that our attack achieves a higher success rate when compared to prior attacks in a limited query setting.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8396,8409,,,,,,,,,,,,,,,,WOS:000860727002039,0
C,"Qiu, Y; Zhang, JC; Zhou, J",,,Assoc Computat Linguist,"Qiu, Yao; Zhang, Jinchao; Zhou, Jie",,,Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre training phase between pre-training and fine-tuning phases to adapt the model on the domain-specific unlabeled data can bring positive effects. However, most of these further pre-training works just keep running the conventional pre-training task, e.g., masked language model, which can be regarded as the domain adaptation to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different tasks may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various tasks at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2318,2327,,,,,,,,,,,,,,,,WOS:000855966302037,0
C,"Rothe, S; Maynez, J; Narayan, S",,,Assoc Computat Linguist,"Rothe, Sascha; Maynez, Joshua; Narayan, Shashi",,,A Thorough Evaluation of Task-Specific Pretraining for Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al., 2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et al., 2020). We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in a controlled study. We also extend our study to a low resource and zero shot setup, to understand how many training examples are needed in order to ablate the task-specific pretraining without quality loss. Our results show that task-agnostic pretraining is sufficient for most cases which hopefully reduces the need for costly task-specific pretraining. We also report new state-of-the-art number for two summarization tasks using a T5 model with 11 billion parameters and an optimal beam search length penalty.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,140,145,,,,,,,,,,,,,,,,WOS:000855966300012,0
C,"Scialom, T; Dray, PA; Gallinari, P; Lamprier, S; Piwowarski, B; Staiano, J; Wang, A",,,Assoc Computat Linguist,"Scialom, Thomas; Dray, Paul-Alexis; Gallinari, Patrick; Lamprier, Sylvain; Piwowarski, Benjamin; Staiano, Jacopo; Wang, Alex",,,QuestEval: Summarization Asks for Fact-based Evaluation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named QUESTEVAL. In contrast to established metrics such as ROUGE or BERTScore, QUESTEVAL does not require any ground-truth reference. Nonetheless, QUESTEVAL substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments. We make code and models available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6594,6604,,,,,,,,,,,,,,,,WOS:000860727000044,0
C,"Srivatsan, N; Wu, S; Barron, JT; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Srivatsan, Nikita; Wu, Si; Barron, Jonathan T.; Berg-Kirkpatrick, Taylor",,,Scalable Font Reconstruction with Dual Latent Manifolds,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong to the same font. This design allows our model to generalize to characters that were not observed during training time, an important task in light of the relative sparsity of most fonts. We also put forward a new loss, adapted from prior work that measures likelihood using an adaptive distribution in a projected space, resulting in more natural images without requiring a discriminator. We evaluate on the task of font reconstruction over various datasets representing character types of many language, and compare favorably to modern style transfer systems according to both automatic and manually-evaluated metrics.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3060,3072,,,,,,,,,,,,,,,,WOS:000855966303016,0
C,"Suster, S; Fivez, P; Totis, P; Kimmig, A; Davis, J; De Raedt, L; Daelemans, W",,,Assoc Computat Linguist,"Suster, Simon; Fivez, Pieter; Totis, Pietro; Kimmig, Angelika; Davis, Jesse; De Raedt, Luc; Daelemans, Walter",,,Mapping probability word problems to executable representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system to provide the answer. Our best performing model incorporates general-domain contextualised word representations that were finetuned using transfer learning on another in-domain dataset. We also apply end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems.",,,,,,"De Raedt, Luc/0000-0002-6860-6303",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3627,3640,,,,,,,,,,,,,,,,WOS:000855966303066,0
C,"Wiegreffe, S; Marasovic, A; Smith, NA",,,Assoc Computat Linguist,"Wiegreffe, Sarah; Marasovic, Ana; Smith, Noah A.",,,Measuring Association Between Labels and Free-Text Rationales,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In interpretable NLP, we require faithful rationales that reflect the model's decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their lessstudied counterpart: free-text natural language rationales. We demonstrate that pipelines, models for faithful rationalization on informationextraction style tasks, do not work as well on reasoning tasks requiring free-text rationales. We turn to models that jointly predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, robustness equivalence and feature importance agreement, we find that stateof-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10266,10284,,,,,,,,,,,,,,,,WOS:000860727004028,0
C,"Wright, D; Augenstein, I",,,Assoc Computat Linguist,"Wright, Dustin; Augenstein, Isabelle",,,Semi-Supervised Exaggeration Detection of Health Science Press Releases,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the original paper, making data collection challenging. We address this by curating a set of labeled press release/abstract pairs from existing expert annotated studies on exaggeration in press releases of scientific papers suitable for benchmarking the performance of machine learning models on the task. Using limited data from this and previous studies on exaggeration detection in science, we introduce MT- PET, a multi-task version of Pattern Exploiting Training (PET), which leverages knowledge from complementary clozestyle QA tasks to improve few-shot learning. We demonstrate that MT-PET outperforms PET and supervised learning both when data is limited, as well as when there is an abundance of data for the main task.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10824,10836,,,,,,,,,,,,,,,,WOS:000860727004069,0
C,"Zhang, ZW; Li, JY; Fukumoto, F; Ye, YM",,,Assoc Computat Linguist,"Zhang, Zhiwei; Li, Jiyi; Fukumoto, Fumiyo; Ye, Yanming",,,"Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction. Such works have the problems of error propagation among the modules in the pipeline and lack of sharing valuable information among modules. We thus propose an approach, named as ARSJOINT, that jointly learns the modules for the three tasks with a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset SCIFACT show that our approach outperforms the existing works.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3580,3586,,,,,,,,,,,,,,,,WOS:000855966303062,0
C,"Zhou, CT; Levy, D; Li, X; Ghazvininejad, M; Neubig, G",,,Assoc Computat Linguist,"Zhou, Chunting; Levy, Daniel; Li, Xian; Ghazvininejad, Marjan; Neubig, Graham",,,Distributionally Robust Multilingual Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5664,5674,,,,,,,,,,,,,,,,WOS:000855966305062,0
C,"Ahrens, M; Ashwin, J; Calliess, JP; Nguyen, V",,,Assoc Computat Linguist,"Ahrens, Maximilian; Ashwin, Julian; Calliess, Jan-Peter; Vu Nguyen",,,Bayesian Topic Regression for Causal Inference,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8162,8188,,,,,,,,,,,,,,,,WOS:000860727002022,0
C,"Chen, SX; Liu, XD; Gao, JF; Jiao, J; Zhang, RF; Ji, YF",,,Assoc Computat Linguist,"Chen, Sanxing; Liu, Xiaodong; Gao, Jianfeng; Jiao, Jian; Zhang, Ruofei; Ji, Yangfeng",,,HittER: Hierarchical Transformers for Knowledge Graph Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entityrelation composition and Relational contextualization based on a source entity's neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10395,10407,,,,,,,,,,,,,,,,WOS:000860727004036,0
C,"Cheng, KW; Yang, ZQ; Zhang, M; Sun, YZ",,,Assoc Computat Linguist,"Cheng, Kewei; Yang, Ziqing; Zhang, Ming; Sun, Yizhou",,,UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic models to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness.",,,,,,"Zhang, Ming/0000-0002-9809-3430",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9753,9771,,,,,,,,,,,,,,,,WOS:000860727003070,0
C,"Das, R; Zaheer, M; Thai, D; Godbole, A; Perez, E; Lee, JY; Tan, LZ; Polymenakos, L; McCallum, A",,,Assoc Computat Linguist,"Das, Rajarshi; Zaheer, Manzil; Thai, Dung; Godbole, Ameya; Perez, Ethan; Lee, Jay-Yoon; Tan, Lizhen; Polymenakos, Lazaros; McCallum, Andrew",,,Case-Based Reasoning for Natural Language Queries over Knowledge Bases,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions - a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the COMPLEXWEBQUESTIONS dataset, CBR-KBQA outperforms the current state of the art by 11% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9594,9611,,,,,,,,,,,,,,,,WOS:000860727003056,0
C,"Gao, JL; Sun, X; Xu, MM; Zhou, X; Ghanem, B",,,Assoc Computat Linguist,"Gao, Jialin; Sun, Xin; Xu, MengMeng; Zhou, Xi; Ghanem, Bernard",,,Relation-aware Video Reading Comprehension for Temporal Language Grounding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose a Relation-aware Network (RaNet) to address it. This framework aims to select a video moment choice from the predefined answer set with the aid of coarse-and-fine choice-query interaction and choice-choice relation construction. A choice-query interactor is proposed to match the visual and textual information simultaneously in sentence-moment and token-moment levels, leading to a coarse-and-fine cross-modal interaction. Moreover, a novel multi-choice relation constructor is introduced by leveraging graph convolution to capture the dependencies among video moment choices for the best choice selection. Extensive experiments on ActivityNet-Captions, TACoS, and CharadesSTA demonstrate the effectiveness of our solution. Codes will be available at https: //github.com/Huntersxsx/RaNet.",,,,,,"Sun, Xin/0000-0002-3092-0583",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3978,3988,,,,,,,,,,,,,,,,WOS:000855966304011,0
C,"He, H; Choi, JD",,,Assoc Computat Linguist,"He, Han; Choi, Jinho D.",,,The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5555,5577,,,,,,,,,,,,,,,,WOS:000855966305055,0
C,"Hewitt, J; Ethayarajh, K; Liang, P; Manning, CD",,,Assoc Computat Linguist,"Hewitt, John; Ethayarajh, Kawin; Liang, Percy; Manning, Christopher D.",,,Conditional probing: measuring usable information beyond a baseline,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Probing experiments investigate the extent to which neural representations make properties-like part-of-speech-predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we're interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called V-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1626,1639,,,,,,,,,,,,,,,,WOS:000855966301055,0
C,"Inoue, N; Trivedi, H; Sinha, S; Balasubramanian, N; Inui, K",,,Assoc Computat Linguist,"Inoue, Naoya; Trivedi, Harsh; Sinha, Steven; Balasubramanian, Niranjan; Inui, Kentaro",,,Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not minimally sufficient for answering a question. Instead, we advocate for an abstractive approach, where we propose to generate a question-focused, abstractive summary of input paragraphs and then feed it to an RC system. Given a limited amount of human-annotated abstractive explanations, we train the abstractive explainer in a semi-supervised manner, where we start from the supervised model and then train it further through trial and error maximizing a conciseness-promoted reward function. Our experiments demonstrate that the proposed abstractive explainer can generate more compact explanations than an extractive explainer with limited supervision (only 2k instances) while maintaining sufficiency.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6064,6080,,,,,,,,,,,,,,,,WOS:000860727000005,0
C,"Izsak, P; Berchansky, M; Levy, O",,,Assoc Computat Linguist,"Izsak, Peter; Berchansky, Moshe; Levy, Omer",,,How to Train BERT with an Academic Budget,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few wellfunded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single lowend deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERTBASE on GLUE tasks at a fraction of the original pretraining cost.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10644,10652,,,,,,,,,,,,,,,,WOS:000860727004055,0
C,"Kiyono, S; Kobayashi, S; Suzuki, J; Inui, K",,,Assoc Computat Linguist,"Kiyono, Shun; Kobayashi, Sosuke; Suzuki, Jun; Inui, Kentaro",,,SHAPE: Shifted Absolute Position Embedding for Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We demonstrate that SHAPE is empirically comparable to its counterpart while being simpler and faster(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3309,3321,,,,,,,,,,,,,,,,WOS:000855966303038,0
C,"Lacerra, C; Tripodi, R; Navigli, R",,,Assoc Computat Linguist,"Lacerra, Caterina; Tripodi, Rocco; Navigli, Roberto",,,GENESIS: A Generative Approach to Substitutes in Context,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as language models. Furthermore, lexical substitution is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we propose GENESIS (Generating Substitutes in contexts), the first generative approach to lexical substitution. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-theart results on different benchmarks. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GENESIS results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets and the code to reproduce the experiments at https://github. com/SapienzaNLP/genesis.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10810,10823,,,,,,,,,,,,,,,,WOS:000860727004068,0
C,"Lavi, O; Rabinovich, E; Shlomov, S; Boaz, D; Ronen, I; Anaby-Tavor, A",,,Assoc Computat Linguist,"Lavi, Ofer; Rabinovich, Ella; Shlomov, Segev; Boaz, David; Ronen, Inbal; Anaby-Tavor, Ateret",,,We've had this conversation before: A Novel Approach to Measuring Dialog Similarity,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialog is a core building block of human natural language interactions. It contains multiparty utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics for contact center calls and virtual agent design. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. Our approach takes into account various conversation aspects such as utterance semantics, conversation flow, and the participants. We evaluate this new approach and compare it to existing document similarity measures on two publicly available datasets. The results demonstrate that our method outperforms the other approaches in capturing dialog flow, and is better aligned with the human perception of conversation similarity.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1169,1177,,,,,,,,,,,,,,,,WOS:000855966301022,0
C,"Li, ZC; Parnow, K; Utiyama, M; Sumita, E; Zhao, H",,,Assoc Computat Linguist,"Li, Zuchao; Parnow, Kevin; Utiyama, Masao; Sumita, Eiichiro; Zhao, Hai",,,MISS: An Assistant for Multi-Style Simultaneous Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we present MISS, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation, flexibility, and measurable translation quality. Compared with the free commercial translation systems commonly used, our translation assistance system regards the machine translation application as a more complete and fully-featured tool for users. By incorporating additional features and giving the user better control over their experience, we improve translation efficiency and performance. Additionally, our assistant system combines machine translation, grammatical error correction, and interactive edits, and uses a crowd-sourcing mode to collect more data for further training to improve both the machine translation and grammatical error correction models. A short video demonstrating our system is available at https : / /www .youtube .com/watch?v-ZGCo7KtRKd8.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,1,10,,,,,,,,,,,,,,,,WOS:000855241500001,0
C,"Meng, YX; Ao, X; He, Q; Sun, XF; Han, QH; Wu, F; Fan, C; Li, JW",,,Assoc Computat Linguist,"Meng, Yuxian; Ao, Xiang; He, Qing; Sun, Xiaofei; Han, Qinghong; Wu, Fei; Fan, Chun; Li, Jiwei",,,ConRPG: Paraphrase Generation using Contexts as Regularizer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A long-standing issue with paraphrase generation is how to obtain reliable supervision signals. In this paper, we propose an unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; and (2) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the effectiveness of the proposed model in both supervised and unsupervised setups.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2551,2562,,,,,,,,,,,,,,,,WOS:000855966302058,0
C,"Min, BN; Rozonoyer, B; Qiu, HL; Zamanian, A; Xue, NAW; MacBride, J",,,Assoc Computat Linguist,"Min, Bonan; Rozonoyer, Ben; Qiu, Haoling; Zamanian, Alex; Xue, Nianwen; MacBride, Jessica",,,ExcavatorCovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for COVID-19,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG). Excavator will help government agencies alleviate the information overload, understand likely downstream effects of political and economic decisions and events related to the pandemic, and respond in a timely manner to mitigate the impact of COVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic: analysts and decision makers will be empowered by Excavator to better understand and solve complex problems in the future. A demonstration video is available at https://virneo.com/528619007.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,63,71,,,,,,,,,,,,,,,,WOS:000855241500008,0
C,"Raji, S; de Melo, G",,,Assoc Computat Linguist,"Raji, Shahab; de Melo, Gerard",,,Guilt by Association: Emotion Intensities in Lexical Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9911,9917,,,,,,,,,,,,,,,,WOS:000860727004005,0
C,"Song, MY; Jing, LP; Xiao, L",,,Assoc Computat Linguist,"Song, Mingyang; Jing, Liping; Xiao, Lin",,,Importance Estimation from Multiple Perspectives for Keyphrase Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its syntactic accuracy, information saliency, and concept consistency simultaneously. However, most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results. In this paper, we propose a new approach to estimate the importance of keyphrase from multiple perspectives (called as KIEMP) and further improve the performance of keyphrase extraction. Specifically, KIEMP estimates the importance of phrase with three modules: a chunking module to measure its syntactic accuracy, a ranking module to check its information saliency, and a matching module to judge the concept (i.e., topic) consistency between phrase and the whole document. These three modules are seamlessly jointed together via an end-to-end multi-task learning model, which is helpful for three parts to enhance each other and balance the effects of three perspectives. Experimental results on six benchmark datasets show that KIEMP outperforms the existing state-of-the-art keyphrase extraction approaches in most cases.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2726,2736,,,,,,,,,,,,,,,,WOS:000855966302074,0
C,"Spangher, A; May, J; Shiang, SR; Deng, LJ",,,Assoc Computat Linguist,"Spangher, Alexander; May, Jonathan; Shiang, Sz-rung; Deng, Lingjia",,,Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9% Micro F1-score over current state-of-the-art benchmarks on the NewsDiscourse dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for under-represented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,498,517,,,,,,,,,,,,,,,,WOS:000855966300040,0
C,"Tan, X; Zhang, LY; Zhou, GD",,,Assoc Computat Linguist,"Tan, Xin; Zhang, Longyin; Zhou, Guodong",,,Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems, and the problems remain challenging due to the scarcity of ZP annotated NLG corpora. In this case, we propose a highly adaptive two-stage approach to couple context modeling with ZP recovering to mitigate the ZP problem in NLG tasks. Notably, we frame the recovery process in a task-supervised fashion where the ZP representation recovering capability is learned during the NLG task learning process, thus our method does not require NLG corpora annotated with ZPs. For system enhancement, we learn an adversarial bot to adjust our model outputs to alleviate the error propagation caused by mis-recovered ZPs. Experiments on three document-level NLG tasks, i.e., machine translation, question answering, and summarization, show that our approach can improve the performance to a great extent, and the improvement on pronoun translation is very impressive.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2530,2540,,,,,,,,,,,,,,,,WOS:000855966302056,0
C,"Vu, TT; He, XL; Phung, D; Haffari, G",,,Assoc Computat Linguist,"Thuy-Trang Vu; He, Xuanli; Dinh Phung; Haffari, Gholamreza",,,Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3335,3346,,,,,,,,,,,,,,,,WOS:000855966303040,0
C,"Weber, L; Garda, S; Munchmeyer, J; Leser, U",,,Assoc Computat Linguist,"Weber, Leon; Garda, Samuele; Muenchmeyer, Jannes; Leser, Ulf",,,"Extend, don't rebuild: Phrasing conditional graph modification as autoregressive sequence labelling",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al., 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as graph extension instead of graph generation. We propose the first model for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in accuracy over the state-of-the-art between 13 and 26 percentage points. Furthermore, we introduce a novel data set from the biomedical domain which has much larger linguistic variability and more complex graphs than the scene graph modification data sets. For this data set, the state-of-the art fails to generalize, while our model can produce meaningful predictions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1213,1224,,,,,,,,,,,,,,,,WOS:000855966301026,0
C,"Wu, K; Wang, LJ; Li, ZH; Zhang, A; Xiao, XY; Wu, H; Zhang, M; Wang, HF",,,Assoc Computat Linguist,"Wu, Kun; Wang, Lijie; Li, Zhenghua; Zhang, Ao; Xiao, Xinyan; Wu, Hua; Zhang, Min; Wang, Haifeng",,,Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8974,8983,,,,,,,,,,,,,,,,WOS:000860727003009,0
C,"Xie, SY; Hong, Y",,,Assoc Computat Linguist,"Xie, Shangyu; Hong, Yuan",,,Reconstruction Attack on Instance Encoding for Language Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction attack to break TextHide by recovering the private training data, and thus unveil the privacy risks of instance encoding. We have experimentally validated the effectiveness of the reconstruction attack with two commonly-used datasets for sentence classification. Our attack would advance the development of privacy preserving machine learning in the context of natural language processing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2038,2044,,,,,,,,,,,,,,,,WOS:000855966302013,0
C,"Yin, D; Li, LH; Hu, ZN; Peng, NY; Chang, KW",,,Assoc Computat Linguist,"Yin, Da; Li, Liunian Harold; Hu, Ziniu; Peng, Nanyun; Chang, Kai-Wei",,,Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Commonsense is defined as the knowledge that is shared by everyone. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenarios of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard multimodal commonsense bench-mark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin 9712 /GD-VCR.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2115,2129,,,,,,,,,,,,,,,,WOS:000855966302021,0
C,"Zhang, T; Xia, CY; Yu, PS; Liu, ZW; Zhao, S",,,Assoc Computat Linguist,"Zhang, Tao; Xia, Congying; Yu, Philip S.; Liu, Zhiwei; Zhao, Shu",,,PDALN: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach - PDALN. It achieves superior domain adaptability by employing three components: (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously; (2) Multi-level Domain invariant features, derived from a multigrained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt highresource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other baselines indicates the state-of-the-art performance of PDALN.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5441,5451,,,,,,,,,,,,,,,,WOS:000855966305046,0
C,"Zhang, XH; Yu, BW; Liu, TW; Zhang, ZY; Sheng, JW; Xue, MG; Xu, HB",,,Assoc Computat Linguist,"Zhang, Xinghua; Yu, Bowen; Liu, Tingwen; Zhang, Zhenyu; Sheng, Jiawei; Xue Mengge; Xu, Hongbo",,,Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotation noise, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the whole training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutuallybeneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10746,10757,,,,,,,,,,,,,,,,WOS:000860727004063,0
C,"Zmigrod, R; Vieira, T; Cotterell, R",,,Assoc Computat Linguist,"Zmigrod, Ran; Vieira, Tim; Cotterell, Ryan",,,Efficient Sampling of Dependency Structures,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to sample dependency trees from a graph subject to the root constraint. Wilson (1996)'s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn et al. (1996)'s sampling algorithm has a running time of O(N-3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(KN3 + (KN)-N-2) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10558,10569,,,,,,,,,,,,,,,,WOS:000860727004048,0
C,"Bostrom, K; Zhao, XY; Chaudhuri, S; Durrett, G",,,Assoc Computat Linguist,"Bostrom, Kaj; Zhao, Xinyu; Chaudhuri, Swarat; Durrett, Greg",,,Flexible Generation of Natural Language Deductions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose - it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe PARAPATTERN, a method for building models to generate deductive inferences from diverse natural language inputs without direct human supervision. We train BART-based models (Lewis et al., 2020) to generate the result of applying a particular logical operation to one or more premise statements. Crucially, we develop a largely automated pipeline for constructing suitable training examples from Wikipedia. We evaluate our models using out-of-domain sentence compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as targeted perturbation sets. Our results show that our models are substantially more accurate and flexible than baseline systems. PARAPATTERN achieves 85% validity on examples of the 'substitution' operation from EntailmentBank without the use of any in-domain training data, matching the performance of a model fine-tuned for EntailmentBank. The full source code for our method is publicly available.(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6266,6278,,,,,,,,,,,,,,,,WOS:000860727000021,0
C,"Buechel, S; Modersohn, L; Hahn, U",,,Assoc Computat Linguist,"Buechel, Sven; Modersohn, Luise; Hahn, Udo",,,Towards Label-Agnostic Emotion Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few wellresourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9231,9249,,,,,,,,,,,,,,,,WOS:000860727003030,0
C,"Burdick, L; Kummerfeld, JK; Mihalcea, R",,,Assoc Computat Linguist,"Burdick, Laura; Kummerfeld, Jonathan K.; Mihalcea, Rada",,,Analyzing the Surprising Variability in Word Embedding Stability Across Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in different embedding spaces) in diverse languages. We discuss linguistic properties that are related to stability, drawing out insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5891,5901,,,,,,,,,,,,,,,,WOS:000855966306006,0
C,"De Cao, N; Aziz, W; Titov, I",,,Assoc Computat Linguist,"De Cao, Nicola; Aziz, Wilker; Titov, Ivan",,,Highly Parallel Autoregressive Entity Linking with Discriminative Correction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e. a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is >70 times faster and more accurate than the previous generative method, outperforming stateof-the-art approaches on the standard English dataset AIDA-CoNLL. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7662,7669,,,,,,,,,,,,,,,,WOS:000860727001052,0
C,"Jing, BY; You, ZY; Yang, T; Fan, W; Tong, HH",,,Assoc Computat Linguist,"Jing, Baoyu; You, Zeyu; Yang, Tao; Fan, Wei; Tong, Hanghang",,,Multiplex Graph Neural Network for Extractive Text Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity & natural connection), nor model intra-sentential relationships (e.g, semantic & syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (MultiGCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,133,139,,,,,,,,,,,,,,,,WOS:000855966300011,0
C,"Kang, YN; Wang, Z; Zhang, HY; Chen, JJ; You, HM",,,Assoc Computat Linguist,"Kang, Yuning; Wang, Zan; Zhang, Hongyu; Chen, Junjie; You, Hanmo",,,APIRecX: Cross-Library API Recommendation via Pre-Trained Language Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models such as N-gram are applied to API recommendation. However, because the software libraries keep changing and new libraries keep emerging, new APIs are common These new APIs can be seen as OOV (out of vocabulary) words and cannot be handled well by existing API recommendation approaches due to the lack of training data. In this paper, we propose APIRecX, the first cross-library API recommendation approach, which uses BPE to split each API call in each API sequence and pre-trains a GPT-based language model. It then recommends APIs by fine-tuning the pre-trained model. APIRecX can migrate the knowledge of existing libraries to a new library, and can recommend APIs that are previously regarded as OOV. We evaluate APIRecX on six libraries and the results confirm its effectiveness by comparing with two typical API recommendation approaches.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3425,3436,,,,,,,,,,,,,,,,WOS:000855966303047,0
C,"Kottur, S; Moon, S; Geramifard, A; Damavandi, B",,,Assoc Computat Linguist,"Kottur, Satwik; Moon, Seungwhan; Geramifard, Alborz; Damavandi, Babak",,,SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user's multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K taskoriented user <-> assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collected using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of the generated utterances to collect diverse referring expressions. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4903,4912,,,,,,,,,,,,,,,,WOS:000855966305005,0
C,"Li, R; Xiao, W; Wang, LJ; Jang, H; Carenini, G",,,Assoc Computat Linguist,"Li, Raymond; Xiao, Wen; Wang, Lanjun; Jang, Hyeju; Carenini, Giuseppe",,,T-3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive visualization, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the framework is useful, and suggest several improvements. Our framework is available at: https : //github.com/raymondzmc/T3-Vis.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,220,230,,,,,,,,,,,,,,,,WOS:000855241500026,0
C,"Lin, WZ; Tseng, BH; Byrne, B",,,Assoc Computat Linguist,"Lin, Weizhe; Tseng, Bo-Hsiang; Byrne, Bill",,,Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue State Tracking is central to multidomain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level. We further report detailed analyses to demonstrate the effectiveness of graph models in DST by showing that the proposed graph modules capture inter-slot dependencies and improve the predictions of values that are common to multiple domains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7871,7881,,,,,,,,,,,,,,,,WOS:000860727001068,0
C,"Longpre, S; Perisetla, K; Chen, A; Ramesh, N; DuBois, C; Singh, S",,,Assoc Computat Linguist,"Longpre, Shayne; Perisetla, Kartik; Chen, Anthony; Ramesh, Nikhil; DuBois, Chris; Singh, Sameer",,,Entity-Based Knowledge Conflicts in Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge which minimizes hallucination and improves out-of-distribution generalization by 4%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7052,7063,,,,,,,,,,,,,,,,WOS:000860727001013,0
C,"Mirzakhalov, J; Babu, A; Ataman, D; Kariev, S; Tyers, F; Abduraufov, O; Hajili, M; Ivanova, S; Khaytbaev, A; Laverghetta, A; Moydinboyev, B; Onal, E; Pulatova, S; Wahab, A; Firat, O; Chellappan, S",,,Assoc Computat Linguist,"Mirzakhalov, Jamshidbek; Babu, Anoop; Ataman, Duygu; Kariev, Sherzod; Tyers, Francis; Abduraufov, Otabek; Hajili, Mammad; Ivanova, Sardana; Khaytbaev, Abror; Laverghetta, Antonio, Jr.; Moydinboyev, Behzodbek; Onal, Esra; Pulatova, Shaxnoza; Wahab, Ahsan; Firat, Orhan; Chellappan, Sriram",,,A Large-Scale Study of Machine Translation in the Turkic Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted for building competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper, we provide the first large-scale case study of the practical application of MT in the Turkic language family in order to realize the gains of NMT for Turkic languages under high-resource to extremely low-resource scenarios. In addition to presenting an extensive analysis that identifies the bottlenecks towards building competitive systems to ameliorate data scarcity, our study has several key contributions, including, i) a large parallel corpus covering 22 Turkic languages consisting of common public datasets in combination with new datasets of approximately 2 million parallel sentences, ii) bilingual baselines for 26 language pairs, iii) novel high-quality test sets in three different translation domains and iv) human evaluation scores. All of our data, software and models are publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5876,5890,,,,,,,,,,,,,,,,WOS:000855966306005,0
C,"Mohebbi, H; Modarressi, A; Pilehvar, MT",,,Assoc Computat Linguist,"Mohebbi, Hosein; Modarressi, Ali; Pilehvar, Mohammad Taher",,,Exploring the Role of BERT Token Representations to Explain Sentence Probing Results,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth analysis on the representation space of BERT in search for distinct and meaningful subspaces that can explain the reasons behind these probing results. Based on a set of probing tasks and with the help of attribution methods we show that BERT tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,792,806,,,,,,,,,,,,,,,,WOS:000855966300061,0
C,"Ouyang, X; Wang, SH; Pang, C; Sun, Y; Tian, H; Wu, H; Wang, HF",,,Assoc Computat Linguist,"Ouyang, Xuan; Wang, Shuohuan; Pang, Chao; Sun, Yu; Tian, Hao; Wu, Hua; Wang, Haifeng",,,ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent studies have demonstrated that pretrained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for low resource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that ERNIE-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,27,38,,,,,,,,,,,,,,,,WOS:000855966300003,0
C,"Pandia, L; Ettinger, A",,,Assoc Computat Linguist,"Pandia, Lalchand; Ettinger, Allyson",,,Sorting through the noise: Testing robustness of information processing in pre-trained language,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models' ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models' use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1583,1596,,,,,,,,,,,,,,,,WOS:000855966301052,0
C,"Pasupat, P; Zhang, Y; Guu, K",,,Assoc Computat Linguist,"Pasupat, Panupong; Zhang, Yuan; Guu, Kelvin",,,Controllable Semantic Parsing via Retrieval Augmentation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving stateof-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7683,7698,,,,,,,,,,,,,,,,WOS:000860727001055,0
C,"Piper, A; So, RJ; Bamman, D",,,Assoc Computat Linguist,"Piper, Andrew; So, Richard Jean; Bamman, David",,,Narrative Theory for Computational Narrative Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical frameworks to the NLP community, situate current research in NLP within distinct narratological traditions, and argue that linking computational work in NLP to theory opens up a range of new empirical questions that would both help advance our understanding of narrative and open up new practical applications.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,298,311,,,,,,,,,,,,,,,,WOS:000855966300026,0
C,"Qi, P; Lee, H; Sido, O; Manning, CD",,,Assoc Computat Linguist,"Qi, Peng; Lee, Haejun; Sido, Oghenetegiri; Manning, Christopher D.",,,Answering Open-Domain Questions of Varying Reasoning Steps from Text,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks-retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents-in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called BEERQA, by combining existing one- and two-step datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https: //beerqa.github . io/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3599,3614,,,,,,,,,,,,,,,,WOS:000855966303064,0
C,"Rao, DN; Miao, X; Jiang, ZH; Li, R",,,Assoc Computat Linguist,"Rao, Dongning; Miao, Xin; Jiang, Zhihua; Li, Ran",,,STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Rumor detection on social media puts pretrained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20(1) by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks coattention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3347,3363,,,,,,,,,,,,,,,,WOS:000855966303041,0
C,"Schick, T; Schutze, H",,,Assoc Computat Linguist,"Schick, Timo; Schuetze, Hinrich",,,Few-Shot Text Generation with Natural Language Instructions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GENPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GENPET gives consistent improvements over strong baselines in few-shot settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,390,402,,,,,,,,,,,,,,,,WOS:000855966300032,0
C,"Taille, B; Guigue, V; Scoutheeten, G; Gallinari, P",,,Assoc Computat Linguist,"Taille, Bruno; Guigue, Vincent; Scoutheeten, Geoffrey; Gallinari, Patrick",,,Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020a) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another heuristic: the mere retention of training relation triples. In this paper we propose several experiments confirming that retention of known facts is a key factor of performance on standard benchmarks. Furthermore, one experiment suggests that a pipeline model able to use intermediate type representations is less prone to over-rely on retention.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10438,10449,,,,,,,,,,,,,,,,WOS:000860727004040,0
C,"Vu, T; Luong, MT; Le, QV; Simon, G; Iyyer, M",,,Assoc Computat Linguist,"Tu Vu; Minh-Thang Luong; Le, Quoc, V; Simon, Grady; Iyyer, Mohit",,,STraTA: Self-Training with Task Augmentation for Better Few-shot Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 fewshot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5715,5731,,,,,,,,,,,,,,,,WOS:000855966305066,0
C,"Varab, D; Schluter, N",,,Assoc Computat Linguist,"Varab, Daniel; Schluter, Natalie",,,"MassiveSumm: a very large-scale, very multilingual, newswire summarisation dataset",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current research in automatic summarisation is unapologetically anglo-centered-a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10150,10161,,,,,,,,,,,,,,,,WOS:000860727004021,0
C,"Zeng, JL; Wu, SZ; Yin, YJ; Jiang, YF; Li, M",,,Assoc Computat Linguist,"Zeng, Jiali; Wu, Shuangzhi; Yin, Yongjing; Jiang, Yufan; Li, Mu",,,Recurrent Attention for Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent research questions the importance of the dot-product self-attention in Transformer models and shows that most attention heads learn simple positional patterns. In this paper, we push further in this research line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN). RAN directly learns attention weights without any token-to-token interaction and further improves their capacity by layer-to-layer interaction. Across an extensive set of experiments on 10 machine translation tasks, we find that RAN models are competitive and outperform their Transformer counterpart in certain scenarios, with fewer parameters and inference time. Particularly, when apply RAN to the decoder of Transformer, there brings consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3216,3225,,,,,,,,,,,,,,,,WOS:000855966303030,0
C,"Zhang, BL; Li, ZC; Gan, Z; Chen, YB; Wan, J; Liu, K; Zhao, J; Liu, SP; Shi, YF",,,Assoc Computat Linguist,"Zhang, Baoli; Li, Zhucong; Gan, Zhen; Chen, Yubo; Wan, Jing; Liu, Kang; Zhao, Jun; Liu, Shengping; Shi, Yafei",,,CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator: CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector: CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer: We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this error system to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our platform, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96% and +2.57% F1 respectively in model performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,275,282,,,,,,,,,,,,,,,,WOS:000855241500032,0
C,"Zhang, BA; Titov, I; Sennrich, R",,,Assoc Computat Linguist,"Zhang, Biao; Titov, Ivan; Sennrich, Rico",,,Sparse Attention with Linear Units,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6507,6520,,,,,,,,,,,,,,,,WOS:000860727000038,0
C,"Zhang, YJ; Zhang, YB; Xu, CP; Li, J; Jiang, ZY; Peng, BL",,,Assoc Computat Linguist,"Zhang, Yuji; Zhang, Yubo; Xu, Chunpu; Li, Jing; Jiang, Ziyan; Peng, Baolin",,,#HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user's hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one's interests in a hashtag are related to what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via joint training. In this way, user interests learned from the past can be customized to match future hashtags, which is beyond the capability of existing methods assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our model significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7811,7820,,,,,,,,,,,,,,,,WOS:000860727001064,0
C,"Zhao, J; Xue, NAW; Van Gysel, J; Choi, JD",,,Assoc Computat Linguist,"Zhao, Jin; Xue, Nianwen; Van Gysel, Jens; Choi, Jinho D.",,,UMR-Writer: A Web Application for Annotating Uniform Meaning Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a tool and how they are addressed.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,160,167,,,,,,,,,,,,,,,,WOS:000855241500019,0
C,"Akula, AR; Changpinyo, S; Gong, BQ; Sharma, P; Zhu, SC; Soricut, R",,,Assoc Computat Linguist,"Akula, Arjun R.; Changpinyo, Soravit; Gong, Boqing; Sharma, Piyush; Zhu, Song-Chun; Soricut, Radu",,,CrossVQA: Scalably Generating Benchmarks For Systematically Testing VQA Generalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2148,2166,,,,,,,,,,,,,,,,WOS:000855966302023,0
C,"Chakrabarty, T; Trivedi, A; Muresan, S",,,Assoc Computat Linguist,"Chakrabarty, Tuhin; Trivedi, Aadit; Muresan, Smaranda",,,Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise, but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6247,6252,,,,,,,,,,,,,,,,WOS:000860727000019,0
C,"Chen, JK; Zheng, RJ; Kita, A; Ma, MB; Huang, L",,,Assoc Computat Linguist,"Chen, Junkun; Zheng, Renjie; Kita, Atsuhito; Ma, Mingbo; Huang, Liang",,,Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still trained on conventional full-sentence bitexts. This is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. We propose a novel method that rewrites the target side of existing full-sentence corpora into simultaneous-style translation. Experiments on Zh -> En and Ja -> En simultaneous translation show substantial improvements (up to +2.7 BLEU) with the addition of these generated pseudo-references.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5857,5864,,,,,,,,,,,,,,,,WOS:000855966306003,0
C,"Ciosici, MR; Cecil, J; Hedges, A; Lee, DH; Freedman, M; Weischedel, R",,,Assoc Computat Linguist,"Ciosici, Manuel R.; Cecil, Joe; Hedges, Alex; Lee, Dong-Ho; Freedman, Marjorie; Weischedel, Ralph",,,Perhaps PTLMs Should Go to School - A Task to Assess Open Book and Closed Book QA,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Our goal is to deliver a new task and leader-board to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true/false statements based on review questions written by the textbook authors, validation/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be similar to 50%. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook's content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5's pre-training), yields at best minor improvement (56%), suggesting that the PTLM may not have understood the textbook (or perhaps misunderstood the questions). Performance is better (similar to 60%) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6104,6111,,,,,,,,,,,,,,,,WOS:000860727000008,0
C,"Dhamecha, TI; Murthy, VR; Bharadwaj, S; Sankaranarayanan, K; Bhattacharyya, P",,,Assoc Computat Linguist,"Dhamecha, Tejas Indulal; Murthy, Rudra, V; Bharadwaj, Samarth; Sankaranarayanan, Karthik; Bhattacharyya, Pushpak",,,Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150% in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8584,8595,,,,,,,,,,,,,,,,WOS:000860727002053,0
C,"Dobrovolskii, V",,,Assoc Computat Linguist,"Dobrovolskii, Vladimir",,,Word-Level Coreference Resolution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is O(n(2)) in the length of text and the number of potential links is O(n(4)), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to O(n(2)) and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7670,7675,,,,,,,,,,,,,,,,WOS:000860727001053,0
C,"Eisenschlos, JM; Gor, M; Muller, T; Cohen, WW",,,Assoc Computat Linguist,"Eisenschlos, Julian Martin; Gor, Maharshi; Mueller, Thomas; Cohen, William W.",,,MATE: Multi-view Attention for Table Transformer Efficiency,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HYBRIDQA (Chen et al., 2020b), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7606,7619,,,,,,,,,,,,,,,,WOS:000860727001048,0
C,"Geva, M; Katz, U; Ben-Arie, A; Berant, J",,,Assoc Computat Linguist,"Geva, Mor; Katz, Uri; Ben-Arie, Aviv; Berant, Jonathan",,,What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8201,8215,,,,,,,,,,,,,,,,WOS:000860727002024,0
C,"Gor, M; Webster, K; Boyd-Graber, J",,,Assoc Computat Linguist,"Gor, Maharshi; Webster, Kellie; Boyd-Graber, Jordan",,,Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The goal of question answering (QA) is to answer any question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, model accuracy analysis reveals little evidence that accuracy is lower for people based on gender or nationality; instead, there is more variation on professions (question topic). But QA's lack of representation could itself hide evidence of bias, necessitating QA datasets that better represent global diversity.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5457,5473,,,,,,,,,,,,,,,,WOS:000855966305048,0
C,"He, WN; Huang, CM; Liu, YM; Zhu, XD",,,Assoc Computat Linguist,"He, Weinan; Huang, Canming; Liu, Yongmei; Zhu, Xiaodan",,,WINOLOGIC: A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The recent success of neural language models (NLMs) on the Winograd Schema Challenge has called for further investigation of the commonsense reasoning ability of these models. Previous diagnostic datasets rely on crowd-sourcing which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality commonsense knowledge. Specifically, we identify and collect formal knowledge formulas verified by theorem provers and translate such formulas into natural language sentences. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new dataset named WINOLOGIC with these sentences. Given a problem in WINOLOGIC, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WINOLOGIC to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3779,3789,,,,,,,,,,,,,,,,WOS:000855966303079,0
C,"Holtzman, A; West, P; Shwartz, V; Choi, YJ; Zettlemoyer, L",,,Assoc Computat Linguist,"Holtzman, Ari; West, Peter; Shwartz, Vered; Choi, Yejin; Zettlemoyer, Luke",,,Surface Form Competition: Why the Highest Probability Answer Isn't Always Right,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large language models have shown promising results in zero-shot settings (Brown et al., 2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. computer and PC. Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. *",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7038,7051,,,,,,,,,,,,,,,,WOS:000860727001012,0
C,"Huang, Y; Giledereli, B; Koksal, A; Ozgur, A; Ozkirimli, E",,,Assoc Computat Linguist,"Huang, Yi; Giledereli, Buse; Koksal, Abdullatif; Ozgur, Arzucan; Ozkirimli, Elif",,,Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Re-sampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multilabel text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8153,8161,,,,,,,,,,,,,,,,WOS:000860727002021,0
C,"Lakhotia, K; Paranjape, B; Ghoshal, A; Yih, WT; Mehdad, Y; Iyer, S",,,Assoc Computat Linguist,"Lakhotia, Kushal; Paranjape, Bhargavi; Ghoshal, Asish; Yih, Wen-tau; Mehdad, Yashar; Iyer, Srinivasan",,,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex(1), which addresses these shortcomings for seq2seq models by: 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3712,3727,,,,,,,,,,,,,,,,WOS:000855966303073,0
C,"Li, H; Ding, WB; Kang, Y; Liu, TQ; Wu, ZQ; Liu, ZT",,,Assoc Computat Linguist,"Li, Hang; Ding, Wenbiao; Kang, Yu; Liu, Tianqiao; Wu, Zhongqin; Liu, Zitao",,,CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Existing approaches for audio-language task-specific prediction focus on building complicated late-fusion mechanisms. However, these models face challenges of overfitting with limited labels and poor generalization. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra- and inter-modalities connections between audio and language through two proxy tasks from a large number of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our CTAL model on multiple downstream audio and-language tasks, we observe significant improvements on different tasks, including emotion classification, sentiment analysis, and speaker verification. Furthermore, we design a fusion mechanism in the fine-tuning phase, which allows CTAL to achieve better performance. Lastly, we conduct detailed ablation studies to demonstrate that both our novel cross-modality fusion component and audio language pre-training methods contribute to the promising results. The code and pretrained models are available at https://github.com/tal-al/CTAL_EMNLP2021.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3966,3977,,,,,,,,,,,,,,,,WOS:000855966304010,0
C,"Liu, C; Zhang, MC; Fu, ZB; Hou, P; Li, Y",,,Assoc Computat Linguist,"Liu, Chen; Zhang, Mengchao; Fu, Zhibin; Hou, Pan; Li, Yu",,,FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In natural language processing (NIP), state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSI, framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient SSL. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSI, benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to 58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo. In addition, compared with the fully supervised method on the full dataset, FLiText just uses less than 1% of labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the datasets of IMDb, Yelp-5, and Yahoo respectively.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2481,2491,,,,,,,,,,,,,,,,WOS:000855966302051,0
C,"Manakul, P; Gales, MJF",,,Assoc Computat Linguist,"Manakul, Potsawee; Gales, Mark J. F.",,,Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer's encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9359,9368,,,,,,,,,,,,,,,,WOS:000860727003078,0
C,"Micheli, V; Fleuret, F",,,Assoc Computat Linguist,"Micheli, Vincent; Fleuret, Francois",,,Language Models are Few-Shot Butlers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine-tuned with only 1.2% of the expert demonstrations and a simple reinforcement learning algorithm achieve a 51% absolute improvement in success rate over existing methods in the ALFWorld environment.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9312,9318,,,,,,,,,,,,,,,,WOS:000860727003036,0
C,"Panapitiya, G; Parks, F; Sepulveda, J; Saldanha, E",,,Assoc Computat Linguist,"Panapitiya, Gihan; Parks, Fred; Sepulveda, Jonathan; Saldanha, Emily",,,Extracting Material Property Measurement Data from Scientific Articles,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training datasets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of reliable property prediction methods. In this work, we describe a methodology for an automatic property extraction framework using material solubility as the target property. We create an annotated dataset containing tags for solubility-related entities using a combination of regular expressions and manual tagging. We then compare five entity recognition models leveraging both token-level and span-level architectures on the task of classifying solute names, solubility values, and solubility units. Additionally, we explore a novel pretraining approach that leverages automated chemical name and quantity extraction tools to generate large datasets that do not rely on intensive manual effort. Finally, we perform an analysis to identify the causes of classification errors.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5393,5402,,,,,,,,,,,,,,,,WOS:000855966305042,0
C,"Park, G; Kim, G; Yang, EO",,,Assoc Computat Linguist,"Park, Geondo; Kim, Gyeongman; Yang, Eunho",,,Distilling Linguistic Context for Language Model Compression,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes, but also in combination with DynaBERT, the recently proposed adaptive size pruning method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,364,378,,,,,,,,,,,,,,,,WOS:000855966300030,0
C,"Plant, R; Giuffrida, V; Gkatzia, D",,,Assoc Computat Linguist,"Plant, Richard; Giuffrida, Valerio; Gkatzia, Dimitra",,,CAPE: Context-Aware Private Embeddings for Private Language Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural language models have contributed to state-of-the-art results in a number of downstream applications including sentiment analysis, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding personally identifiable information learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines differential privacy and adversarial learning to preserve privacy during training of embeddings. Specifically, CAPE firstly applies calibrated noise through differential privacy to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3% reduction in attacker performance compared to the best-performing current method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7970,+,,,,,,,,,,,,,,,,WOS:000860727002006,0
C,"Pu, A; Chung, HW; Parikh, AP; Gehrmann, S; Sellam, T",,,Assoc Computat Linguist,"Pu, Amy; Chung, Hyung Won; Parikh, Ankur P.; Gehrmann, Sebastian; Sellam, Thibault",,,Learning Compact Metrics for MT,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity with RemBERT, a state-of-the-art multilingual language model, using data from the WMT Metrics Shared Task. We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages. Our method yields up to 10.5% improvement over vanilla fine-tuning and reaches 92.6% of RemBERT's performance using only a third of its parameters.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,751,762,,,,,,,,,,,,,,,,WOS:000855966300058,0
C,"Shen, S; Yao, ZW; Kiela, DE; Keutzer, KR; Mahoney, MW",,,Assoc Computat Linguist,"Shen, Sheng; Yao, Zhewei; Kiela, Douwe; Keutzer, Kurt; Mahoney, Michael W.",,,What's Hidden in a One-layer Randomly Weighted Transformer?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we apply different binary masks to the same weight matrix to generate different layers. Hidden within a one-layer randomly weighted Transformer, we find that subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed pretrained embedding layer, the previously found subnetworks are smaller than, but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained Transformer(small/base) on IWSLT14/WMT14. Furthermore, we demonstrate the effectiveness of larger and deeper transformers in this setting, as well as the impact of different initialization methods.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2914,2921,,,,,,,,,,,,,,,,WOS:000855966303003,0
C,"Tan, XW; Pergola, G; He, YL",,,Assoc Computat Linguist,"Tan, Xingwei; Pergola, Gabriele; He, Yulan",,,Extracting Event Temporal Relations via Hyperbolic Geometry,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically oriented at modeling hierarchical structures. We introduce two approaches to encode events and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer event relations through simple geometrical operations. In the second one, we devise an end-to-end architecture composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different geometrical space, resulting in state-of-the-art performance on several standard metrics. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8065,8077,,,,,,,,,,,,,,,,WOS:000860727002014,0
C,"Wang, SF; Thompson, L; Iyyer, M",,,Assoc Computat Linguist,"Wang, Shufan; Thompson, Laure; Iyyer, Mohit",,,Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10837,10851,,,,,,,,,,,,,,,,WOS:000860727004070,0
C,"White, J; Poesia, G; Hawkins, R; Sadigh, D; Goodman, N",,,Assoc Computat Linguist,"White, Julia; Poesia, Gabriel; Hawkins, Robert; Sadigh, Dorsa; Goodman, Noah",,,Open-domain clarification question generation without question examples,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model's ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,563,570,,,,,,,,,,,,,,,,WOS:000855966300044,0
C,"Xu, CW; Zhou, WCS; Ge, T; Xu, K; McAuley, J; Wei, FR",,,Assoc Computat Linguist,"Xu, Canwen; Zhou, Wangchunshu; Ge, Tao; Xu, Ke; McAuley, Julian; Wei, Furu",,,Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module replacing with loyalty and robustness. By combining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10653,10659,,,,,,,,,,,,,,,,WOS:000860727004056,0
C,"Zhang, SJ; Gong, CY; Choi, E",,,Assoc Computat Linguist,"Zhang, Shujian; Gong, Chengyue; Choi, Eunsol",,,Learning with Different Amounts of Annotation: From Zero to Many Labels,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Training NLP systems typically assumes access to annotated data that has a single human label per example. Given imperfect labeling from annotators and inherent ambiguity of language, we hypothesize that single label is not sufficient to learn the spectrum of language interpretation. We explore new annotation distribution schemes, assigning multiple labels per example for a small subset of training examples. Introducing such multi label examples at the cost of annotating fewer examples brings clear gains on natural language inference task and entity typing task, even when we simply first train with a single label data and then fine tune with multi label examples. Extending a MixUp data augmentation framework, we propose a learning algorithm that can learn from training examples with different amount of annotation (with zero, one, or multiple labels). This algorithm efficiently combines signals from uneven training data and brings additional gains in low annotation budget and cross domain settings. Together, our method achieves consistent gains in two tasks, suggesting distributing labels unevenly among training examples can be beneficial for many NLP tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7620,7632,,,,,,,,,,,,,,,,WOS:000860727001049,0
C,"Bianchi, F; Marelli, M; Nicoli, P; Palmonari, M",,,Assoc Computat Linguist,"Bianchi, Federico; Marelli, Marco; Nicoli, Paolo; Palmonari, Matteo",,,SWEAT: Scoring Polarization of Topics across Different Corpora,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.",,,,,,"BIANCHI, FEDERICO/0000-0003-0776-361X",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10065,10072,,,,,,,,,,,,,,,,WOS:000860727004012,0
C,"Chada, R; Natarajan, P",,,Assoc Computat Linguist,"Chada, Rakesh; Natarajan, Pradeep",,,FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of learning from only a few examples (called a few-shot setting) is of key importance and relevance to a real-world setting. For question answering (QA), the current state-of-the-art pre-trained models typically need fine-tuning on tens of thousands of examples to obtain good results. Their performance degrades significantly in a few-shot setting (< 100 examples). To address this, we propose a simple fine-tuning framework that leverages pre-trained text-to-text models and is directly aligned with their pre-training framework. Specifically, we construct the input as a concatenation of the question, a mask token representing the answer span and a context. Given this input, the model is fine-tuned using the same objective as that of its pre-training objective. Through experimental studies on various few-shot configurations, we show that this formulation leads to significant gains on multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when there are only 16 training examples). The gains extend further when used with larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples) and translate well to a multilingual setting. On the multilingual TydiQA benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of upto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64 training examples). We conduct detailed ablation studies to analyze factors contributing to these gains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6081,6090,,,,,,,,,,,,,,,,WOS:000860727000006,0
C,"Chen, P",,,Assoc Computat Linguist,"Chen, Peng",,,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. Permute-Former applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of selfattention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10606,10618,,,,,,,,,,,,,,,,WOS:000860727004052,0
C,"Chi, ZW; Dong, L; Ma, SM; Huang, SH; Mao, XL; Huang, HY; Wei, FR",,,Assoc Computat Linguist,"Chi, Zewen; Dong, Li; Ma, Shuming; Huang, Shaohan; Mao, Xian-Ling; Huang, Heyan; Wei, Furu",,,mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multilingual T5 (MT5; Xue et al. 2020) pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (MT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pretraining. We evaluate the methods on eight multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed MT6 improves cross-lingual transferability over MT5.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1671,1683,,,,,,,,,,,,,,,,WOS:000855966301058,0
C,"Dziri, N; Madotto, A; Zaiane, O; Bose, AJ",,,Assoc Computat Linguist,"Dziri, Nouha; Madotto, Andrea; Zaiane, Osmar; Bose, Avishek Joey",,,Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose NEURAL PATH HUNTER which follows a generate-then-refine strategy whereby a generated response is amended using the KG. NEURAL PATH HUNTER leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2197,2214,,,,,,,,,,,,,,,,WOS:000855966302027,0
C,"Fang, TQ; Wang, WQ; Choi, SY; Hao, SB; Zhang, HM; Song, YQ; He, B",,,Assoc Computat Linguist,"Fang, Tianqing; Wang, Weiqi; Choi, Sehyun; Hao, Shibo; Zhang, Hongming; Song, Yangqiu; He, Bin",,,Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Reasoning over commonsense knowledge bases (CSKBs) whose elements are in the form of free-text is an important yet hard task in NLP. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a highquality human-annotated evaluation set to probe neural models' commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over graphs. Experimental results show that generalizing commonsense reasoning on unseen assertions is inherently a hard task. Models achieving high accuracy during training perform poorly on the evaluation set, with a large gap between human performance. Codes and data are available at https://github.com/ HKUST- KnowComp/CSKB-Population.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8949,8964,,,,,,,,,,,,,,,,WOS:000860727003007,0
C,"Honovich, O; Choshen, L; Aharoni, R; Neeman, E; Szpektor, I; Abend, O",,,Assoc Computat Linguist,"Honovich, Or; Choshen, Leshem; Aharoni, Roee; Neeman, Ella; Szpektor, Idan; Abend, Omri",,,Q(2) : Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q(2), compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q(2) against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7856,7870,,,,,,,,,,,,,,,,WOS:000860727001067,0
C,"Levy, S; Mo, K; Xiong, WH; Wang, WY",,,Assoc Computat Linguist,"Levy, Sharon; Mo, Kevin; Xiong, Wenhan; Wang, William Yang",,,Open-Domain Question-Answering for COVID-19 and Other Emergent Domains,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public space for users to ask questions and receive credible, scientific answers. To fulfill this need, we turn to the task of open-domain question-answering, which we can use to efficiently find answers to free-text questions from a large set of documents. In this work, we present such a system for the emergent domain of COVID-19. Despite the small data size available, we are able to successfully train the system to retrieve answers from a large-scale corpus of published COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking and question-answering techniques, such as document diversity and multiple answer spans. Our open-domain question-answering system can further act as a model for the quick development of similar systems that can be adapted and modified for other developing emergent domains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,259,266,,,,,,,,,,,,,,,,WOS:000855241500030,0
C,"Liu, FX; Wang, YH; Wang, TL; Ordonez, V",,,Assoc Computat Linguist,"Liu, Fuxiao; Wang, Yinghan; Wang, Tianlu; Ordonez, Vicente",,,Visual News: Benchmark and Challenges in News Image Captioning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as events and entities. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our method utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6761,6771,,,,,,,,,,,,,,,,WOS:000860727000057,0
C,"Ma, KX; Ilievski, F; Francis, J; Ozaki, S; Nyberg, E; Oltramari, A",,,Assoc Computat Linguist,"Ma, Kaixin; Ilievski, Filip; Francis, Jonathan; Ozaki, Satoru; Nyberg, Eric; Oltramari, Alessandro",,,Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models may already possess useful knowledge from past experience, but a challenge remains in understanding what parts and to what extent models should be refined for a given task. In this paper, we investigate what models learn from commonsense reasoning datasets. We measure the impact of three different adaptation methods on the generalization and accuracy of models. Our experiments with two models show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. We observe that alternative adaptation methods like prefixtuning have comparable accuracy, but generalize better to unseen answers and are more robust to adversarial splits.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5474,5483,,,,,,,,,,,,,,,,WOS:000855966305049,0
C,"Maimaiti, M; Liu, Y; Zheng, YH; Chen, G; Huang, KY; Zhang, J; Luan, HB; Sun, MS",,,Assoc Computat Linguist,"Maimaiti, Mieradilijiang; Liu, Yang; Zheng, Yuanhang; Chen, Gang; Huang, Kaiyu; Zhang, Ji; Luan, Huanbo; Sun, Maosong",,,"Segment, Mask, and Predict: Augmenting Chinese Word Segmentation with Self-Supervision",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the models with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the robustness of the previous neural methods is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective architecture. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2068,2077,,,,,,,,,,,,,,,,WOS:000855966302017,0
C,"Mekala, D; Gangal, V; Shang, JB",,,Assoc Computat Linguist,"Mekala, Dheeraj; Gangal, Varun; Shang, Jingbo",,,Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only human guidance and weave in rich pre-trained generative language models into the iterative weak supervision strategy. Specifically, we first propose a label-conditioned finetuning formulation to attune these generators for our task. Furthermore, we devise a regularization objective based on the coarse-fine label constraints derived from our problem setting, giving us even further improvements over the prior formulation. Our framework uses the fine-tuned generative models to sample pseudo-training data for training the classifier, and bootstraps on real unlabeled data for model refinement. Extensive experiments and case studies on two real-world datasets demonstrate superior performance over SOTA zero-shot classification baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,583,594,,,,,,,,,,,,,,,,WOS:000855966300046,0
C,"Nguyen, MV; Nguyen, TN; Min, BN; Nguyen, TH",,,Assoc Computat Linguist,"Minh Van Nguyen; Tuan Ngo Nguyen; Min, Bonan; Thien Huu Nguyen",,,Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source language example of a different class. To address this issue, we propose a novel crosslingual alignment method that leverages class information of REE tasks for representation learning. In particular, we propose to learn two versions of representation vectors for each class in an REE task based on either source or target language examples. Representation vectors for corresponding classes will then be aligned to achieve class-aware alignment for crosslingual representations. In addition, we propose to further align representation vectors for languageuniversal word categories (i.e., parts of speech and dependency relations). As such, a novel filtering mechanism is presented to facilitate the learning of word category representations from contextualized representations on input texts based on adversarial learning. We conduct extensive crosslingual experiments with English, Chinese, and Arabic over REE tasks. The results demonstrate the benefits of the proposed method that significantly advances the state-of-the-art performance in these settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5414,5426,,,,,,,,,,,,,,,,WOS:000855966305044,0
C,"Mukherjee, R; Nayak, T; Butala, Y; Bhattacharya, S; Goyal, P",,,Assoc Computat Linguist,"Mukherjee, Rajdeep; Nayak, Tapas; Butala, Yash; Bhattacharya, Sourangshu; Goyal, Pawan",,,PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consisting of an opinion target or aspect, its associated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are majorly tagging-based. Among the methods taking a sequence tagging approach, some fail to capture the strong interdependence between the three opinion factors, whereas others fall short of identifying triplets with overlapping aspect/opinion spans. A recent grid tagging approach on the other hand fails to capture the span-level semantics while predicting the sentiment between an aspect-opinion pair. Different from these, we present a tagging-free solution for the task, while addressing the limitations of the existing works. We adapt an encoder-decoder architecture with a Pointer Network-based decoding framework that generates an entire opinion triplet at each time step thereby making our solution end-to-end. Interactions between the aspects and opinions are effectively captured by the decoder by considering their entire detected spans while predicting their connecting sentiment. Extensive experiments on several benchmark datasets establish the better efficacy of our proposed approach, especially in recall, and in predicting multiple and aspect/opinion-overlapped triplets from the same review sentence. We report our results both with and without BERT and also demonstrate the utility of domainspecific BERT post-training for the task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9279,9291,,,,,,,,,,,,,,,,WOS:000860727003033,0
C,"Reid, M; Hu, JJ; Neubig, G; Matsuo, Y",,,Assoc Computat Linguist,"Reid, Machel; Hu, Junjie; Neubig, Graham; Matsuo, Yutaka",,,AFROMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AFROMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1306,1320,,,,,,,,,,,,,,,,WOS:000855966301032,0
C,"Ruder, S; Constant, N; Botha, J; Siddhant, A; Firat, O; Fu, JL; Liu, PF; Hu, JJ; Garrette, D; Neubig, G; Johnson, M",,,Assoc Computat Linguist,"Ruder, Sebastian; Constant, Noah; Botha, Jan; Siddhant, Aditya; Firat, Orhan; Fu, Jinlan; Liu, Pengfei; Hu, Junjie; Garrette, Dan; Neubig, Graham; Johnson, Melvin",,,XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to humanlevel performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME- R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MULTICHECKLIST) and finegrained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10215,10245,,,,,,,,,,,,,,,,WOS:000860727004026,0
C,"Sanyal, S; Ren, X",,,Assoc Computat Linguist,"Sanyal, Soumya; Ren, Xiang",,,Discretized Integrated Gradients for Explaining Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research (1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10285,10299,,,,,,,,,,,,,,,,WOS:000860727004029,0
C,"Wang, BJ; Zhang, Z; Xu, K; Hao, GY; Zhang, YY; Shang, LF; Li, LL; Chen, X; Jiang, X; Liu, Q",,,Assoc Computat Linguist,"Wang, Baojun; Zhang, Zhao; Xu, Kun; Hao, Guang-Yuan; Zhang, Yuyang; Shang, Lifeng; Li, Linlin; Chen, Xiao; Jiang, Xin; Liu, Qun",,,DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt word-agnostic tag embeddings to avoid re-training the representation while updating the lexicon. Moreover, we employ an effective supervised lexical knowledge denoising method to smooth out matching noise. Finally, we introduce a col-wise attention based knowledge fusion mechanism to guarantee the pluggability of the proposed framework. Experiments on ten datasets of three tasks show that the proposed framework achieves new SOTA, even with very large scale lexicons(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2679,2693,,,,,,,,,,,,,,,,WOS:000855966302070,0
C,"Wu, SX; Li, J; Zhang, P; Zhang, Y",,,Assoc Computat Linguist,"Wu, Sixuan; Li, Jian; Zhang, Peng; Zhang, Yue",,,Natural Language Processing Meets Quantum Physics: A Survey and Categorization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent research has investigated quantum NLP, designing algorithms that process natural language in quantum computers, and also quantum-inspired algorithms that improve NLP performance on classical computers. In this survey, we review representative methods at the intersection of NLP and quantum physics in the past ten years, categorizing them according to the use of quantum theory, the linguistic targets that are modeled, and the downstream application. The literature review ends with a discussion on the key factors to the success that has been achieved by existing work, as well as challenges ahead, with the goal of better understanding the promises and further directions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3172,3182,,,,,,,,,,,,,,,,WOS:000855966303026,0
C,"Xu, RX; Luo, FL; Zhang, ZY; Tan, CAQ; Chang, BB; Huang, SF; Huang, F",,,Assoc Computat Linguist,"Xu, Runxin; Luo, Fuli; Zhang, Zhiyuan; Tan, Chuanqi; Chang, Baobao; Huang, Songfang; Huang, Fei",,,Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, CHILD-TUNING, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that CHILD-TUNING consistently outperforms the vanilla fine-tuning by 1.5 similar to 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 similar to 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that CHILD-TUNING can obtain better generalization performance by large margins.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9514,9528,,,,,,,,,,,,,,,,WOS:000860727003050,0
C,"Yan, ZH; Zhang, C; Fu, JL; Zhang, Q; Wei, ZY",,,Assoc Computat Linguist,"Yan, Zhiheng; Zhang, Chong; Fu, Jinlan; Zhang, Qi; Wei, Zhongyu",,,A Partition Filter Network for Joint Entity and Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of taskspecific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,185,197,,,,,,,,,,,,,,,,WOS:000855966300017,0
C,"Zhang, Y; Kamigaito, H; Okumura, M",,,Assoc Computat Linguist,"Zhang, Ying; Kamigaito, Hidetaka; Okumura, Manabu",,,A Language Model-based Generative Classifier for Sentence-level Discourse Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F-1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F-1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2432,2446,,,,,,,,,,,,,,,,WOS:000855966302047,0
C,"Zhu, KR; Gao, Y; Guo, JQ; Lou, JG",,,Assoc Computat Linguist,"Zhu, Kunrui; Gao, Yan; Guo, Jiaqi; Lou, Jian-Guang",,,Translating Headers of Tabular Data: A Pilot Study of Schema Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in the community, and state-of-the-art neural machine translation models cannot work well on this task because of two intrinsic differences between plain text and tabular data: morphological difference and context difference. To facilitate the research study, we construct the first parallel dataset for schema translation, which consists of 3,158 tables with 11,979 headers written in 6 different languages, including English, Chinese, French, German, Spanish, and Japanese. Also, we propose the first schema translation model called CAST, which is a header-to-header neural machine translation model augmented with schema context. Specifically, we model a target header and its context as a directed graph to represent their entity types and relations. Then CAST encodes the graph with a relational-aware transformer and uses another transformer to decode the header in the target language. Experiments on our dataset demonstrate that CAST significantly outperforms state-of-the-art neural machine translation models. Our dataset will be released at https://github.com/microsoft/ContextualSP.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,56,66,,,,,,,,,,,,,,,,WOS:000855966300005,0
C,"Arora, A; Garcia-Duran, A; West, R",,,Assoc Computat Linguist,"Arora, Akhil; Garcia-Duran, Alberto; West, Robert",,,Low-Rank Subspaces for Unsupervised Entity Linking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, EIGENTHEMES, that relies solely on the availability of entity names and a referent knowledge base. EIGENTHEMES exploits the fact that the entities that are truly mentioned in a document (the gold entities) tend to form a semantically dense subset of the set of all candidate entities in the document. Geometrically speaking, when representing entities as vectors via some given embedding, the gold entities tend to lie in a low-rank subspace of the full embedding space. EIGENTHEMES identifies this subspace using the singular value decomposition and scores candidate entities according to their proximity to the subspace. On the empirical front, we introduce multiple strong baselines that compare favorably to (and sometimes even outperform) the existing state of the art. Extensive experiments on benchmark datasets from a variety of real-world domains showcase the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8037,8054,,,,,,,,,,,,,,,,WOS:000860727002012,0
C,"Bogoychev, N; Van der Linde, J; Heafield, K",,,Assoc Computat Linguist,"Bogoychev, Nikolay; Van der Linde, Jelmer; Heafield, Kenneth",,,TranslateLocally: Blazing-fast translation running on the local CPU,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Every day, millions of people sacrifice their privacy and browsing habits in exchange for online machine translation. Companies and governments with confidentiality requirements often ban online translation or pay a premium to disable logging. To bring control back to the end user and demonstrate speed, we developed translateLocally. Running locally on a desktop or laptop CPU, translateLocally delivers cloud-like translation speed and quality even on 10 year old hardware. The open-source software is based on Marian and runs on Linux, Windows, and macOS.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,168,174,,,,,,,,,,,,,,,,WOS:000855241500020,0
C,"Bursztyn, VS; Healey, J; Lipka, N; Koh, E; Downey, D; Birnbaum, L",,,Assoc Computat Linguist,"Bursztyn, Victor S.; Healey, Jennifer; Lipka, Nedim; Koh, Eunyee; Downey, Doug; Birnbaum, Larry",,,It doesn't look good for a date: Transforming Critiques into Preferences for Conversational Recommendation Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., It doesn't look good for a date), requiring some degree of common sense for a preference to be inferred. In this work, we present a method for transforming a user critique into a positive preference (e.g., I prefer more romantic) in order to retrieve reviews pertaining to potentially better recommendations (e.g., Perfect for a romantic dinner). We leverage a large neural language model (LM) in a fewshot setting to perform critique-to-preference transformation, and we test two methods for retrieving recommendations: one that matches embeddings, and another that fine-tunes an LM for the task. We instantiate this approach in the restaurant domain and evaluate it using a new dataset of restaurant critiques. In an ablation study, we show that utilizing critiqueto-preference transformation improves recommendations, and that there are at least three general cases that explain this improved performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1913,1918,,,,,,,,,,,,,,,,WOS:000855966302004,0
C,"Chen, YW; Teufel, S",,,Assoc Computat Linguist,"Chen, Yiwen; Teufel, Simone",,,Synthetic Textual Features for the Large-Scale Detection of Basic-level Categories in English and Mandarin,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Basic-level categories (BLC) are an important psycholinguistic concept introduced by Rosch et al. (1976); they are defined as the most inclusive categories for which a concrete mental image of the category as a whole can be formed, and also as those categories which are acquired early in life. Rosch's original algorithm for detecting BLC (called cue-validity) is based on the availability of semantic features such as 'has tail' for 'cat', and has remained untested at large. An at-scale algorithm for the automatic determination of BLC exists, but it operates without Rosch-style semantic features, and is thus unable to verify Rosch's hypothesis. We present the first method for the detection of BLC at scale that makes use of Rosch-style semantic features. For both English and Mandarin, we test three methods of generating such features for any synset within Wordnet (WN): extraction of textual features from Wikipedia pages, Distributional Memory (DM) and BART. The best of our methods outperforms the current SoA in BLC detection, with an accuracy of English BLC detection of 75.0%, and of Mandarin BLC detection 80.7% on a test set. When applied to all of WordNet, our model predicts that 1,118 synsets in English Wordnet (1.4%) are BLC, far fewer than existing methods, and with a precision improvement of over 200% over these. As well as confirming the usefulness of Rosch's cue validity algorithm, we also developed and evaluated our own new indicator for BLC, which models the fact that BLC features tend to be BLC themselves.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8294,8305,,,,,,,,,,,,,,,,WOS:000860727002032,0
C,"Di Giovanni, M; Brambilla, M",,,Assoc Computat Linguist,"Di Giovanni, Marco; Brambilla, Marco",,,Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on unsupervised approaches that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter's intrinsic powerful signals of relatedness: replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities.(1)",,,,,"Brambilla, Marco/M-4748-2015","Brambilla, Marco/0000-0002-8753-2434",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9902,9910,,,,,,,,,,,,,,,,WOS:000860727004004,0
C,"Hao, KL; Yu, BT; Hu, W",,,Assoc Computat Linguist,"Hao, Kailong; Yu, Botao; Hu, Wei",,,Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the socalled false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out possible FN samples by heuristically lever-aging the memory mechanism of deep neural networks. Then, it aligns those unlabeled data with the training data into a unified feature space by adversarial training to assign pseudo labels and further utilize the information contained in them. Experiments on two wildly-used benchmark datasets demonstrate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9661,9672,,,,,,,,,,,,,,,,WOS:000860727003062,0
C,"Huang, KH; Tang, S; Peng, NY",,,Assoc Computat Linguist,"Huang, Kung-Hsiang; Tang, Sam; Peng, Nanyun",,,Document-level Entity-based Extraction as Template Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TOPK COPY, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SCIREX dataset show new state-of-the-art results on REE (+3.26%), binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5257,5269,,,,,,,,,,,,,,,,WOS:000855966305030,0
C,"Jiang, YC; Bansal, M",,,Assoc Computat Linguist,"Jiang, Yichen; Bansal, Mohit",,,Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During inference, the model jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from <= 10% to 100%. With only 418 (5%) training instances, our approach still achieves 97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can be induced in Transformers given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention's query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6253,6265,,,,,,,,,,,,,,,,WOS:000860727000020,0
C,"Kasai, J; Peng, H; Zhang, YZ; Yogatama, D; Ilharco, G; Pappas, N; Mao, Y; Chen, WZ; Smith, NA",,,Assoc Computat Linguist,"Kasai, Jungo; Peng, Hao; Zhang, Yizhe; Yogatama, Dani; Ilharco, Gabriel; Pappas, Nikolaos; Mao, Yi; Chen, Weizhu; Smith, Noah A.",,,Finetuning Pretrained Transformers into RNNs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a significant computational cost, as the attention mechanism's complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10630,10643,,,,,,,,,,,,,,,,WOS:000860727004054,0
C,"Lee, JSY; Lim, HH; Webster, C",,,Assoc Computat Linguist,"Lee, John S. Y.; Lim, Ho Hung; Webster, Carol",,,Paraphrasing Compound Nominalizations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of unsupervised methods, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8023,8028,,,,,,,,,,,,,,,,WOS:000860727002010,0
C,"Liu, DZ; Qu, XY; Zhou, P",,,Assoc Computat Linguist,"Liu, Daizong; Qu, Xiaoye; Zhou, Pan",,,Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-toattend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-bystep for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed model performs better than the state-of-the-arts.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9302,9311,,,,,,,,,,,,,,,,WOS:000860727003035,0
C,"Liu, J; Chen, YF; Xu, JN",,,Assoc Computat Linguist,"Liu, Jian; Chen, Yufeng; Xu, Jinan",,,Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach - it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2716,2725,,,,,,,,,,,,,,,,WOS:000855966302073,0
C,"Ma, Y; Li, Q",,,Assoc Computat Linguist,"Ma, Yun; Li, Qing",,,Exploring Non-Autoregressive Text Style Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model sacrifices its transfer performance due to the lack of conditional dependence between output tokens. To this end, we investigate three techniques, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9267,9278,,,,,,,,,,,,,,,,WOS:000860727003032,0
C,"Mahendra, R; Aji, AF; Louvan, S; Rahman, F; Vania, C",,,Assoc Computat Linguist,"Mahendra, Rahmad; Aji, Alham Fikri; Louvan, Samuel; Rahman, Fahrurrozi; Vania, Clara",,,IndoNLI: A Natural Language Inference Dataset for Indonesian,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We adapt the data collection protocol for MNLI and collect similar to 18K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pretrained models in our data. The best performance on the expert-annotated data is still far below human performance (13.4% accuracy gap), suggesting that this test set is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this dataset can help accelerate progress in Indonesian NLP research.",,,,,,"Mahendra, Rahmad/0000-0003-1107-7073",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10511,10527,,,,,,,,,,,,,,,,WOS:000860727004045,0
C,"Orbach, M; Toledo-Ronen, O; Spector, A; Aharonov, R; Katz, Y; Slonim, N",,,Assoc Computat Linguist,"Orbach, Matan; Toledo-Ronen, Orith; Spector, Artem; Aharonov, Ranit; Katz, Yoav; Slonim, Noam",,,YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO - a new TSA evaluation dataset of open-domain user reviews. YASO contains 2215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9154,9173,,,,,,,,,,,,,,,,WOS:000860727003023,0
C,"Pope, Q; Fern, XLZ",,,Assoc Computat Linguist,"Pope, Quintin; Fern, Xiaoli Z.",,,Text Counterfactuals via Latent Optimization and Shapley-Guided Search,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model's prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5578,5593,,,,,,,,,,,,,,,,WOS:000855966305056,0
C,"Saxon, M; Levy, S; Wang, XY; Albalak, A; Wang, WY",,,Assoc Computat Linguist,"Saxon, Michael; Levy, Sharon; Wang, Xinyi; Albalak, Alon; Wang, William Yang",,,Modeling Disclosive Transparency in NLP Application Descriptions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Broader disclosive transparency-truth and clarity in communication regarding the function of AI systems-is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where too much information clouds a reader's understanding of what a system description means. Disclosive transparency's subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2023,2037,,,,,,,,,,,,,,,,WOS:000855966302012,0
C,"Sogaard, A",,,Assoc Computat Linguist,"Sogaard, Anders",,,Locke's Holiday: Belief Bias in Machine Reading,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of 'My kingdom for a cough drop, cried Queen Elizabeth.' Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called AUTO-LOCKE, to quantify such effects. Evaluations of machine reading systems on AUTO-LOCKE show the pervasiveness of belief bias in machine reading.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8240,8245,,,,,,,,,,,,,,,,WOS:000860727002027,0
C,"Song, XY; Salcianu, A; Song, Y; Dopson, D; Zhou, D",,,Assoc Computat Linguist,"Song, Xinying; Salcianu, Alex; Song, Yang; Dopson, Dave; Zhou, Denny",,,Fast WordPiece Tokenization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the Word-Piece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as maximum matching. The best known algorithms so far are O(n(2)) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel algorithm whose tokenization complexity is strictly O(n). Our method is inspired by the Aho-Corasick algorithm. We introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. For general text, we further propose an algorithm that combines pre-tokenization (splitting the text into words) and our linear-time Word-Piece method into a single pass. Experimental results show that our method is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2089,2103,,,,,,,,,,,,,,,,WOS:000855966302019,0
C,"Sun, HH; Zhong, JL; Ma, YP; Han, Z; He, K",,,Assoc Computat Linguist,"Sun, Haohai; Zhong, Jialun; Ma, Yunpu; Han, Zhen; He, Kun",,,TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult and faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing stateof-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8306,8319,,,,,,,,,,,,,,,,WOS:000860727002033,0
C,"Syed, S; Yousef, T; Al-Khatib, K; Janicke, S; Potthast, M",,,Assoc Computat Linguist,"Syed, Shahbaz; Yousef, Tariq; Al-Khatib, Khalid; Janicke, Stefan; Potthast, Martin",,,SUMMARY EXPLORER Visualizing the State of the Art in Text Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper introduces SUMMARY EXPLORER, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The tool complements existing approaches for locally debugging summarization models and improves upon them. The tool is available at https://tldr.webis.de/.",,,,,,"Yousef, Tariq/0000-0001-6136-3970; Potthast, Martin/0000-0003-2451-0665",,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,185,194,,,,,,,,,,,,,,,,WOS:000855241500022,0
C,"Vafa, K; Deng, YT; Blei, DM; Rush, AM",,,Assoc Computat Linguist,"Vafa, Keyon; Deng, Yuntian; Blei, David M.; Rush, Alexander M.",,,Rationales for Sequential Predictions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short finetuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10314,10332,,,,,,,,,,,,,,,,WOS:000860727004031,0
C,"Lai, VD; Dernoncourt, F; Nguyen, TH",,,Assoc Computat Linguist,"Viet Dac Lai; Dernoncourt, Franck; Thien Huu Nguyen",,,Learning Prototype Representations Across Few-Shot Tasks for Event Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among classifiers across tasks to make the model more robust to outliers. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our model is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5270,5277,,,,,,,,,,,,,,,,WOS:000855966305031,0
C,"Wagner, J; Foster, J",,,Assoc Computat Linguist,"Wagner, Joachim; Foster, Jennifer",,,Revisiting Tri-training of Dependency Parsers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised learning can be expected to have the most impact here. Based on treebank size and available ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and Vietnamese. Furthermore, we include English in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9457,9473,,,,,,,,,,,,,,,,WOS:000860727003046,0
C,"Wei, J",,,Assoc Computat Linguist,"Wei, Jason",,,Good-Enough Example Extrapolation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called good-enough example extrapolation (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than upsampling and other hidden-space data augmentation methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5923,5929,,,,,,,,,,,,,,,,WOS:000855966306009,0
C,"Xu, LY; Zhang, XC; Zhao, XJ; Chen, HF; Chen, F; Choi, JD",,,Assoc Computat Linguist,"Xu, Liyan; Zhang, Xuchao; Zhao, Xujiang; Chen, Haifeng; Chen, Feng; Choi, Jinho D.",,,Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer: Language Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 on average for NER and 2.5 accuracy score for NLI.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6716,6723,,,,,,,,,,,,,,,,WOS:000860727000053,0
C,"Yang, ZY; Yang, YF; Cer, D; Darve, E",,,Assoc Computat Linguist,"Yang, Ziyi; Yang, Yinfei; Cer, Daniel; Darve, Eric",,,A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method Language Information Removal (LIR) factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses simple linear operations, e.g. matrix factorization and orthogonal projection. LIR reveals that for weak-alignment multilingual systems, the principal components of semantic spaces primarily encodes language identity information. We first evaluate the LIR on a cross-lingual question answer retrieval task (LAReQA), which requires the strong alignment for the multilingual embedding space. Experiment shows that LIR is highly effectively on this task, yielding almost 100% relative improvement in MAP for weak-alignment models. We then evaluate the LIR on Amazon Reviews and XEVAL dataset, with the observation that removing language information is able to improve the cross-lingual transfer performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5825,5832,,,,,,,,,,,,,,,,WOS:000855966305074,0
C,"Yuan, XD",,,Assoc Computat Linguist,"Yuan, Xingdi",,,Interactive Machine Comprehension with Dynamic Knowledge Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good inductive biases, which can serve as an agent's memory mechanism in iMRC tasks. We explore four different categories of graphs that can capture text information at various levels. We describe methods that dynamically build and update these graphs during information gathering, as well as neural models to encode graph representations in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6734,6750,,,,,,,,,,,,,,,,WOS:000860727000055,0
C,"Zhang, L; Ding, JD; Xu, Y; Liu, YY; Zhou, SG",,,Assoc Computat Linguist,"Zhang, Lu; Ding, Jiandong; Xu, Yi; Liu, Yingyao; Zhou, Shuigeng",,,Weakly-supervised Text Classification Based on Keyword Graph,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat keywords independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our framework is an iterative process. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a text classifier to classify the unlabeled texts. Finally, we re-extract keywords from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2803,2813,,,,,,,,,,,,,,,,WOS:000855966302081,0
C,"Zhao, C; Xiong, CY; Boyd-Graber, J; Daume, H",,,Assoc Computat Linguist,"Zhao, Chen; Xiong, Chenyan; Boyd-Graber, Jordan; Daume, Hal, III",,,Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question-answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DISTDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DISTDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DISTDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9612,9622,,,,,,,,,,,,,,,,WOS:000860727003057,0
C,"Zhu, YC; Pang, L; Lan, YY; Shen, HW; Cheng, XQ",,,Assoc Computat Linguist,"Zhu, Yunchang; Pang, Liang; Lan, Yanyan; Shen, Huawei; Cheng, Xueqi",,,Adaptive Information Seeking for Open-Domain Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Information seeking is an essential step for open-domain question answering to efficiently gather evidence from a large corpus. Recently, iterative approaches have been proven to be effective for complex questions, by recursively retrieving new evidence at each step. However, almost all existing iterative approaches use predefined strategies, either applying the same retrieval function multiple times or fixing the order of different retrieval functions, which cannot fulfill the diverse requirements of various questions. In this paper, we propose a novel adaptive information-seeking strategy for open-domain question answering, namely AISO. Specifically, the whole retrieval and answer process is modeled as a partially observed Markov decision process, where three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and one answer operation are defined as actions. According to the learned policy, AISO could adaptively select a proper retrieval action to seek the missing evidence at each step, based on the collected evidence and the reformulated query, or directly output the answer when the evidence set is sufficient for the question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as single-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms all baseline methods with predefined strategies in terms of both retrieval and answer evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3615,3626,,,,,,,,,,,,,,,,WOS:000855966303065,0
C,"Bar-Haim, R; Kantor, Y; Venezian, E; Katz, Y; Slonim, N",,,Assoc Computat Linguist,"Bar-Haim, Roy; Kantor, Yoav; Venezian, Elad; Katz, Yoav; Slonim, Noam",,,Project Debater APIs: Decomposing the AI Grand Challenge,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics. Engaging in a live debate requires a diverse set of skills, and Project Debater has been developed accordingly as a collection of components, each designed to perform a specific subtask. Project Debater APIs provide access to many of these capabilities, as well as to more recently developed ones. This diverse set of web services, publicly available for academic use, includes core NLP services, argument mining and analysis capabilities, and higher-level services for content summarization. We describe these APIs and their performance, and demonstrate how they can be used for building practical solutions. In particular, we will focus on Key Point Analysis, a novel technology that identifies the main points and their prevalence in a collection of texts such as survey responses and user reviews.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,267,274,,,,,,,,,,,,,,,,WOS:000855241500031,0
C,"Bogin, B; Gupta, S; Gardner, M; Berant, J",,,Assoc Computat Linguist,"Bogin, Ben; Gupta, Shivanshu; Gardner, Matt; Berant, Jonathan",,,COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higherorder operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zero- or few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-ofthe-art pre-trained language-and-vision models struggle to compositionally generalize.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9824,9846,,,,,,,,,,,,,,,,WOS:000860727003075,0
C,"Chakrabarty, T; Saakyan, A; Muresan, S",,,Assoc Computat Linguist,"Chakrabarty, Tuhin; Saakyan, Arkadiy; Muresan, Smaranda",,,Don't Go Far Off : An Empirical Study on Neural Poetry Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zeroshot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7253,7265,,,,,,,,,,,,,,,,WOS:000860727001025,0
C,"Colombo, P; Staerman, G; Clavel, C; Piantanida, P",,,Assoc Computat Linguist,"Colombo, Pierre; Staerman, Guillaume; Clavel, Chloe; Piantanida, Pablo",,,Automatic Text Evaluation through the Lens of Wasserstein Barycenters,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, i.e., Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (e.g., MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10450,10466,,,,,,,,,,,,,,,,WOS:000860727004041,0
C,"Faghihi, HR; Guo, Q; Uszok, A; Nafar, A; Raisi, E; Kordjarnshidi, P",,,Assoc Computat Linguist,"Faghihi, Hossein Rajaby; Guo, Quan; Uszok, Andrzej; Nafar, Aliakbar; Raisi, Elaheh; Kordjarnshidi, Parisa",,,DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models. The domain knowledge can be defined explicitly, which improves the explainability of the models in addition to their performance and generalizability in the low-data regime. Several approaches for such integration of symbolic and sub-symbolic models have been introduced; however, there is no library to facilitate the programming for such integration in a generic way while various underlying algorithms can be used. Our library aims to simplify programming for such integration in both training and inference phases while separating the knowledge representation from learning algorithms. We showcase various NLP benchmark tasks and beyond. The framework is publicly available at Github(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,231,241,,,,,,,,,,,,,,,,WOS:000855241500027,0
C,"Feng, B; Lou, Q; Jiang, L; Fox, G",,,Assoc Computat Linguist,"Feng, Bo; Lou, Qian; Jiang, Lei; Fox, Geoffrey",,,CRYPTOGRU: Low Latency Privacy-Preserving Text Model Inference,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users' privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent unit (GRU) network, CRYPTOGRU, for low-latency secure inferences. CRYPTOGRU replaces computationally expensive GC-based tanh with fast GC-based ReLU, and then quantizes sigmoid and ReLU to smaller bit-length to accelerate activations in a GRU. We evaluate CRYPToGRU with multiple GRU models trained on 4 public datasets. Experimental results show CRYPTOGRU achieves top-notch accuracy and improves the secure inference latency by up to 138 x over one of the state-of-the-art secure networks on the Penn Treebank dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2052,2057,,,,,,,,,,,,,,,,WOS:000855966302015,0
C,"Fried, D; Chiu, JT; Klein, D",,,Assoc Computat Linguist,"Fried, Daniel; Chiu, Justin T.; Klein, Dan",,,Reference-Centric Models for Grounded Collaborative Dialogue,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner's utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa, 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2130,2147,,,,,,,,,,,,,,,,WOS:000855966302022,0
C,"Iki, T; Aizawa, A",,,Assoc Computat Linguist,"Iki, Taichi; Aizawa, Akiko",,,Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We compare five V&L models, including single-stream and dual-stream models, trained with the same pre-training Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V&L extensions.",,,,,"Iki, Taichi/HII-7522-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2189,2196,,,,,,,,,,,,,,,,WOS:000855966302026,0
C,"Lange, L; Strotgen, J; Adel, H; Klakow, D",,,Assoc Computat Linguist,"Lange, Lukas; Stroetgen, Jannik; Adel, Heike; Klakow, Dietrich",,,To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity - as suggested in prior work may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on sequence labeling across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F-1 points.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8744,8753,,,,,,,,,,,,,,,,WOS:000860727002067,0
C,"Li, YJ; Zhao, CY; Caragea, C",,,Assoc Computat Linguist,"Li, Yingjie; Zhao, Chenye; Caragea, Cornelia",,,Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one model on each dataset and datasets of different domains, respectively. We show that models can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the state-of-the-art by a large margin. We publicly release our code.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6332,6345,,,,,,,,,,,,,,,,WOS:000860727000026,0
C,"Li, YY; Wang, J; Yu, B",,,Assoc Computat Linguist,"Li, Yingya; Wang, Jun; Yu, Bei",,,Detecting Health Advice in Medical Research Literature,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as strong advice, weak advice, or no advice, and developed a BERT-based model that can predict, with a macro-averaged F1-score of 0.93, whether a sentence gives strong advice, weak advice, or not. The prediction model generalized well to sentences in both unstructured abstracts and discussion sections, where health advice normally appears. We also conducted a case study that applied this prediction model to retrieve specific health advice on COVID-19 treatments from LitCovid, a large COVID research literature portal, demonstrating the usefulness of retrieving health advice sentences as an advanced research literature navigation function for health researchers and the general public.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6018,6029,,,,,,,,,,,,,,,,WOS:000860727000001,0
C,"Miao, YS; Blunsom, P; Specia, L",,,Assoc Computat Linguist,"Miao, Yishu; Blunsom, Phil; Specia, Lucia",,,A Generative Framework for Simultaneous Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6697,6706,,,,,,,,,,,,,,,,WOS:000860727000051,0
C,"Nayyeri, M; Xu, CJ; Hoffmann, F; Alam, MM; Lehmann, J; Vahdati, S",,,Assoc Computat Linguist,"Nayyeri, Mojtaba; Xu, Chengjin; Hoffmann, Franca; Alam, Mirza Mohtashim; Lehmann, Jens; Vahdati, Sahar",,,Knowledge Graph Representation Learning using Ordinary Differential Equations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space. The capability of KGEs in preserving graph characteristics including structural aspects and semantics, highly depends on the design of their score function, as well as the inherited abilities from the underlying geometry. Many KGEs use the Euclidean geometry which renders them incapable of preserving complex structures and consequently causes wrong inferences by the models. To address this problem, we propose a neuro differential KGE that embeds nodes of a KG on the trajectories of Ordinary Differential Equations (ODEs). To this end, we represent each relation (edge) in a KG as a vector field on several manifolds. We specifically parameterize ODEs by a neural network to represent complex manifolds and complex vector fields on the manifolds. Therefore, the underlying embedding space is capable to assume the shape of various geometric forms to encode heterogeneous subgraphs. Experiments on synthetic and benchmark datasets using state-of-the-art KGE models justify the ODE trajectories as a means to enable structure preservation and consequently avoiding wrong inferences.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9529,9548,,,,,,,,,,,,,,,,WOS:000860727003051,0
C,"Ni, AS; Gardner, M; Dasigi, P",,,Assoc Computat Linguist,"Ni, Ansong; Gardner, Matt; Dasigi, Pradeep",,,Mitigating False-Negative Contexts in Multi-Document Question Answering with Retrieval Marginalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as Wikipedia, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during annotation, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model cannot rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint modeling with marginalization improves model performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6149,6161,,,,,,,,,,,,,,,,WOS:000860727000012,0
C,"Park, S; Li, H; Patel, A; Mudgal, S; Lee, S; Kim, YB; Matsoukas, S; Sarikaya, R",,,Assoc Computat Linguist,"Park, Sunghyun; Li, Han; Patel, Ameen; Mudgal, Sidharth; Lee, Sungjin; Kim, Young-Bum; Matsoukas, Spyros; Sarikaya, Ruhi",,,A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system across 10 domains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6054,6063,,,,,,,,,,,,,,,,WOS:000860727000004,0
C,"Perez-Mayos, L; Ballesteros, M; Wanner, L",,,Assoc Computat Linguist,"Perez-Mayos, Laura; Ballesteros, Miguel; Wanner, Leo",,,How much pretraining data do language models need to learn syntax?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformers-based pretrained language models achieve outstanding results in many wellknown NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1571,1582,,,,,,,,,,,,,,,,WOS:000855966301051,0
C,"Provilkov, I; Malinin, A",,,Assoc Computat Linguist,"Provilkov, Ivan; Malinin, Andrey",,,Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is dataset length-bias - NMT datasets are strongly biased towards short sentences. To mitigate this issue, we propose a new data augmentation technique - Multi-Sentence Resampling (MSR). This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8612,8621,,,,,,,,,,,,,,,,WOS:000860727002055,0
C,"Pyatkin, V; Roit, P; Michael, J; Tsarfaty, R; Goldberg, Y; Dagan, I",,,Assoc Computat Linguist,"Pyatkin, Valentina; Roit, Paul; Michael, Julian; Tsarfaty, Reut; Goldberg, Yoav; Dagan, Ido",,,Asking It All: Generating Contextualized Questions for any Semantic Role,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1429,1441,,,,,,,,,,,,,,,,WOS:000855966301041,0
C,"Subramanian, S; Rahimi, A; Baldwin, T; Cohn, T; Frermann, L",,,Assoc Computat Linguist,"Subramanian, Shivashankar; Rahimi, Afshin; Baldwin, Timothy; Cohn, Trevor; Frermann, Lea",,,Fairness-aware Class Imbalanced Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2045,2051,,,,,,,,,,,,,,,,WOS:000855966302014,0
C,"Sun, SM; Zhao, WL; Manjunatha, V; Jain, R; Morariu, V; Dernoncourt, F; Srinivasan, BV; Iyyer, M",,,Assoc Computat Linguist,"Sun, Simeng; Zhao, Wenlong; Manjunatha, Varun; Jain, Rajiv; Morariu, Vlad; Dernoncourt, Franck; Srinivasan, Balaji Vasan; Iyyer, Mohit",,,IGA: An Intent-Guided Authoring Assistant,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confine the quality of IGA's generated outputs, while a small-scale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into Al-assisted writing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5972,5985,,,,,,,,,,,,,,,,WOS:000855966306013,0
C,"Vaeth, D; Tilli, P; Vu, NT",,,Assoc Computat Linguist,"Vath, Dirk; Tilli, Pascal; Ngoc Thang Vu",,,Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise. Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the data sample level. As proof of concept, we perform a case study on four models. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they cannot recognize text in images Our metrics allow us to quantify which image and question embeddings provide most robustness to a model. All code s is publicly available.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,114,123,,,,,,,,,,,,,,,,WOS:000855241500014,0
C,"Ye, S; Kim, J; Oh, A",,,Assoc Computat Linguist,"Ye, Seonghyeon; Kim, Jiseon; Oh, Alice",,,Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation, we apply contrastive learning on projected embeddings of original and augmented examples. When fine-tuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is achieved with only 70% of computational memory compared to the baseline model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1832,1838,,,,,,,,,,,,,,,,WOS:000855966301071,0
C,"Arefyev, N; Kharchev, D; Shelmanov, A",,,Assoc Computat Linguist,"Arefyev, Nikolay; Kharchev, Dmitrii; Shelmanov, Artem",,,NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand, which are likely more relevant than the most frequent words. The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9114,9124,,,,,,,,,,,,,,,,WOS:000860727003019,0
C,"Batra, S; Jain, S; Heidari, P; Arun, A; Youngs, C; Li, XT; Donmez, P; Mei, S; Kuo, SZ; Bhardwaj, V; Kumar, A; White, M",,,Assoc Computat Linguist,"Batra, Soumya; Jain, Shashank; Heidari, Peyman; Arun, Ankit; Youngs, Catharine; Li, Xintong; Donmez, Pinar; Mei, Shawn; Kuo, Shiun-Zu; Bhardwaj, Vikas; Kumar, Anuj; White, Michael",,,Building Adaptive Acceptability Classifiers for Neural NLG,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and grammatical. We don't make use of any human references making the classifiers suitable for run-time deployment. Training data for the classifiers is obtained using a 2-stage approach of first generating synthetic data using a combination of existing and new model-based approaches followed by a novel validation framework to filter and sort the synthetic data into acceptable and unacceptable classes. Our 2-stage approach adapts to a wide range of data representations and does not require additional data beyond what the NLG models are trained on. It is also independent of the underlying NLG model architecture, and is able to generate more realistic samples close to the distribution of the NLG model-generated responses. We present results on 5 datasets (WebNLG, Cleaned E2E, ViGGO, Alarm, and Weather) with varying data representations. We compare our framework with existing techniques that involve synthetic data generation using simple sentence transformations and/or model-based techniques, and show that building acceptability classifiers using data that resembles the generation model outputs followed by a validation framework outperforms the existing techniques, achieving state-of-the-art results. We also show that our techniques can be used in few-shot settings using self-training.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,682,697,,,,,,,,,,,,,,,,WOS:000855966300053,0
C,"Berard, A; Lee, D; Clinchant, S; Jung, K; Nikoulina, V",,,Assoc Computat Linguist,"Berard, Alexandre; Lee, Dain; Clinchant, Stephane; Jung, Kweonwoo; Nikoulina, Vassilina",,,Efficient Inference for Multilingual Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several light decoder architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to more than x2 faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8563,8583,,,,,,,,,,,,,,,,WOS:000860727002052,0
C,"Bhardwaj, P; Kelleher, J; Costabello, L; O'Sullivan, D",,,Assoc Computat Linguist,"Bhardwaj, Peru; Kelleher, John; Costabello, Luca; O'Sullivan, Dec",,,Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model's predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62% over the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8225,8239,,,,,,,,,,,,,,,,WOS:000860727002026,0
C,"Che, WX; Feng, YL; Qin, LB; Liu, T",,,Assoc Computat Linguist,"Che, Wanxiang; Feng, Yunlong; Qin, Libo; Liu, Ting",,,N-LTP: An Open-source Neural Language Technology Platform for Chinese,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, pan-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https : //github.com/HIT-SCIR/ltp.",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,42,49,,,,,,,,,,,,,,,,WOS:000855241500006,0
C,"Chen, YM; Zhang, Y; Zhang, C; Lee, G; Cheng, R; Li, HZ",,,Assoc Computat Linguist,"Chen, Yiming; Zhang, Yan; Zhang, Chen; Lee, Grandee; Cheng, Ran; Li, Haizhou",,,Revisiting Self-Training for Few-Shot Learning of Language Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As unlabeled data carry rich task-relevant information, they are proven useful for fewshot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and fewshot knowledge transfer across tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9125,9135,,,,,,,,,,,,,,,,WOS:000860727003020,0
C,"Ding, SY; Junczys-Dowmunt, M; Post, M; Koehn, P",,,Assoc Computat Linguist,"Ding, Shuoyang; Junczys-Dowmunt, Marcin; Post, Matt; Koehn, Philipp",,,Levenshtein Training forWord-level Quality Estimation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human postediting data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6724,6733,,,,,,,,,,,,,,,,WOS:000860727000054,0
C,"Han, K; Lee, S; Lee, W; Lee, J; Lee, DH",,,Assoc Computat Linguist,"Han, Kijong; Lee, Seojin; Lee, Wooin; Lee, Joosung; Lee, Dong-hun",,,An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2338,2344,,,,,,,,,,,,,,,,WOS:000855966302039,0
C,"Hong, HW; Zhang, JF; Zhang, Y; Wan, Y; Sui, YL",,,Assoc Computat Linguist,"Hong, Haiwen; Zhang, Jingfeng; Zhang, Yin; Wan, Yao; Sui, Yulei",,,Fix-Filter-Fix: Intuitively Connect Any Models for Effective Bug Fixing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input buggy code (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the buggy code that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or F-3) for bug fixing. F-3 connects models with our filter mechanism to filter out the last model's unchanged fix to the next. We propose an F-3 theory that can quantitatively and accurately calculate the F-3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic F-3 instances, called F-ST+AT(3) and F-AT+ST(3). Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of F-3 and corroborates our findings and methodology.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3495,3504,,,,,,,,,,,,,,,,WOS:000855966303054,0
C,"Khalifa, M; Ballesteros, M; McKeown, K",,,Assoc Computat Linguist,"Khalifa, Muhammad; Ballesteros, Miguel; McKeown, Kathleen",,,A Bag of Tricks for Dialogue Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning about the situation, and informal language understanding. Using a pretrained sequence-to-sequence language model, we explore speaker name substitution, negation scope highlighting, multi-task learning with relevant tasks, and pretraining on in-domain data. Our experiments show that our proposed techniques indeed improve summarization performance, outperforming strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8014,8022,,,,,,,,,,,,,,,,WOS:000860727002009,0
C,"Kung, PN; Chen, YC; Yin, SS; Yang, TH; Chen, YN",,,Assoc Computat Linguist,"Kung, Po-Nien; Chen, Yi-Cheng; Yin, Sheng-Siang; Yang, Tse-Hsuan; Chen, Yun-Nung",,,Efficient Multi-Task Auxiliary Learning: Selecting Auxiliary Data by Feature Similarity,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-task auxiliary learning utilizes a set of relevant auxiliary tasks to improve the performance of a primary task. A common usage is to manually select multiple auxiliary tasks for multi-task learning on all data, which raises two issues: (1) selecting beneficial auxiliary tasks for a primary task is nontrivial; (2) when the auxiliary datasets are large, training on all data becomes time-expensive and impractical. Therefore, this paper focuses on addressing these problems and proposes a time-efficient sampling method to select the data that is most relevant to the primary task. The proposed method allows us to only train on the most beneficial sub-datasets from the auxiliary tasks, achieving efficient multi-task auxiliary learning. The experiments on three benchmark datasets (RTE, MRPC, STS-B) show that our method significantly outperforms random sampling and ST-DNN. Also, by applying our method, the model can surpass fully-trained MT-DNN on RTE, MRPC, STS-B, using only 50%, 66%, and 1% of data, respectively.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,416,428,,,,,,,,,,,,,,,,WOS:000855966300034,0
C,"Lei, T",,,Assoc Computat Linguist,"Lei, Tao",,,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as ENWIK8, WIKI-103 and BILLION WORD datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to topperforming Transformer models. For instance, our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7633,7648,,,,,,,,,,,,,,,,WOS:000860727001050,0
C,"Liu, C; Wang, R; Liu, JH; Sun, J; Huang, F; Si, L",,,Assoc Computat Linguist,"Liu, Che; Wang, Rui; Liu, Jinghua; Sun, Jian; Huang, Fei; Si, Luo",,,DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a contextaware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman's correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2396,2406,,,,,,,,,,,,,,,,WOS:000855966302044,0
C,"Liu, HL; Yu, T; Li, P",,,Assoc Computat Linguist,"Liu, Haoliang; Yu, Tan; Li, Ping",,,Inflate and Shrink: Enriching and Reducing Interactions for Fast Text-Image Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"By exploiting the cross-modal attention, cross-BERT methods have achieved state-of-the-art accuracy in cross-modal retrieval. Nevertheless, the heavy text-image interactions in the cross-BERT model are prohibitively slow for large-scale retrieval. Late-interaction methods trade off retrieval accuracy and efficiency by exploiting cross-modal interaction only in the late stage, attaining a satisfactory retrieval speed. In this work, we propose an inflating and shrinking approach to further boost the efficiency and accuracy of late-interaction methods. The inflating operation plugs several codes in the input of the encoder to exploit the text-image interactions more thoroughly for higher retrieval accuracy. Then the shrinking operation gradually reduces the text-image interactions through knowledge distilling for higher efficiency. Through an inflating operation followed by a shrinking operation, both efficiency and accuracy of a late-interaction model are boosted. Systematic experiments on public benchmarks demonstrate the effectiveness of our inflating and shrinking approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9796,9809,,,,,,,,,,,,,,,,WOS:000860727003073,0
C,"Ma, ZY; Li, JJ; Zhang, ZZ; Li, GH; Cheng, YJ",,,Assoc Computat Linguist,"Ma, Zhiyuan; Li, Jianjun; Zhang, Zezheng; Li, Guohui; Cheng, Yongjing",,,Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models' generated response is still limited, mainly due to their lack of fine-grained reasoning on deterministic knowledge (w.rt. conceptual tokens), which makes them difficult to capture the concept shifts and identify user's real intention in cross-task scenarios. To address these issues, we propose a novel intention mechanism to better model deterministic entity knowledge. Based on such a mechanism, we further propose an intention reasoning network (IR-Net), which consists of joint and multi-hop reasoning, to obtain intention-aware representations of conceptual tokens that can be used to capture the concept shifts involved in task-oriented conversations, so as to effectively identify user's intention and generate more accurate responses. Experimental results verify the effectiveness of IR-Net, showing that it achieves the stateof-the-art performance on two representative multi-domain dialog datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2273,2285,,,,,,,,,,,,,,,,WOS:000855966302033,0
C,"Madotto, A; Lin, ZJ; Zhou, ZP; Moon, S; Crook, P; Liu, B; Yu, Z; Cho, E; Fung, P; Wang, ZG",,,Assoc Computat Linguist,"Madotto, Andrea; Lin, Zhaojiang; Zhou, Zhenpeng; Moon, Seungwhan; Crook, Paul; Liu, Bing; Yu, Zhou; Cho, Eunjoon; Fung, Pascale; Wang, Zhiguang",,,Continual Learning in Task-Oriented Dialogue Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Continual learning in task-oriented dialogue systems allows the system to add new domains and functionalities over time after deployment, without incurring the high cost of retraining the whole system each time. In this paper, we propose a first-ever continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in both modularized and end-to-end learning settings. In addition, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. We also suggest that the upper bound performance of continual learning should be equivalent to multitask learning when data from all domain is available at once. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform better, by a large margin, compared to other continuous learning techniques, and only slightly worse than the multitask learning upper bound while being 20X faster in learning new domains. We also report several trade-offs in terms of parameter usage, memory size and training time, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released to promote more research in this direction(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7452,7467,,,,,,,,,,,,,,,,WOS:000860727001038,0
C,"Min, S; Lee, K; Chang, MW; Toutanova, K; Hajishirzi, H",,,Assoc Computat Linguist,"Min, Sewon; Lee, Kenton; Chang, Ming-Wei; Toutanova, Kristina; Hajishirzi, Hannaneh",,,Joint Passage Ranking for Diverse Multi-Answer Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study multi-answer retrieval, an underexplored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. In this paper, we introduce JPR, the first joint passage retrieval model for multi-answer retrieval. JPR makes use of an autoregressive reranker that selects a sequence of passages, each conditioned on previously selected passages. JPR is trained to select passages that cover new answers at each timestep and uses a tree-decoding algorithm to enable flexibility in the degree of diversity. Compared to prior approaches, JPR achieves significantly better answer coverage on three multianswer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6997,7008,,,,,,,,,,,,,,,,WOS:000860727001008,0
C,"O'Neill, J; Rozenshtein, P; Kiryo, R; Kubota, M; Bollegala, D",,,Assoc Computat Linguist,"O'Neill, James; Rozenshtein, Polina; Kiryo, Ryuichi; Kubota, Motoko; Bollegala, Danushka",,,"I Wish I Would Have Loved This One, But I Didn't - A Multilingual Dataset for Counterfactual Detection in Product Reviews",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of ecommerce reviews, and provides high quality professional annotations. We train CFD models using different text representation methods and classifiers. We find that these models are robust against the selectional biases introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying machine translation on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7092,7108,,,,,,,,,,,,,,,,WOS:000860727001016,0
C,"Rehbein, I; Ponzetto, SP; Stuckenschmidt, H; Adendorf, A; Bahnsen, O; Stoetzer, L",,,Assoc Computat Linguist,"Rehbein, Ines; Ponzetto, Simone Paolo; Stuckenschmidt, Heiner; Adendorf, Anna; Bahnsen, Oke; Stoetzer, Lukas",,,Come Hither or Go Away? Recognising Pre-Electoral Coalition Signals in the News,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct tasks: (i) predicting whether a reported statement from a politician or a journalist refers to a potential coalition and (ii) predicting the polarity of the signal - namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of multi-task learning and investigate which setup and task formulation is best suited for each sub-task. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, Germany, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.",,,,,"Bahnsen, Oke/AAZ-2731-2020","Bahnsen, Oke/0000-0003-3198-2804; Ponzetto, Simone Paolo/0000-0001-7484-2049; Stuckenschmidt, Heiner/0000-0002-0209-3859",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7798,7810,,,,,,,,,,,,,,,,WOS:000860727001063,0
C,"Xiao, ST; Liu, Z; Shao, YX; Lian, DF; Xie, X",,,Assoc Computat Linguist,"Xiao, Shitao; Liu, Zheng; Shao, Yingxia; Lian, Defu; Xie, Xing",,,Matching-oriented Product Quantization For Ad-hoc Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Product quantization (PQ) is a widely used technique for ad-hoc retrieval. Recent studies propose supervised PQ, where the embedding and quantization models can be jointly trained with supervised learning. However, there is a lack of appropriate formulation of the joint training objective; thus, the improvements over previous non-supervised baselines are limited in reality. In this work, we propose the Matching-oriented Product Quantization (MoPQ), where a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the minimization of MCL, we are able to maximize the matching probability of query and ground-truth key, which contributes to the optimal retrieval accuracy. Given that the exact computation of MCL is intractable due to the demand of vast contrastive samples, we further propose the Differentiable Cross-device Sampling (DCS), which significantly augments the contrastive samples for precise approximation of MCL. We conduct extensive experimental studies on four real-world datasets, whose results verify the effectiveness of MoPQ.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8119,8129,,,,,,,,,,,,,,,,WOS:000860727002018,0
C,"Yang, WK; Lin, YK; Li, P; Zhou, J; Sun, X",,,Assoc Computat Linguist,"Yang, Wenkai; Lin, Yankai; Li, Peng; Zhou, Jie; Sun, Xu",,,RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Backdoor attacks, which maliciously control a well-trained model's outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods.",,,,,,"Li, Peng/0000-0003-1374-5979",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8365,8381,,,,,,,,,,,,,,,,WOS:000860727002037,0
C,"Yu, D; Sagae, K",,,Assoc Computat Linguist,"Yu, Dian; Sagae, Kenji",,,Automatically Exposing Problems with Dialog Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including reinforcement learning to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,456,470,,,,,,,,,,,,,,,,WOS:000855966300037,0
C,"Yuan, Z; Taslimipoor, S; Davis, C; Bryant, C",,,Assoc Computat Linguist,"Yuan, Zheng; Taslimipoor, Shiva; Davis, Christopher; Bryant, Christopher",,,Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for English. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines GED and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8722,8736,,,,,,,,,,,,,,,,WOS:000860727002065,0
C,"Zhang, ZS; Strubell, E; Hovy, E",,,Assoc Computat Linguist,"Zhang, Zhisong; Strubell, Emma; Hovy, Eduard",,,On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant. Cross-lingual learning can improve performance in this setting by transferring knowledge from high-resource languages to low-resource ones. Moreover, we hypothesize that annotations of syntactic dependencies can be leveraged to further facilitate cross-lingual transfer. In this work, we perform an empirical exploration of the helpfulness of syntactic supervision for cross-lingual SRL within a simple multitask learning scheme. With comprehensive evaluations across ten languages (in addition to English) and three SRL benchmark datasets, including both dependency- and span-based SRL, we show the effectiveness of syntactic supervision in low-resource scenarios.",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6229,6246,,,,,,,,,,,,,,,,WOS:000860727000018,0
C,"Zhou, WX; Chen, MH",,,Assoc Computat Linguist,"Zhou, Wenxuan; Chen, Muhao",,,Learning from Noisy Labels for Entity-Centric Information Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5381,5392,,,,,,,,,,,,,,,,WOS:000855966305041,0
C,"Bai, F; Ritter, A; Xu, W",,,Assoc Computat Linguist,"Bai, Fan; Ritter, Alan; Xu, Wei",,,Pre-train or Annotate? Domain Adaptation with a Constrained Budget,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we view domain adaptation with a constrained budget as a consumer choice problem, where the goal is to select an optimal combination of data annotation and pre-training. We measure annotation costs of three procedural text datasets, along with the pre-training costs of several in-domain language models. The utility of different combinations of pre-training and data annotation are evaluated under varying budget constraints to assess which combination strategy works best. We find that for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, however, a combination of data annotation and in-domain pre-training yields better performance. Our experiments suggest task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5002,5015,,,,,,,,,,,,,,,,WOS:000855966305013,0
C,"Berger, N; Riezler, S; Sokolov, A; Ebert, S",,,Assoc Computat Linguist,"Berger, Nathaniel; Riezler, Stefan; Sokolov, Artem; Ebert, Sebastian",,,Don't Search for a Search Method - Simple Heuristics Suffice for Adversarial Text Attacks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple heuristics exploiting nearest neighbors without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8216,8224,,,,,,,,,,,,,,,,WOS:000860727002025,0
C,"Chen, H; Xia, R; Yu, JF",,,Assoc Computat Linguist,"Chen, Hao; Xia, Rui; Yu, Jianfei",,,Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to word embeddings, which cannot address the spurious association problem. In this work, we propose an end-toend reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics: 1) the generator automatically generates massive and diverse antonymous sentences; 2) the discriminator contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples; 3) the discriminator is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach's advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.",,,,,"Yu, Jianfei/GYD-3660-2022","Yu, Jianfei/0000-0001-8380-0609",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,269,278,,,,,,,,,,,,,,,,WOS:000855966300024,0
C,"Chi, TC; Rudnicky, AI",,,Assoc Computat Linguist,"Chi, Ta-Chung; Rudnicky, Alexander, I",,,Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all reply-to links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a zero-shot dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10% of the data, we achieve nearly the same performance of using the full dataset(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4897,4902,,,,,,,,,,,,,,,,WOS:000855966305004,0
C,"Due, D; dos Santos, CN; Ng, P; Athiwaratkun, B; Xiang, B; Gardner, M; Singh, S",,,Assoc Computat Linguist,"Due, Dheeru; dos Santos, Cicero Nogueira; Ng, Patrick; Athiwaratkun, Ben; Xiang, Bing; Gardner, Matt; Singh, Sameer",,,Generative Context Pair Selection for Multi-hop Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Compositional reasoning tasks such as multihop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, our proposed generative passage selection model, while being comparable to the state-of-the-art answering performance, has a better performance (4.9% higher than baseline) on an adversarial held-out set that tests the robustness of model's multi-hop reasoning capabilities.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7009,7015,,,,,,,,,,,,,,,,WOS:000860727001009,0
C,"Gao, LY; Callan, J",,,Assoc Computat Linguist,"Gao, Luyu; Callan, Jamie",,,Condenser: a Pre-training Architecture for Dense Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs' internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,981,993,,,,,,,,,,,,,,,,WOS:000855966301008,0
C,"Gravellier, L; Hunter, J; Muller, P; Pellegrini, T; Ferrane, I",,,Assoc Computat Linguist,"Gravellier, Lila; Hunter, Julie; Muller, Philippe; Pellegrini, Thomas; Ferrane, Isabelle",,,Weakly supervised discourse segmentation for multiparty oral conversations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has linguistic features foreign to written text. Segmentation is less studied for this type of language, where annotated data is scarce, and existing corpora more heterogeneous. We develop a weak supervision approach to adapt, using minimal annotation, a state of the art discourse segmenter trained on written text to French conversation transcripts. Supervision is given by a latent model boot-strapped by manually defined heuristic rules that use linguistic and acoustic information. The resulting model improves the original segmenter, especially in contexts where information on speaker turns is lacking or noisy, gaining up to 13% in F-score. Evaluation is performed on data like those used to define our heuristic rules, but also on transcripts from two other corpora.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1381,1392,,,,,,,,,,,,,,,,WOS:000855966301037,0
C,"Guibon, G; Labeau, M; Flamein, H; Lefeuvre, L; Clavel, C",,,Assoc Computat Linguist,"Guibon, Gael; Labeau, Matthieu; Flamein, Helene; Lefeuvre, Luce; Clavel, Chloe",,,Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones. The code for ProtoSeq is available at https://github.com/gguibon/ProtoSeq.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6858,6870,,,,,,,,,,,,,,,,WOS:000860727000064,0
C,"Jain, N; Gupta, V; Rai, A; Kumar, G",,,Assoc Computat Linguist,"Jain, Nupur; Gupta, Vivek; Rai, Anshul; Kumar, Gaurav",,,TABPERT: An Effective Platform for Tabular Perturbation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To truly grasp reasoning ability, a Natural Language Inference model should be evaluated on counterfactual data. TABPERT facilitates this by assisting in the generation of such counterfactual data for assessing model tabular reasoning issues. TABPERT allows a user to update a table, change its associated hypotheses, change their labels, and highlight rows that are relevant for the hypothesis classification. TABPERT also captures information about the techniques used to automatically produce the table, as well as the strategies employed to generate the challenging hypotheses. These counterfactual tables and hypotheses, as well as the metadata, can then be used to explore an existing model's shortcomings methodically and quantitatively.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,350,360,,,,,,,,,,,,,,,,WOS:000855241500039,0
C,"Jain, P; Jain, A; Zhang, TJ; Abbeel, P; Gonzalez, JE; Stoica, I",,,Assoc Computat Linguist,"Jain, Paras; Jain, Ajay; Zhang, Tianjun; Abbeel, Pieter; Gonzalez, Joseph E.; Stoica, Ion",,,Contrastive Code Representation Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training outperforms RoBERTa on an adversarial code clone detection benchmark by 39% AUROC. Surprisingly, improved adversarial robustness translates to better accuracy over natural code; ContraCode improves sumnarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines. All source is available at https://github.com/parasj/contracode.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5954,5971,,,,,,,,,,,,,,,,WOS:000855966306012,0
C,"Jiang, CY; Jin, ZJ; Tu, KW",,,Assoc Computat Linguist,"Jiang, Chengyue; Jin, Zijian; Tu, Kewei",,,Neuralizing Regular Expressions for Slot Filling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural models and symbolic rules such as regular expressions have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting regular expressions into neural networks. Specifically, we first convert regular expressions into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via sequence labeling. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9481,9498,,,,,,,,,,,,,,,,WOS:000860727003048,0
C,"Li, ML; Li, S; Wang, ZHL; Huang, LF; Cho, K; Ji, H; Han, JW; Voss, C",,,Assoc Computat Linguist,"Li, Manling; Li, Sha; Wang, Zhenhailong; Huang, Lifu; Cho, Kyunghyun; Ji, Heng; Han, Jiawei; Voss, Clare",,,The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and argument relations. In addition, we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema. To build and evaluate such schemas, we release a new schema learning corpus containing 6,399 documents accompanied with event graphs, and we have manually constructed gold-standard schemas. Intrinsic evaluations by schema matching and instance graph perplexity, prove the superior quality of our probabilistic graph schema library compared to linear representations. Extrinsic evaluation on schema-guided future event prediction further demonstrates the predictive power of our event graph model, significantly outperforming human schemas and baselines by more than 23.8% on HITS@1.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5203,5215,,,,,,,,,,,,,,,,WOS:000855966305026,0
C,"Li, ZH; Tomar, Y; Passonneau, RJ",,,Assoc Computat Linguist,"Li, Zhaohui; Tomar, Yajur; Passonneau, Rebecca J.",,,A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Automatic short answer grading (ASAG) is the task of assessing students' short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging multiple choice questions. We propose a Semantic Feature-wise transformation Relation Network (SFRN) that exploits the multiple components of ASAG datasets more effectively. SFRN captures relational knowledge among the questions (Q), reference answers or rubrics (R), and labeled student answers (A). A relation network learns vector representations for the elements of QRA triples, then combines the learned representations using learned semantic feature-wise transformations. We apply translation-based data augmentation to address the two problems of limited training data, and high data skew for multi-class ASAG tasks. Our model has up to 11% performance improvement over state-of-the-art results on the benchmark SemEval-2013 datasets, and surpasses custom approaches designed for a Kaggle challenge, demonstrating its generality.",,,,,,"Passonneau, Rebecca/0000-0001-8626-811X",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6030,6040,,,,,,,,,,,,,,,,WOS:000860727000002,0
C,"Liang, B; Su, H; Yin, RD; Gui, L; Yang, M; Zhao, Q; Yu, XQ; Xu, RF",,,Assoc Computat Linguist,"Liang, Bin; Su, Hang; Yin, Rongdi; Gui, Lin; Yang, Min; Zhao, Qin; Yu, Xiaoqi; Xu, Ruifeng",,,Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly sentimen-trelated to the aspects in the context and determine their importance based on the public knowledge base, so as to naturally learn the aspect-related contextual sentiment dependencies with these words in ACSA. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the aspect from external affective commonsense knowledge. Then, we employ Beta Distribution to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct graphs for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,208,218,,,,,,,,,,,,,,,,WOS:000855966300019,0
C,"Liu, Y; Zhang, JG; Wan, Y; Xia, CY; He, LF; Yu, PS",,,Assoc Computat Linguist,"Liu, Ye; Zhang, Jian-Guo; Wan, Yao; Xia, Congying; He, Lifang; Yu, Philip S.",,,HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HET-FORMER, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer. Extensive experiments on both single- and multi-document summarization tasks show that HETFORMER achieves stateof-the-art performance in Rouge F1 while using less memory and fewer parameters.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,146,154,,,,,,,,,,,,,,,,WOS:000855966300013,0
C,"Ma, Y; Chen, YB; Mao, XD; Li, Q",,,Assoc Computat Linguist,"Ma, Yun; Chen, Yangbin; Mao, Xudong; Li, Qing",,,Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and undertransfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each decoder is regularized by knowledge from its peer which has a different knowledge acquisition process. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and undertransfer problems. Experimental results on two benchmark datasets show that our framework achieves strong empirical results on both style compatibility and content preservation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9250,9266,,,,,,,,,,,,,,,,WOS:000860727003031,0
C,"Oprea, SV; Wilson, SR; Magdy, W",,,Assoc Computat Linguist,"Oprea, Silviu Vlad; Wilson, Steven R.; Magdy, Walid",,,Chandler: An Explainable Sarcastic Response Generator,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce Chandler, a system that generates sarcastic responses to a given utterance. Previous sarcasm generators assume the intended meaning that sarcasm conceals is the opposite of the literal meaning. We argue that this traditional theory of sarcasm provides a grounding that is neither necessary, nor sufficient, for sarcasm to occur. Instead, we ground our generation process on a formal theory that specifies conditions that unambiguously differentiate sarcasm from non-sarcasm. Furthermore, Chandler not only generates sarcastic responses, but also explanations for why each response is sarcastic. This provides accountability, crucial for avoiding miscommunication between humans and conversational agents, particularly considering that sarcastic communication can be offensive. In human evaluation, Chandler achieves comparable or higher sarcasm scores, compared to state-of-the-art generators, while generating more diverse responses, that are more specific and more coherent to the input.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,339,349,,,,,,,,,,,,,,,,WOS:000855241500038,0
C,"Patel, R; Pavlick, E",,,Assoc Computat Linguist,"Patel, Roma; Pavlick, Ellie",,,Was it said or was it claimed? How linguistic bias affects generative language models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author's opinion on the matter. Several works have identified classes of words that induce such framing effects. In this paper, we test whether generative language models are sensitive to these linguistic cues. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find qualitative and quantitative evidence that they lead to measurable style and topic differences in the generated text, leading to language that is more polarised (both positively and negatively) and, anecdotally, appears more skewed towards controversial entities and events.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10080,10095,,,,,,,,,,,,,,,,WOS:000860727004014,0
C,"Qin, H; Tian, YH; Song, Y",,,Assoc Computat Linguist,"Qin, Han; Tian, Yuanhe; Song, Yan",,,Relation Extraction with Word Graphs from N-grams,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for indomain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2860,2868,,,,,,,,,,,,,,,,WOS:000855966302087,0
C,"Safavi, T; Koutra, D",,,Assoc Computat Linguist,"Safavi, Tara; Koutra, Danai",,,Relational World Knowledge Representation in Contextual Language Models: A Review,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1053,1067,,,,,,,,,,,,,,,,WOS:000855966301014,0
C,"Ustun, A; Berard, A; Besacier, L; Galle, M",,,Assoc Computat Linguist,"Ustun, Ahmet; Berard, Alexandre; Besacier, Laurent; Galle, Matthias",,,Multilingual Unsupervised Neural Machine Translation with Denoising Adapters,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is back-translation, which is computationally costly and hard to tune. In this paper we propose instead to use denoising adapters, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the modularity and flexibility of such an approach we show that the resulting translations are on-par with back-translating as measured by BLEU, and furthermore it allows adding unseen languages incrementally.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6650,6662,,,,,,,,,,,,,,,,WOS:000860727000048,0
C,"Wang, WX; Peng, W; Zhang, M; Liu, Q",,,Assoc Computat Linguist,"Wang, Weixuan; Peng, Wei; Zhang, Meng; Liu, Qun",,,Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English -> German and English -> French translation tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3197,3202,,,,,,,,,,,,,,,,WOS:000855966303028,0
C,"Wang, ZY; Gupta, S; Hao, J; Fan, X; Li, DC; Li, AH; Guo, CL",,,Assoc Computat Linguist,"Wang, Zhuoyi; Gupta, Saurabh; Hao, Jie; Fan, Xing; Li, Dingcheng; Li, Alexander Hanbo; Guo, Chenlei",,,Contextual Rephrase Detection for Reducing Friction in Dialogue Systems,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"For voice assistants like Alexa, Google Assistant and Siri, correctly interpreting users' intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend to rephrase their query until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users' implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multiturn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including user's implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1899,1905,,,,,,,,,,,,,,,,WOS:000855966302002,0
C,"Wilner, S; Woolridge, D; Glick, M",,,Assoc Computat Linguist,"Wilner, Sean; Woolridge, Daniel; Glick, Madeleine",,,Narrative Embedding: Re-Contextualization through Attention,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Narrative analysis is becoming increasingly important for a number of linguistic tasks including summarization, knowledge extraction, and question answering. We present a novel approach for narrative event representation using attention to re-contextualize events across the whole story. Comparing to previous analysis we find an unexpected attachment of event semantics to predicate tokens within a popular transformer model. We test the utility of our approach on narrative completion prediction, achieving state of the art performance on Multiple Choice Narrative Cloze and scoring competitively on the Story Cloze Task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1393,1405,,,,,,,,,,,,,,,,WOS:000855966301038,0
C,"Wu, H; Shi, XD",,,Assoc Computat Linguist,"Wu, Hui; Shi, Xiaodong",,,Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Joint entity and relation extraction is challenging due to the complex interaction of interaction between named entity recognition and relation extraction. Although most existing works tend to jointly train these two tasks through a shared network, they fail to fully utilize the interdependence between entity types and relation types. In this paper, we design a novel synchronous dual network (SDN) with cross-type attention via separately and interactively considering the entity types and relation types. On the one hand, SDN adopts two isomorphic bi-directional type-attention LSTM to encode the entity type enhanced representations and the relation type enhanced representations, respectively. On the other hand, SDN explicitly models the interdependence between entity types and relation types via cross-type attention mechanism. In addition, we also propose a new multi-task learning strategy via modeling the interaction of two types of information. Experiments on NYT and WebNLG datasets verify the effectiveness of the proposed model, achieving state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2769,2779,,,,,,,,,,,,,,,,WOS:000855966302078,0
C,"Yao, WL; Pan, XM; Jin, LF; Chen, JS; Yu, DA; Yu, D",,,Assoc Computat Linguist,"Yao, Wenlin; Pan, Xiaoman; Jin, Lifeng; Chen, Jianshu; Yu, Dian; Yu, Dong",,,Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one predefined word sense inventory (e.g., WordNet). To address the data sparsity problem and generalize the model to be independent of one predefined inventory, we propose a gloss alignment algorithm that can align definition sentences (glosses) with the same meaning from different sense inventories to collect rich lexical knowledge. We then train a model to identify semantic equivalence between a target word in context and one of its glosses using these aligned inventories, which exhibits strong transfer capability to many WSD tasks1. Experiments on benchmark datasets show that the proposed method improves predictions on both frequent and rare word senses, outperforming prior work by 1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on WiC Task also indicates that our method can better capture word meanings in context.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7741,7751,,,,,,,,,,,,,,,,WOS:000860727001058,0
C,"Yu, PF; Ji, H; Natarajan, P",,,Assoc Computat Linguist,"Yu, Pengfei; Ji, Heng; Natarajan, Premkumar",,,Lifelong Event Detection with Knowledge Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but they are limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or semantic relatedness exist among hierarchical knowledge element types. In our proposed framework, knowledge is being transferred between learned old event types and new event types. Specifically, we update old knowledge with the mentions of new event types using a self-training loss. In addition, we aggregate the representations of old event types based on their similarities with new event types to initialize the representations of new event types. Experimental results show that our framework outperforms competitive baselines with a 5.1% absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30% absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves the performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel knowledge acquisition.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5278,5290,,,,,,,,,,,,,,,,WOS:000855966305032,0
C,"Zheng, YH; Tan, ZX; Zhang, M; Maimaiti, M; Luan, HB; Sun, MS; Liu, Q; Liu, Y",,,Assoc Computat Linguist,"Zheng, Yuanhang; Tan, Zhixing; Zhang, Meng; Maimaiti, Mieradilijiang; Luan, Huanbo; Sun, Maosong; Liu, Qun; Liu, Yang",,,Self-Supervised Quality Estimation for Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and laborintensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3322,3334,,,,,,,,,,,,,,,,WOS:000855966303039,0
C,"Aghajanyan, A; Gupta, A; Shrivastava, A; Chen, XL; Zettlemoyer, L; Gupta, S",,,Assoc Computat Linguist,"Aghajanyan, Armen; Gupta, Anchit; Shrivastava, Akshat; Chen, Xilun; Zettlemoyer, Luke; Gupta, Sonal",,,Muppet: Massive Multi-task Representations with Pre-Finetuning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that prefinetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5799,5811,,,,,,,,,,,,,,,,WOS:000855966305072,0
C,"Al-Negheimish, H; Madhyastha, P; Russo, A",,,Assoc Computat Linguist,"Al-Negheimish, Hadeel; Madhyastha, Pranava; Russo, Alessandra",,,Numerical reasoning in machine reading comprehension tasks: are we there yet?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting, and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggest that the models have achieved near-human performance. However, does this mean that these models have learned to reason? In this paper, we present a controlled study on some of the top-performing model architectures for the task of numerical reasoning. Our observations suggest that the standard metrics are incapable of measuring progress towards such tasks.",,,,,,"Madhyastha, Pranava/0000-0002-4438-8161; Russo, Alessandra/0000-0002-3318-8711",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9643,9649,,,,,,,,,,,,,,,,WOS:000860727003060,0
C,"Bikaun, T; French, T; Hodkiewicz, M; Stewart, M; Liu, W",,,Assoc Computat Linguist,"Bikaun, Tyler; French, Tim; Hodkiewicz, Melinda; Stewart, Michael; Liu, Wei",,,LexiClean: An annotation tool for rapid multi-task lexical normalisation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such corpora, but currently lacks suitable tools to acquire high-quality annotated data to support deep learning based approaches. In this paper, we present LexiClean(1), the first open-source web-based annotation tool for multi-task lexical normalisation. LexiClean's main contribution is support for simultaneous in situ token-level modification and annotation that can be rapidly applied corpus wide. We demonstrate the usefulness of our tool through a case study on two sets of noisy corpora derived from the specialised-domain of industrial mining We show that LexiClean allows for the rapid and efficient development of high-quality parallel corpora. A demo of our system is available at: https: //youtu.be/P7_ooKrQPDU.",,,,,,"Bikaun, Tyler/0000-0001-6600-6323; Hodkiewicz, Melinda/0000-0002-7336-3932; French, Tim/0000-0002-0748-8040; Liu, Wei/0000-0002-7409-0948",,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,212,219,,,,,,,,,,,,,,,,WOS:000855241500025,0
C,"Cao, SY; Wang, L",,,Assoc Computat Linguist,"Cao, Shuyang; Wang, Lu",,,CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailmentbased reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6633,6649,,,,,,,,,,,,,,,,WOS:000860727000047,0
C,"Chai, JY; He, YJ; Hashemi, H; Li, B; Parveen, D; Kondapally, R; Xu, WJ",,,Assoc Computat Linguist,"Chai, Junyi; He, Yujie; Hashemi, Homa; Li, Bing; Parveen, Daraksha; Kondapally, Ranganath; Xu, Wenjin",,,Automatic Construction of Enterprise Knowledge Base,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we present an automatic knowledge base construction system from large scale enterprise documents with minimal efforts of human intervention. In the design and deployment of such a knowledge mining system for enterprise, we faced several challenges including data distributional shift, performance evaluation, compliance requirements and other practical issues. We leveraged state-of-the-art deep learning models to extract information (named entities and definitions) at per document level, then further applied classical machine learning techniques to process global statistical information to improve the knowledge base. Experimental results are reported on actual enterprise documents. This system is currently serving as part of a Microsoft 365 service.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,11,19,,,,,,,,,,,,,,,,WOS:000855241500002,0
C,"Chalkidis, I; Fergadiotis, M; Androutsopoulos, I",,,Assoc Computat Linguist,"Chalkidis, Ilias; Fergadiotis, Manos; Androutsopoulos, Ion",,,MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union ( EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift and the importance of chronological, instead of random splits. We use the dataset as a testbed for zeroshot cross-lingual transfer, where we exploit annotated training documents in one language (source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic forgetting of multilingual knowledge and, consequently, poor zero-shot transfer to other languages. Adaptation strategies, namely partial fine-tuning, adapters, BITFIT, LNFIT, originally proposed to accelerate finetuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer, but their impact also depends on the pretrained model used and the size of the label set.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6974,6996,,,,,,,,,,,,,,,,WOS:000860727001007,0
C,"Dash, S; Rossiello, G; Bagchi, S; Mihindukulasooriya, N; Gliozzo, A",,,Assoc Computat Linguist,"Dash, Sarthak; Rossiello, Gaetano; Bagchi, Sugato; Mihindukulasooriya, Nandana; Gliozzo, Alfio",,,Open Knowledge Graphs Canonicalization using Variational Autoencoders,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational Autoencoders (CUVA)1, a joint model to learn both embeddings and cluster assignments in an end-to-end approach, which leads to a better vector representation for the noun and relation phrases. Our evaluation over multiple benchmarks shows that CUVA outperforms the existing state-of-the-art approaches. Moreover, we introduce CANONICNELL, a novel dataset to evaluate entity canonicalization systems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10379,10394,,,,,,,,,,,,,,,,WOS:000860727004035,0
C,"Dong, XY; Yu, WH; Zhu, CG; Jiang, M",,,Assoc Computat Linguist,"Dong, Xiangyu; Yu, Wenhao; Zhu, Chenguang; Jiang, Meng",,,Injecting Entity Types into Entity-Guided Text Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of entity in NLG, in this paper, we aim to model the entity type in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our model has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,734,741,,,,,,,,,,,,,,,,WOS:000855966300056,0
C,"Emelin, D; Le Bras, R; Hwang, JD; Forbes, M; Choi, Y",,,Assoc Computat Linguist,"Emelin, Denis; Le Bras, Ronan; Hwang, Jena D.; Forbes, Maxwell; Choi, Yejin",,,"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,698,718,,,,,,,,,,,,,,,,WOS:000855966300054,0
C,"Feng, LY; Qiu, MH; Li, YL; Zheng, HT; Shen, Y",,,Assoc Computat Linguist,"Feng, Lingyun; Qiu, Minghui; Li, Yaliang; Zheng, Hai-Tao; Shen, Ying",,,Wasserstein Selective Transfer Learning for Cross-domain Text Mining,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein Selective Transfer Learning (WSTL) method. Specifically, the proposed method considers a reinforced selector to select helpful data for transfer learning. We further use a Wasserstein-based discriminator to maximize the empirical distance between the selected source data and target data. The TL module is then trained to minimize the estimated Wasserstein distance in an adversarial manner and provides domain invariant features for the reinforced selector. We adopt an evaluation metric based on the performance of the TL module as delayed reward and a Wasserstein-based metric as immediate rewards to guide the reinforced selector learning. Compared with the competing TL approaches, the proposed method selects data samples that are closer to the target domain. It also provides better state features and reward signals that lead to better performance with faster convergence. Extensive experiments on three real-world text mining tasks demonstrate the effectiveness of the proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9772,9783,,,,,,,,,,,,,,,,WOS:000860727003071,0
C,"Hoover, JL; Sordoni, A; Du, WY; O'Donnell, TJ",,,Assoc Computat Linguist,"Hoover, Jacob Louis; Sordoni, Alessandro; Du, Wenyu; O'Donnell, Timothy J.",,,Linguistic Dependencies and Statistical Dependence,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most approximate to 0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2941,2963,,,,,,,,,,,,,,,,WOS:000855966303006,0
C,"Jang, KR; Kang, J; Hong, G; Myaeng, SH; Park, J; Yoon, T; Seo, H",,,Assoc Computat Linguist,"Jang, Kyoung-Rok; Kang, Junmo; Hong, Giwon; Myaeng, Sung-Hyon; Park, Joohee; Yoon, Taewon; Seo, Heecheol",,,Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models' dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly controllable sparsity. UHD's large capacity and minimal noise and interference among the dimensions allow for binarized representations, which are highly efficient for storage and search. Also proposed is a bucketing method, where the embeddings from multiple layers of BERT are selected/merged to represent diverse linguistic aspects. We test our models with MS MARCO and TREC CAR, showing that our models outperforms other sparse models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1016,1029,,,,,,,,,,,,,,,,WOS:000855966301011,0
C,"Koto, F; Lau, JH; Baldwin, T",,,Assoc Computat Linguist,"Koto, Fajri; Lau, Jey Han; Baldwin, Timothy",,,INDOBERTWEET: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present INDOBERTWEET, the first largescale pretrained model for Indonesian Twitter that is trained by extending a monolinguallytrained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective than proposed methods for vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based datasets.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10660,10668,,,,,,,,,,,,,,,,WOS:000860727004057,0
C,"Leblond, I; Alayrac, JB; Sifre, L; Pislar, M; Lespiau, JB; Antonoglou, I; Simonyan, K; Vinyals, O",,,Assoc Computat Linguist,"Leblond, Itemi; Alayrac, Jean-Baptiste; Sifre, Laurent; Pislar, Miruna; Lespiau, Jean-Baptiste; Antonoglou, Ioannis; Simonyan, Karen; Vinyals, Oriol",,,Machine Translation Decoding beyond Beam Search,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8410,8434,,,,,,,,,,,,,,,,WOS:000860727002040,0
C,"Li, Z; Qu, LZ; Haffari, G",,,Assoc Computat Linguist,"Li, Zhuang; Qu, Lizhen; Haffari, Gholamreza",,,Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper investigates continual learning for semantic parsing. In this setting, a neural semantic parser learns tasks sequentially without accessing full training data from previous tasks. Direct application of the SOTA continual learning algorithms to this problem fails to achieve comparable performance with retraining models with all seen tasks, because they have not considered the special properties of structured outputs, yielded by semantic parsers. Therefore, we propose TOTAL RECALL, a continual learning method designed for neural semantic parsers from two aspects: i) a sampling method for memory replay that diversifies logical form templates and balances distributions of parse actions in a memory; ii) a two-stage training method that significantly improves generalization capability of the parsers across tasks. We conduct extensive experiments to study the research problems involved in continual semantic parsing, and demonstrate that a neural semantic parser trained with TOTAL RECALL achieves superior performance than the one trained directly with the SOTA continual learning algorithms, and achieve a 3-6 times speedup compared to retraining from scratch. Code and datasets are available at: https://github.com/zhuang-li/cl_nsp.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3816,3831,,,,,,,,,,,,,,,,WOS:000855966303082,0
C,"Liu, D; Du, MG; Li, XX; Li, Y; Chen, EH",,,Assoc Computat Linguist,"Liu, Dan; Du, Mengge; Li, Xiaoxi; Li, Ya; Chen, Enhong",,,Cross Attention Augmented Transducer Networks for Simultaneous Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,39,55,,,,,,,,,,,,,,,,WOS:000855966300004,0
C,"Liu, ZY; Chen, NF",,,Assoc Computat Linguist,"Liu, Zhengyuan; Chen, Nancy F.",,,Controllable Neural Dialogue Summarization with Personal Named Entity Planning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the under-constrained problem in summarization tasks. This framework supports two types of use cases: (1) Comprehensive Perspective, which is a general purpose case with no user-preference specified, considering summary points from all conversational interlocutors and all mentioned persons; (2) Focus Perspective, positioning the summary based on a user-specified personal named entity, which could be one of the interlocutors or one of the persons mentioned in the conversation. During training, we exploit occurrence planning of personal named entities and coheference information to improve temporal coherence and to minimize hallucination in neural generation. Experimental results show that our proposed framework generates fluent and factually consistent summaries under various planning controls using both objective metrics and human evaluations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,92,106,,,,,,,,,,,,,,,,WOS:000855966300008,0
C,"Madaan, A; Tandon, N; Rajagopal, D; Clark, P; Yang, YM; Hovy, E",,,Assoc Computat Linguist,"Madaan, Aman; Tandon, Niket; Rajagopal, Dheeraj; Clark, Peter; Yang, Yiming; Hovy, Eduard",,,Think about it! Improving defeasible reasoning by first modeling the question scenario,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to think about a question and explicitly model the scenario, rather than answering reflexively.(1)",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6291,6310,,,,,,,,,,,,,,,,WOS:000860727000023,0
C,"Mao, X; Wang, WT; Wu, YB; Lan, M",,,Assoc Computat Linguist,"Mao, Xin; Wang, Wenting; Wu, Yuanbin; Lan, Man",,,From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Cross-lingual entity alignment (EA) aims to find the equivalent entities between cross-lingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2843,2853,,,,,,,,,,,,,,,,WOS:000855966302085,0
C,"Mao, YN; Ma, WC; Lei, DR; Han, JW; Ren, X",,,Assoc Computat Linguist,"Mao, Yuning; Ma, Wenchang; Lei, Deren; Han, Jiawei; Ren, Xiang",,,"Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Prior studies on text-to-text generation typically assume that the model could figure out what to attend to in the input and what to include in the output via seq2seq learning, with only the parallel training data and no additional guidance. However, it remains unclear whether current models can preserve important concepts in the source input, as seq2seq learning does not have explicit focus on the concepts and commonly used evaluation metrics also treat concepts equally important as other tokens. In this paper, we present a systematic analysis that studies whether current seq2seq models, especially pre-trained language models, are good enough for preserving important input concepts and to what extent explicitly guiding generation with the concepts as lexical constraints is beneficial. We answer the above questions by conducting extensive analytical experiments on four representative text-to-text generation tasks. Based on the observations, we then propose a simple yet effective framework to automatically extract, denoise, and enforce important input concepts as lexical constraints. This new method performs comparably or better than its unconstrained counterpart on automatic metrics, demonstrates higher coverage for concept preservation, and receives better ratings in the human evaluation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5063,5074,,,,,,,,,,,,,,,,WOS:000855966305017,0
C,"Paik, C; Aroca-Ouellette, S; Roncone, A; Kann, K",,,Assoc Computat Linguist,"Paik, Cory; Aroca-Ouellette, Stephane; Roncone, Alessandro; Kann, Katharina",,,The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human's perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,823,835,,,,,,,,,,,,,,,,WOS:000855966300063,0
C,"Qiu, LL; Hu, HX; Zhang, BW; Shaw, P; Sha, F",,,Assoc Computat Linguist,"Qiu, Linlu; Hu, Hexiang; Zhang, Bowen; Shaw, Peter; Sha, Fei",,,Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed to study systematic generalization for grounded language understanding. First, we study which aspects of the original benchmark can be solved by commonly used methods in multimodal research. We find that a general-purpose Transformer-based model with cross-modal attention achieves strong performance on a majority of the gSCAN splits, surprisingly outperforming more specialized approaches from prior work. Furthermore, our analysis suggests that many of the remaining errors reveal the same fundamental challenge in systematic generalization of linguistic constructs regardless of visual context. Second, inspired by this finding, we propose challenging new tasks for gSCAN by generating data to incorporate relations between objects in the visual environment. Finally, we find that current models are surprisingly data inefficient given the narrow scope of commands in gSCAN, suggesting another challenge for future work.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2180,2188,,,,,,,,,,,,,,,,WOS:000855966302025,0
C,"Rashid, A; Lioutas, V; Ghaddar, A; Rezagholizadeh, M",,,Assoc Computat Linguist,"Rashid, Ahmad; Lioutas, Vasileios; Ghaddar, Abbas; Rezagholizadeh, Mehdi",,,Towards Zero-Shot Knowledge Distillation for Natural Language Processing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher's training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and adversarial training to learn the teacher's output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher's classification score (accuracy or F1) while compressing the model 30 times.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6551,6561,,,,,,,,,,,,,,,,WOS:000860727000041,0
C,"Sinha, K; Jia, R; Hupkes, D; Pineau, J; Williams, A; Kiela, D",,,Assoc Computat Linguist,"Sinha, Koustuv; Jia, Robin; Hupkes, Dieuwke; Pineau, Joelle; Williams, Adina; Kiela, Douwe",,,Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks mostly due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and we show that these models still achieve high accuracy after fine-tuning on many downstream tasks - including tasks specifically designed to be challenging for models that ignore word order. Our models also perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and they underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2888,2913,,,,,,,,,,,,,,,,WOS:000855966303002,0
C,"Tian, JD; Li, YT; Chen, WQ; Xiao, LQ; He, H; Jin, YH",,,Assoc Computat Linguist,"Tian, Jidong; Li, Yitian; Chen, Wenqing; Xiao, Liqiang; He, Hao; Jin, Yaohui",,,Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and traceability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.",,,,,"浩, 何/GZG-6941-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3738,3747,,,,,,,,,,,,,,,,WOS:000855966303075,0
C,"Wang, CH; Hsu, WN; Adi, Y; Polyak, A; Lee, A; Chen, PJ; Gu, JT; Pino, J",,,Assoc Computat Linguist,"Wang, Changhan; Hsu, Wei-Ning; Adi, Yossi; Polyak, Adam; Lee, Ann; Chen, Peng-Jen; Gu, Jiatao; Pino, Juan",,,FAIRSEQ S-2 : A Scalable and Integrable Speech Synthesis Toolkit,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper presents FAIRSEQ S-2, a FAIRSEQ extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, FAIRSEQ S-2 also benefits from the scalability offered by FAIRSEQ and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models will be made available at lit tps://github.com/ pytorch/fairseq/tree/master/ examples/speech_synthesis.",,,,,,"Adi, Yossi/0000-0003-2237-3898",,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,143,152,,,,,,,,,,,,,,,,WOS:000855241500017,0
C,"Wang, R; Su, XA; Long, SY; Dai, XY; Huang, SJ; Chen, JJ",,,Assoc Computat Linguist,"Wang, Ran; Su, Xi'ao; Long, Siyu; Dai, Xinyu; Huang, Shujian; Chen, Jiajun",,,Meta-LMTC: Meta-Learning for Large-Scale Multi-Label Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the dataset that can guide models to learn with few samples. In this paper, for the first time, this problem is addressed from a meta-learning perspective. However, the simple extension of meta-learning approaches to multi-label classification is sub-optimal for LMTC tasks due to long-tailed label distribution and coexisting of few- and zero-shot scenarios. We propose a meta-learning approach named META-LMTC. Specifically, it constructs more faithful and more diverse tasks according to well-designed sampling strategies and directly incorporates the objective of adapting to new low-resource tasks into the meta-learning phase. Extensive experiments show that META-LMTC achieves state-of-the-art performance against strong baselines and can still enhance powerful BERTlike models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8633,8646,,,,,,,,,,,,,,,,WOS:000860727002057,0
C,"Yang, XW; Karaman, S; Tetreault, J; Jaimes, A",,,Assoc Computat Linguist,"Yang, Xuewen; Karaman, Svebor; Tetreault, Joel; Jaimes, Alex",,,Journalistic Guidelines Aware News Image Captioning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this task, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5162,5175,,,,,,,,,,,,,,,,WOS:000855966305023,0
C,"Zhang, S; Niu, JW; Wei, CY",,,Assoc Computat Linguist,"Zhang, Sen; Niu, Jianwei; Wei, Chuyuan",,,Fine-grained Factual Consistency Assessment for Abstractive Summarization Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the top-K most relevant sentences with the summary sentence from the document. In the second stage, the model performs fine-grained consistency reasoning at the sentence level, and then aggregates all sentences' consistency scores to obtain the final assessment result. We get the training data pairs by data synthesis and adopt contrastive loss of data pairs to help the model identify subtle cues. Experiment results show that SumFC has made a significant improvement over the previous state-of-the-art methods. Our experiments also indicate that SumFC distinguishes detailed differences better.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,107,116,,,,,,,,,,,,,,,,WOS:000855966300009,0
C,"Zhang, SL; Feng, Y",,,Assoc Computat Linguist,"Zhang, Shaolei; Feng, Yang",,,Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7306,7317,,,,,,,,,,,,,,,,WOS:000860727001029,0
C,"Zhou, H; Huang, ML; Liu, Y; Chen, W; Zhu, XY",,,Assoc Computat Linguist,"Zhou, Hao; Huang, Minlie; Liu, Yong; Chen, Wei; Zhu, Xiaoyan",,,EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these models have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce knowledge graphs to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2383,2395,,,,,,,,,,,,,,,,WOS:000855966302043,0
C,"Ahn, J; Oh, A",,,Assoc Computat Linguist,"Ahn, Jaimeen; Oh, Alice",,,Mitigating Language-Dependent Ethnic Bias in BERT,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"BERT and other large-scale language models (LMs) contain gender and racial bias. They also exhibit other dimensions of social bias, most of which have not been studied in depth, and some of which vary depending on the language. In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,533,549,,,,,,,,,,,,,,,,WOS:000855966300042,0
C,"Alva-Manchego, F; Obamuyide, A; Gajbhiye, A; Blain, F; Fomicheva, M; Specia, L",,,Assoc Computat Linguist,"Alva-Manchego, Fernando; Obamuyide, Abiola; Gajbhiye, Amit; Blain, Frederic; Fomicheva, Marina; Specia, Lucia",,,deepQuest-py: Large and Distilled Models for Quality Estimation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuestpy is available at https://github.com/sheffieldnlp/deepQuest-py under a CC BY-NC-SA licence.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,382,389,,,,,,,,,,,,,,,,WOS:000855241500042,0
C,"Chen, JW; Lin, HY; Han, XP; Sun, L",,,Assoc Computat Linguist,"Chen, Jiawei; Lin, Hongyu; Han, Xianpei; Sun, Le",,,Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event detection has long been troubled by the trigger curse: overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance. This problem is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during training. Experiments show that our method significantly improves the FSED on ACE05, MAVEN and KBP17 datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8078,8088,,,,,,,,,,,,,,,,WOS:000860727002015,0
C,"Chen, ZY; Chen, WH; Smiley, C; Borova, I; Langdon, D; Moussa, R; Beane, M; Huang, TH; Routledge, B; Wang, WY",,,Assoc Computat Linguist,"Chen, Zhiyu; Chen, Wenhu; Smiley, Charese; Borova, Iana; Langdon, Dylan; Moussa, Reema; Beane, Matt; Huang, Ting-Hao; Routledge, Bryan; Wang, William Yang",,,FINQA: A Dataset of Numerical Reasoning over Financial Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The sheer volume of financial statements makes it difficult for humans to access and analyze a business's financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FINQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset - the first of its kind - should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3697,3711,,,,,,,,,,,,,,,,WOS:000855966303072,0
C,"Deng, MK; Tan, BW; Liu, ZZ; Xing, EP; Hu, ZT",,,Assoc Computat Linguist,"Deng, Mingkai; Tan, Bowen; Liu, Zhengzhong; Xing, Eric P.; Hu, Zhiting",,,"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g., summarization), transduction (e.g., text rewriting), and creation (e.g., dialog). Information alignment between input, context, and output text plays a common central role in characterizing the generation. With automatic alignment prediction models, we develop a family of interpretable metrics that are suitable for evaluating key aspects of different NLG tasks, often without need of gold reference data. Experiments show the uniformly designed metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics in each of diverse tasks, including text summarization, style transfer, and knowledgegrounded dialog.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7580,7605,,,,,,,,,,,,,,,,WOS:000860727001047,0
C,"Fan, C; Li, JW; Ao, X; Wu, F; Meng, YX; Sun, XF",,,Assoc Computat Linguist,"Fan, Chun; Li, Jiwei; Ao, Xiang; Wu, Fei; Meng, Yuxian; Sun, Xiaofei",,,Layer-wise Model Pruning based on Mutual Information,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Inspired by mutual information (MI) based feature selection in SVMs and logistic regression, in this paper, we propose MI-based layer-wise pruning: for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques: (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3079,3090,,,,,,,,,,,,,,,,WOS:000855966303018,0
C,"Feldhus, N; Schwarzenberg, R; Moller, S",,,Assoc Computat Linguist,"Feldhus, Nils; Schwarzenberg, Robert; Moeller, Sebastian",,,THERMOSTAT: A Large Collection of NLP Model Explanations and Analysis Tools,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present THERMOSTAT which consists of a large collection of model explanations and accompanying analysis tools. THERMOSTAT allows easy access to over 200k explanations for the decisions of prominent stateo-f-the-art models spanning across different NLP tasks, generated with multiple explainers. The dataset took over 10k GPU hours (> one year) to compile; compute time that the community now saves. The accompanying software tools allow to analyse explanations instance-wise but also accumulatively on corpus level. Users can investigate and compare models, datasets and explainers without the need to orchestrate implementation details. THERMOSTAT is fully open source, democratizes explainability research in the language domain, circumvents redundant computations and increases comparability and replicability.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,87,95,,,,,,,,,,,,,,,,WOS:000855241500011,0
C,"Ge, HB; Sun, CX; Xiong, DY; Liu, Q",,,Assoc Computat Linguist,"Ge, Huibin; Sun, Chenxi; Xiong, Deyi; Liu, Qun",,,Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character idioms. We also observe that linguistic relations between target words and long-range context exhibit diversity, including lexical match, synonym, summary and reasoning. Experiment results show that the Chinese pretrained language model PanGu-alpha (Zeng et al., 2021) is 45 points behind human in terms of top-1 word prediction accuracy, indicating that Chinese WPLC is a challenging dataset. The dataset is publicly available at https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese_WPLC.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3770,3778,,,,,,,,,,,,,,,,WOS:000855966303078,0
C,"Ivgi, M; Berant, J",,,Assoc Computat Linguist,"Ivgi, Maor; Berant, Jonathan",,,Achieving Model Robustness through Discrete Adversarial Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only. Concretely, given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the model. We propose (i) a new discrete attack, based on best-first search, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that random sampling leads to impressive gains in robustness, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of similar to 10x. Furthermore, online augmentation with search-based attacks justifies the higher training cost, significantly improving robustness on three datasets. Last, we show that our new attack substantially improves robustness compared to prior methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1529,1544,,,,,,,,,,,,,,,,WOS:000855966301048,0
C,"Jin, Y; Zhao, H; Liu, M; Du, L; Buntine, W",,,Assoc Computat Linguist,"Jin, Yuan; Zhao, He; Liu, Ming; Du, Lan; Buntine, Wray",,,Neural Attention-Aware Hierarchical Topic Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, thereby paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.",,,,,,"Liu, Ming/0000-0002-2160-6111",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1042,1052,,,,,,,,,,,,,,,,WOS:000855966301013,0
C,"Kim, J; Jeong, M; Choi, S; Hwang, SW",,,Assoc Computat Linguist,"Kim, Jihyuk; Jeong, Myeongho; Choi, Seungtaek; Hwang, Seung-won",,,Structure-Augmented Keyphrase Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (e.g., tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then encoding these information, using existing keyphrases of other documents, complementing missing/incomplete titles. Specifically, we first extend the given document with related but absent keyphrases from existing keyphrases, to augment missing contexts (generating structure), and then, build a graph of keyphrases and the given document, to obtain structure-aware representation of the augmented text (encoding structure). Our empirical results validate that our proposed structure augmentation and structure-aware encoding can improve KG for both scenarios, outperforming the state-of-the-art(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2657,2667,,,,,,,,,,,,,,,,WOS:000855966302068,0
C,"Lee, BW; Jang, YS; Lee, JHJ",,,Assoc Computat Linguist,"Lee, Bruce W.; Jang, Yoo Sung; Lee, Jason Hyung-Jong",,,Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10669,10686,,,,,,,,,,,,,,,,WOS:000860727004058,0
C,"Lekhtman, E; Ziser, Y; Reichart, R",,,Assoc Computat Linguist,"Lekhtman, Entony; Ziser, Yftah; Reichart, Roi",,,"DILBERT: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, finetuning a pre-trained language model on a source domain and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP tasks, the output categories substantially differ between domains, making adaptation even more challenging. This, for example, happens in the task of aspect extraction, where the aspects of interest of reviews of, e.g., restaurants or electronic devices may be very different. This paper presents a new fine-tuning scheme for BERT, which aims to address the above challenges. We name this scheme DILBERT: Domain Invariant Learning with BERT, and customize it for aspect extraction in the unsupervised domain adaptation setting. DILBERT harnesses the categorical information of both the source and the target domains to guide the pre-training process towards a more domain and category invariant representation, thus closing the gap between the domains. We show that DILBERT yields substantial improvements over state-of-the-art baselines while using a fraction of the unlabeled data, particularly in more challenging domain adaptation setups.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,219,230,,,,,,,,,,,,,,,,WOS:000855966300020,0
C,"Li, ZY; Zou, YC; Zhang, C; Zhang, Q; Wei, ZY",,,Assoc Computat Linguist,"Li, Zhengyan; Zou, Yicheng; Zhang, Chong; Zhang, Qi; Wei, Zhongyu",,,Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect-based sentiment analysis aims to identify the sentiment polarity of a specific aspect in product reviews. We notice that about 30% of reviews do not contain obvious opinion words, but still convey clear human-aware sentiment orientation, which is known as implicit sentiment. However, recent neural networkbased approaches paid little attention to implicit sentiment entailed in the reviews. To overcome this issue, we adopt Supervised Contrastive Pre-training on large-scale sentimentannotated corpora retrieved from in-domain language resources. By aligning the representation of implicit sentiment expressions to those with the same sentiment label, the pretraining process leads to better capture of both implicit and explicit sentiment orientation towards aspects in reviews. Experimental results show that our method achieves state-of-the-art performance on SemEval2014 benchmarks, and comprehensive analysis validates its effectiveness on learning implicit sentiment.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,246,256,,,,,,,,,,,,,,,,WOS:000855966300022,0
C,"Lin, HZ; Ma, J; Cheng, MF; Yang, ZW; Chen, LL; Chen, G",,,Assoc Computat Linguist,"Lin, Hongzhan; Ma, Jing; Cheng, Mingfei; Yang, Zhiwei; Chen, Liangliang; Chen, Guang",,,Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim-guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10035,10047,,,,,,,,,,,,,,,,WOS:000860727004010,0
C,"Lin, ZC; Sun, YH; Zhang, MS",,,Assoc Computat Linguist,"Lin, Zhichao; Sun, Yueheng; Zhang, Meishan",,,A Graph-Based Neural Model for End-to-End Frame Semantic Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three suhtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three sub tasks arc closely related while previous studies model them individually, which ignores their intern connections and meanwhile induces error propagation problem. In this work, we propose an end-to-end neural model to tackle the task jointly. Concretely, we exploit a graph based method, regarding frame semantic parsing as a graph construction problem. All predicates and roles are treated as graph nodes, and their relations are taken as graph edges. Experiment results on two benchmark damsels of frame semantic parsing show that our method is highly competitive, resulting in better performance than pipeline models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3864,3874,,,,,,,,,,,,,,,,WOS:000855966304001,0
C,"Liu, B; Scelis, H; Zuccon, G; Hua, W; Zhao, GH",,,Assoc Computat Linguist,"Liu, Bing; Scelis, Harrisen; Zuccon, Guido; Hua, Wen; Zhao, Genghong",,,ActiveEA: Active Learning for Neural Entity Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods - neural EA models - rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In this paper, we devise a novel Active Learning (AL) framework for neural EA, aiming to create highly informative seed alignment to obtain more effective EA models with less annotation cost. Our framework tackles two main challenges encountered when applying AL to EA: (1) How to exploit dependencies between entities within the AL strategy. Most AL strategies assume that the data instances to sample are independent and identically distributed. However, entities in KGs are related. To address this challenge, we propose a structure-aware uncertainty sampling strategy that can measure the uncertainty of each entity as well as its impact on its neighbour entities in the KG. (2) How to recognise entities that appear in one KG but not in the other KG (i.e., bachelors). Identifying bachelors would likely save annotation budget. To address this challenge, we devise a bachelor recognizer paying attention to alleviate the effect of sampling bias. Empirical results show that our proposed AL strategy can significantly improve sampling quality with good generality across different datasets, EA models and amount of bachelors.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3364,3374,,,,,,,,,,,,,,,,WOS:000855966303042,0
C,"Liu, X; Chen, QC; Chen, JY; Zhou, WX; Liu, TY; Yang, XL; Peng, WH",,,Assoc Computat Linguist,"Liu, Xin; Chen, Qingcai; Chen, Junying; Zhou, Wenxiu; Liu, Tingyu; Yang, Xinlan; Peng, Weihua",,,Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and dynamic routing, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with medical literature hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKL can more accurately associate knowledge with medical literature than the mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the medical literature, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub(1).",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3518,3532,,,,,,,,,,,,,,,,WOS:000855966303057,0
C,"Maharana, A; Bansal, M",,,Assoc Computat Linguist,"Maharana, Adyasha; Bansal, Mohit",,,"Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and relevance. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters/objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6772,6786,,,,,,,,,,,,,,,,WOS:000860727000058,0
C,"Meng, Y; Zhang, YY; Huang, JX; Wang, X; Zhang, Y; Ji, H; Han, JW",,,Assoc Computat Linguist,"Meng, Yu; Zhang, Yunyi; Huang, Jiaxin; Wang, Xuan; Zhang, Yu; Ji, Heng; Han, Jiawei",,,Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantlysupervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantlysupervised NER models by significant margins(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10367,10378,,,,,,,,,,,,,,,,WOS:000860727004034,0
C,"Ni, AS; Azerbayev, Z; Mutuma, M; Feng, T; Zhang, YS; Yu, T; Awadallah, AH; Radev, D",,,Assoc Computat Linguist,"Ni, Ansong; Azerbayev, Zhangir; Mutuma, Mutethia; Feng, Troy; Zhang, Yusen; Yu, Tao; Awadallah, Ahmed Hassan; Radev, Dragomir",,,SummerTime: Text Summarization Toolkit for Non-experts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent advances in summarization provide models that can generate summaries of higher quality. Such models now exist for a number of summarization tasks, including query-based summarization, dialogue summarization, and multi-document summarization. While such models and tasks are rapidly growing in the research field, it has also become challenging for non-experts to keep track of them. To make summarization methods more accessible to a wider audience, we develop Summertime by rethinking the summarization task from the perspective of an NLP non-expert. SummerTime is a complete toolkit for text summarization, including various models, datasets and evaluation metrics, for a full spectrum of summarization-related tasks. SummerTime integrates with libraries designed for NLP researchers, and enables users with easy-touse APIs. With SummerTime, users can locate pipeline solutions and search for the best model with their own data, and visualize the differences, all with a few lines of code. We also provide explanations for models and evaluation metrics to help users understand the model behaviors and select models that best suit their needs. Our library, along with a notebook demo, is available at https://github.com/Yale-LILY/ Summer Time.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,329,338,,,,,,,,,,,,,,,,WOS:000855241500037,0
C,"Qian, J; Liu, YB; Liu, LM; Li, YM; Jiang, HY; Zhang, HS; Shi, SM",,,Assoc Computat Linguist,"Qian, Jing; Liu, Yibin; Liu, Lemao; Li, Yangming; Jiang, Haiyun; Zhang, Haisong; Shi, Shuming",,,Fine-grained Entity Typing without Knowledge Base,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Existing work on Fine-grained Entity Typing (FET) typically trains automatic models on the datasets obtained by using Knowledge Bases (KB) as distant supervision. However, the reliance on KB means this training setting can be hampered by the lack of or the incompleteness of the KB. To alleviate this limitation, we propose a novel setting for training FET models: FET without accessing any knowledge base. Under this setting, we propose a two-step framework to train FET models. In the first step, we automatically create pseudo data with fine-grained labels from a large unlabeled dataset. Then a neural network model is trained based on the pseudo data, either in an unsupervised way or using self-training under the weak guidance from a coarse-grained Named Entity Recognition (NER) model. Experimental results show that our method achieves competitive performance with respect to the models trained on the original KB-supervised datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5309,5319,,,,,,,,,,,,,,,,WOS:000855966305035,0
C,"Rasooli, MS; Callison-Burch, C; Wijaya, DT",,,Assoc Computat Linguist,"Rasooli, Mohammad Sadegh; Callison-Burch, Chris; Wijaya, Derry Tanti",,,Wikily Supervised Neural Translation Tailored to Cross-Lingual Tasks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily supervised translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multitasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a translated version of the English captioning data, using our wikily-supervised translation models. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1655,1670,,,,,,,,,,,,,,,,WOS:000855966301057,0
C,"Song, J; Kim, S; Yoon, S",,,Assoc Computat Linguist,"Song, Jongyoon; Kim, Sungwon; Yoon, Sungroh",,,AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En <-> De and WMT16 Ro -> En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En <-> De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.",,,,,"song, jiahao/GQP-4395-2022","Song, Jiahao/0000-0001-6296-1905",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1,14,,,,,,,,,,,,,,,,WOS:000855966300001,0
C,"Ushio, A; Liberatore, F; Camacho-Collados, J",,,Assoc Computat Linguist,"Ushio, Asahi; Liberatore, Federico; Camacho-Collados, Jose",,,Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical comparison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interesting findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statistical and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8089,8103,,,,,,,,,,,,,,,,WOS:000860727002016,0
C,"Xu, HR; Van Durme, B; Murray, K",,,Assoc Computat Linguist,"Xu, Haoran; van Durme, Benjamin; Murray, Kenton",,,"BERT, MBERT, or BIBERT? A Study on Contextualized Embeddings for Neural Machine Translation",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pretrained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BIBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En -> De and 38.61 for De -> En on the IWSLT'14 dataset, and 31.26 for En -> De and 34.94 for De -> En on the WMT'14 dataset, which exceeds all published numbers(12).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6663,6675,,,,,,,,,,,,,,,,WOS:000860727000049,0
C,"Yu, WJ; Wen, YP; Zheng, FD; Xiao, N",,,Assoc Computat Linguist,"Yu, Weijiang; Wen, Yingpeng; Zheng, Fudan; Xiao, Nong",,,Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The recent algorithms for math word problems (MWP) neglect to use outside knowledge not present in the problems. Most of them only capture the word-level relationship and ignore to build hierarchical reasoning like the human being for mining the contextual structure between words and sentences. In this paper, we propose a Reasoning with Pre-trained Knowledge and Hierarchical Structure (RPKHS) network, which contains a pre-trained knowledge encoder and a hierarchical reasoning encoder. Firstly, our pretrained knowledge encoder aims at reasoning the MWP by using outside knowledge from the pre-trained transformer-based models. Secondly, the hierarchical reasoning encoder is presented for seamlessly integrating the word-level and sentence-level reasoning to bridge the entity and context domain on MWP. Extensive experiments show that our RPKHS significantly outperforms state-of-the-art approaches on two large-scale commonly-used datasets, and boosts performance from 77.4% to 83.9% on Math23K, from 75.5 to 82.2% on Math23K with 5-fold cross-validation and from 83.7% to 89.8% on MAWPS. More extensive ablations are shown to demonstrate the effectiveness and interpretability of our proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3384,3394,,,,,,,,,,,,,,,,WOS:000855966303044,0
C,"Yu, WH; Zhu, CG; Zhao, T; Guo, ZC; Jiang, M",,,Assoc Computat Linguist,"Yu, Wenhao; Zhu, Chenguang; Zhao, Tong; Guo, Zhichun; Jiang, Meng",,,Sentence-Permuted Paragraph Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for both training phase and inference phase. Experiments on three paragraph generation benchmarks demonstrate PermGen generates more diverse outputs with a higher quality than existing models.,,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5051,5062,,,,,,,,,,,,,,,,WOS:000855966305016,0
C,"Yu, XT; Zhang, HM; Song, YQ; Zhang, CS; Xu, K; Yu, D",,,Assoc Computat Linguist,"Yu, Xintong; Zhang, Hongming; Song, Yangqiu; Zhang, Changshui; Xu, Kun; Yu, Dong",,,Exophoric Pronoun Resolution in Dialogues with Topic Regularization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3832,3845,,,,,,,,,,,,,,,,WOS:000855966303083,0
C,"Zhang, ZS; Ouyang, SR; Zhao, H; Utiyama, M; Sumita, E",,,Assoc Computat Linguist,"Zhang, Zhuosheng; Ouyang, Siru; Zhao, Hai; Utiyama, Masao; Sumita, Eiichiro",,,Smoothing Dialogue States for Open Conversational Machine Reading,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial by using hard-label decisions to activate question generation, which eventually hinders the model performance. In this work, we propose an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference. Experiments on the OR-ShARC dataset show the effectiveness of our method, which achieves new state-of-the-art results.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3685,3696,,,,,,,,,,,,,,,,WOS:000855966303071,0
C,"Zou, YC; Zhu, BL; Hu, XW; Gui, T; Zhang, Q",,,Assoc Computat Linguist,"Zou, Yicheng; Zhu, Bolin; Hu, Xingwu; Gui, Tao; Zhang, Qi",,,Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for low-resource dialogue summarization directly pretrain models in other domains, e.g., the news domain, but they generally neglect the huge difference between dialogues and conventional articles. To bridge the gap between out-of-domain pretraining and in-domain fine-tuning, in this work, we propose a multi-source pretraining paradigm to better leverage the external summary data. Specifically, we exploit large-scale in-domain non-summary data to separately pretrain the dialogue encoder and the summary decoder. The combined encoder-decoder model is then pretrained on the out-of-domain summary data using adversarial critics, aiming to facilitate domain-agnostic summarization. The experimental results on two public datasets show that with only limited training data, our approach achieves competitive performance and generalizes well in different dialogue scenarios.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,80,91,,,,,,,,,,,,,,,,WOS:000855966300007,0
C,"Alinejad, A; Shavarani, HS; Sarkar, A",,,Assoc Computat Linguist,"Alinejad, Ashkan; Shavarani, Hassan S.; Sarkar, Anoop",,,Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences. These oracle sequences can then be used to train a supervised model for action generation at inference time. Our approach provides an alternative to current heuristic methods in simultaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1734,1744,,,,,,,,,,,,,,,,WOS:000855966301063,0
C,"Arkhangorodsky, A; Chu, C; Fang, S; Huang, YQ; Jiang, DL; Nagesh, A; Zhang, BL; Knight, K",,,Assoc Computat Linguist,"Arkhangorodsky, Arkady; Chu, Christopher; Fang, Scot; Huang, Yiqi; Jiang, Denglin; Nagesh, Ajay; Zhang, Boliang; Knight, Kevin",,,MeetDot: Videoconferencing with Live Translation Captions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the retranslation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,195,202,,,,,,,,,,,,,,,,WOS:000855241500023,0
C,"Church, K; Bian, YC",,,Assoc Computat Linguist,"Church, Kenneth; Bian, Yuchen",,,Data Collection vs. Knowledge Graph Completion: What is Needed to Improve Coverage?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This survey/position paper discusses ways to improve coverage of resources such as Word-Net. Rapp estimated correlations, rho, between corpus statistics and psycholinguistic norms. rho improves with quantity (corpus size) and quality (balance). 1M words are enough for simple estimates (unigram frequencies), but at least 100M are required for pairs of words (word associations, edges). Knowledge Graph Completion (KGC) attempts to learn missing links in WN18. Unfortunately, WN18 is flawed with information leaking from train to test. More seriously, WN18 is based on SemCor (just 200k words) and dated (collected in 1960s). KGC cannot learn anything that happened since the 1960s, or associations requiring 100M words.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6210,6215,,,,,,,,,,,,,,,,WOS:000860727000016,0
C,"de Souza, E; Freitas, C",,,Assoc Computat Linguist,"de Souza, Elvis; Freitas, Claudia",,,"ET: A Workstation for Querying, Editing and Evaluating Annotated Corpora",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper we explore the functionalities of ET, a suite designed to support linguistic research and natural language processing tasks using corpora annotated in the CoNLL-U format. These goals are achieved by two integrated environments - Interrogatorio, an environment for querying and editing annotated corpora, and Julgamento, an environment for assessing their quality. ET is open-source, built on different Python Web technologies and has Web demonstrations available on-line. ET has been intensively used in our research group for over two years, being the chosen framework for several linguistic and NLP-related studies conducted by its researchers.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,35,41,,,,,,,,,,,,,,,,WOS:000855241500005,0
C,"Eddine, MK; Tixier, AJP; Vazirgiannis, M",,,Assoc Computat Linguist,"Eddine, Moussa Kamal; Tixier, Antoine J-P; Vazirgiannis, Michalis",,,BARThez: a Skilled Pretrained French Sequence-to-Sequence Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two generative tasks from a novel summarization dataset, Orange-Sum, that we created for this research. We show BARThez to be very competitive with state-of-the-art BERT-based French language models such as CamemBERT and FlauBERT. We also continue the pretraining of a multilingual BART on BARThez' corpus, and show our resulting model, mBARThez, to significantly boost BARThez' generative performance. Code, data and models are publicly available.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9369,9390,,,,,,,,,,,,,,,,WOS:000860727003041,0
C,"Fei, ZC; Zhang, Q; Zhou, YQ",,,Assoc Computat Linguist,"Fei, Zichu; Zhang, Qi; Zhou, Yaqian",,,Iterative GNN-based Decoder for Question Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural question generation (QG) aims to generate questions from a passage, and generated questions are answered from the passage. Most models with state-of-the-art performance model the previously generated text at each decoding step. However, (1) they ignore the rich structure information that is hidden in the previously generated text. (2) they ignore the impact of copied words on the passage. We perceive that information in previously generated words serves as auxiliary information in subsequent generation. To address these problems, we design the Iterative Graph Network-based Decoder (IGND) to model the previous generation using a Graph Neural Network at each decoding step. Moreover, our graph model captures dependency relations in the passage that boost the generation. Experimental results demonstrate that our model outperforms the state-of-the-art models with sentence-level QG tasks on SQuAD and MARCO datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2573,2582,,,,,,,,,,,,,,,,WOS:000855966302060,0
C,"Guan, Y; Guo, SR; Li, R; Li, XL; Zhang, H",,,Assoc Computat Linguist,"Guan, Yong; Guo, Shaoru; Li, Ru; Li, Xiaoli; Zhang, Hu",,,Integrating Semantic Scenario and Word Relations for Abstractive Sentence Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently graph-based methods have been adopted for Abstractive Text Summarization. However, existing graph-based methods only consider either word relations or structure information, which neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Dual Graph network for Abstractive Sentence Summarization (DG-ABS). Specifically, we first construct semantic scenario graph and semantic word relation graph based on FrameNet, and subsequently learn their representations and design graph fusion method to enhance their correlation and obtain better semantic representation for summary generation. Experimental results show our model outperforms existing state-of-the-art methods on two popular benchmark datasets, i.e., Gigaword and DUC 2004.",,,,,"Li, Xiaoli/GYQ-7384-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2522,2529,,,,,,,,,,,,,,,,WOS:000855966302055,0
C,"Guerreiro, NM; Martins, AFT",,,Assoc Computat Linguist,"Guerreiro, Nuno M.; Martins, Andre F. T.",,,SPECTRA: Sparse Structured Text Rationalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6534,6550,,,,,,,,,,,,,,,,WOS:000860727000040,0
C,"Han, RJ; Hsu, IH; Sun, J; Baylon, J; Ning, Q; Roth, D; Peng, NY",,,Assoc Computat Linguist,"Han, Rujun; Hsu, I-Hung; Sun, Jiao; Baylon, Julia; Ning, Qiang; Roth, Dan; Peng, Nanyun",,,ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines' ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce ESTER, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (EM), F1 and event-based HIT@1 scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7543,7559,,,,,,,,,,,,,,,,WOS:000860727001045,0
C,"Hasler, E; Domhan, T; Trenous, J; Tran, K; Byrne, B; Hieber, F",,,Assoc Computat Linguist,"Hasler, Eva; Domhan, Tobias; Trenous, Jonay; Tran, Ke; Byrne, Bill; Hieber, Felix",,,Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8470,8477,,,,,,,,,,,,,,,,WOS:000860727002044,0
C,"Huang, H; Kajiwara, T; Arase, Y",,,Assoc Computat Linguist,"Huang, Han; Kajiwara, Tomoyuki; Arase, Yuki",,,Definition Modelling for Appropriate Specificity,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under- specificity problems. Over-specific definitions present narrow word meanings, whereas under-specific definitions present general and context-insensitive meanings. Herein, we propose a method for definition generation with appropriate specificity. The proposed method addresses the aforementioned problems by leveraging a pre-trained encoder-decoder model, namely Text-to-Text Transfer Transformer, and introducing a re-ranking mechanism to model specificity in definitions. 1 Experimental results on standard evaluation datasets indicate that our method significantly outperforms the previous state-of-the-art method. Moreover, manual evaluation confirms that our method effectively addresses the over/under-specificity problems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2499,2509,,,,,,,,,,,,,,,,WOS:000855966302053,0
C,"Kahardipraja, P; Madureira, B; Schlangen, D",,,Assoc Computat Linguist,"Kahardipraja, Patrick; Madureira, Brielen; Schlangen, David",,,Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1178,1189,,,,,,,,,,,,,,,,WOS:000855966301023,0
C,"Kil, J; Zhang, C; Xuan, D; Chao, WL",,,Assoc Computat Linguist,"Kil, Jihyung; Zhang, Cheng; Xuan, Dong; Chao, Wei-Lun",,,Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples - there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the unknowns to the learned VQA model are indeed known in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the how many question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SIMPLEAUG to turn this known knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models' performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6346,6361,,,,,,,,,,,,,,,,WOS:000860727000027,0
C,"Lin, SC; Yang, JH; Lin, J",,,Assoc Computat Linguist,"Lin, Sheng-Chieh; Yang, Jheng-Hong; Lin, Jimmy",,,Contextualized Query Embeddings for Conversational Search,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L-2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1004,1015,,,,,,,,,,,,,,,,WOS:000855966301010,0
C,"Moghe, N; Steedman, M; Birch, A",,,Assoc Computat Linguist,"Moghe, Nikita; Steedman, Mark; Birch, Alexandra",,,Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pre-trained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pre-trained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -> Chinese, Chinese -> English) and Multilingual WoZ (English -> German, English -> Italian) datasets. We achieve impressive improvements (> 20% on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10% of the target language task data and zero-shot setup respectively.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1137,1150,,,,,,,,,,,,,,,,WOS:000855966301020,0
C,"Montero, I; Pappas, N; Smith, NA",,,Assoc Computat Linguist,"Montero, Ivan; Pappas, Nikolaos; Smith, Noah A.",,,Sentence Bottleneck Autoencoders from Transformer Language,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1822,1831,,,,,,,,,,,,,,,,WOS:000855966301070,0
C,"Pasunuru, R; Stoyanov, V; Bansal, M",,,Assoc Computat Linguist,"Pasunuru, Ramakanth; Stoyanov, Veselin; Bansal, Mohit",,,Continual Few-Shot Learning for Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize sentiment, handle numbers, perform coreference, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data: two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various baselines from diverse paradigms (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite1 and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5688,5702,,,,,,,,,,,,,,,,WOS:000855966305064,0
C,"Qi, FC; Chen, YY; Li, MK; Yao, Y; Liu, ZY; Sun, MS",,,Assoc Computat Linguist,"Qi, Fanchao; Chen, Yangyi; Li, Mukai; Yao, Yuan; Liu, Zhiyuan; Sun, Maosong",,,ONION: A Simple and Effective Defense Against Textual Backdoor Attacks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https: //github.com/thunlp/ONION.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9558,9566,,,,,,,,,,,,,,,,WOS:000860727003053,0
C,"Safavi, T; Zhu, J; Koutra, D; Safavi, T",,,Assoc Computat Linguist,"Safavi, Tara; Zhu, Jing; Koutra, Danai; Safavi, Tara",,,NEGATER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB statements, even though negative (false) statements are often also important for discriminative reasoning over commonsense KBs. As a first step toward the latter, this paper proposes NegatER, a framework that ranks potential negatives in commonsense KBs using a contextual language model (LM). Importantly, as most KBs do not contain negatives, NegatER relies only on the positive knowledge in the LM and does not require ground-truth negative examples. Experiments demonstrate that, compared to multiple contrastive data augmentation approaches, NegatER yields negatives that are more grammatical, coherent, and informative-leading to statistically significant accuracy improvements in a challenging KB completion task and confirming that the positive knowledge in LMs can be re-purposed to generate negative knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5633,5646,,,,,,,,,,,,,,,,WOS:000855966305060,0
C,"Sheno, R; Herbig, N; Kruger, A; van Genabith, J",,,Assoc Computat Linguist,"Sheno, Raksha; Herbig, Nico; Krueger, Antonio; van Genabith, Josef",,,Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Compared to fully manual translation, postediting (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not known (i) at what quality threshold QE is actually beginning to be useful for human PE, and (ii), how to best present wordlevel QE information to translators. In particular, should word-level QE visualization indicate uncertainty of the QE model or not? In this paper, we address both research questions with real and simulated word-level QE, visualizations, and user studies, where time, subjective ratings, and quality of the final translations are assessed. Results show that current wordlevel QE models are not yet good enough to support PE. Instead, quality levels of similar to 80% F1 are required. For helpful quality levels, a visualization reflecting the uncertainty of the QE model is preferred. Our analysis further shows that speed gains achieved through QE are not merely a result of blindly trusting the QE system, but that the quality of the final translations also improves. The threshold results from the paper establish a quality goal for future wordlevel QE research.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10173,10185,,,,,,,,,,,,,,,,WOS:000860727004023,0
C,"Wang, L; Zhao, W; Liu, JM",,,Assoc Computat Linguist,"Wang, Liang; Zhao, Wei; Liu, Jingming",,,Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the quality of alignment. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3807,3815,,,,,,,,,,,,,,,,WOS:000855966303081,0
C,"Welivita, A; Xie, YB; Pu, P",,,Assoc Computat Linguist,"Welivita, Anuradha; Xie, Yubo; Pu, Pearl",,,A Large-Scale Dataset for Empathetic Response Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emotion plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using offline experiments and visual validation methods. The resultant procedure can be used to create similar datasets in the same domain as well as in other domains.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1251,1264,,,,,,,,,,,,,,,,WOS:000855966301029,0
C,"Wu, ZQ; Lu, BR; Hajishirzi, H; Ostendorf, M",,,Assoc Computat Linguist,"Wu, Zeqiu; Lu, Bo-Ru; Hajishirzi, Hannaneh; Ostendorf, Mari",,,DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.,,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1852,1863,,,,,,,,,,,,,,,,WOS:000855966301073,0
C,"Xia, P; Van Durme, B",,,Assoc Computat Linguist,"Xia, Patrick; Van Durme, Benjamin",,,Moving on from OntoNotes: Coreference Resolution Model Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which often differ from those of OntoNotes. We aim to quantify transferability of coref models based on the number of annotated documents available in the target dataset. We examine eleven target datasets and find that continued training is consistently effective and especially beneficial when there are few target documents. We establish new benchmarks across several datasets, including state-of-the-art results on PreCo.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5241,5256,,,,,,,,,,,,,,,,WOS:000855966305029,0
C,"Xu, JT; Yvon, F",,,Assoc Computat Linguist,"Xu, Jitao; Yvon, Francois",,,"One Source, Two Targets: Challenges and Rewards of Dual Decoding",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four applications. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8533,8546,,,,,,,,,,,,,,,,WOS:000860727002049,0
C,"Yam, YM; Li, RM; Wang, SR; Zhang, HZ; Zan, DG; Zhang, FZ; Wu, W; Xu, WR",,,Assoc Computat Linguist,"Yam, Yuanmeng; Li, Rumei; Wang, Sirui; Zhang, Hongzhi; Zan, Daoguang; Zhang, Fuzheng; Wu, Wei; Xu, Weiran",,,Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3653,3660,,,,,,,,,,,,,,,,WOS:000855966303068,0
C,"Zhang, XY; Chen, MH; May, J",,,Assoc Computat Linguist,"Zhang, Xiyang; Chen, Muhao; May, Jonathan",,,Salience-Aware Event Chain Modeling for Narrative Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1418,1428,,,,,,,,,,,,,,,,WOS:000855966301040,0
C,"Zhou, K; Zhao, WX; Wang, SR; Zhang, FZ; Wu, W; We, JR",,,Assoc Computat Linguist,"Zhou, Kun; Zhao, Wayne Xin; Wang, Sirui; Zhang, Fuzheng; Wu, Wei; We, Ji-Rong",,,Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the robustness of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the Gaussian noise provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the robustness of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at https://github.com/RUCAIBox/VDA.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3875,3887,,,,,,,,,,,,,,,,WOS:000855966304002,0
C,"Zhuang, SY; Zuccon, GO",,,Assoc Computat Linguist,"Zhuang, Shengyao; Zuccon, Guido",,,Dealing with Typos for BERT-based Passage Retrieval and Ranking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a passage retrieval method, and the BERT re-ranker, a popular passage re-ranking method. In this context, we formally investigate how these models respond and adapt to a specific type of keyword mismatch that caused by keyword typos occurring in queries. Through empirical investigation, we find that typos can lead to a significant drop in retrieval and ranking effectiveness. We then propose a simple typos-aware training framework for DR and BERT re-ranker to address this issue. Our experimental results on the MS MARCO passage ranking dataset show that, with our proposed typos-aware training, DR and BERT re-ranker can become robust to typos in queries, resulting in significantly improved effectiveness compared to models trained without appropriately accounting for typos.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2836,2842,,,,,,,,,,,,,,,,WOS:000855966302084,0
C,"Abaho, M; Bollegala, D; Williamson, P; Dodd, S",,,Assoc Computat Linguist,"Abaho, Micheal; Bollegala, Danushka; Williamson, Paula; Dodd, Susanna",,,Detect and Classify - Joint Span Detection and Classification for Health Outcomes,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a pre-defined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8709,8721,,,,,,,,,,,,,,,,WOS:000860727002064,0
C,"Adelani, DI; Zhang, MR; Shen, XY; Davody, A; Kleinbauer, T; Klakow, D",,,Assoc Computat Linguist,"Adelani, David Ifeoluwa; Zhang, Miaoran; Shen, Xiaoyu; Davody, Ali; Kleinbauer, Thomas; Klakow, Dietrich",,,Preventing Author Profiling through Zero-Shot Multilingual Back-Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of current state-of-the-art approaches the improved privacy is accompanied by an undesirable drop in the downstream utility of the transformed data. In this paper, we propose a simple, zero-shot way to effectively lower the risk of author profiling through multilingual back-translation using off-the-shelf translation models. We compare our models with five representative text style transfer models on three datasets across different domains. Results from both an automatic and a human evaluation show that our approach achieves the best overall performance while requiring no training data. We are able to lower the adversarial prediction of gender and race by up to 22% while retaining 95% of the original utility on downstream tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8687,8695,,,,,,,,,,,,,,,,WOS:000860727002062,0
C,"Branco, R; Branco, A; Silva, J; Rodrigues, J",,,Assoc Computat Linguist,"Branco, Ruben; Branco, Antonio; Silva, Joao; Rodrigues, Joao",,,Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets.",,,,,,"Branco, Ruben/0000-0002-8126-8513",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1504,1521,,,,,,,,,,,,,,,,WOS:000855966301046,0
C,"Choenni, R; Shutova, E; van Rooij, R",,,Assoc Computat Linguist,"Choenni, Rochelle; Shutova, Ekaterina; van Rooij, Robert",,,Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Warning: this paper contains content that may be offensive or upsetting. In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1477,1491,,,,,,,,,,,,,,,,WOS:000855966301044,0
C,"Ferdowsi, S; Borissov, N; Knafou, J; Amini, P; Teodoro, D",,,Assoc Computat Linguist,"Ferdowsi, Sohrab; Borissov, Nikolay; Knafou, Julien; Amini, Poorya; Teodoro, Douglas",,,Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across different documents. We applied our model to classify clinical trial (CT) protocols into completed and terminated categories. We use bag-of-words based, as well as pre-trained transformer-based embeddings to featurize the graph nodes, achieving f1-scores similar or equal to 0.85 on a publicly available large scale CT registry of around 360K protocols. We further demonstrate how the selective pooling can add insights into the CT termination status prediction. We make the source code and dataset splits accessible.",,,,,"Teodoro, Douglas/H-5122-2012","Teodoro, Douglas/0000-0001-6238-4503",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,608,618,,,,,,,,,,,,,,,,WOS:000855966300048,0
C,"Friedman, D; Dodge, B; Chen, DQ",,,Assoc Computat Linguist,"Friedman, Dan; Dodge, Ben; Chen, Danqi",,,Single-dataset Experts for Multi-dataset Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub-distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems.(1)",,,,,,"Dodge, Benjamin/0000-0002-2519-2219",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6128,6137,,,,,,,,,,,,,,,,WOS:000860727000010,0
C,"Goldman, O; Tsarfaty, R",,,Assoc Computat Linguist,"Goldman, Omer; Tsarfaty, Reut",,,Minimal Supervision for Morphological Inflection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small as five labeled inflection tables, accompanied by a large bulk of unlabeled text. Our bootstrapping method exploits the orthographic and semantic regularities in morphological systems in a two-phased setup, where word tagging based on analogies is followed by word pairing based on distances. Our experiments with the Paradigm Cell Filling Problem over eight typologically different languages show that in languages with relatively simple morphology, orthographic regularities on their own allow inflection models to achieve respectable accuracy. Combined orthographic and semantic regularities alleviate difficulties with particularly complex morpho-phonological systems. We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2078,2088,,,,,,,,,,,,,,,,WOS:000855966302018,0
C,"Jahan, L; Mittal, R; Finlayson, MA",,,Assoc Computat Linguist,"Jahan, Labiba; Mittal, Rahul; Finlayson, Mark A.",,,Inducing Stereotypical Character Roles from Plot Structure,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in narratives: they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters' roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp's structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp's dramatis personae with F-1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The bestperforming feature set comprises plot functions, unigrams, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,492,497,,,,,,,,,,,,,,,,WOS:000855966300039,0
C,"Klein, T; Nabi, M",,,Assoc Computat Linguist,"Klein, Tassilo; Nabi, Moin",,,Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.(1)",,,,,,"Klein, Tassilo/0000-0002-0631-2940",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8737,8743,,,,,,,,,,,,,,,,WOS:000860727002066,0
C,"Lane, W; Bird, S",,,Assoc Computat Linguist,"Lane, William; Bird, Steven",,,Local Word Discovery for Interactive Transcription,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17%. Further, we find that 75% of breath groups in the test set receive at least one correct partial or full-word suggestion.",,,,,"Bird, Steven/E-7945-2019","Bird, Steven/0000-0003-3782-7733",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2058,2067,,,,,,,,,,,,,,,,WOS:000855966302016,0
C,"Liang, ZJ; Hu, H; Xu, C; Miao, J; He, YY; Chen, YN; Geng, XB; Liang, F; Jiang, DX",,,Assoc Computat Linguist,"Liang, Zujie; Hu, Huang; Xu, Can; Miao, Jian; He, Yingying; Chen, Yining; Geng, Xiubo; Liang, Fan; Jiang, Daxin",,,Learning Neural Templates for Recommender Dialogue System,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Though recent end-to-end neural models have shown the promising progress on Conversational Recommender System (CRS), two key challenges still remain. First, the recommended items cannot be always incorporated into the generated replies precisely and appropriately. Second, only the items mentioned in the training corpus have a chance to be recommended in the conversation. To tackle these challenges, we introduce a novel framework called NTRD for recommender dialogue system that decouples the dialogue generation from the item recommendation. NTRD has two key components, i.e., response template generator and item selector. The former adopts an encoder-decoder model to generate a response template with slot locations tied to target items, while the latter fills in slot locations with the proper items using a sufficient attention mechanism. Our approach combines the strengths of both classical slot filling approaches (that are generally controllable) and modern neural NLG approaches (that are generally more natural and accurate). Extensive experiments on the benchmark REDIAL show our NTRD significantly outperforms the previous state-of-the-art methods. Besides, our approach has the unique advantage to produce novel items that do not appear in the training set of dialogue corpus. The code is available at https://github. com/jokieleung/NTRD.",,,,,"Liang, Fan/HGA-0248-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7821,7833,,,,,,,,,,,,,,,,WOS:000860727001065,0
C,"Liu, ZC; Xu, HY; Wen, YL; Jiang, N; Wu, HY; Yuan, XJ",,,Assoc Computat Linguist,"Liu, Zichen; Xu, Hongyuan; Wen, Yanlong; Jiang, Ning; Wu, Haiying; Yuan, Xiaojie",,,TEMP: Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose TEMP, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. For the first time, TEMP employs pre-trained contextual encoders in taxonomy construction and hypernym detection problems. Experiments prove that pre-trained contextual embeddings are able to capture hypernym-hyponym relations. To learn more detailed differences between taxonomy-paths, we train the model with dynamic margin loss by a novel dynamic margin function. Extensive evaluations exhibit that TEMP outperforms prior state-of-the-art taxonomy expansion approaches by 14.3% in accuracy and 15.8% in mean reciprocal rank on three public benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3854,3863,,,,,,,,,,,,,,,,WOS:000855966303085,0
C,"Lowell, D; Howard, BE; Lipton, ZC; Wallace, BC",,,Assoc Computat Linguist,"Lowell, David; Howard, Brian E.; Lipton, Zachary C.; Wallace, Byron C.",,,Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Unsupervised Data Augmentation (UDA) is a semi-supervised technique that applies a consistency loss to penalize differences between a model's predictions on (a) observed (unlabeled) examples; and (b) corresponding `noised' examples produced via data augmentation. While UDA has gained popularity for text classification, open questions linger over which of its components are important, and how to extend the method to sequence labeling tasks; this paper addresses these questions. Our main contribution is an empirical study of UDA to establish which components of the algorithm confer benefits in NLP. Notably, although prior work has emphasized use of clever augmentation techniques including back-translation, we find that enforcing consistency between predictions assigned to observed and randomly substituted words often yields comparable (or greater) benefits compared to these more complex perturbation models. Furthermore, we find that applying UDA's consistency loss affords meaningful gains without any unlabeled data at all, i.e., in a standard supervised setting. In short, UDA need not be unsupervised to realize much of its noted benefits, and does not require complex data augmentation to be effective.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4992,5001,,,,,,,,,,,,,,,,WOS:000855966305012,0
C,"Lv, X; Cao, YX; Hou, L; Li, JZ; Liu, ZY; Zhang, YC; Dai, ZL",,,Assoc Computat Linguist,"Lv, Xin; Cao, Yixin; Hou, Lei; Li, Juanzi; Liu, Zhiyuan; Zhang, Yichi; Dai, Zelin",,,Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate these metrics using the interpretability scores of rules. Furthermore, we manually annotate all possible rules and establish a Benchmark to detect the Interpretability of Multi-hop Reasoning (BIMR). In experiments, we verify the effectiveness of our benchmark. Besides, we run nine representative baselines on our benchmark, and the experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is 51.7% lower than the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., how to better incorporate rule information into the multihop reasoning model. Our codes and datasets can be obtained from https://github. com/THU-KEG/BIMR.",,,,,,"Liu, Zhiyuan/0000-0002-7709-2543",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8899,8911,,,,,,,,,,,,,,,,WOS:000860727003002,0
C,"Mensah, S; Sun, K; Aletras, N",,,Assoc Computat Linguist,"Mensah, Samuel; Sun, Kai; Aletras, Nikolaos",,,An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on pretrained word embeddings or language models that leverage part-of-speech and position embeddings, aiming to examine the actual contribution of each component in TOWE. We also adapt a graph convolutional network (GCN) to enhance word representations by incorporating syntactic information. Our experimental results demonstrate that BiLSTM-based models can effectively encode position information into word representations while using a GCN only achieves marginal gains. Interestingly, our simple methods outperform several state-of-the-art complex neural structures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9174,9179,,,,,,,,,,,,,,,,WOS:000860727003024,0
C,"Merrill, W; Ramanujan, V; Goldberg, Y; Schwartz, R; Smith, NA",,,Assoc Computat Linguist,"Merrill, William; Ramanujan, Vivek; Goldberg, Yoav; Schwartz, Roy; Smith, Noah A.",,,Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (l(2) norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such saturated networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1766,1781,,,,,,,,,,,,,,,,WOS:000855966301066,0
C,"Shen, JM; Zhang, YY; Ji, H; Han, JW",,,Assoc Computat Linguist,"Shen, Jiaming; Zhang, Yunyi; Ji, Heng; Han, Jiawei",,,Corpus-based Open-Domain Event Type Induction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Traditional event extraction methods require predefined event types and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of hpredicate sense, object headi pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering hpredicate sense, object headi pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5427,5440,,,,,,,,,,,,,,,,WOS:000855966305045,0
C,"Si, CL; Zhao, C; Boyd-Graber, J",,,Assoc Computat Linguist,"Si, Chenglei; Zhao, Chen; Boyd-Graber, Jordan",,,What's in a Name? Answer Equivalence For Open-Domain Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers (i.e., equivalent answers). We incorporate answers for two settings: evaluation with additional answers and model training with equivalent answers. We analyse three QA benchmarks: Natural Questions, TriviaQA and SQuAD. Answer expansion increases the exact match score on all datasets for evaluation, while incorporating it helps model training over real-world datasets. We ensure the additional answers are valid through a human post hoc evaluation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9623,9629,,,,,,,,,,,,,,,,WOS:000860727003058,0
C,"Subramanian, S; Han, XD; Baldwin, T; Cohn, T; Frermann, L",,,Assoc Computat Linguist,"Subramanian, Shivashankar; Han, Xudong; Baldwin, Timothy; Cohn, Trevor; Frermann, Lea",,,Evaluating Debiasing Techniques for Intersectional Biases,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Bias is pervasive in NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider 'gerrymandering' groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple protected attributes.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2492,2498,,,,,,,,,,,,,,,,WOS:000855966302052,0
C,"Sun, S; El-Kishky, A; Chaudhary, V; Cross, J; Guzman, F; Specia, L",,,Assoc Computat Linguist,"Sun, Shuo; El-Kishky, Ahmed; Chaudhary, Vishrav; Cross, James; Guzman, Francisco; Specia, Lucia",,,Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence-level Quality Estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and thus infeasible for many real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5865,5875,,,,,,,,,,,,,,,,WOS:000855966306004,0
C,"Takeoka, K; Akimoto, K; Oyamada, M",,,Assoc Computat Linguist,"Takeoka, Kunihiro; Akimoto, Kosuke; Oyamada, Masafumi",,,Low-resource Taxonomy Enrichment with Pretrained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task fail to find optimal parents of new terms in low-resource settings where only small taxonomies are available because of overfitting to hierarchical relationships in the taxonomies. To tackle the problem of low-resource taxonomy enrichment, we propose Musubu, an efficient framework for taxonomy enrichment in low-resource settings with pretrained language models (LMs) as knowledge bases to compensate for the shortage of information. Musubu leverages an LM-based classifier to determine whether or not inputted term pairs have hierarchical relationships. Musubu also utilizes Hearst patterns to generate queries to leverage implicit knowledge from the LM efficiently for more accurate prediction. We empirically demonstrate the effectiveness of our method in extensive experiments on taxonomies from both a SemEval task and real-world retailer datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2747,2758,,,,,,,,,,,,,,,,WOS:000855966302076,0
C,"Tang, R; Kumar, K; Chalkley, K; Xin, J; Zhang, LM; Li, WY; Yang, GF; Mao, YJ; Shin, J; Murray, GC; Lin, J",,,Assoc Computat Linguist,"Tang, Raphael; Kumar, Karun; Chalkley, Kendra; Xin, Ji; Zhang, Liming; Li, Wenyan; Yang, Gefei; Mao, Yajie; Shin, Junho; Murray, G. Craig; Lin, Jimmy",,,Voice Query Auto Completion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Query auto completion (QAC) is the task of predicting a search engine user's final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcripts as users speak. Naively applying existing methods fails because the intermediate transcripts often don't form prefixes or even substrings of the final transcript. To address this issue, we propose to condition QAC approaches on intermediate transcripts to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best method obtains an 18% relative improvement in mean reciprocal rank over previous methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,900,906,,,,,,,,,,,,,,,,WOS:000855966301001,0
C,"van Niekerk, C; Malinin, A; Geishauser, C; Heck, M; Lin, HC; Lubis, N; Feng, ST; Gasic, M",,,Assoc Computat Linguist,"van Niekerk, Carel; Malinin, Andrey; Geishauser, Christian; Heck, Michael; Lin, Hsien-chin; Lubis, Nurul; Feng, Shutong; Gasic, Milica",,,Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The ability to identify and resolve uncertainty is crucial for the robustness of a dialogue system. Indeed, this has been confirmed empirically on systems that utilise Bayesian approaches to dialogue belief tracking. However, such systems consider only confidence estimates and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take uncertainties into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of uncertainty to the feature space of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the robustness of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take uncertainty into account.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7901,7914,,,,,,,,,,,,,,,,WOS:000860727002001,0
C,"Wang, CG; Liu, X; Chen, Z; Hong, HY; Tang, J; Song, D",,,Assoc Computat Linguist,"Wang, Chenguang; Liu, Xiao; Chen, Zui; Hong, Haoyun; Tang, Jie; Song, Dawn",,,Zero-Shot Information Extraction as a Unified Text-to-Triple Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1225,1238,,,,,,,,,,,,,,,,WOS:000855966301027,0
C,"Wu, SX; Li, Y; Wang, MH; Zhang, DW; Zhou, Y; Wu, ZH",,,Assoc Computat Linguist,"Wu, Sixing; Li, Ying; Wang, Minghui; Zhang, Dawei; Zhou, Yang; Wu, Zhonghai",,,More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper proposes a novel dialogue generation model, MSKE-Dialog, to solve this issue with three unique advantages: (1) Rather than only one, MSKE-Dialog can simultaneously leverage multiple heterogeneous knowledge sources (it includes but is not limited to commonsense knowledge facts, text knowledge, infobox knowledge) to improve the knowledge coverage; (2) To avoid the topic conflict among the context and different knowledge sources, we propose a Multi-Reference Selection to better select context/knowledge; (3) We propose a Multi-Reference Generation to generate informative responses by referring to multiple generation references at the same time. Extensive evaluations on a Chinese dataset show the superior performance of this work against various state-of-the-art approaches. To our best knowledge, this work is the first to use the multi-source heterogeneous knowledge in the open-domain knowledge-enhanced dialogue generation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2286,2300,,,,,,,,,,,,,,,,WOS:000855966302034,0
C,"Yang, B; Wu, LJ",,,Assoc Computat Linguist,"Yang, Bo; Wu, Lijun",,,How to Leverage Multimodal EHR Data for Better Medical Predictions?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of medical service. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature, and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction. The results on two medical prediction tasks show that our fused model with different data outperforms the state-of-the-art method that without clinical notes, which illustrates the importance of our fusion method and the value of clinical note features. Our code is available at https: //github.com/emnlp-mimic/mimic.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4029,4038,,,,,,,,,,,,,,,,WOS:000855966304016,0
C,"Yu, T; Joty, S",,,Assoc Computat Linguist,"Yu, Tao; Joty, Shafiq",,,Effective Fine-tuning Methods for Cross-lingual Adaptation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as complementary to multilingual language modeling using the unlabeled data in the target language. We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method's effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8492,8501,,,,,,,,,,,,,,,,WOS:000860727002046,0
C,"Zhang, DJ; Li, SW; Xiao, W; Zhu, H; Nallapati, R; Arnold, AO; Xiang, B",,,Assoc Computat Linguist,"Zhang, Dejiao; Li, Shang-Wen; Xiao, Wei; Zhu, Henghui; Nallapati, Ramesh; Arnold, Andrew O.; Xiang, Bing",,,Pairwise Supervised Contrastive Learning of Sentence Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%-13% averaged improvement on eight clustering tasks, and 5%-6% averaged improvement on seven semantic textual similarity (STS) tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5786,5798,,,,,,,,,,,,,,,,WOS:000855966305071,0
C,"Zhao, JW; Luo, W; Chen, BX; Gilman, A",,,Assoc Computat Linguist,"Zhao, Jiawei; Luo, Wei; Chen, Boxing; Gilman, Andrew",,,Mutual-Learning Improves End-to-End Speech Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A currently popular research area in end-to end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously allows only a one way transfer, limiting the overall effectiveness of the approach by the performance of the pre-trained teacher model. Therefore, we pose that in this respect knowledge distillation based approaches are sub-optimal. We propose an alternative a trainable mutual-learning scenario, where the MT and ST models are collaboratively trained and are considered as peers, rather than teacher/student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, models can effectively utilise the auxiliary information from peer models and achieve compelling results on MuST-C datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3989,3994,,,,,,,,,,,,,,,,WOS:000855966304012,0
C,"Ait-Mokhtar, S; Brun, C; Hoppenot, Y; Sandor, A",,,Assoc Computat Linguist,"Ait-Mokhtar, Salah; Brun, Caroline; Hoppenot, Yves; Sandor, Agnes",,,Semantic Context Path Labeling for Semantic Exploration of User Reviews,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper we present a prototype demonstrator showcasing a novel method to perform semantic exploration of user reviews. The system enables effective navigation in a rich contextual semantic schema with a large number of hierarchically structured classes indicating relevant information. In order to identify instances of the structured classes in the reviews, we defined a new Information Extraction task called Semantic Context Path (SCP) labeling, which simultaneously assigns types and semantic roles to entity mentions. Reviews can thus rapidly be explored based on the fine-grained and structured semantic classes. As a proof-of-concept, we have implemented this system for reviews on Points-of-Interest, in English and Korean.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,106,113,,,,,,,,,,,,,,,,WOS:000855241500013,0
C,"Blloshmi, R; Bevilacqua, M; Fabiano, E; Caruso, V; Navigli, R",,,Assoc Computat Linguist,"Blloshmi, Rexhina; Bevilacqua, Michele; Fabiano, Edoardo; Caruso, Valentina; Navigli, Roberto",,,SPRING Goes Online: End-to-End AMR Parsing and Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper we present SPRING Online Services, a Web interface and RESTful APIs for our state-of-the-art AMR parsing and generation system, SPRING (Symmetric PaRsIng aNd Generation). The Web interface has been developed to be easily used by the Natural Language Processing community, as well as by the general public. It provides, among other things, a highly interactive visualization platform and a feedback mechanism to obtain user suggestions for further improvements of the system's output. Moreover, our RESTful APIs enable easy integration of SPRING in downstream applications where AMR structures are needed. Finally, we make SPRING Online Services freely available at hup://nlp.uniroma I it/spring and, in addition, we release extra model checkpoints to be used with the original SPRING Python code.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,134,142,,,,,,,,,,,,,,,,WOS:000855241500016,0
C,"Bondarenko, Y; Nagel, M; Blankevoort, T",,,Assoc Computat Linguist,"Bondarenko, Yelysei; Nagel, Markus; Blankevoort, Tijmen",,,Understanding and Overcoming the Challenges of Efficient Transformer Quantization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges - namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme - per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7947,7969,,,,,,,,,,,,,,,,WOS:000860727002005,0
C,"Cao, M; Chen, L; Shou, Z; Zhang, C; Zou, YX",,,Assoc Computat Linguist,"Cao, Meng; Chen, Long; Shou, Zheng; Zhang, Can; Zou, Yuexian",,,On Pursuit of Designing Multi-modal Transformer for Video Grounding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottomup model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-toend multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two encoders for video and language encoding, and a crossmodal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve recordbreaking performance on all datasets and metrics, with several times faster inference speed. Our project is available at GTR.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9810,9823,,,,,,,,,,,,,,,,WOS:000860727003074,0
C,"Chowdhury, SBR; Brahman, F; Chaturvedi, S",,,Assoc Computat Linguist,"Chowdhury, Somnath Basu Roy; Brahman, Faeze; Chaturvedi, Snigdha",,,Is Everything in Order? A SimpleWay to Order Sentences,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine's understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (RE- BART) that leverages a pre-trained Transformer-based model to identify a coherent order for a given set of shuffled sentences. The model takes a set of shuffled sentences with sentence-specific markers as input and generates a sequence of position markers of the sentences in the ordered text. RE-BART achieves the state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and Kendall's tau (gamma). We perform evaluations in a zero-shot setting, showcasing that our model is able to generalize well across other datasets. We additionally perform several experiments to understand the functioning and limitations of our framework.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10769,10779,,,,,,,,,,,,,,,,WOS:000860727004065,0
C,"Christophe, C; Velcin, J; Cugliari, J; Boumghar, M; Suignard, P",,,Assoc Computat Linguist,"Christophe, Clement; Velcin, Julien; Cugliari, Jairo; Boumghar, Manel; Suignard, Philippe",,,Monitoring geometrical properties of word embeddings for detecting the emergence of new topics,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and language evolution, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of weak signals at the word level. We propose to monitor the behavior of words representation in an embedding space and use one of its geometrical properties to characterize the emergence of topics. As evaluation is typically hard for this kind of task, we present a framework for quantitative evaluation. We show positive results that outperform state-of-the-art methods on two public datasets of press and scientific articles.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,994,1003,,,,,,,,,,,,,,,,WOS:000855966301009,0
C,"Dev, S; Monajatipoor, M; Ovalle, A; Subramonian, A; Phillips, JM; Chang, KW",,,Assoc Computat Linguist,"Dev, Sunipa; Monajatipoor, Masoud; Ovalle, Anaelia; Subramonian, Arjun; Phillips, Jeff M.; Chang, Kai-Wei",,,Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Content Warning: This paper contains examples of` stcreotypes and associations, misgendering, erasure, and other harms that could be offensive and triggering to trans and non binary individuals. Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non -binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non -binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1968,1994,,,,,,,,,,,,,,,,WOS:000855966302009,0
C,"DeYonng, J; Beltagy, I; van Zuylen, M; Kuehl, B; Wang, LL",,,Assoc Computat Linguist,"DeYonng, Jay; Beltagy, Iz; van Zuylen, Madeleine; Kuehl, Bailey; Wang, Lucy Lu",,,MS2: A Dataset for Multi-Document Summarization of Medical Studies,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MS<^>2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system's generated summaries. Data and models are available at https:// github.com/allenai/ms2.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7494,7513,,,,,,,,,,,,,,,,WOS:000860727001042,0
C,"Donatelli, L; Schmidt, T; Biswas, D; Kohn, A; Zhai, FZ; Koller, A",,,Assoc Computat Linguist,"Donatelli, Lucia; Schmidt, Theresa; Biswas, Debanjali; Koehn, Arne; Zhai, Fangzhou; Koller, Alexander",,,Aligning Actions Across Recipe Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recipe texts are an idiosyncratic form of instructional language that pose unique challenges for automatic understanding. One challenge is that a cooking step in one recipe can be explained in another recipe in different words, at a different level of abstraction, or not at all. Previous work has annotated correspondences between recipe instructions at the sentence level, often glossing over important correspondences between cooking steps across recipes. We present a novel and fully-parsed English recipe corpus, ARA (Aligned Recipe Actions), which annotates correspondences between individual actions across similar recipes with the goal of capturing information implicit for accurate recipe understanding. We represent this information in the form of recipe graphs, and we train a neural model for predicting correspondences on ARA. We find that substantial gains in accuracy can be obtained by taking fine-grained structural information about the recipes into account.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6930,6942,,,,,,,,,,,,,,,,WOS:000860727001002,0
C,"Gu, JC; Ling, ZH; Wu, Y; Liu, Q; Chen, ZG; Zhu, XD",,,Assoc Computat Linguist,"Gu, Jia-Chen; Ling, Zhen-Hua; Wu, Yu; Liu, Quan; Chen, Zhigang; Zhu, Xiaodan",,,Detecting Speaker Personas from Conversational Texts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Personas are useful for dialogue response prediction. However, the personas used in current studies are pre-defined and hard to obtain before a conversation. To tackle this issue, we study a new task, named Speaker Persona Detection (SPD), which aims to detect speaker personas based on the plain conversational text. In this task, a best-matched persona is searched out from candidates given the conversational text. This is a many-to-many semantic matching task because both contexts and personas in SPD are composed of multiple sentences. The long-term dependency and the dynamic redundancy among these sentences increase the difficulty of this task. We build a dataset for SPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate several baseline models and propose utterance-to-profile (U2P) matching networks for this task. The U2P models operate at a fine granularity which treat both contexts and personas as sets of multiple sequences. Then, each sequence pair is scored and an interpretable overall score is obtained for a context-persona pair through aggregation. Evaluation results show that the U2P models outperform their baseline counterparts significantly.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1126,1136,,,,,,,,,,,,,,,,WOS:000855966301019,0
C,"Han, W; Chen, H; Poria, S",,,Assoc Computat Linguist,"Han, Wei; Chen, Hui; Poria, Soujanya",,,Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain task-related information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9180,9192,,,,,,,,,,,,,,,,WOS:000860727003025,0
C,"He, JX; Neubig, G; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"He, Junxian; Neubig, Graham; Berg-Kirkpatrick, Taylor",,,Efficient Nearest Neighbor Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k -nearest neighbors language model (Khandelwal et al., 2019) as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5703,5714,,,,,,,,,,,,,,,,WOS:000855966305065,0
C,"Hu, XM; Zhang, CW; Yang, YW; Li, XH; Lin, L; Wen, LJ; Yu, PS",,,Assoc Computat Linguist,"Hu, Xuming; Zhang, Chenwei; Yang, Yawen; Li, Xiaohe; Lin, Li; Wen, Lijie; Yu, Philip S.",,,Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to encourage pseudo label data to imitate the gradient descent direction on labeled data and bootstrap its optimization capability through trial and error. We also propose a framework called GradLRE, which handles two major scenarios in low-resource relation extraction. Besides the scenario where unlabeled data is sufficient, GradLRE handles the situation where no unlabeled data is available, by exploiting a contextualized augmentation method to generate data. Experimental results on two public datasets demonstrate the effectiveness of GradLRE on low resource relation extraction when comparing with baselines. Source code is available(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2737,2746,,,,,,,,,,,,,,,,WOS:000855966302075,0
C,"Jang, JY; Kim, S; Jung, M; Shin, S; Gweon, G",,,Assoc Computat Linguist,"Jang, Jin Yea; Kim, San; Jung, Minyoung; Shin, Saim; Gweon, Gahgene",,,BPM_MT: Enhanced Backchannel Prediction Model using Multi-Task Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Backchannel (BC), a short reaction signal of a listener to a speaker's utterances, helps to improve the quality of the conversation. Several studies have been conducted to predict BC in conversation; however, the utilization of advanced natural language processing techniques using lexical information presented in the utterances of a speaker has been less considered. To address this limitation, we present a BC prediction model called BPM_MT (Backchannel prediction model with multitask learning), which utilizes KoBERT, a pre-trained language model. The BPM_MT simultaneously carries out two tasks at learning: 1) BC category prediction using acoustic and lexical features, and 2) sentiment score prediction based on sentiment cues. BPM_MT exhibited 14.24% performance improvement compared to the existing baseline in the four BC categories: continuer, understanding, empathic response, and No BC. In particular, for empathic response category, a performance improvement of 17.14% was achieved.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3447,3452,,,,,,,,,,,,,,,,WOS:000855966303049,0
C,"Kalyan, A; Kumar, A; Chandrasekaran, A; Sabharwal, A; Clark, P",,,Assoc Computat Linguist,"Kalyan, Ashwin; Kumar, Abhinav; Chandrasekaran, Arjun; Sabharwal, Ashish; Clark, Peter",,,How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many real-world problems require the combined application of multiple reasoning abilities-employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, How much would the sea level rise if all ice in the world melted? FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7318,7328,,,,,,,,,,,,,,,,WOS:000860727001030,0
C,"Kang, J; Kim, J; Shin, S; Myaeng, SH",,,Assoc Computat Linguist,"Kang, Junmo; Kim, Jeonghwan; Shin, Suwon; Myaeng, Sung-Hyon",,,Leveraging Order-Free Tag Relations for Context-Aware Recommendation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the interdependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We propose a sequence-oblivious generation method for tag recommendation, in which the next tag to be generated is independent of the order of the generated tags and the order of the ground truth tags occurring in training data. Empirical results on two different domains, Instagram and Stack Overflow, show that our method is significantly superior to the previous approaches.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3464,3476,,,,,,,,,,,,,,,,WOS:000855966303051,0
C,"Kassner, N; Tafjord, O; Schutze, H; Clark, P",,,Assoc Computat Linguist,"Kassner, Nora; Tafjord, Oyvind; Schuetze, Hinrich; Clark, Peter",,,BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually believes about the world, making it susceptible to inconsistent behavior and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs - a BeliefBank - that records but then may modify the raw PTLM answers. We describe two mechanisms to improve belief consistency in the overall system. First, a reasoning component - a weighted MaxSAT solver - revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the accuracy and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8849,8861,,,,,,,,,,,,,,,,WOS:000860727002075,0
C,"Lu, SQ; He, D; Xiong, CY; Ke, GL; Malik, W; Dou, ZC; Bennett, P; Liu, TY; Overwijk, A",,,Assoc Computat Linguist,"Lu, Shuqi; He, Di; Xiong, Chenyan; Ke, Guolin; Malik, Waleed; Dou, Zhicheng; Bennett, Paul; Liu, Tie-Yan; Overwijk, Arnold",,,Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/ SEED-Encoder/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2780,2791,,,,,,,,,,,,,,,,WOS:000855966302079,0
C,"Margatina, K; Vernikos, G; Barrault, L; Aletras, N",,,Assoc Computat Linguist,"Margatina, Katerina; Vernikos, Giorgos; Barrault, Loic; Aletras, Nikolaos",,,Active Learning by Acquiring Contrastive Examples,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,650,663,,,,,,,,,,,,,,,,WOS:000855966300051,0
C,"Oren, I; Herzig, J; Berant, J",,,Assoc Computat Linguist,"Oren, Inbar; Herzig, Jonathan; Berant, Jonathan",,,Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing. Given a small training set of annotated examples and an infinite pool of synthetic examples, we select a subset of synthetic examples that are structurally-diverse and use them to improve compositional generalization. We evaluate our approach on a new split of the schema2QA dataset, and show that it leads to dramatic improvements in compositional generalization as well as moderate improvements in the traditional i.i.d setup. Moreover, structurally-diverse sampling achieves these improvements with as few as 5K examples, compared to 1M examples when sampling uniformly at random - a 200x improvement in data efficiency.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10793,10809,,,,,,,,,,,,,,,,WOS:000860727004067,0
C,"Pujari, R; Goldwasser, D",,,Assoc Computat Linguist,"Pujari, Rajkumar; Goldwasser, Dan",,,Understanding Politics via Contextualized Discourse Processing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1353,1367,,,,,,,,,,,,,,,,WOS:000855966301035,0
C,"Ravishankar, V; Sogaard, A",,,Assoc Computat Linguist,"Ravishankar, Vinit; Sogaard, Anders",,,The Impact of Positional Encodings on Multilingual Compression,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is: Sinusoidal encodings were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, compositionality becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the inductive bias to effectively learn compositionality and cross-lingual alignment. In other words, while sinusoidal positional encodings were originally designed for monolingual applications, they are particularly useful in multilingual language models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,763,777,,,,,,,,,,,,,,,,WOS:000855966300059,0
C,"Shi, Q; Zhang, Y; Yin, QY; Liu, T",,,Assoc Computat Linguist,"Shi, Qi; Zhang, Yu; Yin, Qingyu; Liu, Ting",,,Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the verification process. However, due to the lack of fully supervised signals in the program generation process, spurious programs can be derived and employed, which leads to the inability of the model to catch helpful logical operations. To address the aforementioned problems, in this work, we formulate the table-based fact verification task as an evidence retrieval and reasoning framework, proposing the Logic-level Evidence Retrieval and Graph-based Verification network (LERGV). Specifically, we first retrieve logic-level program-like evidence from the given table and statement as supplementary evidence for the table. After that, we construct a logic-level graph to capture the logical relations between entities and functions in the retrieved evidence, and design a graph-based verification network to perform logic-level graph-based reasoning based on the constructed graph to classify the final entailment relation. Experimental results on the large-scale benchmark TABFACT show the effectiveness of the proposed approach(1).",,,,,"liu, ting/GZM-3326-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,175,184,,,,,,,,,,,,,,,,WOS:000855966300016,0
C,"Steffen, J; van Genabith, J",,,Assoc Computat Linguist,"Steffen, Joerg; van Genabith, Josef",,,TransIns: Document Translation with Markup Reinsertion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but non-contiguous in target etc. Here we present TransIns, a system for non-plain text document translation that builds on the Okapi framework and MT models trained with Marian NMT. We develop, implement and evaluate different strategies for reinserting markup into translated sentences using token alignments between source and target sentences. We propose a simple and effective strategy that compiles down all markup to single source tokens and transfers them to aligned target tokens. Our evaluation shows that this strategy yields highly accurate markup in the translated documents that outperforms the markup quality found in documents translated with popular translation services. We release TransIns under the MIT License as open-source software on https:// github.com/DFKI-MLT/TransIns. An online demonstrator is available at https://transins. dfki.de.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,28,34,,,,,,,,,,,,,,,,WOS:000855241500004,0
C,"Tian, JC; Chen, SZ; Zhang, XW; Feng, ZY; Xiong, DY; Wu, SJ; Dou, CL",,,Assoc Computat Linguist,"Tian, Jiachen; Chen, Shizhan; Zhang, Xiaowang; Feng, Zhiyong; Xiong, Deyi; Wu, Shaojuan; Dou, Chunliu",,,Difficult Samples Re-embedding via Mutual Information Constrained Semantically Oversampling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help classifiers achieve significant improvements over strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3148,3161,,,,,,,,,,,,,,,,WOS:000855966303024,0
C,"Wang, CY; Pan, HJ; Qiu, MH; Yang, F; Huang, J; Zhang, Y",,,Assoc Computat Linguist,"Wang, Chengyu; Pan, Haojie; Qiu, Minghui; Yang, Fei; Huang, Jun; Zhang, Yin",,,Meta Distant Transfer Learning for Pre-trained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize nontransferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9742,9752,,,,,,,,,,,,,,,,WOS:000860727003069,0
C,"Wang, ZC; Lan, AS; Baraniuk, RG",,,Assoc Computat Linguist,"Wang, Zichao; Lan, Andrew S.; Baraniuk, Richard G.",,,Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study the problem of generating arithmetic math word problems (MWPs) given a math equation that specifies the mathematical computation and a context that specifies the problem scenario. Existing approaches are prone to generating MWPs that are either mathematically invalid or have unsatisfactory language quality. They also either ignore the context or require manual specification of a problem template, which compromises the diversity of the generated MWPs. In this paper, we develop a novel MWP generation approach that leverages i) pre-trained language models and a context keyword selection model to improve the language quality of the generated MWPs and ii) an equation consistency constraint for math equations to improve the mathematical validity of the generated MWPs. Extensive quantitative and qualitative experiments on three real-world MWP datasets demonstrate the superior performance of our approach compared to various baselines.",,,,,"lan, A/HHZ-6588-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5986,5999,,,,,,,,,,,,,,,,WOS:000855966306014,0
C,"Wilmot, D; Keller, F",,,Assoc Computat Linguist,"Wilmot, David; Keller, Frank",,,Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,851,865,,,,,,,,,,,,,,,,WOS:000855966300065,0
C,"Wu, H; Xu, K; Song, LQ",,,Assoc Computat Linguist,"Wu, Han; Xu, Kun; Song, Linqi",,,CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Conversational semantic role labeling (CSRL) is believed to be a crucial step towards dialogue understanding. However, it remains a major challenge for existing CSRL parser to handle conversational structural information. In this paper, we present a simple and effective architecture for CSRL which aims to address this problem. Our model is based on a conversational structure-aware graph network which explicitly encodes the speaker dependent information. We also propose a multi-task learning method to further improve the model. Experimental results on benchmark datasets show that our model with our proposed training objectives significantly outperforms previous baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2312,2317,,,,,,,,,,,,,,,,WOS:000855966302036,0
C,"Ye, QY; Li, BZ; Wang, SN; Bolte, B; Ma, H; Yih, WT; Ren, X; Khabsa, M",,,Assoc Computat Linguist,"Ye, Qinyuan; Li, Belinda Z.; Wang, Sinong; Bolte, Benjamin; Ma, Hao; Yih, Wen-tau; Ren, Xiang; Khabsa, Madian",,,On the Influence of Masking Policies in Intermediate Pre-training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current NLP models are predominantly trained through a two-stage pre-train then fine-tune pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it is still unclear (1) in what cases such intermediate pre-training is helpful, (2) whether hand-crafted heuristic objectives are optimal for a given task, and (3) whether a masking policy designed for one task is generalizable beyond that task. In this paper, we perform a large-scale empirical study to investigate the effect of various masking policies in intermediate pre-training with nine selected tasks across three categories. Crucially, we introduce methods to automate the discovery of optimal masking policies via direct supervision or meta-learning. We conclude that the success of intermediate pre-training is dependent on appropriate pre-train corpus, selection of output format (i.e., masked spans or full sentence), and clear understanding of the role that MLM plays for the downstream task. In addition, we find our learned masking policies outperform the heuristic of masking named entities on TriviaQA, and policies learned from one task can positively transfer to other tasks in certain cases, inviting future research in this direction.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7190,7202,,,,,,,,,,,,,,,,WOS:000860727001021,0
C,"Zhao, JM; Arthur, P; Haffari, G; Cohn, T; Shareghi, E",,,Assoc Computat Linguist,"Zhao, Jinming; Arthur, Philip; Haffari, Gholamreza; Cohn, Trevor; Shareghi, Ehsan",,,It is Not as Good as You Think! Evaluating Simultaneous Machine Translation on Interpretation Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data. In the absence of interpretation training data, we propose a translationto-interpretation (T2I) style transfer method which allows converting existing offline translations into interpretation-style data, leading to up-to 2.8 BLEU improvement. However, the evaluation gap remains notable, calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6707,6715,,,,,,,,,,,,,,,,WOS:000860727000052,0
C,"Chen, GH; Ma, SM; Chen, Y; Dong, L; Zhang, DD; Pan, J; Wang, WP; Wei, FR",,,Assoc Computat Linguist,"Chen, Guanhua; Ma, Shuming; Chen, Yun; Dong, Li; Zhang, Dongdong; Pan, Jia; Wang, Wenping; Wei, Furu",,,Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoder-decoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,15,26,,,,,,,,,,,,,,,,WOS:000855966300002,0
C,"Chen, MY; Li, W; Liu, JC; Xiao, XY; Wu, H; Wang, HF",,,Assoc Computat Linguist,"Chen, Moye; Li, Wei; Liu, Jiachen; Xiao, Xinyan; Wu, Hua; Wang, Haifeng",,,SgSum:Transforming Multi-document Summarization into Sub-graph Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4063,4074,,,,,,,,,,,,,,,,WOS:000855966304020,0
C,"Chen, SG; Aguilar, G; Neves, L; Solorio, T",,,Assoc Computat Linguist,"Chen, Shuguang; Aguilar, Gustavo; Neves, Leonardo; Solorio, Thamar",,,Data Augmentation for Cross-Domain Named Entity Recognition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Current work in named entity recognition (NER) shows that data augmentation techniques can produce more robust models. However, most existing techniques focus on augmenting in-domain data in low-resource scenarios where annotated data is quite limited. In contrast, we study cross-domain data augmentation for the NER task. We investigate the possibility of leveraging data from high-resource domains by projecting it into the low-resource domains. Specifically, we propose a novel neural architecture to transform the data representation from a high-resource to a low-resource domain by learning the patterns (e.g. style, noise, abbreviations, etc.) in the text that differentiate them and a shared feature space where both domains are aligned. We experiment with diverse datasets and show that transforming the data to the low-resource domain representation achieves significant improvements over only using data from highresource domains.(1)",,,,,,"Solorio, Thamar/0000-0002-3541-9405",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5346,5356,,,,,,,,,,,,,,,,WOS:000855966305038,0
C,"Chen, Y; Jiang, HY; Liu, LM; Shi, SM; Fan, C; Yang, M; Xu, RF",,,Assoc Computat Linguist,"Chen, Yi; Jiang, Haiyun; Liu, Lemao; Shi, Shuming; Fan, Chuang; Yang, Min; Xu, Ruifeng",,,An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this paper, we empirically study three kinds of auxiliary information: context consistency, type hierarchy and background knowledge (e.g., prototypes and descriptions) of types, and propose a multi-source fusion model (MSF) targeting these sources. The performance obtains up to 11.42% and 22.84% absolute gains over state-of-the-art baselines on BBN and Wiki respectively with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarily among them.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2668,2678,,,,,,,,,,,,,,,,WOS:000855966302069,0
C,"Dudy, S; Bedrick, S; Webber, B",,,Assoc Computat Linguist,"Dudy, Shiran; Bedrick, Steven; Webber, Bonnie",,,Refocusing on Relevance: Personalization in NLG,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many NLG tasks such as summarization, dialogue response, or open domain question answering focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user's intent or context of work is not easily recoverable based solely on that source text-a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that relevance (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such personalization, and argue that value-sensitive design represents a crucial path forward through these challenges.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5190,5202,,,,,,,,,,,,,,,,WOS:000855966305025,0
C,"Gan, YJ; Chen, XY; Purver, M",,,Assoc Computat Linguist,"Gan, Yujian; Chen, Xinyun; Purver, Matthew",,,Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zeroshot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8926,8931,,,,,,,,,,,,,,,,WOS:000860727003004,0
C,"He, TX; Zhang, JZ; Zhou, ZM; Glass, J",,,Assoc Computat Linguist,"He, Tianxing; Zhang, Jingzhao; Zhou, Zhiming; Glass, James",,,Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. In this work, we focus on the task of open-ended language generation, propose metrics to quantify the impact of exposure bias in the aspects of quality, diversity, and consistency. Our key intuition is that if we feed ground-truth data prefixes (instead of prefixes generated by the model itself) into the model and ask it to continue the generation, the performance should become much better because the training-generation discrepancy in the prefix is removed. Both automatic and human evaluations are conducted in our experiments. On the contrary to the popular belief in exposure bias, we find that the the distortion induced by the prefix discrepancy is limited, and does not seem to be incremental during the generation. Moreover, our analysis reveals an interesting self-recovery ability of the LM, which we hypothesize to be countering the harmful effects from exposure bias.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5087,5102,,,,,,,,,,,,,,,,WOS:000855966305019,0
C,"Hwang, W; Lee, H; Yim, J; Kim, G; Seo, M",,,Assoc Computat Linguist,"Hwang, Wonseok; Lee, Hyunji; Yim, Jinyeong; Kim, Geewook; Seo, Minjoon",,,Cost-effective End-to-end Information Extraction for Semi-structured Document Images,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However, such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from our existing pipeline-based IE system to an end-to-end system focusing on practical challenges that are associated with replacing and deploying the system in real, large-scale production. By carefully formulating document IE as a sequence generation task, we show that a single end-to-end IE system can be built and still achieve competent performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3375,3383,,,,,,,,,,,,,,,,WOS:000855966303043,0
C,"Kimura, D; Ono, M; Chaudhury, S; Kohita, R; Wachi, A; Agravante, DJ; Tatsubori, M; Munawar, A; Gray, A",,,Assoc Computat Linguist,"Kimura, Daiki; Ono, Masaki; Chaudhury, Subhajit; Kohita, Ryosuke; Wachi, Akifumi; Agravante, Don Joven; Tatsubori, Michiaki; Munawar, Asim; Gray, Alexander",,,Neuro-Symbolic Reinforcement Learning with First-Order Logic,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neurosymbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.",,,,,"Chaudhury, Subhajit/T-1625-2019","Chaudhury, Subhajit/0000-0003-3435-2584",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3505,3511,,,,,,,,,,,,,,,,WOS:000855966303055,0
C,"Liu, SL; Zhao, XF; Li, BC; Ren, FL; Zhang, LH; Yin, SJ",,,Assoc Computat Linguist,"Liu, Shilei; Zhao, Xiaofeng; Li, Bochao; Ren, Feiliang; Zhang, Longhui; Yin, Shujuan",,,A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new domains with limited training samples. Therefore, building a knowledge-grounded dialogue system under the low-resource setting is a still crucial issue. In this paper, we propose a novel three-stage learning framework based on weakly supervised learning which benefits from large scale ungrounded dialogues and unstructured knowledge base. To better cooperate with this framework, we devise a variant of Transformer with decoupled decoder which facilitates the disentangled learning of response generation and knowledge incorporation. Evaluation results on two benchmarks indicate that our approach can outperform other state-of-the-art methods with less training data, and even in zero-resource scenario, our approach still performs well.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2262,2272,,,,,,,,,,,,,,,,WOS:000855966302032,0
C,"Lu, J; Ng, V",,,Assoc Computat Linguist,"Lu, Jing; Ng, Vincent",,,Conundrums in Event Coreference Resolution: Making Sense of the State of the Art,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite recent promising results achieved by span-based approaches to event coreference resolution, there is a lack of understanding of what has been improved. We present an empirical analysis of our state-of-the-art span-based event coreference resolver (Lu and Ng, 2021) with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1368,1380,,,,,,,,,,,,,,,,WOS:000855966301036,0
C,"Mehrabi, N; Zhou, P; Morstatter, F; Pujara, J; Ren, X; Galstyan, A",,,Assoc Computat Linguist,"Mehrabi, Ninareh; Zhou, Pei; Morstatter, Fred; Pujara, Jay; Ren, Xiang; Galstyan, Aram",,,Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms, overgeneralization of polarized perceptions and representation disparity across different demographic groups in both CSKBs. Next, we find similar representational harms for downstream models that use ConceptNet. Finally, we propose a filtering-based approach for mitigating such harms, and observe that our filtered-based approach can reduce the issues in both resources and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models.",,,,,,"Galstyan, Aram/0000-0003-4215-0886",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5016,5033,,,,,,,,,,,,,,,,WOS:000855966305014,0
C,"Pylypenko, D; Amponsah-Kaakyire, K; Chowdhury, KD; van Genabith, J; Espana-Bonet, C",,,Assoc Computat Linguist,"Pylypenko, Dania; Amponsah-Kaakyire, Kwabena; Chowdhury, Koel Dutta; van Genabith, Josef; Espana-Bonet, Cristina",,,Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the variance in the neural models' predictions. We use pre-trained neural word embeddings, as well as several end-to-end neural architectures in both monolingual and multilingual settings and compare them to feature-engineering-based SVM classifiers. We show that (i) neural architectures outperform other approaches by more than 20 accuracy points, with the BERT-based model performing the best in both the monolingual and multilingual settings; (ii) while many individual hand-crafted translationese features correlate with neural model predictions, feature importance analysis shows that the most important features for neural and classical architectures differ; and (iii) our multilingual experiments provide empirical evidence for translationese universals across languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8596,8611,,,,,,,,,,,,,,,,WOS:000860727002054,0
C,"Schick, T; Schutze, H",,,Assoc Computat Linguist,"Schick, Timo; Schuetze, Hinrich",,,Generating Datasets with Pretrained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6943,6951,,,,,,,,,,,,,,,,WOS:000860727001003,0
C,"Sui, DAB; Wang, CH; Chen, YB; Liu, K; Zhao, J; Bi, W",,,Assoc Computat Linguist,"Sui, Dianbo; Wang, Chenhao; Chen, Yubo; Liu, Kang; Zhao, Jun; Bi, Wei",,,Set Generation Networks for End-to-End Knowledge Base Population,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by transformers with nonautoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the networks, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9650,9660,,,,,,,,,,,,,,,,WOS:000860727003061,0
C,"Xiao, SN; Chen, L; Shao, J; Zhuang, YT; Xiao, J",,,Assoc Computat Linguist,"Xiao, Shaoning; Chen, Long; Shao, Jian; Zhuang, Yueting; Xiao, Jun",,,Natural Language Video Localization with Learnable Moment Proposals,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by the query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposalfree models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that propose-and-rank approach is underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LP-Net (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve the performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods(1).",,,,,,"Chen, Long/0000-0001-6148-9709",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4008,4017,,,,,,,,,,,,,,,,WOS:000855966304014,0
C,"Yu, BW; Wang, YC; Liu, TW; Zhu, HS; Sun, LM; Wang, B",,,Assoc Computat Linguist,"Yu, Bowen; Wang, Yucheng; Liu, Tingwen; Zhu, Hongsong; Sun, Limin; Wang, Bin",,,Maximal Clique Based Non-Autoregressive Open Information Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Open Information Extraction (OpenIE) aims to discover textual facts from a given sentence. In essence, the facts contained in plain text are unordered. However, the popular OpenIE systems usually output facts sequentially in the way of predicting the next fact conditioned on the previous decoded ones, which enforce an unnecessary order on the facts and involve the error accumulation between autoregressive steps. To break this bottleneck, we propose MacroIE, a novel non-autoregressive framework for OpenIE. MacroIE firstly constructs a fact graph based on the table filling scheme, in which each node denotes a fact element, and an edge links two nodes that belong to the same fact. Then OpenIE can be reformulated as a non-parametric process of finding maximal cliques from the graph. It directly outputs the final set of facts in one go, thus getting rid of the burden of predicting fact order, as well as the error propagation between facts. Experiments conducted on two benchmark datasets show that our proposed model significantly outperforms current state-of-theart methods, beats the previous systems by as much as 5.7 absolute gain in F1 score.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9696,9706,,,,,,,,,,,,,,,,WOS:000860727003065,0
C,"Zhang, Y; Zhang, B; Wang, R; Cao, JJ; Li, C; Bao, ZY",,,Assoc Computat Linguist,"Zhang, Yue; Zhang, Bo; Wang, Rui; Cao, Junjie; Li, Chen; Bao, Zuyi",,,Entity Relation Extraction as Dependency Parsing in Visually Rich Documents,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e., semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity, different VRD encoders, and different relation decoders. For the model training, we explore multi-task learning to combine entity labeling and relation extraction tasks; and for the evaluation, we conduct experiments on different datasets with filtering and augmentation. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the real-world application, our model has been applied to the in-house customs data, achieving reliable performance in the production setting.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2759,2768,,,,,,,,,,,,,,,,WOS:000855966302077,0
C,"Zhou, G; Devlin, J",,,Assoc Computat Linguist,"Zhou, Giulio; Devlin, Jacob",,,Multi-Vector Attention Models for Deep Re-ranking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is based on the joint query-document representation. Dual encoder models are typically used for retrieval and deep re-ranking, while cross-attention models are typically used for shallow re-ranking. In this paper, we present a lightweight architecture that explores this joint cost vs. accuracy trade-off based on multi-vector attention (MVA). We thoroughly evaluate our method on the MS-MARCO passage retrieval dataset and show how to efficiently trade off retrieval accuracy with joint computation and offline document storage cost. We show that a highly compressed document representation and inexpensive joint computation can be achieved through a combination of learned pooling tokens and aggressive downprojection. Our code and model checkpoints are available on GitHub.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5452,5456,,,,,,,,,,,,,,,,WOS:000855966305047,0
C,"Zhou, WCS; Ge, T; Xu, CW; Xu, K; Wei, FR",,,Assoc Computat Linguist,"Zhou, Wangchunshu; Ge, Tao; Xu, Canwen; Xu, Ke; Wei, Furu",,,Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original textinfilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pretraining, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,571,582,,,,,,,,,,,,,,,,WOS:000855966300045,0
C,"Zhu, J; Jurgens, D",,,Assoc Computat Linguist,"Zhu, Jian; Jurgens, David",,,Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"An individual's variation in writing style is often a function of both social and personal attributes. While structured social variation has been extensively studied, e.g., gender based variation, far less is known about how to characterize individual styles due to their idiosyncratic nature. We introduce a new approach to studying idiolects through a massive cross-author comparison to identify and encode stylistic features. The neural model achieves strong performance at authorship identification on short texts and through an analogy-based probing task, showing that the learned representations exhibit surprising regularities that encode qualitative and quantitative shifts of idiolectal styles. Through text perturbation, we quantify the relative contributions of different linguistic elements to idiolectal variation. Furthermore, we provide a description of idiolects through measuring inter- and intra-author variation, showing that variation in idiolects is often distinctive yet consistent.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,279,297,,,,,,,,,,,,,,,,WOS:000855966300025,0
C,"Chen, D; Yu, Z",,,Assoc Computat Linguist,"Chen, Derek; Yu, Zhou",,,GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,429,442,,,,,,,,,,,,,,,,WOS:000855966300035,0
C,"Curry, AC; Abercrombie, G; Rieser, V",,,Assoc Computat Linguist,"Curry, Amanda Cercas; Abercrombie, Gavin; Rieser, Verena",,,"ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present the first English corpus study on abusive language towards three conversational AI systems gathered `in the wild': an opendomain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more `nuanced' approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90%.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7388,7403,,,,,,,,,,,,,,,,WOS:000860727001035,0
C,"Guan, Y; Guo, SR; Li, R; Li, XL; Tan, HY",,,Assoc Computat Linguist,"Guan, Yong; Guo, Shaoru; Li, Ru; Li, Xiaoli; Tan, Hongye",,,Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization (FS3), which leverages Frame semantics to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN/DM and NYT demonstrate that FS3 model outperforms six state-of-the-art methods",,,,,"Li, Xiaoli/GYQ-7384-2022",,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4045,4052,,,,,,,,,,,,,,,,WOS:000855966304018,0
C,"Jain, A; Srivastava, S",,,Assoc Computat Linguist,"Jain, Ayush; Srivastava, Shashank",,,Does Social Pressure Drive Persuasion in Online Fora?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Online forums such as ChangeMyView have been explored to research aspects of persuasion and argumentative quality in language. While previous research has focused on arguments between a view-holder and a persuader, we explore the premise that apart from the merits of arguments, persuasion is influenced by the ambient social community. We hypothesize that comments from the rest of the community can either affirm the original view or implicitly exert pressure to change it. We develop a structured model to capture the ambient community's sentiment towards the discussion and its effect on persuasion. Our experiments show that social features themselves are significantly predictive of persuasion (even without looking at the actual content of discussion), with performance comparable to some earlier approaches that use content features. Combining community and content features leads to an overall performance of 78:5% on the persuasion prediction task. Our analyses suggest that the effect of social pressure is comparable to the difference between persuasive and non-persuasive language strategies in driving persuasion and that social pressure might be a causal factor for persuasion.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9201,9208,,,,,,,,,,,,,,,,WOS:000860727003027,0
C,"Ji, T; Yan, H; Qiu, XP",,,Assoc Computat Linguist,"Ji, Tuo; Yan, Hang; Qiu, Xipeng",,,SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these features with character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3544,3551,,,,,,,,,,,,,,,,WOS:000855966303059,0
C,"Konno, R; Kiyono, S; Matsubayashi, Y; Ouchi, H; Inui, K",,,Assoc Computat Linguist,"Konno, Ryuto; Kiyono, Shun; Matsubayashi, Yuichiroh; Ouchi, Hiroki; Inui, Kentaro",,,Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pre-training task that trains MLMs on anaphoric relations with explicit supervision, and the second proposal is a new finetuning method that remedies a notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost the state-of-the-art performance, and our detailed analysis provides new insights on the remaining challenges.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3790,3806,,,,,,,,,,,,,,,,WOS:000855966303080,0
C,"Li, JD; Ataman, D; Sennrich, R",,,Assoc Computat Linguist,"Li, Jiaoda; Ataman, Duygu; Sennrich, Rico",,,Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8556,8562,,,,,,,,,,,,,,,,WOS:000860727002051,0
C,"Li, J; Zhong, SP; Chen, KZ",,,Assoc Computat Linguist,"Li, Jing; Zhong, Shangping; Chen, Kaizhi",,,MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8862,8874,,,,,,,,,,,,,,,,WOS:000860727002076,0
C,"Li, SQ; Yang, L; He, WD; Zhang, SQ; Zeng, JJ; Lin, HF",,,Assoc Computat Linguist,"Li, Shuqun; Yang, Liang; He, Weidong; Zhang, Shiqi; Zeng, Jingjie; Lin, Hongfei",,,Label-Enhanced Hierarchical Contextualized Representation for Sequential Metaphor Identification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse information. In this paper, we propose a model augmented with hierarchical contextualized representation to extract more information from both sentence-level and discourse-level. At the sentence level, we leverage the metaphor information of words that except the target word in the sentence to strengthen the reasoning ability of our model via a novel label-enhanced contextualized representation. At the discourse level, the position-aware global memory network is adopted to learn long-range dependency among the same words within a discourse. Finally, our model combines the representations obtained from these two parts. The experiment results on two tasks of the VUA dataset show that our model outperforms every other state-of-the-art method that also does not use any external knowledge except what the pre-trained language model contains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3533,3543,,,,,,,,,,,,,,,,WOS:000855966303058,0
C,"Li, ZY; Xu, JH; Zeng, JH; Li, LY; Zheng, XQ; Zhang, Q; Chang, KW; Hsieh, CJ",,,Assoc Computat Linguist,"Li, Zongyi; Xu, Jianhan; Zeng, Jiehang; Li, Linyang; Zheng, Xiaoqing; Zhang, Qi; Chang, Kai-Wei; Hsieh, Cho-Jui",,,Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent studies have shown that deep neural network-based models are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on comparing different defense approaches under the same attacking setting. In this paper, we seek to fill the gap through comprehensive studies on the behavior of neural text classifiers trained with various defense methods against representative adversarial attacks. In addition, we propose an effective method to further improve the robustness of neural text classifiers against such attacks, and achieved the highest accuracy on both clean and adversarial examples on AGNEWS and IMDB datasets, outperforming existing methods by a significant margin. We hope this study could provide useful clues for future research on text adversarial defense. Codes are available at https: // github.com/RockyLzy/TextDefender.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3137,3147,,,,,,,,,,,,,,,,WOS:000855966303023,0
C,"Liu, G; Mao, YZ; Huang, HL; Gao, WG; Li, X",,,Assoc Computat Linguist,"Liu, Guang; Mao, Yuzhao; Huang, Hailong; Gao, Weiguo; Li, Xuan",,,Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model's input space. However, such strict linear constraints often lead to under-fitting which degrades the effects of regularization. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small adversarial perturbation to the mixing coefficients rather than the examples. Thus, slight non-linearity is injected in-between the synthetic examples and synthetic labels. By training on these data, the deep networks are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3%), especially in low-resource conditions (up to 17.5%).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2998,3008,,,,,,,,,,,,,,,,WOS:000855966303010,0
C,"Lu, J; Abrego, GH; Ma, J; Ni, JM; Yang, YF",,,Assoc Computat Linguist,"Lu, Jing; Abrego, Gustavo Hernandez; Ma, Ji; Ni, Jianmo; Yang, Yinfei",,,Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data and negative sampling at both stages. We study six negative sampling strategies and apply them to the fine-tuning stage and, as a noteworthy novelty, to the synthetic data that we use for pre-training. Also, we explore fusion methods that combine negatives from different strategies. We evaluate our system using two passage retrieval tasks for open-domain QA and using MS MARCO. Our experiments show that augmenting the negative contrast in both stages is effective to improve passage retrieval accuracy and, importantly, they also show that synthetic data generation and negative sampling have additive benefits. Moreover, using fusion of different kinds allows us to reach performance that establishs a new state-of-the-art level in two of the tasks we evaluated.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6091,6103,,,,,,,,,,,,,,,,WOS:000860727000007,0
C,"Maneriker, P; He, YT; Parthasarathy, S",,,Assoc Computat Linguist,"Maneriker, Pranav; He, Yuntian; Parthasarathy, Srinivasan",,,SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Darknet market forums are frequently used to exchange illegal goods and services between parties who use encryption to conceal their identities. The Tor network is used to host these markets, which guarantees additional anonymization from IP and location tracking, making it challenging to link across malicious users using multiple accounts (sybils). Additionally, users migrate to new forums when one is closed further increasing the difficulty of linking users across multiple forums. We develop a novel stylometry-based multitask learning approach for natural language and model interactions using graph embeddings to construct low-dimensional representations of short episodes of user activity for authorship attribution. We provide a comprehensive evaluation of our methods across four different darknet forums demonstrating its efficacy over the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X on Recall@10.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6844,6857,,,,,,,,,,,,,,,,WOS:000860727000063,0
C,"Mendelson, M; Belinkov, Y",,,Assoc Computat Linguist,"Mendelson, Michael; Belinkov, Yonatan",,,Debiasing Methods in Natural Language Understanding Make Bias More Accessible,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model's inner representations. We propose a general probing-based framework that allows for posthoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model's representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1545,1557,,,,,,,,,,,,,,,,WOS:000855966301049,0
C,"Mittal, A; Jeevan, P; Gandhi, P; Kanojia, D; Bhattacharyya, P",,,Assoc Computat Linguist,"Mittal, Anirudh; Jeevan, Pranav; Gandhi, Prerak; Kanojia, Diptesh; Bhattacharyya, Pushpak",,,So You Think You're Funny?: Rating the Humour Quotient in Standup Comedy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset (similar to 40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience's laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a funniness score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0:813 in terms of Quadratic Weighted Kappa (QWK). Our Open Mic dataset is released for further research along with the code.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10073,10079,,,,,,,,,,,,,,,,WOS:000860727004013,0
C,"Perry, T",,,Assoc Computat Linguist,"Perry, Tal",,,LightTag: Text Annotation Platform,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text annotation tools assume that their user's goal is to create a labeled corpus. However, users view annotation as a necessary evil on the way to deliver business value through NLP. Thus an annotation tool should optimize for the throughput of the global NLP process, not only the productivity of individual annotators. LightTag is a text annotation tool designed and built on that principle. This paper shares our design rationale, data modeling choices, and user interface decisions then illustrates how those choices serve the full NLP lifecycle.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,20,27,,,,,,,,,,,,,,,,WOS:000855241500003,0
C,"Pimentel, T; Cotterell, R",,,Assoc Computat Linguist,"Pimentel, Tiago; Cotterell, Ryan",,,A Bayesian Framework for Information-Theoretic Probing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pimentel et al. (2020b) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents-allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2869,2887,,,,,,,,,,,,,,,,WOS:000855966303001,0
C,"Sainz, O; de Lacalle, OL; Labaka, G; Barrena, A; Agirre, E",,,Assoc Computat Linguist,"Sainz, Oscar; de Lacalle, Oier Lopez; Labaka, Gorka; Barrena, Ander; Agirre, Eneko",,,Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 minutes per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short of the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, giving the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are especially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1199,1212,,,,,,,,,,,,,,,,WOS:000855966301025,0
C,"Sanchez-Cartagena, VM; Espla-Gomis, M; Perez-Ortiz, JA; Sanchez-Martinez, F",,,Assoc Computat Linguist,"Sanchez-Cartagena, Victor M.; Espla-Gomis, Miquel; Antonio Perez-Ortiz, Juan; Sanchez-Martinez, Felipe",,,Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations.",,,,,"Sanchez-Martinez, Felipe/G-9689-2016","Sanchez-Martinez, Felipe/0000-0002-2295-2630",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8502,8516,,,,,,,,,,,,,,,,WOS:000860727002047,0
C,"Schmitt, M; Schutze, H",,,Assoc Computat Linguist,"Schmitt, Martin; Schuetze, Hinrich",,,Continuous Entailment Patterns for Lexical Inference in Context,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design patterns that closely resemble the text seen during self-supervised pretraining because the model has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM's vocabulary, patterns can be adapted more flexibly to a PLM's idiosyncrasies. Contrasting patterns where a token can be any continuous vector vs. those where a discrete choice between vocabulary elements has to be made, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively small training sets. In a direct comparison with discrete patterns, CONAN consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights into the kind of pattern that enhances a PLM's performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6952,6959,,,,,,,,,,,,,,,,WOS:000860727001004,0
C,"Sciavolino, C; Zhong, ZX; Lee, J; Chen, DQ",,,Assoc Computat Linguist,"Sciavolino, Christopher; Zhong, Zexuan; Lee, Jinhyuk; Chen, Danqi",,,Simple Entity-Centric Questions Challenge Dense Retrievers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., Where was Arve Furset born?), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6138,6148,,,,,,,,,,,,,,,,WOS:000860727000011,0
C,"Wang, HY; Zhang, HM; Chen, MH; Roth, D",,,Assoc Computat Linguist,"Wang, Haoyu; Zhang, Hongming; Chen, Muhao; Roth, Dan",,,Learning Constraints and Descriptive Segmentation for Subevent Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EVENTSEG) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks together, we propose an approach to learning and enforcing constraints that capture dependencies between subevent detection and EVENTSEG prediction, as well as guiding the model to make globally consistent inference. Specifically, we adopt Rectifier Networks for constraint learning and then convert the learned constraints to a regularization term in the loss function of the neural model. Experimental results show that the proposed method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for subevent detection, HiEve and IC, respectively, while achieving a decent performance on EVENTSEG prediction(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5216,5226,,,,,,,,,,,,,,,,WOS:000855966305027,0
C,"Wolfe, R; Caliskan, A",,,Assoc Computat Linguist,"Wolfe, Robert; Caliskan, Aylin",,,Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman's rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman's rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as -.702. Moreover, we find Spearman's rho between racial bias and name frequency in BERT of.92, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,518,532,,,,,,,,,,,,,,,,WOS:000855966300041,0
C,"Xiao, LQ; Ma, J; Dong, XLN; Martinez-Gomez, P; Zalmout, N; Zhang, CW; Zhao, T; He, H; Jin, YH",,,Assoc Computat Linguist,"Xiao, Liqiang; Ma, Jun; Dong, Xin Luna; Martinez-Gomez, Pascual; Zalmout, Nasser; Zhang, Chenwei; Zhao, Tong; He, Hao; Jin, Yaohui",,,End-to-End Conversational Search for Online Shopping with Utterance Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.",,,,,"Zhang, Chenwei/ABD-7742-2021; 浩, 何/GZG-6941-2022","Zhang, Chenwei/0000-0002-0606-3649; ",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3477,3486,,,,,,,,,,,,,,,,WOS:000855966303052,0
C,"Yan, C; Zhang, YZ; Liu, K; Zhao, J; Shi, YF; Liu, SP",,,Assoc Computat Linguist,"Yan, Cheng; Zhang, Yuanzhe; Liu, Kang; Zhao, Jun; Shi, Yafei; Liu, Shengping",,,Biomedical Concept Normalization by Leveraging Hypernyms,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperform the previous state-of-the-art model on the NCBI dataset. Code will be available at https / /github com/ yan- cheng/BCNH.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3512,3517,,,,,,,,,,,,,,,,WOS:000855966303056,0
C,"Yao, HX; Wu, YX; Al-Shedivat, M; Xing, EP",,,Assoc Computat Linguist,"Yao, Huaxiu; Wu, Yingxin; Al-Shedivat, Maruan; Xing, Eric P.",,,Knowledge-Aware Meta-learning for Low-Resource Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new task. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional representation for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three datasets demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1814,1821,,,,,,,,,,,,,,,,WOS:000855966301069,0
C,"Zhou, P; Khanna, R; Lee, S; Lin, BYC; Ho, D; Pujara, J; Ren, X",,,Assoc Computat Linguist,"Zhou, Pei; Khanna, Rahul; Lee, Seyeon; Lin, Bill Yuchen; Ho, Daniel; Pujara, Jay; Ren, Xiang",,,RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7560,7579,,,,,,,,,,,,,,,,WOS:000860727001046,0
C,"Briakou, E; Agrawals, S; Tetreault, J; Carpuat, M",,,Assoc Computat Linguist,"Briakou, Eleftheria; Agrawals, Sweta; Tetreault, Joel; Carpuat, Marine",,,Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading ST automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus solely on English, we expand our focus to Brazilian-Portuguese, French, and Italian, making this work the first multilingual evaluation of metrics in ST. We outline best practices for automatic evaluation in (formality) style transfer and identify several models that correlate well with human judgments and are robust across languages. We hope that this work will help accelerate development in ST, where human evaluation is often challenging to collect.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1321,1336,,,,,,,,,,,,,,,,WOS:000855966301033,0
C,"Chen, PC; Tsai, HR; Bhojanapalli, S; Chung, YW; Chang, YW; Ferng, CS",,,Assoc Computat Linguist,"Chen, Pu-Chin; Tsai, Henry; Bhojanapalli, Srinadh; Chung, Hyung Won; Chang, Yin-Wen; Ferng, Chun-Sung",,,A Simple and Effective Positional Encoding for Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled positional attEntion for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2974,2988,,,,,,,,,,,,,,,,WOS:000855966303008,0
C,"Chowdhury, SBR; Ghosh, S; Li, YY; Oliva, JB; Srivastava, S; Chaturvedi, S",,,Assoc Computat Linguist,"Chowdhury, Somnath Basu Roy; Ghosh, Sayan; Li, Yiyuan; Oliva, Junier B.; Srivastava, Shashank; Chaturvedi, Snigdha",,,Adversarial Scrubbing of Demographic Information for Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target task. In this paper, we present an adversarial learning framework Adversarial Scrubber (ADS), to debias contextual representations. We perform theoretical analysis to show that our framework converges without leaking demographic information under certain conditions. We extend previous evaluation techniques by evaluating debiasing performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that ADS generates representations with minimal information about demographic attributes while being maximally informative about the target task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,550,562,,,,,,,,,,,,,,,,WOS:000855966300043,0
C,"Dale, D; Voronov, A; Dementieva, D; Logacheva, V; Kozlova, O; Semenov, N; Panchenko, A",,,Assoc Computat Linguist,"Dale, David; Voronov, Anton; Dementieva, Daryna; Logacheva, Varvara; Kozlova, Olga; Semenov, Nikita; Panchenko, Alexander",,,Text Detoxification using Large Pre-trained Neural Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the method more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our models with a number of methods for style transfer. The models are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both methods we suggest yield new SOTA results.",,,,,,"Dementieva, Daryna/0000-0003-0929-4140",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7979,7996,,,,,,,,,,,,,,,,WOS:000860727002007,0
C,"Dev, S; Li, T; Phillips, JM; Srikumar, V",,,Assoc Computat Linguist,"Dev, Sunipa; Li, Tao; Phillips, Jeff M.; Srikumar, Vivek",,,OSCAR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Language representations are known to carry certain associations (e.g., gendered connotations) which may lead to invalid and harmful predictions in downstream tasks. While existing methods are effective at mitigating such unwanted associations by linear projection, we argue that they are too aggressive: not only do they remove such associations, they also erase information that should be retained. To address this issue, we propose OSCAR (Orthogonal Subspace Correction and Rectification), a balanced approach of mitigation that focuses on disentangling associations between concepts that are deemed problematic, instead of removing concepts wholesale. We develop new measurements for evaluating information retention relevant to the debiasing goal. Our experiments on gender-occupation associations show that OSCAR is a well-balanced approach that ensures that semantic information is retained in the embeddings and unwanted associations are also effectively mitigated.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5034,5050,,,,,,,,,,,,,,,,WOS:000855966305015,0
C,"Dou, ZY; Peng, NY",,,Assoc Computat Linguist,"Dou, Zi-Yi; Peng, Nanyun",,,Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing representation generality.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6362,6371,,,,,,,,,,,,,,,,WOS:000860727000028,0
C,"Falis, M; Dong, H; Birch, A; Alex, B",,,Assoc Computat Linguist,"Falis, Matus; Dong, Hang; Birch, Alexandra; Alex, Beatrice",,,CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC els for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,907,912,,,,,,,,,,,,,,,,WOS:000855966301002,0
C,"Geng, XW; Feng, XC; Qin, B",,,Assoc Computat Linguist,"Geng, Xinwei; Feng, Xiaocheng; Qin, Bing",,,Learning to Rewrite for Non-Autoregressive Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named REWRITENAT to explicitly learn to rewrite the erroneous translation pieces. Specifically, REWRITENAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consistency of data distribution with iterative decoding, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that REWRITENAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, REWRITENAT can obtain competitive results with autoregressive translation on WMT14 En <-> De, En <-> Fr and WMT16 Ro -> En translation benchmarks(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3297,3308,,,,,,,,,,,,,,,,WOS:000855966303037,0
C,"Gerz, D; Su, PH; Kusztos, R; Mondal, A; Lis, M; Singhal, E; Mrksic, N; Wen, TH; Vulic, I",,,Assoc Computat Linguist,"Gerz, Daniela; Su, Pei-Hao; Kusztos, Razvan; Mondal, Avishek; Lis, Michal; Singhal, Eshan; Mrksic, Nikola; Wen, Tsung-Hsien; Vulic, Ivan",,,Multilingual and Cross-Lingual Intent Detection from Spoken Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, termed MINDS14, a first training and evaluation resource for the ID task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) yield strong intent detectors in the majority of target languages covered in MINDS-14, and offer comparative analyses across different axes: e.g., translation direction, impact of speech recognition, data augmentation from a related domain. We see this work as an important step towards more inclusive development and evaluation of multilingual ID from spoken data, hopefully in a much wider spectrum of languages compared to prior work.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7468,7475,,,,,,,,,,,,,,,,WOS:000860727001039,0
C,"Glass, M; Rossiello, G; Chowdhury, MFM; Gliozzo, A",,,Assoc Computat Linguist,"Glass, Michael; Rossiello, Gaetano; Chowdhury, Md Faisal Mahbub; Gliozzo, Alfio",,,Robust Retrieval Augmented Generation for Zero-shot Slot Filling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling In this task, given an entity query in form of [ENTITY, SLOT, ?], a system is asked to 'fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both TREx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1939,1949,,,,,,,,,,,,,,,,WOS:000855966302007,0
C,"Grover, I; Huggins, M; Breazeal, C; Park, HW",,,Assoc Computat Linguist,"Grover, Ishaan; Huggins, Matthew; Breazeal, Cynthia; Park, Hae Won",,,MRF-Chat: Improving Dialogue with Markov Random Fields,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among others. While neural models have shown reasonable results, modelling the cognitive processes that humans use when conversing with each other may improve the agent's quality of responses. A key element of natural conversation is to tailor one's response such that it accounts for concepts that the speaker and listener may or may not know and the contextual relevance of all prior concepts used in conversation. We show that a rich representation and explicit modeling of these psychological processes can improve predictions made by existing neural network models. In this work, we propose a novel probabilistic approach using Markov Random Fields (MRF) to augment existing deep-learning methods for improved next utterance prediction. Using human and automatic evaluations, we show that our augmentation approach significantly improves the performance of existing state-of-the-art retrieval models for open-domain conversational agents.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4925,4936,,,,,,,,,,,,,,,,WOS:000855966305007,0
C,"Hasan, MK; Spann, J; Hasan, M; Islam, MS; Haut, K; Mihalcea, R; Hoque, E",,,Assoc Computat Linguist,"Hasan, Md Kamrul; Spann, James; Hasan, Masum; Islam, Md Saiful; Haut, Kurtis; Mihalcea, Rada; Hoque, Ehsan",,,Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The combination of gestures, intonations, and textual content plays a key role in argument delivery. However, the current literature mostly considers textual content while assessing the quality of an argument, and is limited to datasets containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate-centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of argumentation quality. Second, we design the Multimodal ARgument Quality assessor (MARQ) - a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate-centric features. Our proposed MARQ model achieves an accuracy of 81.91% on the argument quality prediction task and outperforms established baseline models with an error rate reduction of 22.7%. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6387,6397,,,,,,,,,,,,,,,,WOS:000860727000030,0
C,"Joseph, K; Shugars, S; Gallagher, R; Green, J; Mathe, AQ",,,Assoc Computat Linguist,"Joseph, Kenneth; Shugars, Sarah; Gallagher, Ryan; Green, Jon; Mathe, Alexi Quintana",,,(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture stance as measured by public opinion polls. We demonstrate this by directly comparing an individual's self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both Pro and Anti stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,312,324,,,,,,,,,,,,,,,,WOS:000855966300027,0
C,"Kim, H; Li, JL; Bansal, M",,,Assoc Computat Linguist,"Kim, Hyounghun; Li, Jialu; Bansal, Mohit",,,NDH-FULL: Learning and Evaluating Navigational Agents on Full-Length Dialogue,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent's ability to interact with humans for assistance and navigate based on natural language responses. In this paper, we explore the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset, and present a state-of-the-art model which is built upon Vision-Language transformers. However, despite achieving competitive performance, we find that the agent in the NDH task is not evaluated appropriately by the primary metric - Goal Progress. By analyzing the performance mismatch between Goal Progress and other metrics (e.g., normalized Dynamic Time Warping) from our state-of-the-art model, we show that NDH's sub-path based task setup (i.e., navigating partial trajectory based on its correspondent subset of the full dialogue) does not provide the agent with enough supervision signal towards the goal region. Therefore, we propose a new task setup called NDH-FULL which takes the full dialogue and the whole navigation path as one instance. We present a strong baseline model and show initial results on this new task. We further describe several approaches that we try, in order to improve the model performance (based on curriculum learning, pre-training, and data-augmentation), suggesting potential useful training methods on this new NDH-FULL task.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6432,6442,,,,,,,,,,,,,,,,WOS:000860727000033,0
C,"Li, ZC; Utiyama, M; Sumita, E; Zhao, H",,,Assoc Computat Linguist,"Li, Zuchao; Utiyama, Masao; Sumita, Eiichiro; Zhao, Hai",,,Unsupervised Neural Machine Translation with Universal Grammar,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine translation usually relies on parallel corpora to provide parallel signals for training The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky's Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3249,3264,,,,,,,,,,,,,,,,WOS:000855966303033,0
C,"Lin, ZJ; Liu, B; Madotto, A; Moon, S; Crook, P; Zhou, ZP; Wang, ZG; Yu, Z; Cho, E; Subba, R; Fung, P",,,Assoc Computat Linguist,"Lin, Zhaojiang; Liu, Bing; Madotto, Andrea; Moon, Seungwhan; Crook, Paul; Zhou, Zhenpeng; Wang, Zhiguang; Yu, Zhou; Cho, Eunjoon; Subba, Rajen; Fung, Pascale",,,Zero-Shot Dialogue State Tracking via Cross-Task Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the crosstask knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multichoice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7890,7900,,,,,,,,,,,,,,,,WOS:000860727001070,0
C,"Liu, QC; Ponti, EM; McCarthy, D; Vulic, I; Korhonen, A",,,Assoc Computat Linguist,"Liu, Qianchu; Ponti, Edoardo M.; McCarthy, Diana; Vulic, Ivan; Korhonen, Anna",,,AM(2)ICO: EvaluatingWord Meaning in Context across Low-Resource Languages with Adversarial Examples,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Capturing word meaning in context and distinguishing between correspondences and variations across languages is key to building successful multilingual and cross-lingual text representation models. However, existing multilingual evaluation datasets that evaluate lexical semantics in-context have various limitations. In particular, 1) their language coverage is restricted to high-resource languages and skewed in favor of only a few language families and areas, 2) a design that makes the task solvable via superficial cues, which results in artificially inflated (and sometimes super-human) performances of pretrained encoders, and 3) no support for crosslingual evaluation. In order to address these gaps, we present AM(2)ICO (Adversarial and Multilingual Meaning in Context), a widecoverage cross-lingual and multilingual evaluation set; it aims to faithfully assess the ability of state-of-the-art (SotA) representation models to understand the identity of word meaning in cross-lingual contexts for 14 language pairs. We conduct a series of experiments in a wide range of setups and demonstrate the challenging nature of AM(2)ICO. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7151,7162,,,,,,,,,,,,,,,,WOS:000860727001019,0
C,"Ma, WC; Takanobu, R; Huang, ML",,,Assoc Computat Linguist,"Ma, Wenchang; Takanobu, Ryuichi; Huang, Minlie",,,CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we propose CR-Walker in this paper, a model that performs tree-structured reasoning on a knowledge graph, and generates informative dialog acts to guide language generation. The unique scheme of tree-structured reasoning views traversed entity at each hop as part of dialog acts to facilitate language generation, which links how entities are selected and expressed. Automatic and human evaluations show that CR-Walker can arrive at more accurate recommendation, and generate more informative and engaging responses.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1839,1851,,,,,,,,,,,,,,,,WOS:000855966301072,0
C,"Mozes, M; Bartolo, M; Stenetorp, P; Kleinberg, B; Griffin, LD",,,Assoc Computat Linguist,"Mozes, Maximilian; Bartolo, Max; Stenetorp, Pontus; Kleinberg, Bennett; Griffin, Lewis D.",,,Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TEXTFOOLER, GENETIC, BAE and SEMEMEPSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8258,8270,,,,,,,,,,,,,,,,WOS:000860727002029,0
C,"Pei, JX; Jurgens, D",,,Assoc Computat Linguist,"Pei, Jiaxin; Jurgens, David",,,Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific findings, we demonstrate that hedges alone account for only a partial explanation of certainty. We show that both the overall certainty and individual aspects can be predicted with pre-trained language models, providing a more complete picture of the author's intended communication. Downstream analyses on 431K scientific findings from news and scientific abstracts demonstrate that modeling sentence-level and aspect-level certainty is meaningful for areas like science communication. Both the model and datasets used in this paper are released at https://blablablab.si. umich.edu/projects/certainty/",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9959,10011,,,,,,,,,,,,,,,,WOS:000860727004008,0
C,"Rodriguez, P; Boyd-Graber, J",,,Assoc Computat Linguist,"Rodriguez, Pedro; Boyd-Graber, Jordan",,,Evaluation Paradigms in Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Question answering (QA) primarily descends from two branches of research: (1) Alan Turing's investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon's comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other paradigm values building QA systems that appeal to users. By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9630,9642,,,,,,,,,,,,,,,,WOS:000860727003059,0
C,"Ruckle, A; Geigle, G; Glockner, M; Beck, T; Pfeiffer, J; Reimers, N; Gurevych, I",,,Assoc Computat Linguist,"Rueckle, Andreas; Geigle, Gregor; Glockner, Max; Beck, Tilman; Pfeiffer, Jonas; Reimers, Nils; Gurevych, Iryna",,,AdapterDrop: On the Efficiency of Adapters in Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7930,7946,,,,,,,,,,,,,,,,WOS:000860727002004,0
C,"Wu, YM; Rezagholizadeh, M; Ghaddar, A; Haidar, MA; Ghodsi, A",,,Assoc Computat Linguist,"Wu, Yimeng; Rezagholizadeh, Mehdi; Ghaddar, Abbas; Haidar, Md Akmal; Ghodsi, Ali",,,Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies matching in the hidden spaces of two different networks (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD cannot easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits: (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7649,7661,,,,,,,,,,,,,,,,WOS:000860727001051,0
C,"Yauney, G; Mimno, D",,,Assoc Computat Linguist,"Yauney, Gregory; Mimno, David",,,Comparing Text Representations: A Theory-Driven Approach,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Much of the progress in contemporary NLP has come from learning representations, such as masked language model (MLM) contextual embeddings, that turn challenging problems into simple classification tasks. But how do we quantify and explain this effect? We adapt general tools from computational learning theory to fit the specific characteristics of text datasets and present a method to evaluate the compatibility between representations and tasks. Even though many tasks can be easily solved with simple bag-of-words (BOW) representations, BOW does poorly on hard natural language inference tasks. For one such task we find that BOWcannot distinguish between real and randomized labelings, while pre-trained MLM representations show 72x greater distinction between real and random labelings than BOW. This method provides a calibrated, quantitative measure of the difficulty of a classification-based NLP task, enabling comparisons between representations without requiring empirical evaluations that may be sensitive to initializations and hyperparameters. The method provides a fresh perspective on the patterns in a dataset and the alignment of those patterns with specific labels.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5527,5539,,,,,,,,,,,,,,,,WOS:000855966305053,0
C,"Zhang, S; Wong, C; Usuyama, N; Jain, S; Naumann, T; Poon, H",,,Assoc Computat Linguist,"Zhang, Sheng; Wong, Cliff; Usuyama, Naoto; Jain, Sarthak; Naumann, Tristan; Poon, Hoifung",,,Modular Self-Supervision for Document-Level Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans, document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans, state-of-the-art neural architectures are less effective and task-specific self-supervision such as distant supervision becomes very noisy. In this paper, we propose decomposing document-level relation extraction into relation detection and argument resolution, taking inspiration from Davidsonian semantics. This enables us to incorporate explicit discourse modeling and leverage modular self-supervision for each sub-problem, which is less noise-prone and can be further refined end-to-end via variational EM. We conduct a thorough evaluation in biomedical machine reading for precision oncology, where cross-paragraph relation mentions are prevalent. Our method outperforms prior state of the art, such as multi-scale learning and graph neural networks, by over 20 absolute F1 points. The gain is particularly pronounced among the most challenging relation instances whose arguments never co-occur in a paragraph.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5291,5302,,,,,,,,,,,,,,,,WOS:000855966305033,0
C,"Zhang, XY; Van de Meent, JW; Wallace, BC",,,Assoc Computat Linguist,"Zhang, Xiongyi; van de Meent, Jan-Willem; Wallace, Byron C.",,,Disentangling Representations of Text by Masking Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as - and often better than - previously proposed methods based on variational autoencoders and adversarial training.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,778,791,,,,,,,,,,,,,,,,WOS:000855966300060,0
C,"Zhang, Y; Liang, HR; Jatowt, A; Lei, WQ; Wei, X; Jiang, N; Yang, ZL",,,Assoc Computat Linguist,"Zhang, Yao; Liang, Hongru; Jatowt, Adam; Lei, Wenqiang; Wei, Xin; Jiang, Ning; Yang, Zhenglu",,,GMH: A General Multi-hop Reasoning Model for KG Completion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current models typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model: i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules: 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our model with significant improvements against baselines in both short and long distance reasoning scenarios.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3437,3446,,,,,,,,,,,,,,,,WOS:000855966303048,0
C,"Zouhar, V; Tamchyna, A; Popel, M; Bojar, O",,,Assoc Computat Linguist,"Zouhar, Vilem; Tamchyna, Ales; Popel, Martin; Bojar, Ondrej",,,Neural Machine Translation Quality and Post-Editing Performance,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English!Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10204,10214,,,,,,,,,,,,,,,,WOS:000860727004025,0
C,"Aggarwal, V; Garimella, A; Srinivasan, BV; Anandhavelu, N; Jain, R",,,Assoc Computat Linguist,"Aggarwal, Vinay; Garimella, Aparna; Srinivasan, Balaji Vasan; Anandhavelu, N.; Jain, Rajiv",,,CLAUSEREC: A Clause Recommendation Framework for AI-aided Contract Authoring,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Contracts are a common type of legal document that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such documents, and even lesser in generating them. These contracts are made up of clauses, and the unique nature of these clauses calls for specific methods to understand and generate such documents. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a contract, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our prediction and recommendation. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8770,8776,,,,,,,,,,,,,,,,WOS:000860727002069,0
C,"Bai, JQ; Zhou, L; Blanco, A; Liu, SJ; Wei, FR; Zhou, M; Li, ZJ",,,Assoc Computat Linguist,"Bai, Jiaqi; Zhou, Long; Blanco, Ambrosio; Liu, Shujie; Wei, Furu; Zhou, Ming; Li, Zhoujun",,,Jointly Learning to Repair Code and Generate Commit Message,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose a novel task of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for software development. However, existing work usually performs the two tasks independently. We construct a multilingual triple dataset including buggy code, fixed code, and commit messages for this novel task. We provide the cascaded models as baseline, which are enhanced with different training approaches, including the teacher-student method, the multi-task method, and the backtranslation method. To deal with the error propagation problem of the cascaded method, the joint model is proposed that can both repair the code and generate the commit message in a unified framework. Experimental results show that the enhanced cascaded model with teacher-student method and multitask-learning method achieves the best score on different metrics of automated code repair, and the joint model behaves better than the cascaded model on commit message generation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9784,9795,,,,,,,,,,,,,,,,WOS:000860727003072,0
C,"Chen, Y; Ritter, A",,,Assoc Computat Linguist,"Chen, Yang; Ritter, Alan",,,Model Selection for Cross-Lingual Transfer,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformers that are pre-trained on multilingual corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive crosslingual transfer capabilities. In the zero-shot transfer setting, only English training data is used, and the fine-tuned model is evaluated on another target language. While this works surprisingly well, substantial variance has been observed in target language performance between different fine-tuning runs, and in the zero-shot setup, no target-language development data is available to select among multiple fine-tuned models. Prior work has relied on English dev data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices. In this paper, we show that it is possible to select consistently better models when small amounts of annotated data are available in auxiliary pivot languages. We propose a machine learning approach to model selection that uses the finetuned model's own internal representations to predict its cross-lingual capabilities. In extensive experiments we find that this method consistently selects better models than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5675,5687,,,,,,,,,,,,,,,,WOS:000855966305063,0
C,"Deng, JR; Wang, CH; Meng, XR; Wang, YJ; Li, J; Lin, S; Han, S; Miao, F; Rajasekaran, S; Ding, CW",,,Assoc Computat Linguist,"Deng, Jieren; Wang, Chenghong; Meng, Xianrui; Wang, Yijue; Li, Ji; Lin, Sheng; Han, Shuo; Miao, Fei; Rajasekaran, Sanguthevar; Ding, Caiwen",,,A Secure and Efficient Federated Learning Framework for NLP,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks. Existing solutions either involve a trusted aggregator or require heavyweight cryptographic primitives, which degrades performance significantly. Moreover, many existing secure FL designs work only under the restrictive assumption that none of the clients can be dropped out from the training protocol. To tackle these problems, we propose SEFL, a secure and efficient FL framework that (1) eliminates the need for the trusted entities; (2) achieves similar and even better model accuracy compared with existing FL designs; (3) is resilient to client dropouts. Through extensive experimental studies on natural language processing (NLP) tasks, we demonstrate that the SEFL achieves comparable accuracy compared to existing FL solutions, and the proposed pruning technique can improve runtime performance up to 13.7x.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7676,7682,,,,,,,,,,,,,,,,WOS:000860727001054,0
C,"Geva, M; Schuster, R; Berant, J; Levy, O",,,Assoc Computat Linguist,"Geva, Mor; Schuster, Roei; Berant, Jonathan; Levy, Omer",,,Transformer Feed-Forward Layers Are Key-Value Memories,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5484,5495,,,,,,,,,,,,,,,,WOS:000855966305050,0
C,"Han, RJ; Ren, X; Peng, NY",,,Assoc Computat Linguist,"Han, Rujun; Ren, Xiang; Peng, Nanyun",,,ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This Effective CONtinual pre-training framework for Event Temporal reasoning (ECONET) improves the PTLMs' fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5367,5380,,,,,,,,,,,,,,,,WOS:000855966305040,0
C,"Hao, J; Song, LF; Wang, LW; Xu, K; Tu, ZP; Yu, D",,,Assoc Computat Linguist,"Hao, Jie; Song, Linfeng; Wang, Liwei; Xu, Kun; Tu, Zhaopeng; Yu, Dong",,,RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model's outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4913,4924,,,,,,,,,,,,,,,,WOS:000855966305006,0
C,"Hessel, J; Holtzman, A; Forbes, M; Le Bras, R; Choi, Y",,,Assoc Computat Linguist,"Hessel, Jack; Holtzman, Ari; Forbes, Maxwell; Le Bras, Ronan; Choi, Yejin",,,CLIPScore: A Reference-free Evaluation Metric for Image Captioning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7514,7528,,,,,,,,,,,,,,,,WOS:000860727001043,0
C,"Jacovi, A; Swayamdipta, S; Ravfogel, S; Elazar, Y; Choi, YJ; Goldberg, Y",,,Assoc Computat Linguist,"Jacovi, Alon; Swayamdipta, Swabha; Ravfogel, Shauli; Elazar, Yanai; Choi, Yejin; Goldberg, Yoav",,,Contrastive Explanations for Model Interpretability,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Additionally, for a given input feature, our contrastive explanations can answer for which label, and against which alternative label, is the feature useful. We produce contrastive explanations via both highlevel abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1597,1611,,,,,,,,,,,,,,,,WOS:000855966301053,0
C,"Jiang, QN; Wang, MX; Cao, J; Cheng, SB; Huang, SJ; Li, L",,,Assoc Computat Linguist,"Jiang, Qingnan; Wang, Mingxuan; Cao, Jun; Cheng, Shanbo; Huang, Shujian; Li, Lei",,,Learning Kernel-Smoothed Machine Translation with Retrieved Examples,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained models are released at https://github.com/jiangqn/KSTER.",,,,,,"Li, Lei/0000-0003-3095-9776",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7280,7290,,,,,,,,,,,,,,,,WOS:000860727001027,0
C,"Karpinska, M; Akoury, N; Iyyer, M",,,Assoc Computat Linguist,"Karpinska, Marzena; Akoury, Nader; Iyyer, Mohit",,,The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1265,1285,,,,,,,,,,,,,,,,WOS:000855966301030,0
C,"Kim, B; Kim, H; Lee, SW; Lee, G; Kwak, D; Jeon, DH; Park, S; Kim, S; Kim, S; Seo, D; Lee, H; Jeong, M; Lee, S; Kim, M; Ko, SH; Kim, S; Park, T; Kim, J; Kang, S; Ryu, NH; Yoo, KM; Chang, M; Suh, S; In, S; Park, J; Kim, K; Kim, H; Jeong, J; Yeo, YG; Ham, D; Park, D; Lee, MY; Kang, J; Kang, I; Ha, JW; Park, W; Sung, N",,,Assoc Computat Linguist,"Kim, Boseop; Kim, HyoungSeok; Lee, Sang-Woo; Lee, Gichang; Kwak, Donghyun; Jeon, Dong Hyeon; Park, Sunghyun; Kim, Sungju; Kim, Seonhoon; Seo, Dongpil; Lee, Heungsub; Jeong, Minyoung; Lee, Sungjae; Kim, Minsub; Ko, Suk Hyun; Kim, Seokhun; Park, Taeyong; Kim, Jinuk; Kang, Soyoung; Ryu, Na-Hyeon; Yoo, Kang Min; Chang, Minsuk; Suh, Soobin; In, Sookyo; Park, Jinseong; Kim, Kyungduk; Kim, Hiun; Jeong, Jisu; Yeo, Yong Goo; Ham, Donghoon; Park, Dongju; Lee, Min Young; Kang, Jaewook; Kang, Inho; Ha, Jung-Woo; Park, Woomyoung; Sung, Nako",,,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3405,3424,,,,,,,,,,,,,,,,WOS:000855966303046,0
C,"Liu, YJ; Meng, FD; Chen, YF; Xu, JA; Zhou, J",,,Assoc Computat Linguist,"Liu, Yijin; Meng, Fandong; Chen, Yufeng; Xu, Jinan; Zhou, Jie",,,Scheduled Sampling Based on Decoding Steps for Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3285,3296,,,,,,,,,,,,,,,,WOS:000855966303036,0
C,"Ma, XY; Jiang, Y; Bach, N; Wang, T; Huang, ZQ; Huang, F; Lu, WM",,,Assoc Computat Linguist,"Ma, Xinyin; Jiang, Yong; Bach, Nguyen; Wang, Tao; Huang, Zhongqiang; Huang, Fei; Lu, Weiming",,,MuVER: Improving First-Stage Entity Retrieval with Multi-View Entity Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Entity retrieval, which aims at disambiguating mentions to canonical entities from massive KBs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if entities are only identified by descriptions. However, they ignore the property that meanings of entity mentions diverge in different contexts and are related to various portions of descriptions, which are treated equally in previous works. In this work, we propose Multi-View Entity Representations (MuVER), a novel approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. Our method achieves the state-of-the-art performance on ZESHEL and improves the quality of candidates on three standard Entity Linking datasets(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2617,2624,,,,,,,,,,,,,,,,WOS:000855966302064,0
C,"Olivier, R; Raj, B",,,Assoc Computat Linguist,"Olivier, Raphael; Raj, Bhiksha",,,Sequential Randomized Smoothing for Adversarially Robust Speech Recognition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6372,6386,,,,,,,,,,,,,,,,WOS:000860727000029,0
C,"Pfeiffer, J; Vulic, I; Gurevych, I; Ruder, S",,,Assoc Computat Linguist,"Pfeiffer, Jonas; Vulic, Ivan; Gurevych, Iryna; Ruder, Sebastian",,,UNKs Everywhere: Adapting Multilingual Language Models to New Scripts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such lowresource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10186,10203,,,,,,,,,,,,,,,,WOS:000860727004024,0
C,"Pimentel, T; Meister, C; Salesky, E; Teufel, S; Blasi, D; Cotterell, R",,,Assoc Computat Linguist,"Pimentel, Tiago; Meister, Clara; Salesky, Elizabeth; Teufel, Simone; Blasi, Damian; Cotterell, Ryan",,,A surprisal-duration trade-off across and within the world's languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal-duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal-duration trade-off in operation, both across and within the world's languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,949,962,,,,,,,,,,,,,,,,WOS:000855966301006,0
C,"Rajagopal, D; Balachandran, V; Hovy, E; Tsvetkov, Y",,,Assoc Computat Linguist,"Rajagopal, Dheeraj; Balachandran, Vidhisha; Hovy, Eduard; Tsvetkov, Yulia",,,SELF EXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce SELF EXPLAIN, a novel self-explaining model that explains a text classifier's predictions using phrase-based concepts. SELF EXPLAIN augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SELF EXPLAIN facilitates interpretability without sacrificing performance. Most importantly, explanations from SELF EXPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.(1)",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,836,850,,,,,,,,,,,,,,,,WOS:000855966300064,0
C,"Saparina, I; Osokin, A",,,Assoc Computat Linguist,"Saparina, Irina; Osokin, Anton",,,SPARQLing Database Queries from Intermediate Question Decompositions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To translate natural language questions into executable database queries, most approaches rely on a fully annotated training set. Annotating a large dataset with queries is difficult as it requires query-language expertise. We reduce this burden using grounded in databases intermediate question representations. These representations are simpler to collect and were originally crowdsourced within the Break dataset (Wolfson et al., 2020). Our pipeline consists of two parts: a neural semantic parser that converts natural language questions into the intermediate representations and a non-trainable transpiler to the SPARQL query language (a standard language for accessing knowledge graphs and semantic web). We chose SPARQL because its queries are structurally closer to our intermediate representations (compared to SQL). We observe that the execution accuracy of queries constructed by our model on the challenging Spider dataset is comparable with the state-of-the-art text-to-SQL methods trained with annotated SQL queries. Our code and data are publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8984,8998,,,,,,,,,,,,,,,,WOS:000860727003010,0
C,"Sen, I; Samory, M; Flock, F; Wagner, C; Augenstein, I",,,Assoc Computat Linguist,"Sen, Indira; Samory, Mattia; Flock, Fabian; Wagner, Claudia; Augenstein, Isabelle",,,How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs- sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,325,344,,,,,,,,,,,,,,,,WOS:000855966300028,0
C,"Shin, R; Lin, CH; Thomson, S; Chen, C; Roy, S; Platanios, EA; Pauls, A; Klein, D; Eisner, J; Van Durme, B",,,Assoc Computat Linguist,"Shin, Richard; Lin, Christopher H.; Thomson, Sam; Chen, Charles; Roy, Subhro; Platanios, Emmanouil Antonios; Pauls, Adam; Klein, Dan; Eisner, Jason; Van Durme, Benjamin",,,Constrained Language Models Yield Few-Shot Semantic Parsers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7699,7715,,,,,,,,,,,,,,,,WOS:000860727001056,0
C,"Shuster, K; Smith, EM; Ju, D; Weston, J",,,Assoc Computat Linguist,"Shuster, Kurt; Smith, Eric Michael; Ju, Da; Weston, Jason",,,Multi-Modal Open-Domain Dialogue,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4863,4883,,,,,,,,,,,,,,,,WOS:000855966305002,0
C,"Sun, XW; Cui, YL; Tang, HY; Zhang, FZ; Jin, BH; Wang, S",,,Assoc Computat Linguist,"Sun, Xingwu; Cui, Yanling; Tang, Hongyin; Zhang, Fuzheng; Jin, Beihong; Wang, Shi",,,Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase fine-tuning, the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile, the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand, we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT, STRM improves the matching perfromance of OOV words between a query and a document. Notably, our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20, 2020.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3570,3579,,,,,,,,,,,,,,,,WOS:000855966303061,0
C,"Wang, ZH; Dong, CY; Shang, JB",,,Assoc Computat Linguist,"Wang, Zihan; Dong, Chengyu; Shang, Jingbo",,,Average Approximates First Principal Component? An Empirical Analysis on Representations from Neural Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations-average approximate to first principal component. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong baseline. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a normal distribution for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5594,5603,,,,,,,,,,,,,,,,WOS:000855966305057,0
C,"Yamaguchi, A; Chrysostomou, G; Margatina, K; Aletras, N",,,Assoc Computat Linguist,"Yamaguchi, Atsuki; Chrysostomou, George; Margatina, Katerina; Aletras, Nikolaos",,,Frustratingly Simple Pretraining Alternatives to Masked Language Modeling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE'S parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3116,3125,,,,,,,,,,,,,,,,WOS:000855966303021,0
C,"Yarmohammadi, M; Wu, SJ; Marone, M; Xu, HR; Ebner, S; Qin, GH; Chen, YM; Guo, JL; Harman, C; Murray, K; White, AS; Dredze, M; Van Durme, B",,,Assoc Computat Linguist,"Yarmohammadi, Mahsa; Wu, Shijie; Marone, Marc; Xu, Haoran; Ebner, Seth; Qin, Guanghui; Chen, Yunmo; Guo, Jialiang; Harman, Craig; Murray, Kenton; White, Aaron Steven; Dredze, Mark; Van Durme, Benjamin",,,Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically English. While the advance of pretrained multilingual encoders suggests an easy optimism of train on English, run on any language, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including data projection and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, named entity recognition, part-of-speech tagging, and dependency parsing. We then apply data projection and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training.",,,,,,"Qin, Guanghui/0000-0002-3009-8614",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1950,1967,,,,,,,,,,,,,,,,WOS:000855966302008,0
C,"Ye, X; Nair, R; Durrett, G",,,Assoc Computat Linguist,"Ye, Xi; Nair, Rohan; Durrett, Greg",,,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model's prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level attributions that have been extensively studied in other NLP tasks such as sentiment analysis are less suitable to represent the reasoning that RC models perform. We construct counterfactual sets for three different RC settings, and through heuristics that can connect attribution methods' outputs to high-level model behavior, we can evaluate how useful different attribution methods and even different formats are for understanding counterfactuals. We find that pairwise attributions are better suited to RC than tokenlevel attributions across these different RC settings, with our best performance coming from a modification that we propose to an existing pairwise attribution method.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5496,5512,,,,,,,,,,,,,,,,WOS:000855966305051,0
C,"Yi, JW; Wu, FZ; Wu, CH; Liu, RX; Sun, GZ; Xie, X",,,Assoc Computat Linguist,"Yi, Jingwei; Wu, Fangzhao; Wu, Chuhan; Liu, Ruixuan; Sun, Guangzhong; Xie, Xing",,,Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users' historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model and news representations from the server, and send their locally computed gradients to the server for aggregation. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.",,,,,,"Wu, Chuhan/0000-0001-5730-8792",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2814,2824,,,,,,,,,,,,,,,,WOS:000855966302082,0
C,"Yin, K; DeHaan, K; Alikhani, M",,,Assoc Computat Linguist,"Yin, Kayo; DeHaan, Kenneth; Alikhani, Malihe",,,Signed Coreference Resolution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in Sign Language Processing. In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language. In this paper, we: (1) introduce Signed Coreference Resolution (SCR), a new challenge for coreference modeling and Sign Language Processing; (2) collect an annotated corpus of German Sign Language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture, iconicity, and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4950,4961,,,,,,,,,,,,,,,,WOS:000855966305009,0
C,"Yuan, RF; Wang, ZL; Li, WJ",,,Assoc Computat Linguist,"Yuan, Ruifeng; Wang, Zili; Li, Wenjie",,,Event Graph based Sentence Fusion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence fusion is a conditional generation task that merges several related sentences into a coherent one, which can be deemed as a summary sentence. The importance of sentence fusion has long been recognized by communities in natural language generation, especially in text summarization. It remains challenging for a state-of-the-art neural abstractive summarization model to generate a well-integrated summary sentence. In this paper, we explore the effective sentence fusion method in the context of text summarization. We propose to build an event graph from the input sentences to effectively capture and organize related events in a structured way and use the constructed event graph to guide sentence fusion. In addition to make use of the attention over the content of sentences and graph nodes, we further develop a graph flow attention mechanism to control the fusion process via the graph structure. When evaluated on sentence fusion data built from two summarization datasets, CNN/DaliyMail and Multi-News, our model shows to achieve state-of-the-art performance in terms of Rouge and other metrics like fusion rate and faithfulness.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4075,4084,,,,,,,,,,,,,,,,WOS:000855966304021,0
C,"Zhang, WX; He, RD; Peng, HY; Bing, LD; Lam, W",,,Assoc Computat Linguist,"Zhang, Wenxuan; He, Ruidan; Peng, Haiyun; Bing, Lidong; Lam, Wai",,,Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised crosslingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudolabeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9220,9230,,,,,,,,,,,,,,,,WOS:000860727003029,0
C,"Zhou, JN; Bhat, S",,,Assoc Computat Linguist,"Zhou, Jianing; Bhat, Suma",,,Paraphrase Generation: A Survey of the State of the Art,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper focuses on paraphrase generation, which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating fluent, diverse and human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods.",,,,,,"Bhat, Suma/0000-0003-0324-5890",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5075,5086,,,,,,,,,,,,,,,,WOS:000855966305018,0
C,"Bjorklund, J; Lindstrom, AD; Drewes, F",,,Assoc Computat Linguist,"Bjorklund, Johanna; Lindstrom, Adam Dahlgren; Drewes, Frank",,,"Bridging Perception, Memory, and Inference through Semantic Relations",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge bases, and inference engines. The network provides perception, that is, the interface between the system and its environment. The knowledge base provides explicit memory and thus immediate access to established facts. Finally, inference capabilities are provided by the inference engine which reflects on the perception, supported by memory, to reason and discover new facts. In this work, we probe six popular language models for semantic relations and outline a future line of research to study how the constituent subsystems can be jointly realised and integrated.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9136,9142,,,,,,,,,,,,,,,,WOS:000860727003021,0
C,"Clark, C; Salvador, J; Schwenk, D; Bonafilia, D; Yatskar, M; Kolve, E; Herrasti, A; Choi, J; Mehta, S; Skjonsberg, S; Schoenick, C; Sarnat, A; Hajishirzi, H; Kembhavi, A; Etzioni, O; Farhadi, A",,,Assoc Computat Linguist,"Clark, Christopher; Salvador, Jordi; Schwenk, Dustin; Bonafilia, Derrick; Yatskar, Mark; Kolve, Eric; Herrasti, Alvaro; Choi, Jonghyun; Mehta, Sachin; Skjonsberg, Sam; Schoenick, Carissa; Sarnat, Aaron; Hajishirzi, Hannaneh; Kembhavi, Aniruddha; Etzioni, Oren; Farhadi, Ali",,,Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Communicating with humans is challenging for AIs because it requires a shared understanding of the world, complex semantics (e.g., metaphors or analogies), and at times multi-modal gestures (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on Pictionary, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing icons, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses canonical scenes, visual metaphor, or icon compositions to express challenging words, making it an ideal test for mixing language and visual/symbolic communication in AI. We propose models to play Iconary and train them on over 55,000 games between human players. Our models are skillful players and are able to employ world knowledge in language models to play with words unseen during training. Elite human players outperform our models, particularly at the drawing task, leaving an important gap for future research to address. We release our dataset, code, and evaluation setup as a challenge to the community at github.com/allenai/iconary.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1864,1886,,,,,,,,,,,,,,,,WOS:000855966301074,0
C,"Ding, L; Wu, D; Tao, DC",,,Assoc Computat Linguist,"Ding, Liang; Wu, Di; Tao, Dacheng",,,Improving Neural Machine Translation by Bidirectional Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a simple and effective pretraining strategy - bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from src -> tgt to src -> tgt -> tgt+src without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3278,3284,,,,,,,,,,,,,,,,WOS:000855966303035,0
C,"Guo, C; Sablayrolles, A; Jegou, H; Kiela, D",,,Assoc Computat Linguist,"Guo, Chuan; Sablayrolles, Alexandre; Jegou, Herve; Kiela, Douwe",,,Gradient-based Adversarial Attacks against Text Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5747,5757,,,,,,,,,,,,,,,,WOS:000855966305068,0
C,"Hanawa, K; Nagata, R; Inui, K",,,Assoc Computat Linguist,"Hanawa, Kazuaki; Nagata, Ryo; Inui, Kentaro",,,Exploring Methods for Generating Feedback Comments for Writing Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based methods in generating feedback comments for preposition use. Retrieval-based methods have limitations in that they can only output feedback comments existing in a given training data. Furthermore, feedback comments can be made on other grammatical and writing items than preposition use, which is still unaddressed. To shed light on these points, we investigate a wider range of methods for generating many feedback comments in this study. Our close analysis of the type of task leads us to investigate three different architectures for comment generation: (i) a neural-retrieval-based method as a baseline, (ii) a pointer-generator-based generation method as a neural seq2seq method, (iii) a retrieve-and-edit method, a hybrid of (i) and (ii). Intuitively, the pointer-generator should outperform neural-retrieval, and retrieve-andedit should perform best. However, in our experiments, this expectation is completely overturned. We closely analyze the results to reveal the major causes of these counter-intuitive results and report on our findings from the experiments.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9719,9730,,,,,,,,,,,,,,,,WOS:000860727003067,0
C,"Hu, C; Wang, CL; Ma, XN; Meng, X; Li, YQ; Xiao, T; Zhu, JB; Li, CL",,,Assoc Computat Linguist,"Hu, Chi; Wang, Chenglong; Ma, Xiangnan; Meng, Xia; Li, Yinqiao; Xiao, Tong; Zhu, Jingbo; Li, Changliang",,,RankNAS: Efficient Neural Architecture Search by Pairwise Ranking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between good and bad candidates. Here we do not resort to performance predictors. Instead, we propose a performance ranking method (RankNAS) via pairwise ranking. It enables efficient architecture search using much fewer training examples. Moreover, we develop an architecture selection method to prune the search space and concentrate on more promising candidates. Extensive experiments on machine translation and language modeling tasks show that RankNAS can design high-performance architectures while being orders of magnitude faster than state-of-the-art NAS systems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2469,2480,,,,,,,,,,,,,,,,WOS:000855966302050,0
C,"Jiang, PJ; Long, DK; Sun, YH; Zhang, MS; Xu, GW; Xie, PJ",,,Assoc Computat Linguist,"Jiang, Peijie; Long, Dingkun; Sun, Yueheng; Zhang, Meishan; Xu, Guangwei; Xie, Pengjun",,,A Fine-Grained Domain Adaption Model for Joint Word Segmentation and POS Tagging,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS tagging, presenting a fine-grained domain adaption method to model the gaps accurately. We measure the gaps by one simple and intuitive metric, and adopt it to develop a pseudo target domain corpus based on fine-grained subdomains incrementally. A novel domain-mixed representation learning model is proposed accordingly to encode the multiple subdomains effectively. The whole process is performed progressively for both corpus construction and model training. Experimental results on a benchmark dataset show that our method can gain significant improvements over a vary of baselines. Extensive analyses are performed to show the advantages of our final domain adaption model as well.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3587,3598,,,,,,,,,,,,,,,,WOS:000855966303063,0
C,"Jones, A; Wang, WY; Mahowald, K",,,Assoc Computat Linguist,"Jones, Alex; Wang, William Yang; Mahowald, Kyle",,,A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In cross-lingual language models, representations for many different languages live in the same space. Here, we investigate the linguistic and non-linguistic factors affecting sentence-level alignment in cross-lingual pretrained language models for 101 languages and 5,050 language pairs. Using BERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our corpus, we compute a task-based measure of cross-lingual alignment in the form of bitext retrieval performance, as well as four intrinsic measures of vector space alignment and isomorphism. We then examine a range of linguistic, quasi-linguistic, and training-related features as potential predictors of these alignment metrics. The results of our analyses show that word order agreement and agreement in morphological complexity are two of the strongest linguistic predictors of cross-linguality. We also note in family training data as a stronger predictor than language-specific training data across the board. We verify some of our linguistic findings by looking at the effect of morphological segmentation on English-Inuktitut alignment, in addition to examining the effect of word order agreement on isomorphism for 66 zero-shot language pairs from a different corpus. We make the data and code for our experiments publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5833,5847,,,,,,,,,,,,,,,,WOS:000855966306001,0
C,"Kato, Y; Matsubara, S",,,Assoc Computat Linguist,"Kato, Yoshihide; Matsubara, Shigeki",,,A New Representation for Span-based CCG Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-theshelf span-based parser with our representation is comparable with previous CCG parsers.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10579,10584,,,,,,,,,,,,,,,,WOS:000860727004050,0
C,"Kim, H; Kim, B; Kim, G",,,Assoc Computat Linguist,"Kim, Hyunwoo; Kim, Byeongchang; Kim, Gunhee",,,Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Empathy is a complex cognitive ability based on the reasoning of others' affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other's emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level annotations, which can be demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on pragmatics to make dialogue models focus on targeted words in the input during generation. Our method is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2227,2240,,,,,,,,,,,,,,,,WOS:000855966302029,0
C,"Liang, XN; Wu, SZ; Li, M; Li, ZJ",,,Assoc Computat Linguist,"Liang, Xinnian; Wu, Shuangzhi; Li, Mu; Li, Zhoujun",,,Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,155,164,,,,,,,,,,,,,,,,WOS:000855966300014,0
C,"Marion, P; Nowak, PK; Piccinno, F",,,Assoc Computat Linguist,"Marion, Pierre; Nowak, Pawel Krzysztof; Piccinno, Francesco",,,Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We tackle the problem of weakly-supervised conversational Question Answering over large Knowledge Graphs using a neural semantic parsing approach. We introduce a new Logical Form (LF) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate supervision data efficiently. Our Transformer-based model takes a JSON-like structure as input, allowing us to easily incorporate both Knowledge Graph and conversational contexts. This structured input is transformed to lists of embeddings and then fed to standard attention layers. We validate our approach, both in terms of grammar coverage and LF execution accuracy, on two publicly available datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA, our approach increases the coverage from 80% to 96.2%, and the LF execution accuracy from 70.6% to 75.6%, with respect to previous state-of-the-art results. On ConvQuestions, we achieve competitive results with respect to the state-of-the-art.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8813,8829,,,,,,,,,,,,,,,,WOS:000860727002073,0
C,"Mireshghallah, F; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Mireshghallah, Fatemehsadat; Berg-Kirkpatrick, Taylor",,,Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text style can reveal sensitive attributes of the author (e.g. race or age) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in hiring decisions, regardless of whether hiring decisions are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer by automatically re-writing the text itself. Our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style: (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style pooling's effect on fluency, semantic consistency, and attribute removal from text, in two and three domain style obfuscation.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2009,2022,,,,,,,,,,,,,,,,WOS:000855966302011,0
C,"Roy, A; Pan, SM",,,Assoc Computat Linguist,"Roy, Arpita; Pan, Shimei",,,Incorporating medical knowledge in BERT for clinical relation extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question/Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia text versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperformed the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5357,5366,,,,,,,,,,,,,,,,WOS:000855966305039,0
C,"Ru, DY; Sun, CZ; Feng, JT; Qiu, L; Zhou, H; Zhang, WN; Yu, Y; Li, L",,,Assoc Computat Linguist,"Ru, Dongyu; Sun, Changzhi; Feng, Jiangtao; Qiu, Lin; Zhou, Hao; Zhang, Weinan; Yu, Yong; Li, Lei",,,Learning Logic Rules for Document-level Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that LogiRE significantly outperforms several strong baselines in terms of relation performance (similar to 1.8 F1 score) and logical consistency (over 3.3 logic score). Our code is available at https://github. com/rudongyu/LogiRE.",,,,,,"Li, Lei/0000-0003-3095-9776",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1239,1250,,,,,,,,,,,,,,,,WOS:000855966301028,0
C,"Schuster, T; Fisch, A; Jaakkola, T; Barzilay, R",,,Assoc Computat Linguist,"Schuster, Tal; Fisch, Adam; Jaakkola, Tommi; Barzilay, Regina",,,Consistent Accelerated Inference via Confident Adaptive Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs-Confident Adaptive Transformers-in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4962,4979,,,,,,,,,,,,,,,,WOS:000855966305010,0
C,"Sharma, A; Chhablani, G; Pandey, H; Patil, R",,,Assoc Computat Linguist,"Sharma, Abheesht; Chhablani, Gunjan; Pandey, Harshit; Patil, Rajaswa",,,DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The analysis methods are collated from well-cited research works, with a few of our own methods added for good measure. Succinctly put, some of the analysis methods are: keyword extraction, word clouds, predicting declining/stagnant/growing trends using Productivity, tracking bi-grams using Acceleration plots, finding the Semantic Drift of words, tracking trends using similarity, etc. To demonstrate the utility and efficacy of our tool, we perform a case study on the cs. CL corpus of the arXiv repository and draw inferences from the analysis methods. The toolkit and the associated code are available here.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,361,371,,,,,,,,,,,,,,,,WOS:000855241500040,0
C,"Wang, LW; Li, XF; Liu, JC; He, KQ; Yan, YM; Xu, WR",,,Assoc Computat Linguist,"Wang, Liwen; Li, Xuefeng; Liu, Jiachi; He, Keqing; Yan, Yuanmeng; Xu, Weiran",,,Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our model achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9474,9480,,,,,,,,,,,,,,,,WOS:000860727003047,0
C,"Wang, YQ; Wang, S; Yao, QM; Dou, DJ",,,Assoc Computat Linguist,"Wang, Yaqing; Wang, Song; Yao, Quanming; Dou, Dejing",,,Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.(1)",,,,,"Yao, Quanming/Y-6095-2019","Yao, Quanming/0000-0001-8944-8618",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3091,3101,,,,,,,,,,,,,,,,WOS:000855966303019,0
C,"Xu, MZ; Li, LY; Wong, DF; Liu, Q; Chao, LDS",,,Assoc Computat Linguist,"Xu, Mingzhou; Li, Liangyou; Wong, Derek F.; Liu, Qun; Chao, Lidia S.",,,Document Graph for Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods only consider a few number of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English-French, Chinese-English, WMT English-German and Opensubtitle English-Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8435,8448,,,,,,,,,,,,,,,,WOS:000860727002041,0
C,"Zhang, JQ; Bolanos, L; Li, T; Tanwar, A; Freire, G; Yang, X; Ive, J; Gupta, V; Guo, YK",,,Assoc Computat Linguist,"Zhang, Jingqing; Bolanos, Luis; Li, Tong; Tanwar, Ashwani; Freire, Guilherme; Yang, Xian; Ive, Julia; Gupta, Vibhor; Guo, Yike",,,Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from electronic health records. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After fine-tuning with as little as 20% of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the phenotypes annotated by our model as features.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8754,8769,,,,,,,,,,,,,,,,WOS:000860727002068,0
C,"Zi, KL; Wang, S; Cao, YA; Liu, Y; Li, JC; Cao, CG",,,Assoc Computat Linguist,"Zi, Kangli; Wang, Shi; Cao, Yanan; Liu, Yu; Li, Jicun; Cao, Cungen",,,SOM-NCSCM: An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce of parallel corpora, different segmentation granularity of Chinese sentences, and imperfect performance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated so far. In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answering system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner.(1) Experimental results show that our SOM-NCSCM can significantly benefit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on the Chinese SC task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,403,415,,,,,,,,,,,,,,,,WOS:000855966300033,0
C,"Zou, YC; Liu, ZH; Hu, XW; Zhang, Q",,,Assoc Computat Linguist,"Zou, Yicheng; Liu, Zhihua; Hu, Xingwu; Zhang, Qi",,,"Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2215,2226,,,,,,,,,,,,,,,,WOS:000855966302028,0
C,"Baheti, A; Sap, M; Ritter, A; Riedl, M",,,Assoc Computat Linguist,"Baheti, Ashutosh; Sap, Maarten; Ritter, Alan; Riedl, Mark",,,Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on TOXICHAT that achieve 0.71 F-1 for offensive labels and 0.53 Macro-F-1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4846,4862,,,,,,,,,,,,,,,,WOS:000855966305001,0
C,"Bai, L; Guan, SP; Guo, JF; Li, ZX; Jin, XL; Cheng, XQ",,,Assoc Computat Linguist,"Bai, Long; Guan, Saiping; Guo, Jiafeng; Li, Zixuan; Jin, Xiaolong; Cheng, Xueqi",,,Integrating Deep Event-Level and Script-Level Information for Script Event Prediction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At the script level, most existing studies only consider a single event sequence corresponding to one common protagonist. In this paper, we propose a Transformer-based model, called MCPredictor, which integrates deep event-level and script-level information for script event prediction. At the event level, MCPredictor utilizes the rich information in the text to obtain more comprehensive event semantic representations. At the script-level, it considers multiple event sequences corresponding to different participants of the subsequent event. The experimental results on the widely-used New York Times corpus demonstrate the effectiveness and superiority of the proposed model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9869,9878,,,,,,,,,,,,,,,,WOS:000860727004001,0
C,"Bansal, T; Gunasekaran, K; Wang, T; Munkhdalai, T; McCallum, A",,,Assoc Computat Linguist,"Bansal, Trapit; Gunasekaran, Karthick; Wang, Tong; Munkhdalai, Tsendsuren; McCallum, Andrew",,,Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diversity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribution, some inducing significant improvements in downstream few-shot accuracy of the meta-learned models. Empirically, results on 20 downstream tasks show significant improvements in few-shot learning - adding up to +4.2.% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised methods on the FewRel 2.0 benchmark.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5812,5824,,,,,,,,,,,,,,,,WOS:000855966305073,0
C,"Ben Veyseh, AP; Nguyen, MV; Trung, NN; Min, BN; Nguyen, TH",,,Assoc Computat Linguist,"Ben Veyseh, Amir Pouran; Minh Van Nguyen; Nghia Ngo Trung; Min, Bonan; Thien Huu Nguyen",,,Modeling Document-Level Context for Event Detection via Important Context Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event Detection (ED) aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context with BERT for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by BERT for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5403,5413,,,,,,,,,,,,,,,,WOS:000855966305043,0
C,"Blloshmi, R; Pasini, T; Campolungo, N; Banerjee, S; Navigli, R; Pasi, G",,,Assoc Computat Linguist,"Blloshmi, Rexhina; Pasini, Tommaso; Campolungo, Niccolo; Banerjee, Somnath; Navigli, Roberto; Pasi, Gabriella",,,IR like a SIR Sense-enhanced Information Retrieval for Multiple Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the advent of contextualized embeddings, attention towards neural ranking approaches for Information Retrieval increased considerably. However, two aspects have remained largely neglected: i) queries usually consist of few keywords only, which increases ambiguity and makes their contextualization harder, and ii) performing neural ranking on non-English documents is still cumbersome due to shortage of labeled datasets. In this paper we present SIR (Sense-enhanced Information Retrieval) to mitigate both problems by leveraging word sense information. At the core of our approach lies a novel multilingual query expansion mechanism based on Word Sense Disambiguation that provides sense definitions as additional semantic information for the query. Importantly, we use senses as a bridge across languages, thus allowing our model to perform considerably better than its supervised and unsupervised alternatives across French, German, Italian and Spanish languages on several CLEF benchmarks, while being trained on English Robust04 data only. We release SIR at https://github.com/SapienzaNLP/sir.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1030,1041,,,,,,,,,,,,,,,,WOS:000855966301012,0
C,"Brody, S; Wu, SC; Benton, A",,,Assoc Computat Linguist,"Brody, Sam; Wu, Sichao; Benton, Adrian",,,Towards Realistic Few-Shot Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In recent years, few-shot models have been applied successfully to a variety of NLP tasks. Han et al. (2018) introduced a few-shot learning framework for relation classification, and since then, several models have surpassed human performance on this task, leading to the impression that few-shot relation classification is solved. In this paper we take a deeper look at the efficacy of strong few-shot classification models in the more common relation extraction setting, and show that typical few-shot evaluation metrics obscure a wide variability in performance across relations. In particular, we find that state of the art few-shot relation classification models overly rely on entity type information, and propose modifications to the training routine to encourage models to better discriminate between relations involving similar entity types.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5338,5345,,,,,,,,,,,,,,,,WOS:000855966305037,0
C,"Cahyawijaya, S; Winata, GI; Wilie, B; Vincentio, K; Li, XH; Kuncoro, A; Ruder, S; Lim, ZY; Bahar, S; Khodra, ML; Purwarianti, A; Fung, P",,,Assoc Computat Linguist,"Cahyawijaya, Samuel; Winata, Genta Indra; Wilie, Bryan; Vincentio, Karissa; Li, Xiaohong; Kuncoro, Adhiguna; Ruder, Sebastian; Lim, Zhi Yuan; Bahar, Syafri; Khodra, Masayu Leylia; Purwarianti, Ayu; Fung, Pascale",,,IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource-yet widely spokenlanguages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks-despite using only one-fifth the parameters of a larger multilingual model, mBARTLARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8875,8898,,,,,,,,,,,,,,,,WOS:000860727003001,0
C,"Cao, K; Rimell, L",,,Assoc Computat Linguist,"Cao, Kris; Rimell, Laura",,,You should evaluate your language model on marginal likelihood over tokenisations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate pretrained English and German language models on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2104,2114,,,,,,,,,,,,,,,,WOS:000855966302020,0
C,"Chaudhury, S; Sen, P; Ono, M; Kimura, D; Tatsubori, M; Munawar, A",,,Assoc Computat Linguist,"Chaudhury, Subhajit; Sen, Prithviraj; Ono, Masaki; Kimura, Daiki; Tatsubori, Michiaki; Munawar, Asim",,,Neuro-Symbolic Approaches for Text-Based Policy Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text-Rased Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for end-to-end differentiable symbolic rule learning and show that such symbolic policies outperform previous stateof-the-art methods in text-based RL for the coin collector environment from 5 - 10x fewer training games. Additionally, our method provides human-understandable policy rules that can be readily verified for their logical consistency and can be easily debugged.",,,,,"Chaudhury, Subhajit/T-1625-2019","Chaudhury, Subhajit/0000-0003-3435-2584",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3073,3078,,,,,,,,,,,,,,,,WOS:000855966303017,0
C,"De Raedt, M; Godin, F; Buteneers, P; Develder, C; Demeester, T",,,Assoc Computat Linguist,"De Raedt, Maarten; Godin, Frederic; Buteneers, Pieter; Develder, Chris; Demeester, Thomas",,,A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. For efficient learning, we investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and crosslingual settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10108,10114,,,,,,,,,,,,,,,,WOS:000860727004016,0
C,"Dzendzik, D; Vogel, C; Foster, J",,,Assoc Computat Linguist,"Dzendzik, Dania; Vogel, Carl; Foster, Jennifer",,,English Machine Reading Comprehension Datasets: A Survey,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size, vocabulary, data source, method of creation, human performance level, and first question word. Our analysis reveals that Wikipedia is by far the most common data source and that there is a relative lack of why, when, and where questions across datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8784,8804,,,,,,,,,,,,,,,,WOS:000860727002071,0
C,"Feng, WJ; Liu, BB; Xu, DP; Zheng, QL; Xu, Y",,,Assoc Computat Linguist,"Feng, Weijie; Liu, Binbin; Xu, Dongpeng; Zheng, Qilong; Xu, Yun",,,GraphMR Graph Neural Network for Mathematical Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Mathematical reasoning aims to infer satisfiable solutions based on the given mathematics questions. Previous natural language processing researches have proven the effectiveness of sequence-to-sequence (Seq2Seq) or related variants on mathematics solving. However, few works have been able to explore structural or syntactic information hidden in expressions (e.g., precedence and associativity). This dissertation set out to investigate the usefulness of such untapped information for neural architectures. Firstly, mathematical questions are represented in the format of graphs within syntax analysis. The structured nature of graphs allows them to represent relations of variables or operators while preserving the semantics of the expressions. Having transformed to the new representations, we proposed a graph-to-sequence neural network GraphMR, which can effectively learn the hierarchical information of graphs inputs to solve mathematics and speculate answers. A complete experimental scenario with four classes of mathematical tasks and three Seq2Seq baselines is built to conduct a comprehensive analysis, and results show that GraphMR outperforms others in hidden information learning and mathematics resolving.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3395,3404,,,,,,,,,,,,,,,,WOS:000855966303045,0
C,"Gan, ZF; Xu, HF; Zan, HY",,,Assoc Computat Linguist,"Gan, Zifa; Xu, Hongfei; Zan, Hongying",,,Self-Supervised Curriculum Learning for Spelling Error Correction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model's performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).",,,,,,"Xu, Hongfei/0000-0001-8397-1459",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3487,3494,,,,,,,,,,,,,,,,WOS:000855966303053,0
C,"Giulianelli, M; Sinclair, A; Fernandez, R",,,Assoc Computat Linguist,"Giulianelli, Mario; Sinclair, Arabella; Fernandez, Raquel",,,Is Information Density Uniform in Task-Oriented Dialogues?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations in the density of the information transmitted. In this paper, we test whether, and within which contextual units this principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following. Our study underlines the importance of identifying the relevant contextual components, showing that information content increases particularly within topically and referentially related contextual units.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8271,8283,,,,,,,,,,,,,,,,WOS:000860727002030,0
C,"Guo, H; Rajani, NF; Hase, P; Bansal, M; Xiong, CM",,,Assoc Computat Linguist,"Guo, Han; Rajani, Nazneen Fatema; Hase, Peter; Bansal, Mohit; Xiong, Caiming",,,FASTIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Influence functions approximate the influences of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FASTIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (k NN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80X speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can explain test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MultiNLI model by 2:5% on the HANS dataset. Finally, we experiment with a similar setup but fine-tuning on datapoints not seen during training, improving the model accuracy by 2:8% and 1:7% on HANS and ANLI datasets respectively. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10333,10350,,,,,,,,,,,,,,,,WOS:000860727004032,0
C,"Imani, A; Sabet, MJ; Senel, LK; Dufter, P; Yvon, F; Schutze, H",,,Assoc Computat Linguist,"Imani, Ayyoob; Sabet, Masoud Jalili; Senel, Luetfi Kerem; Dufter, Philipp; Yvon, Francois; Schuetze, Hinrich",,,Graph Algorithms for Multiparallel Word Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements of F-1 of up to 28% over the baseline bilingual word aligner in different datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8457,8469,,,,,,,,,,,,,,,,WOS:000860727002043,0
C,"Ju, YM; Zhang, YZ; Tian, ZX; Liu, K; Cao, XH; Zhao, WT; Li, JL; Zhao, J",,,Assoc Computat Linguist,"Ju, Yiming; Zhang, Yuanzhe; Tian, Zhixing; Liu, Kang; Cao, Xiaohuan; Zhao, Wenting; Li, Jinlong; Zhao, Jun",,,Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines' ability to understand human language. Multiple-choice MRC is one of the most studied tasks in MRC due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input features. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our method can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3641,3652,,,,,,,,,,,,,,,,WOS:000855966303067,0
C,"Klimaszewski, M; Wroblewska, A",,,Assoc Computat Linguist,"Klimaszewski, Mateusz; Wroblewska, Alina",,,COMBO: State-of-the-Art Morphosyntactic Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce COMBO - a fully neural NLP system for accurate part-of-speech tagging, morphological analysis, lemmatisation, and (enhanced) dependency parsing. It predicts categorical morphosyntactic features whilst also exposes their vector representations, extracted from hidden layers. COMBO is an easy to install Python package with automatically downloadable pre-trained models for over 40 languages. It maintains a balance between efficiency and quality. As it is an end-to-end system and its modules are jointly trained, its training is competitively fast. As its models are optimised for accuracy, they achieve often better prediction quality than SOTA. The COMBO library is available at: https : /gitlab.clarin-pl.eu/ syntactic-tools/combo.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,50,62,,,,,,,,,,,,,,,,WOS:000855241500007,0
C,"Liu, JW; Song, KS; Kang, YY; He, GX; Jiang, ZR; Sun, CL; Lu, W; Liu, XZ",,,Assoc Computat Linguist,"Liu, Jiawei; Song, Kaisong; Kang, Yangyang; He, Guoxiu; Jiang, Zhuoren; Sun, Changlong; Lu, Wei; Liu, Xiaozhong",,,A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, RoleSelected Sharing Network (RSSN), which integrates both dialogue satisfaction estimation and handoff prediction in one multi-task learning framework. Unlike prior efforts in dialog mining, by utilizing local user satisfaction as a bridge, global satisfaction detector and handoff predictor can effectively exchange critical information. Specifically, we decouple the relation and interaction between the two tasks by the role information after the shared encoder. Extensive experiments on two public datasets demonstrate the effectiveness of our model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9731,9741,,,,,,,,,,,,,,,,WOS:000860727003068,0
C,"Riabi, A; Scialom, T; Keraron, R; Sagot, B; Seddah, D; Staiano, J",,,Assoc Computat Linguist,"Riabi, Arij; Scialom, Thomas; Keraron, Rachel; Sagot, Benoit; Seddah, Djame; Staiano, Jacopo",,,Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on Question Answering tasks. However, most of those datasets are in English, and the performances of state-of-theart multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only, establishing thus a new state-of-the-art on four multilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7016,7030,,,,,,,,,,,,,,,,WOS:000860727001010,0
C,"Salemi, A; Kebriaei, E; Minaei, GN; Shakery, A",,,Assoc Computat Linguist,"Salemi, Alireza; Kebriaei, Emad; Minaei, Ghazal Neisi; Shakery, Azadeh",,,ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the semantic similarity between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In ARMAN, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed models on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in textual entailment, question paraphrasing, and multiple choice question answering. Finally, we established a human evaluation and show that using the semantic score significantly improves summarization results.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9391,9407,,,,,,,,,,,,,,,,WOS:000860727003042,0
C,"Seoh, R; Birle, I; Tak, M; Chang, HS; Pinette, B; Hough, A",,,Assoc Computat Linguist,"Seoh, Ronald; Birle, Ian; Tak, Mrinal; Chang, Haw-Shivan; Pinette, Brian; Hough, Alfred",,,Open Aspect Target Sentiment Classification with Natural Language Prompts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with natural language prompts, enabling the task under zero-shot cases and enhancing supervised settings, especially for few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop domain, our method of reformulating ATSC as an NLI task outperforms supervised SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points. Moreover, we demonstrate that our prompts could handle implicitly stated aspects as well: our models reach about 77% accuracy on detecting sentiments for aspect categories (e.g., food), which do not necessarily appear within the text, even though we trained the models only with explicitly mentioned aspect terms (e.g., fajitas) from just 16 reviews - while the accuracy of the no-prompt baseline is only around 65%.",,,,,"Chang, Haw-Shiuan/HDN-6000-2022","Chang, Haw-Shiuan/0000-0003-4607-936X",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6311,6322,,,,,,,,,,,,,,,,WOS:000860727000024,0
C,"Stanojevic, M; Cohen, SB",,,Assoc Computat Linguist,"Stanojevic, Milos; Cohen, Shay B.",,,A Root of a Problem: Optimizing Single-Root Dependency Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We describe two approaches to single-root dependency parsing that yield significant speed ups in such parsing. One approach has been previously used in dependency parsers in practice, but remains undocumented in the parsing literature, and is considered a heuristic. We show that this approach actually finds the optimal dependency tree. The second approach relies on simple reweighting of the inference graph being input to the dependency parser and has an optimal running time. Here, we again show that this approach is fully correct and identifies the highest-scoring parse tree. Our experiments demonstrate a manyfold speed up compared to a previous graph-based state-of-the-art parser without any loss in accuracy or optimality.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10540,10557,,,,,,,,,,,,,,,,WOS:000860727004047,0
C,"Vanmassenhove, E; Emmery, C; Shterionov, D",,,Assoc Computat Linguist,"Vanmassenhove, Eva; Emmery, Chris; Shterionov, Dimitar",,,NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender-Neutral Alternatives,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rulebased and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18% on synthetic, in-domain and out-domain test sets.",,,,,,"Vanmassenhove, Eva/0000-0003-1162-820X; Shterionov, Dimitar/0000-0001-6300-797X; Emmery, Chris/0000-0002-2179-559X",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8940,8948,,,,,,,,,,,,,,,,WOS:000860727003006,0
C,"Vulic, I; Su, PH; Coope, S; Gerz, D; Budzianowski, P; Casanueva, I; Mrksic, N; Wen, TH",,,Assoc Computat Linguist,"Vulic, Ivan; Su, Pei-Hao; Coope, Sam; Gerz, Daniela; Budzianowski, Pawel; Casanueva, Inigo; Mrksic, Nikola; Wen, Tsung-Hsien",,,CONVFIT: Conversational Fine-Tuning of Pretrained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose CONVFIT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 CONVFIT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the CONVFIT framework with such similarity-based inference on the standard ID evaluation sets: CONVFIT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1151,1168,,,,,,,,,,,,,,,,WOS:000855966301021,0
C,"Wu, MH; Li, YT; Zhang, M; Li, LY; Haffari, G; Liu, Q",,,Assoc Computat Linguist,"Wu, Minghao; Li, Yitong; Zhang, Meng; Li, Liangyou; Haffari, Gholamreza; Liu, Qun",,,Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the highresource ones. However, automatic balancing methods usually depend on the intra- and interdataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MULTIUAT, that dynamically adjusts the training data usage based on the model's uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiment with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MULTIUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the crossdomain transfer and show the deficiency of static and similarity based methods.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7291,7305,,,,,,,,,,,,,,,,WOS:000860727001028,0
C,"Xu, H; Ghosh, G; Huang, PY; Okhonko, D; Aghajanyan, A; Metze, F; Zettlemoyer, L; Feichtenhofer, C",,,Assoc Computat Linguist,"Xu, Hu; Ghosh, Gargi; Huang, Po-Yao; Okhonko, Dmytro; Aghajanyan, Armen; Metze, Florian; Zettlemoyer, Luke; Feichtenhofer, Christoph",,,VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6787,6800,,,,,,,,,,,,,,,,WOS:000860727000059,0
C,"Zhang, HT; Zheng, TH; Li, YL; Gao, J; Su, L; Li, B",,,Assoc Computat Linguist,"Zhang, Hengtong; Zheng, Tianhang; Li, Yaliang; Gao, Jing; Su, Lu; Li, Bo",,,Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output profanity. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of profanity examples to prevent seq2seq models from generating a broader spectrum of profanity. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with profanity in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating profanity.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5151,5161,,,,,,,,,,,,,,,,WOS:000855966305022,0
C,"Zhao, J; Mandieh, M; Zhang, Y; Cao, Y; Wu, YH",,,Assoc Computat Linguist,"Zhao, Jeffrey; Mandieh, Mahdis; Zhang, Ye; Cao, Yuan; Wu, Yonghui",,,Effective Sequence-to-Sequence Dialogue State Tracking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for text summarization, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the model may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7486,7493,,,,,,,,,,,,,,,,WOS:000860727001041,0
C,"Zhao, MJ; Schutze, H",,,Assoc Computat Linguist,"Zhao, Mengjie; Schutze, Hinrich",,,Discrete and Soft Prompting for Multilingual Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8547,8555,,,,,,,,,,,,,,,,WOS:000860727002050,0
C,"Zhen, RR; Wang, R; Fu, GH; Lv, CG; Zhang, MS",,,Assoc Computat Linguist,"Zhen, Ranran; Wang, Rui; Fu, Guohong; Lv, Chengguo; Zhang, Meishan",,,Chinese Opinion Role Labeling with Corpus Translation: A Pivot Study,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual BERT with Contextual Parameter Generator and Adapter methods to examine the potentials of unsupervised cross-lingual learning and our experiments and analyses for both bilingual and multilingual transfers establish a foundation for the future research of this task(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10139,10149,,,,,,,,,,,,,,,,WOS:000860727004020,0
C,"Zhou, JW; Naseem, T; Astudillo, RF; Lee, YS; Florian, R; Roukos, S",,,Assoc Computat Linguist,"Zhou, Jiawei; Naseem, Tahira; Astudillo, Ramon Fernandez; Lee, Young-Suk; Florian, Radu; Roukos, Salim",,,Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6279,6290,,,,,,,,,,,,,,,,WOS:000860727000022,0
C,"Blaier, E; Malkiel, I; Wolf, L",,,Assoc Computat Linguist,"Blaier, Efrat; Malkiel, Itzik; Wolf, Lior",,,Caption Enriched Samples for Improving Hateful Memes Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast between the image content and the overlayed text, we suggest applying an off-the-shelf image captioning tool in order to capture the first. We demonstrate that the incorporation of such automatic captions during fine-tuning improves the results for various unimodal and multimodal models. Moreover, in the unimodal case, continuing the pre-training of language models on augmented and original caption pairs, is highly beneficial to the classification accuracy. Our code is publicly available(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9350,9358,,,,,,,,,,,,,,,,WOS:000860727003040,0
C,"Cao, LL; Larsson, E; von Ehrenheim, V; Rocha, DDC; Martin, A; Horn, S",,,Assoc Computat Linguist,"Cao, Lele; Larsson, Emil; von Ehrenheim, Vilhelm; Rocha, Dhiana Deva Cavalcanti; Martin, Anna; Horn, Sonja",,,PAUSE: Positive and Annealed Unlabeled Sentence Embedding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared to the unsupervised methods, the supervised ones make less assumptions about optimization objectives and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach - PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our dataset without the burden of extensive manual annotation work.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10096,10107,,,,,,,,,,,,,,,,WOS:000860727004015,0
C,"Cao, PF; Chen, YB; Yang, YQ; Liu, K; Zhao, J",,,Assoc Computat Linguist,"Cao, Pengfei; Chen, Yubo; Yang, Yuqing; Liu, Kang; Zhao, Jun",,,Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of document-level event factuality identification, which determines the event factuality from the view of a document. For this task, we need to consider two important characteristics: Local Uncertainty and Global Structure, which can be utilized to improve performance. In this paper, we propose an Uncertain Local-to-Global Network (ULGN) to make use of these two characteristics. Specifically, we devise a Local Uncertainty Estimation module to model the uncertainty of local information. Moreover, we propose an Uncertain Information Aggregation module to leverage the global structure for integrating the local information. Experimental results demonstrate the effectiveness of our proposed method, outperforming the previous state-of-the-art model by 8.4% and 11.45% of F1 score on two widely used datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2636,2645,,,,,,,,,,,,,,,,WOS:000855966302066,0
C,"Chheda, T; Goyal, P; Tran, T; Patel, D; Boratko, M; Dasgupta, SS; McCallum, A",,,Assoc Computat Linguist,"Chheda, Tejas; Goyal, Purujit; Trang Tran; Patel, Dhruvesh; Boratko, Michael; Dasgupta, Shib Sankar; McCallum, Andrew",,,Box Embeddings: An open-source library for representation learning using geometric structures,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A major factor contributing to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with geometric structures (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacities. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings. (1) Fundamental geometric operations on boxes are implemented in a numerically stable way, as are modern approaches to training boxes which mitigate gradient sparsity. The library is fully open-source, and compatible with both PyTorch and TensorFlow, which allows existing neural network layers to be replaced with or transformed into boxes effortlessly. In this work, we present the implementation details of the fundamental components of the library, and the concepts required to use box representations alongside existing neural network architectures.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,203,211,,,,,,,,,,,,,,,,WOS:000855241500024,0
C,"Dognin, PL; Padhi, I; Melnyk, I; Das, P",,,Assoc Computat Linguist,"Dognin, Pierre L.; Padhi, Inkit; Melnyk, Igor; Das, Payel",,,ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TEKGEN datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in https://github.com/IBM/regen.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1084,1099,,,,,,,,,,,,,,,,WOS:000855966301016,0
C,"Elazar, Y; Zhang, HM; Goldberg, Y; Roth, D",,,Assoc Computat Linguist,"Elazar, Yanai; Zhang, Hongming; Goldberg, Yoav; Roth, Dan",,,"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses twin sentences for evaluation. We also propose two new baselines that indicate the existence of artifacts in WS benchmarks. We then develop a method for evaluating WS-like sentences in a zero-shot setting to account for the commonsense reasoning abilities acquired during the pretraining and observe that popular language models perform randomly in this setting when using our more strict evaluation. We conclude that the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required commonsense reasoning skills and knowledge.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10486,10500,,,,,,,,,,,,,,,,WOS:000860727004043,0
C,"Falke, T; Lehnen, P",,,Assoc Computat Linguist,"Falke, Tobias; Lehnen, Patrick",,,Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such feedback is often available in real-world dialog systems, however, the modularized architecture commonly used in large-scale systems prevents the direct application of such algorithms. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1190,1198,,,,,,,,,,,,,,,,WOS:000855966301024,0
C,"Hardalov, M; Arora, A; Nakov, P; Augenstein, I",,,Assoc Computat Linguist,"Hardalov, Momchil; Arora, Arnav; Nakov, Preslav; Augenstein, Isabelle",,,Cross-Domain Label-Adaptive Stance Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9011,9028,,,,,,,,,,,,,,,,WOS:000860727003012,0
C,"He, XW",,,Assoc Computat Linguist,"He, Xingwei",,,Parallel Refinements for Lexically Constrained Text Generation with BART,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Lexically constrained text generation aims to control the generated text by incorporating some pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to generate generic or ungrammatical sentences, and has high computational complexity. To address these challenges, we propose Constrained BART (CBART) for lexically constrained text generation. CBART leverages the pre-trained model BART and transfers part of the generation burden from the decoder to the encoder by decomposing this task into two sub-tasks, thereby improving the sentence quality. Concretely, we extend BART by adding a token-level classifier over the encoder, aiming at instructing the decoder where to replace and insert. Guided by the encoder, the decoder refines multiple tokens of the input in one step by inserting tokens before specific positions and re-predicting tokens with low confidence. To further reduce the inference latency, the decoder predicts all tokens in parallel. Experiment results on One-Billion-Word and Yelp show that CBART can generate plausible text with high quality and diversity while significantly accelerating inference.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8653,8666,,,,,,,,,,,,,,,,WOS:000860727002059,0
C,"Howcroft, DM; Rieser, V",,,Assoc Computat Linguist,"Howcroft, David M.; Rieser, Verena",,,What happens if you treat ordinal ratings as interval data? Human evaluations in NLP are even more under-powered than you think,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences they are hoping to detect are often subtle. We demonstrate through simulation that ordinal mixed effects models are better able to detect small differences between models, especially in high variance settings common in evaluations of generated texts. We release tools for researchers to conduct their own power analysis and test their assumptions. We also make recommendations for improving statistical power.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8932,8939,,,,,,,,,,,,,,,,WOS:000860727003005,0
C,"Hu, MT; Guo, HL; Zhao, SW; Gao, H; Su, Z",,,Assoc Computat Linguist,"Hu, Mengting; Guo, Honglei; Zhao, Shiwan; Gao, Hang; Su, Zhong",,,Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph Refinement,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A mind-map is a diagram that represents the central concept and key ideas in a hierarchical way. Converting plain text into a mind-map will reveal its key semantic structure and be easier to understand. Given a document, the existing automatic mind-map generation method extracts the relationships of every sentence pair to generate the directed semantic graph for this document. The computation complexity increases exponentially with the length of the document. Moreover, it is difficult to capture the overall semantics. To deal with the above challenges, we propose an efficient mind-map generation network that converts a document into a graph via sequence-to-graph. To guarantee a meaningful mindmap, we design a graph refinement module to adjust the relation graph in a reinforcement learning manner. Extensive experimental results demonstrate that the proposed approach is more effective and efficient than the existing methods. The inference time is reduced by thousands of times compared with the existing methods. The case studies verify that the generated mind-maps better reveal the underlying semantic structures of the document.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8130,8141,,,,,,,,,,,,,,,,WOS:000860727002019,0
C,"Huang, KH; Ahmad, WU; Peng, NY; Chang, KW",,,Assoc Computat Linguist,"Huang, Kuan-Hao; Ahmad, Wasi Uddin; Peng, Nanyun; Chang, Kai-Wei",,,Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding space usually requires sentence-level or word-level parallel corpora, which are expensive to be obtained for low-resource languages. An alternative is to make the multilingual encoders more robust; when fine-tuning the encoder using downstream task, we train the encoder to tolerate noise in the contextual embedding spaces such that even if the representations of different languages are not aligned well, the model can still achieve good performance on zero-shot cross-lingual transfer. In this work, we propose a learning strategy for training robust models by drawing connections between adversarial examples and the failure cases of zero-shot cross-lingual transfer. We adopt two widely used robust training methods, adversarial training and randomized smoothing, to train the desired robust model. The experimental results demonstrate that robust training improves zero-shot cross-lingual transfer on text classification tasks. The improvement is more significant in the generalized cross-lingual transfer setting, where the pair of input sentences belong to two different languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1684,1697,,,,,,,,,,,,,,,,WOS:000855966301059,0
C,"Jing, HJ; Li, ZC; Zhao, H; Jiang, S",,,Assoc Computat Linguist,"Jing, Hongjiang; Li, Zuchao; Zhao, Hai; Jiang, Shu",,,"Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three sub tasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However, most of the existing joint models only focus on the benefits of encoder sharing between subtasks but ignore the difference. Therefore, we propose a joint ABSA model, which not only enjoys the benefits of encoder sharing but also focuses on the difference to improve the effectiveness of the model. In detail, we introduce a dual-encoder design, in which a pair encoder especially focuses on candidate aspect-opinion pair classification, and the original encoder keeps attention on sequence labeling. Empirical results show that our proposed model shows robustness and significantly outperforms the previous state-of-the-art on four benchmark datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3910,+,,,,,,,,,,,,,,,,WOS:000855966304005,0
C,"Kushnareva, L; Cherniayskii, D; Mikhailov, V; Artemova, E; Barannikov, S; Bernstein, A; Piontkovskaya, I; Piontkovski, D; Burnaev, E",,,Assoc Computat Linguist,"Kushnareva, Laida; Cherniayskii, Daniil; Mikhailov, Vladislav; Artemova, Ekaterina; Barannikov, Serguei; Bernstein, Alexander; Piontkovskaya, Irina; Piontkovski, Dmitri; Burnaev, Evgeny",,,Artificial Text Detection via Examining the Topology of Attention Maps,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,635,649,,,,,,,,,,,,,,,,WOS:000855966300050,0
C,"Li, XY; Li, JW; Sun, XF; Fan, C; Zhang, TW; Wu, F; Meng, YX; Zhang, J",,,Assoc Computat Linguist,"Li, Xiaoya; Li, Jiwei; Sun, Xiaofei; Fan, Chun; Zhang, Tianwei; Wu, Fei; Meng, Yuxian; Zhang, Jun",,,kFolden: k-Fold Ensemble for Out-Of-Distribution Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework kFolden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with k training labels, kFolden induces k sub-models, each of which is trained on a subset with k - 1 categories with the left category masked unknown to the sub-model. Exposing an unknown label to the sub-model during training, the model is encouraged to learn to equally attribute the probability to the seen k - 1 labels for the unknown label, enabling this framework to simultaneously resolve in- and out-distribution examples in a natural way via OOD simulations. Taking text classification as an archetype, we develop benchmarks for OOD detection using existing text classification datasets. By conducting comprehensive comparisons and analyses on the developed benchmarks, we demonstrate the superiority of kFolden against current methods in terms of improving OOD detection performances while maintaining improved in-domain classification accuracy.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3102,3115,,,,,,,,,,,,,,,,WOS:000855966303020,0
C,"Liu, H; Shi, Z; Zhu, XD",,,Assoc Computat Linguist,"Liu, Hui; Shi, Zhan; Zhu, Xiaodan",,,Unsupervised Conversation Disentanglement through Co-Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which are expensive to obtain in practice. In this work, we explore to train a conversation disentanglement model without referencing any human annotations. Our method is built upon a deep co-training algorithm, which consists of two neural networks: a message-pair classifier and a session classifier. The former is responsible for retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both networks are initialized respectively with pseudo data built from an unannotated corpus. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to the previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2345,2356,,,,,,,,,,,,,,,,WOS:000855966302040,0
C,"Malkin, N; Lanka, S; Goel, P; Jojic, N",,,Assoc Computat Linguist,"Malkin, Nikolay; Lanka, Sameera; Goel, Pranav; Jojic, Nebojsa",,,Studying word order through iterative shuffling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying substantially different information. Our surprising result relies on inference by iterative shuffling (IBIS), a novel, efficient procedure that finds the ordering of a bag of words having the highest likelihood under a fixed language model. IBIS can use any black-box model without additional training and is superior to existing word ordering algorithms. Coalescing our findings, we discuss how shuffling inference procedures such as IBIS can benefit language modeling and constrained generation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10351,10366,,,,,,,,,,,,,,,,WOS:000860727004033,0
C,"Miletic, F; Przewozny-Desriaux, A; Tanguy, L",,,Assoc Computat Linguist,"Miletic, Filip; Przewozny-Desriaux, Anne; Tanguy, Ludovic",,,Detecting Contact-Induced Semantic Shifts: What Can Embedding-Based Methods Do in Practice?,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in Quebec English. We contrast synchronic data from different regions in order to identify the meanings that are specific to Quebec and potentially related to language contact. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contactinduced semantic shifts observed in synchrony, obtaining results comparable to the state of the art on similar tasks in diachrony. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10852,10865,,,,,,,,,,,,,,,,WOS:000860727004071,0
C,"Nangi, SR; Tyagi, A; Mundra, J; Mukherjee, S; Raj, S; Garimella, A; Chhaya, N",,,Assoc Computat Linguist,"Nangi, Sharmila Reddy; Tyagi, Athary; Mundra, Jay; Mukherjee, Sagnik; Raj, Snehal; Garimella, Aparna; Chhaya, Niyati",,,AUTOSUMM: Automatic Model Creation for Text Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep learning specifics like model architecture, tuning parameters etc., and is often extremely challenging for a non-expert. In this paper, we propose methods to automatically create deep learning models for the tasks of extractive and abstractive text summarization. Based on the recent advances in Automated Machine Learning and the success of large language models such as BERT and GPT-2 in encoding knowledge, we use a combination of Neural Architecture Search (NAS) and Knowledge Distillation (KD) techniques to perform model search and compression using the vast knowledge provided by these language models to develop smaller, customized models for any given dataset. We present extensive empirical results to illustrate the effectiveness of our model creation methods in terms of inference time and model size, while achieving near state-of-the-art performances in terms of accuracy across a range of datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10162,10172,,,,,,,,,,,,,,,,WOS:000860727004022,0
C,"Nedelchev, R; Lehmann, J; Usbeck, R",,,Assoc Computat Linguist,"Nedelchev, Rostislav; Lehmann, Jens; Usbeck, Ricardo",,,Proxy Indicators for the Quality of Open-domain Dialogues,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Thus, despite the abundance of work done in the field, human judges have to evaluate dialogues' quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a deep-learning model trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the method can infer various quality metrics and derive a component-based overall score. We achieve statistically significant correlation coefficients of up to 0.7.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7834,7855,,,,,,,,,,,,,,,,WOS:000860727001066,0
C,"Pimentel, T; Meister, C; Teufel, S; Cotterell, R",,,Assoc Computat Linguist,"Pimentel, Tiago; Meister, Clara; Teufel, Simone; Cotterell, Ryan",,,On Homophony and Renyi Entropy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Renyi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony-a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8284,8293,,,,,,,,,,,,,,,,WOS:000860727002031,0
C,"Pratapa, A; Anastasopoulos, A; Rijhwani, S; Chaudhary, A; Mortensen, DR; Neubig, G; Tsvetkov, Y",,,Assoc Computat Linguist,"Pratapa, Adithya; Anastasopoulos, Antonios; Rijhwani, Shruti; Chaudhary, Aditi; Mortensen, David R.; Neubig, Graham; Tsvetkov, Yulia",,,Evaluating the Morphosyntactic Well-formedness of Generated Texts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L'AMBRE - a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7131,7150,,,,,,,,,,,,,,,,WOS:000860727001018,0
C,"Raganato, A; Vazquez, R; Creutz, M; Tiedemann, J",,,Assoc Computat Linguist,"Raganato, Alessandro; Vazquez, Raul; Creutz, Mathias; Tiedemann, Jorg",,,An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs.",,,,,,"Tiedemann, Jorg/0000-0003-3065-7989",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8449,8456,,,,,,,,,,,,,,,,WOS:000860727002042,0
C,"Saha, S; Yadav, P; Bauer, L; Bansal, M",,,Assoc Computat Linguist,"Saha, Swarnadeep; Yadav, Prateek; Bauer, Lisa; Bansal, Mohit",,,EXPLAGRAPHS: An Explanation Graph Generation Task for Structured Commonsense Reasoning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model's ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be right for the right reasons. In this work, we present EXPLAGRAPHS, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90%) via multiple rounds of verification and refinement. A significant 79% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task.(1)",,,,,,"Bauer, Lisa/0000-0002-4725-1232",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7716,7740,,,,,,,,,,,,,,,,WOS:000860727001057,0
C,"Yang, EG; Liu, MT; Xiong, DY; Zhang, YJ; Meng, Y; Hu, CJ; Xu, JA; Chen, YF",,,Assoc Computat Linguist,"Yang, Erguang; Liu, Mingtong; Xiong, Deyi; Zhang, Yujie; Meng, Yao; Hu, Changjian; Xu, Jinan; Chen, Yufeng",,,Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that are not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with non-parallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the model using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its syntactic structure. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed model trained only on non-parallel data is capable of generating diverse paraphrases with specified structures. Additionally, we further validate the effectiveness of our method for generating syntactically adversarial examples on a sentiment analysis task. Source codes are available at https://github.com/lanse-sir/sup.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2594,2604,,,,,,,,,,,,,,,,WOS:000855966302062,0
C,"Yang, S; Urbani, J",,,Assoc Computat Linguist,"Yang, Song; Urbani, Jacopo",,,Tribrid: Stance Classification with Neural Inconsistency Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study the problem of performing automatic stance classification on social media with neural architectures such as BERT. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The model is jointly learned to make simultaneously multiple predictions, which can be used either to improve the classification of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the confidence scores to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6831,6843,,,,,,,,,,,,,,,,WOS:000860727000062,0
C,"Yang, ZY; Yang, YF; Cer, D; Law, J; Darve, E",,,Assoc Computat Linguist,"Yang, Ziyi; Yang, Yinfei; Cer, Daniel; Law, Jax; Darve, Eric",,,Universal Sentence Representation Learning with Conditional Masked Language Model,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval (Conneau and Kiela, 2018), even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10% improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6216,6228,,,,,,,,,,,,,,,,WOS:000860727000017,0
C,"Zhao, YZ; Ni, CJ; Leung, CC; Joty, S; Chng, ES; Ma, B",,,Assoc Computat Linguist,"Zhao, Yingzhu; Ni, Chongjia; Leung, Cheung-Chi; Joty, Shafiq; Chng, Eng Siong; Ma, Bin",,,A Unified Speaker Adaptation Approach for ASR,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9339,9349,,,,,,,,,,,,,,,,WOS:000860727003039,0
C,"Zhou, L; Small, K; Zhang, Y; Atluri, S",,,Assoc Computat Linguist,"Zhou, Li; Small, Kevin; Zhang, Yong; Atluri, Sandeep",,,Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5103,5135,,,,,,,,,,,,,,,,WOS:000855966305020,0
C,"Bara, CP; Ch-Wang, S; Chai, J",,,Assoc Computat Linguist,"Bara, Cristian-Paul; Ch-Wang, Sky; Chai, Joyce",,,MINDCRAFT: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1112,1125,,,,,,,,,,,,,,,,WOS:000855966301018,0
C,"Bhatia, V; Smith, A; Akavoor, V; Tofu, D; Ishwar, P; Paik, S; Halim, E; Guo, L; Sun, Y; Wijaya, DT; Jalal, M; Betke, M",,,Assoc Computat Linguist,"Bhatia, Vibhu; Smith, Alyssa; Akavoor, Vidya; Tofu, David; Ishwar, Prakash; Paik, Sejin; Halim, Edward; Guo, Lei; Sun, Yimeng; Wijaya, Derry Tanti; Jalal, Mona; Betke, Margrit",,,OpenFraming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called frames, and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user's corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and lever-ages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general.",,,,,"Jalal, Mona/K-6148-2019","Jalal, Mona/0000-0001-9904-9354",,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,242,250,,,,,,,,,,,,,,,,WOS:000855241500028,0
C,"Chen, YZ; Hasan, MR",,,Assoc Computat Linguist,"Chen, Yuanzhi; Hasan, Mohammad Rashedul",,,Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, fine-grained, and overlapping categories) nature, it is imperative for an effective model to capture both the local and global context of the target domain By conducting a systematic investigation, we show that: (i) the deep Transformer-based pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local context, thus exhibits poor generalization, and (ii) a combination of shallow network-based domain-specific models and convolutional neural networks can efficiently extract local as well as global context directly from the target data in a hierarchical fashion, enabling it to offer a more generalizable solution.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6000,6017,,,,,,,,,,,,,,,,WOS:000855966306015,0
C,"Du, YP; Fang, QX; Nguyen, D",,,Assoc Computat Linguist,"Du, Yupei; Fang, Qixiang; Dong Nguyen",,,Assessing the Reliability of Word Embedding Gender Bias Measures,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures' reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10012,10034,,,,,,,,,,,,,,,,WOS:000860727004009,0
C,"Goswami, K; Dutta, S; Assem, H; Fransen, T; McCrae, JP",,,Assoc Computat Linguist,"Goswami, Koustava; Dutta, Sourav; Assem, Haytham; Fransen, Theodorus; McCrae, John P.",,,Cross-lingual Sentence Embedding using Multi-Task Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn efficiently, which confines the scope of these models. In this paper, we propose a novel sentence embedding framework based on an unsupervised loss function for generating effective multilingual sentence embeddings, eliminating the need for parallel corpora. We capture semantic similarity and relatedness between sentences using a multi-task loss function for training a dual encoder model mapping different languages onto the same vector space. We demonstrate the efficacy of an unsupervised as well as a weakly supervised variant of our framework on STS, BUCC and Tatoeba benchmark tasks. The proposed unsupervised sentence embedding framework outperforms even supervised state-of-the-art methods for certain under-resourced languages on the Tatoeba dataset and on a monolingual benchmark. Further, we show enhanced zero-shot learning capabilities for more than 30 languages, with the model being trained on only 13 languages. Our model can be extended to a wide range of languages from any language family, as it overcomes the requirement of parallel corpora for training.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9099,9113,,,,,,,,,,,,,,,,WOS:000860727003018,0
C,"Held, W; Iter, D; Jurafsky, D",,,Assoc Computat Linguist,"Held, William; Iter, Dan; Jurafsky, Dan",,,Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full n(2) pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle inter-cluster coreference, common in many applications. As a result cross-document coreference algorithms are rarely applied to downstream tasks. We draw on an insight from discourse coherence theory: potential coreferences are constrained by the reader's discourse focus. We model the entities/events in a reader's focus as a neighborhood within a learned latent embedding space which minimizes the distance between mentions and the centroids of their gold coreference clusters. We then use these neighborhoods to sample only hard negatives to train a fine-grained classifier on mention pairs and their local discourse features. Our approach(1) achieves state-of-the-art results for both events and entities on the ECB+, Gun Violence, Football Coreference, and Cross-Domain CrossDocument Coreference corpora. Furthermore, training on multiple corpora improves average performance across all datasets by 17.2 F1 points, leading to a robust coreference resolution model for use in downstream tasks where link distribution is unknown.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1406,1417,,,,,,,,,,,,,,,,WOS:000855966301039,0
C,"Jansen, PA; Smith, K; Moreno, D; Ortiz, H",,,Assoc Computat Linguist,"Jansen, Peter A.; Smith, Kelly; Moreno, Dan; Ortiz, Huitzilin",,,"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these multi-hop explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and produce valid explanations that are different than gold explanations. To address this, we construct a large corpus of 126k domain-expert (science teacher) relevance ratings that augment a corpus of explanations to standardized science exam questions, discovering 80k additional relevant facts not rated as gold. We build three strong models based on different methodologies (generation, ranking, and schemas), and empirically show that while expert-augmented ratings provide better estimates of explanation quality, both original (gold) and expertaugmented automatic evaluations still substantially underestimate performance by up to 36% when compared with full manual expert judgements, with different models being disproportionately affected. This poses a significant methodological challenge to accurately evaluating explanations produced by compositional reasoning models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7529,7542,,,,,,,,,,,,,,,,WOS:000860727001044,0
C,"Kawamoto, S; Sawai, Y; Wakimoto, K; Zhang, PN",,,Assoc Computat Linguist,"Kawamoto, Shunyo; Sawai, Yu; Wakimoto, Kohei; Zhang, Peinan",,,FAST: Fast Annotation tool for SmarT devices,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Working with a wide range of annotators with the same attributes is crucial, as in real-world applications. Although such application cases often use crowd-sourcing mechanisms to gather a variety of annotators, most real-world users use mobile devices. In this paper, we propose FAST, an annotation tool for application tasks that focuses on the user experience of mobile devices, which has not yet been focused on thus far. We designed FAST as a web application for use on any device with a flexible interface that can be customized to fit various tasks. In our experiments, we conducted crowd-sourced annotation for a sentiment analysis task with several annotators and evaluated annotation metrics such as speed, quality, and ease of use from the tool's logs and user surveys. Based on the results of our experiments, we conclude that our system can annotate faster than existing methods while maintaining the annotation quality.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,372,381,,,,,,,,,,,,,,,,WOS:000855241500041,0
C,"Kementchedjhieva, Y; Sogaard, A",,,Assoc Computat Linguist,"Kementchedjhieva, Yova; Sogaard, Anders",,,Dynamic Forecasting of Conversation Derailment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend it in several ways. We apply a pretrained language encoder to the task, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results: in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7915,7919,,,,,,,,,,,,,,,,WOS:000860727002002,0
C,"Kim, J; Griggs, E; Kim, IS; Oh, A",,,Assoc Computat Linguist,"Kim, Jiseon; Griggs, Elden; Kim, In Song; Oh, Alice",,,Learning Bill Similarity with Annotated and Augmented Corpora of Bills,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Bill writing is a critical element of representative democracy. However, it is often overlooked that most legislative bills are derived, or even directly copied, from other bills. Despite the significance of bill-to-bill linkages for understanding the legislative process, existing approaches fail to address semantic similarities across bills, let alone reordering or paraphrasing which are prevalent in legal document writing. In this paper, we overcome these limitations by proposing a 5-class classification task that closely reflects the nature of the bill generation process. In doing so, we construct a human-labeled dataset of 4,721 bill-to-bill relationships at the subsection-level and release this annotated dataset to the research community. To augment the dataset, we generate synthetic data with varying degrees of similarity, mimicking the complex bill writing process. We use BERT variants and apply multi-stage training, sequentially fine-tuning our models with synthetic and human-labeled datasets. We find that the predictive performance significantly improves when training with both human-labeled and synthetic data. Finally, we apply our trained model to infer section- and bill-level similarities. Our analysis shows that the proposed methodology successfully captures the similarities across legal documents at various levels of aggregation. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10048,10064,,,,,,,,,,,,,,,,WOS:000860727004011,0
C,"Li, L; Lin, YK; Ren, SH; Li, P; Zhou, J; Sun, X",,,Assoc Computat Linguist,"Li, Lei; Lin, Yankai; Ren, Shuhuai; Li, Peng; Zhou, Jie; Sun, Xu",,,Dynamic Knowledge Distillation for Pre-trained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,379,389,,,,,,,,,,,,,,,,WOS:000855966300031,0
C,"Ma, NAZ; Politowicz, A; Mazumder, S; Chen, JH; Liu, B; Robertson, E; Grigsby, S",,,Assoc Computat Linguist,"Ma, Nianzu; Politowicz, Alexander; Mazumder, Sahisnu; Chen, Jiahua; Liu, Bing; Robertson, Eric; Grigsby, Scott",,,Semantic Novelty Detection in Natural Language Descriptions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says A man is walking a chicken in the park, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective model (called GAT-MA) to solve the problem and also contributes a new dataset. Experimental evaluation shows that GAT-MA outperforms 11 baselines by large margins.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,866,882,,,,,,,,,,,,,,,,WOS:000855966300066,0
C,"Manchanda, S; Karypis, G",,,Assoc Computat Linguist,"Manchanda, Saurav; Karypis, George",,,Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate content-aware bibliometrics to quantitatively measure the scholarly impact of a publication. Our approaches assess the impact of a cited publication by the extent to which the cited publication informs the citing publication. We introduce a new metric, called Content Informed Index (CII), that uses the content of the paper as a source of distant-supervision, to quantify how much the cited-node informs the citing-node. We evaluate the weights estimated by our approach on three manually annotated datasets, where the annotations quantify the extent of information in the citation. Particularly, we evaluate how well the ranking imposed by our approach associates with the ranking imposed by the manual annotations. CII achieves up to 103% improvement in performance as compared to the second-best performing approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6041,6053,,,,,,,,,,,,,,,,WOS:000860727000003,0
C,"Meister, C; Amini, A; Vieira, T; Cotterell, R",,,Assoc Computat Linguist,"Meister, Clara; Amini, Afra; Vieira, Tim; Cotterell, Ryan",,,Conditional Poisson Stochastic Beam Search,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)'s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,664,681,,,,,,,,,,,,,,,,WOS:000855966300052,0
C,"Nguyen, HTN; Nie, D; Badamdorj, T; Liu, YJ; Zhu, YY; Truong, AO; Cheng, L",,,Assoc Computat Linguist,"Nguyen, Hoang T. N.; Nie, Dong; Badamdorj, Taivanbat; Liu, Yujie; Zhu, Yingying; Truong, Jason; Cheng, Li",,,Automated Generation of Accurate & Fluent Medical X-ray Reports,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding; the embedding representation is then passed to our transformer-based generator, to produce the medical report; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning language fluency and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3552,3563,,,,,,,,,,,,,,,,WOS:000855966303060,0
C,"Raychaudhuri, S; Wani, S; Patel, S; Jain, U; Chang, AX",,,Assoc Computat Linguist,"Raychaudhuri, Sonia; Wani, Saim; Patel, Shivansh; Jain, Unnat; Chang, Angel X.",,,Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In the Vision-arid-Language Navigation (VLN) task an embodied agent navigates a 31) environment, following natural language instructions, A challenge in this task is how to handle 'off the path' scenarios where an agent veers from a reference path, Prior work supervises the agent with actions based on the shortest path from the agent's location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4018,4028,,,,,,,,,,,,,,,,WOS:000855966304015,0
C,"Ren, RY; Qu, YQ; Liu, J; Zhao, WX; She, QQ; Wu, H; Wang, HF; Wen, JR",,,Assoc Computat Linguist,"Ren, Ruiyang; Qu, Yingqi; Liu, Jing; Zhao, Wayne Xin; She, Qiaoqiao; Wu, Hua; Wang, Haifeng; Wen, Ji-Rong",,,RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage re-ranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2825,2835,,,,,,,,,,,,,,,,WOS:000855966302083,0
C,"Scholak, T; Schucher, N; Bandanau, D",,,Assoc Computat Linguist,"Scholak, Torsten; Schucher, Nathan; Bandanau, Dzmitry",,,PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD(1), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9895,9901,,,,,,,,,,,,,,,,WOS:000860727004003,0
C,"Shi, ES; Wang, YL; Du, L; Zhang, HY; Han, S; Zhang, DM; Sun, HB",,,Assoc Computat Linguist,"Shi, Ensheng; Wang, Yanlin; Du, Lun; Zhang, Hongyu; Han, Shi; Zhang, Dongmei; Sun, Hongbin",,,CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size/depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large AST into a set of subtrees and utilize a recursive neural network to encode the subtrees. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for code summarization. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.coc/DeepSoftwareAnalytics/CAST.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4053,4062,,,,,,,,,,,,,,,,WOS:000855966304019,0
C,"Situ, X; Maruf, S; Zukerman, I; Paris, C; Haffari, G",,,Assoc Computat Linguist,"Situ, Xuelin; Maruf, Sameen; Zukerman, Ingrid; Paris, Cecile; Haffari, Gholamreza",,,Lifelong Explainer for Lifelong Learners,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME (Ribeiro et al., 2016), which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher - an arbitrary explanation algorithm - on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10 2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at hfips://github.com/situsnow/LLE.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2933,2940,,,,,,,,,,,,,,,,WOS:000855966303005,0
C,"Stoehr, N; Hennigen, LT; Ahbab, S; West, R; Cotterell, R",,,Assoc Computat Linguist,"Stoehr, Niklas; Hennigen, Lucas Torroba; Ahbab, Samin; West, Robert; Cotterell, Ryan",,,Classifying Dyads for Militarized Conflict Analysis,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7775,7784,,,,,,,,,,,,,,,,WOS:000860727001061,0
C,"Vamvas, J; Sennrich, R",,,Assoc Computat Linguist,"Vamvas, Jannis; Sennrich, Rico",,,Contrastive Conditioning for Assessing Disambiguation in MT: A Case Study of Distilled Bias,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others. Identifying patterns of overgeneralization requires evaluation methods that are both reliable and scalable. We propose contrastive conditioning as a reference-free blackbox method for detecting disambiguation errors. Specifically, we score the quality of a translation by conditioning on variants of the source that provide contrastive disambiguation cues. After validating our method, we apply it in a case study to perform a targeted evaluation of sequence-level knowledge distillation. By probing word sense disambiguation and translation of gendered occupation names, we show that distillation-trained models tend to overgeneralize more than other models with a comparable BLEU score. Contrastive conditioning thus highlights a side effect of distillation that is not fully captured by standard evaluation metrics. Code and data to reproduce our findings are publicly available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10246,10265,,,,,,,,,,,,,,,,WOS:000860727004027,0
C,"Villegas, DS; Aletras, N",,,Assoc Computat Linguist,"Villegas, Danae Sanchez; Aletras, Nikolaos",,,Point-of-Interest Type Prediction using Text and Images,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI's type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across eight categories significantly outperforming the state-of-the-art method for POI type prediction based on textonly methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7785,7797,,,,,,,,,,,,,,,,WOS:000860727001062,0
C,"Wang, X; Hu, V; Song, XC; Garg, S; Xiao, JF; Han, JW",,,Assoc Computat Linguist,"Wang, Xuan; Hu, Vivian; Song, Xiangchen; Garg, Shweta; Xiao, Jinfeng; Han, Jiawei",,,CHEMNER: Fine-Grained Chemistry Named Entity Recognition with Ontology-Guided Distant Supervision,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation difficult even for crowds of domain experts. On the other hand, domain-specific ontologies and knowledge bases (KBs) can be easily accessed, constructed, or integrated, which makes distant supervision realistic for fine-grained chemistry NER. In distant supervision, training labels are generated by matching mentions in a document with the concepts in the knowledge bases (KBs). However, this kind of KB-matching suffers from two major challenges: incomplete annotation and noisy annotation. We propose CHEMNER, an ontology-guided, distantly-supervised method for fine-grained chemistry NER to tackle these challenges. It leverages the chemistry type ontology structure to generate distant labels with novel methods of flexible KB-matching and ontology-guided multi-type disambiguation. It significantly improves the distant label generation for the subsequent sequence labeling model training. We also provide an expert-labeled, chemistry NER dataset with 62 fine-grained chemistry types (e.g., chemical compounds and chemical reactions). Experimental results show that CHEMNER is highly effective, outperforming substantially the state-of-the-art NER methods (with .25 absolute F1 score improvement).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5227,5240,,,,,,,,,,,,,,,,WOS:000855966305028,0
C,"Wang, ZA; Zhang, XF; Du, HW",,,Assoc Computat Linguist,"Wang, Ziao; Zhang, Xiaofeng; Du, Hongwei",,,Building the Directed Semantic Graph for Coherent Long Text Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Generating long text conditionally depending on the short input text has recently attracted more and more research efforts. Most existing approaches focus more on introducing extra knowledge to supplement the short input text, but ignore the coherence issue of the generated texts. To address aforementioned research issue, this paper proposes a novel two-stage approach to generate coherent long text. Particularly, we first build a document-level path for each output text with each sentence embedding as its node, and a revised self-organising map (SOM) is proposed to cluster similar nodes of a family of document-level paths to construct the directed semantic graph. Then, three subgraph alignment methods are proposed to extract the maximum matching paths or subgraphs. These directed subgraphs are considered to well preserve extra but relevant content to the short input text, and then they are decoded by the employed pre-trained model to generate coherent long text. Extensive experiments have been performed on three real-world datasets, and the promising results demonstrate that the proposed approach is superior to the state-of-the-art approaches w.r.t. a number of evaluation criteria.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2563,2572,,,,,,,,,,,,,,,,WOS:000855966302059,0
C,"Wen, HY; Ji, H",,,Assoc Computat Linguist,"Wen, Haoyang; Ji, Heng",,,Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBankDense have event-time links. In this paper, we propose a joint model for event-event temporal relation classification and an auxiliary task, relative event time prediction, which predicts the event time as real numbers. We adopt a Stack-Propagation framework to incorporate predicted relative event time for temporal relation classification and keep the differentiability. Our experiments on MATRES dataset show that our model can significantly improve the RoBERTa-based baseline and achieve state-of-the-art performance. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10431,10437,,,,,,,,,,,,,,,,WOS:000860727004039,0
C,"Xu, HR; Zhang, HN; Zou, YY; Chen, HS; Ding, ZY; Lan, YY",,,Assoc Computat Linguist,"Xu, Haoran; Zhang, Hainan; Zou, Yanyan; Chen, Hongshen; Ding, Zhuoye; Lan, Yanyan",,,Adaptive Bridge between Training and Inference for Dialogue Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario. In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task. What's more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as I don't know and HaHa? In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2541,2550,,,,,,,,,,,,,,,,WOS:000855966302057,0
C,"Yang, Y; Panagopoulou, A; Lyu, Q; Zhang, L; Yatskar, M; Callison-Burch, C",,,Assoc Computat Linguist,"Yang, Yue; Panagopoulou, Artemis; Lyu, Qing; Zhang, Li; Yatskar, Mark; Callison-Burch, Chris",,,Visual Goal-Step Inference using wikiHow,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2167,2179,,,,,,,,,,,,,,,,WOS:000855966302024,0
C,"Yousef, T; Schlaf, A; Borst, J; Niekler, A; Heyer, G",,,Assoc Computat Linguist,"Yousef, Tariq; Schlaf, Antje; Borst, Janos; Niekler, Andreas; Heyer, Gerhard",,,Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in Twitter and News Articles,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom organisations to support their daily monitoring and to trigger rapid response actions. The Press Freedom Monitor enables the monitoring experts to get a swift overview of recently reported incidents and it has performed impressively in this regard. This paper presents our work on the tool, starting with the training phase, which comprises defining the topic-related keywords to be used for querying APIs for news and Twitter content and evaluating different machine learning models based on a training dataset specifically created for our use case. Then, we describe the components of the production pipeline, including data gathering, duplicates removal, country mapping, case mapping and the user interface. We also conducted a usability study to evaluate the effectiveness of the user interface, and describe improvement plans for future work.",,,,,,"Yousef, Tariq/0000-0001-6136-3970",,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,153,159,,,,,,,,,,,,,,,,WOS:000855241500018,0
C,"Arabshahi, F; Lee, J; Bosselut, A; Choi, Y; Mitchell, T",,,Assoc Computat Linguist,"Arabshahi, Forough; Lee, Jennifer; Bosselut, Antoine; Choi, Yejin; Mitchell, Tom",,,Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users' commands, a task trivial for humans due to their common sense. In this paper, we propose a zeroshot commonsense reasoning system for conversational agents in an attempt to achieve this. Our reasoner uncovers unstated presumptions from user commands satisfying a general template of if-(state), then-(action), because-(goal). Our reasoner uses a state-ofthe-art transformer-based generative commonsense knowledge base (KB) as its source of background knowledge for reasoning. We propose a novel and iterative knowledge query mechanism to extract multi-hop reasoning chains from the neural KB which uses symbolic logic rules to significantly reduce the search space. Similar to any KBs gathered to date, our commonsense KB is prone to missing knowledge. Therefore, we propose to conversationally elicit the missing knowledge from human users with our novel dynamic question generation strategy, which generates and presents contextualized queries to human users. We evaluate the model with a user study with human users that achieves a 35% higher success rate compared to SOTA.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7404,7418,,,,,,,,,,,,,,,,WOS:000860727001036,0
C,"Bartolo, M; Thrush, T; Jia, R; Riedel, S; Stenetorp, P; Kiela, D",,,Assoc Computat Linguist,"Bartolo, Max; Thrush, Tristan; Jia, Robin; Riedel, Sebastian; Stenetorp, Pontus; Kiela, Douwe",,,Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or relabels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F(1) and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples: crowdworkers can fool our model only 8.8% of the time on average, compared to 17.6% for a model trained without synthetic data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8830,8848,,,,,,,,,,,,,,,,WOS:000860727002074,0
C,"Bugert, M; Gurevych, I",,,Assoc Computat Linguist,"Bugert, Michael; Gurevych, Iryna",,,Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from hyperlinks in online news: When referring to a significant real-world event, writers often add a hyperlink to another article covering this event. We demonstrate that collecting hyperlinks which point to the same article(s) produces extensive and high-quality CDCR data and create a corpus of 2M documents and 2.7M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that models trained on small subsets of HyperCoref are highly competitive, with performance similar to models trained on gold-standard data. With our work, we free CDCR research from depending on costly humanannotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,471,491,,,,,,,,,,,,,,,,WOS:000855966300038,0
C,"Colombo, P; Chapuis, E; Labeau, M; Clavel, C",,,Assoc Computat Linguist,"Colombo, Pierre; Chapuis, Emile; Labeau, Matthieu; Clavel, Chloe",,,Improving Multimodal fusion via Mutual Dependency Maximisation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multimodal sentiment analysis is a trending area of research, and the multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as L-1 or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4:3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,231,245,,,,,,,,,,,,,,,,WOS:000855966300021,0
C,"Deng, X; Su, Y; Lees, A; Wu, Y; Yu, C; Sun, H",,,Assoc Computat Linguist,"Deng, Xiang; Su, Yu; Lees, Alyssa; Wu, You; Yu, Cong; Sun, Huan",,,ReasonBERT: Pre-trained to Reason with Distant Supervision,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present ReasonBERT, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid, contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. We conduct a comprehensive evaluation on a variety of extractive question answering datasets ranging from single-hop to multi-hop and from text-only to table-only to hybrid that require various reasoning capabilities and show that ReasonBERT achieves remarkable improvement over an array of strong baselines. Few-shot experiments further demonstrate that our pre-training method substantially improves sample efficiency.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6112,6127,,,,,,,,,,,,,,,,WOS:000860727000009,0
C,"Done, CH; Li, YL; Shen, Y; Qui, MH",,,Assoc Computat Linguist,"Done, Chenhe; Li, Yaliang; Shen, Ying; Qui, Minghui",,,HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https: //github.com/cheneydon/hrkd.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3126,3136,,,,,,,,,,,,,,,,WOS:000855966303022,0
C,"Fang, Z; Cao, YA; Li, T; Jia, RP; Fang, F; Shang, YM; Lu, YH",,,Assoc Computat Linguist,"Fang, Zheng; Cao, Yanan; Li, Tai; Jia, Ruipeng; Fang, Fang; Shang, Yanmin; Lu, Yuhai",,,TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective neural models. In this paper, we propose a novel dictionary extension method which extracts new entities through the type expanded model. Moreover, we design a multi-granularity boundaryaware network which detects entity boundaries from both local and global perspectives. We conduct experiments on different types of datasets, the results show that our model outperforms previous state-of-the-art distantly supervised systems and even surpasses the supervised models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,198,207,,,,,,,,,,,,,,,,WOS:000855966300018,0
C,"Feng, S; Patel, SS; Wan, H; Joshi, S",,,Assoc Computat Linguist,"Feng, Song; Patel, Siva Sankalp; Wan, Hui; Joshi, Sachindra",,,MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. In this work, we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. To facilitate such a task, we introduce a new dataset that contains dialogues grounded in multiple documents from four different domains. We also explore modeling the dialogue-based and document-based context in the dataset. We present strong baseline approaches and various experimental results, aiming to support further research efforts on such a task.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6162,6176,,,,,,,,,,,,,,,,WOS:000860727000013,0
C,"Frank, S; Bugliarello, E; Elliott, D",,,Assoc Computat Linguist,"Frank, Stella; Bugliarello, Emanuele; Elliott, Desmond",,,Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9847,9857,,,,,,,,,,,,,,,,WOS:000860727003076,0
C,"Gupta, N; Singh, S; Gardner, M; Roth, D",,,Assoc Computat Linguist,"Gupta, Nitish; Singh, Sameer; Gardner, Matt; Roth, Dan",,,Paired Examples as Indirect Supervision in Latent Decision Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the model is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the model failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP: (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5774,5785,,,,,,,,,,,,,,,,WOS:000855966305070,0
C,"Hirsch, E; Eirew, A; Shapira, O; Caciularu, A; Cattan, A; Ernst, O; Pasunuru, R; Ronen, H; Bansal, M; Dagan, I",,,Assoc Computat Linguist,"Hirsch, Eran; Eirew, Alon; Shapira, Ori; Caciularu, Avi; Cattan, Arie; Ernst, Ori; Pasunuru, Ramakanth; Ronen, Hadar; Bansal, Mohit; Dagan, Ido",,,iFACETSUM: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We introduce iFACETSUM,(1) a web application for exploring topical document sets. iFACETSUM integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user's selections. This approach offers both a comprehensive overview as well as concise details regarding subtopics of choice. Fine-grained facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies, which suggest the usefulness of our approach.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,283,297,,,,,,,,,,,,,,,,WOS:000855241500033,0
C,"Jayaram, S; Allaway, E",,,Assoc Computat Linguist,"Jayaram, Sahil; Allaway, Emily",,,Human Rationales as Attribution Priors for Explainable Stance Detection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier-particularly for inputs containing challenging phenomena such as sarcasm-at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model's predictions, thus serving as a computationally cheap and reliable source of attributions for our model.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5540,5554,,,,,,,,,,,,,,,,WOS:000855966305054,0
C,"Jhamtani, H; Gangal, V; Hovy, E; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Jhamtani, Harsh; Gangal, Varun; Hovy, Eduard; Berg-Kirkpatrick, Taylor",,,Investigating Robustness of Dialog Models to Popular Figurative Language Constructs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible.",,,,,,"Hovy, Eduard/0000-0002-3270-7903",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7476,7485,,,,,,,,,,,,,,,,WOS:000860727001040,0
C,"Jiang, F; Fan, YX; Chu, XM; Li, PF; Zhu, QM",,,Assoc Computat Linguist,"Jiang, Feng; Fan, Yaxin; Chu, Xiaomin; Li, Peifeng; Zhu, Qiaoming",,,Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2418,2431,,,,,,,,,,,,,,,,WOS:000855966302046,0
C,"Jiang, HM; Dai, B; Yang, MJ; Zhao, T; Wei, W",,,Assoc Computat Linguist,"Jiang, Haoming; Dai, Bo; Yang, Mengjiao; Zhao, Tuo; Wei, Wei",,,Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large scale experiments. Though researchers have attempted to use metrics for language generation tasks (e.g., perplexity, BLEU) or some model-based reinforcement learning methods (e.g., self-play evaluation) for automatic evaluation, these methods only show very weak correlation with the actual human evaluation in practice. To bridge such a gap, we propose a new framework named ENIGMA for estimating human evaluation scores based on recent advances of off-policy evaluation in reinforcement learning. ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data (see details in Section 2), which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors. Our experiments show that ENIGMA significantly outperforms existing methods in terms of correlation with human evaluation scores.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7419,7451,,,,,,,,,,,,,,,,WOS:000860727001037,0
C,"Jin, LF; Song, LF; Xu, K; Yu, D",,,Assoc Computat Linguist,"Jin, Lifeng; Song, Linfeng; Xu, Kun; Yu, Dong",,,Instance-adaptive training with noise-robust losses against noisy labels,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are not robust to label noise induced in the annotation generation process. Several noise-robust losses have been proposed and evaluated on tasks in computer vision, but they generally use a single dataset-wise hyperparamter to control the strength of noise resistance. This work proposes novel instance-adaptive training frameworks to change dataset-wise hyperparameters of noise resistance in such losses to be instance-specific. Such instance-specific noise resistance hyperparameters are predicted by special instance-level label quality predictors, which are trained along with the main models. Experiments on noisy and corrupted NLP datasets show that proposed instance-adaptive training frameworks help increase the noise-robustness provided by such losses, promoting the use of the frameworks and associated losses in training NLP models with noisy data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5647,5663,,,,,,,,,,,,,,,,WOS:000855966305061,0
C,"Karidi, T; Zhou, YC; Schneider, N; Abend, O; Srikumar, V",,,Assoc Computat Linguist,"Karidi, Taelin; Zhou, Yichu; Schneider, Nathan; Abend, Omri; Srikumar, Vivek",,,PuttingWords in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized pseudoword as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally sense voids-regions that do not correspond to any intelligible sense. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10300,10313,,,,,,,,,,,,,,,,WOS:000860727004030,0
C,"Kim, J; Hong, G; Kim, KM; Kang, J; Myaeng, SH",,,Assoc Computat Linguist,"Kim, Jeonghwan; Hong, Giwon; Kim, Kyung-min; Kang, Junmo; Myaeng, Sung-Hyon",,,Have You Seen That Number? Investigating Extrapolation in Question Answering Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7031,7037,,,,,,,,,,,,,,,,WOS:000860727001011,0
C,"Kulshreshtha, D; Belfer, R; Serban, IV; Reddy, S",,,Assoc Computat Linguist,"Kulshreshtha, Devang; Belfer, Robert; Serban, Iulian Vlad; Reddy, Siva",,,Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA) from source to target domain. While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between the target domain and synthetic data distribution, and reduces model overfitting to the source domain. We run UDA experiments on question generation and passage retrieval from the Natural Questions domain to machine learning and biomedical domains. We find that back-training vastly outperforms selftraining by a mean improvement of 7.8 BLEU4 points on generation, and 17.6% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation datasetMLQuestions containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7064,7078,,,,,,,,,,,,,,,,WOS:000860727001014,0
C,"Li, XM; Luo, XT; Dong, CH; Yang, DCA; Luan, BD; He, Z",,,Assoc Computat Linguist,"Li, Xianming; Luo, Xiaotian; Dong, Chenghao; Yang, Daichuan; Luan, Beidi; He, Zhen",,,TDEER: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Joint extraction of entities and relations from unstructured texts to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding relation. However, it is still challenging to handle this task efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands for Translating Decoding Schema for Joint Extraction of Entities and Relations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as subject + relation -> objects. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8055,8064,,,,,,,,,,,,,,,,WOS:000860727002013,0
C,"Lin, BY; Gao, WY; Yan, J; Moreno, R; Ren, X",,,Assoc Computat Linguist,"Lin, Bill Yuchen; Gao, Wenyang; Yan, Jun; Moreno, Ryan; Ren, Xiang",,,RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class inWikidata; at the context level, we use pre-trained language models (e.g., BERT) to generate word substitutions. Together, the two levels of attack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best model has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the robustness of NER models.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3728,3737,,,,,,,,,,,,,,,,WOS:000855966303074,0
C,"Liu, Y; Cheng, H; Klopfer, R; Schaaf, T; Gormley, MR",,,Assoc Computat Linguist,"Liu, Yang; Cheng, Hua; Klopfer, Russell; Schaaf, Thomas; Gormley, Matthew R.",,,Effective Convolutional Attention Network for Multi-label Clinical Document Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multilayer and sum-pooling attention to extract the most informative features from these multiscale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5941,5953,,,,,,,,,,,,,,,,WOS:000855966306011,0
C,"Luo, FL; Yang, PC; Li, SC; Ren, XC; Sun, X; Huang, SF; Huang, F",,,Assoc Computat Linguist,"Luo, Fuli; Yang, Pengcheng; Li, Shicheng; Ren, Xuancheng; Sun, Xu; Huang, Songfang; Huang, Fei",,,Rethinking Denoised Auto-Encoding in Language Pre-Training,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6% absolute gain on GLUE benchmarks and 0.8% absolute increment on NLVR2.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2922,2932,,,,,,,,,,,,,,,,WOS:000855966303004,0
C,"McKenna, N; Guillou, L; Hosseini, MJ; de Vroe, SB; Johnson, M; Steedman, M",,,Assoc Computat Linguist,"McKenna, Nick; Guillou, Liane; Hosseini, Mohammad Javad; de Vroe, Sander Bijl; Johnson, Mark; Steedman, Mark",,,Multivalent Entailment Graphs for Question Answering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) satisfies WIN(Biden); (2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10758,10768,,,,,,,,,,,,,,,,WOS:000860727004064,0
C,"Ren, FL; Zhang, LH; Yin, SJ; Zhao, XF; Liu, SL; Li, BC; Liu, YD",,,Assoc Computat Linguist,"Ren, Feiliang; Zhang, Longhui; Yin, Shujuan; Zhao, Xiaofeng; Liu, Shilei; Li, Bochao; Liu, Yaduo",,,A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated table features. Next, the mined global associations are integrated into the table feature of each relation. This generate-mine-integrate process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation's table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed model on three benchmark datasets. Experimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: https://github.com/neukg/GRTE.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2646,2656,,,,,,,,,,,,,,,,WOS:000855966302067,0
C,"Schioppa, A; Sokolov, A; Vilar, D; Filippova, K",,,Assoc Computat Linguist,"Schioppa, Andrea; Sokolov, Artem; Vilar, David; Filippova, Katja",,,Controlling Machine Translation for Multiple Attributes with Additive Interventions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users' trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a model trained without annotations to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better control over a wider range of tasks compared to tagging, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable control in an already trained model after a relatively cheap fine-tuning stage.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6676,6696,,,,,,,,,,,,,,,,WOS:000860727000050,0
C,"Sun, RL; Jin, HQ; Wan, XJ",,,Assoc Computat Linguist,"Sun, Renliang; Jin, Hanqi; Wan, Xiaojun",,,"Document-Level Text Simplification: Dataset, Criteria and Baseline",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the baseline models.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7997,8013,,,,,,,,,,,,,,,,WOS:000860727002008,0
C,"Tan, F; Hu, YF; Yen, K; Hu, CW",,,Assoc Computat Linguist,"Tan, Fei; Hu, Yifan; Yen, Kevin; Hu, Changwei",,,BERT-beta: A Proactive Probabilistic Approach to Text Moderation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and interpretation benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the linear model offers useful insights beyond this work.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8667,8675,,,,,,,,,,,,,,,,WOS:000860727002060,0
C,"Toney-Wails, A; Caliskan, A",,,Assoc Computat Linguist,"Toney-Wails, Autumn; Caliskan, Aylin",,,ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Word embeddings learn implicit biases from linguistic regularities captured by word cooccurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from social psychology. We apply ValNorm on static word embeddings from seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high accuracy in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a Pearson correlation of rho = 0:88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in English. In contrast, we measure gender stereotypes using the same set of word embeddings and find that social biases vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7203,7218,,,,,,,,,,,,,,,,WOS:000860727001022,0
C,"Tu, YB; Li, L; Yan, CG; Gao, SX; Yu, ZT",,,Assoc Computat Linguist,"Tu, Yunbin; Li, Liang; Yan, Chenggang; Gao, Shengxiang; Yu, Zhengtao",,,R-3 Net:Relation-embedded Representation Reconstruction Network for Change Captioning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (R3Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes. Specifically, a relation-embedded module is first devised to explore potential changed objects in the large amount of clutter. Then, based on the semantic similarities of corresponding locations in the two images, a representation reconstruction module (RRM) is designed to learn the reconstruction representation and further model the difference representation. Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the semantic interaction between change localization and caption generation. Extensive experiments show that the proposed method achieves the state-of-the-art results on two public datasets",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9319,9329,,,,,,,,,,,,,,,,WOS:000860727003037,0
C,"Utama, PA; Moosavi, NS; Sanh, V; Gurevych, I",,,Assoc Computat Linguist,"Utama, Prasetya Ajie; Moosavi, Nafise Sadat; Sanh, Victor; Gurevych, Iryna",,,Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9063,9074,,,,,,,,,,,,,,,,WOS:000860727003015,0
C,"Wang, M; Zhang, JZ; Wang, YL",,,Assoc Computat Linguist,"Wang, Ming; Zhang, Jianzhang; Wang, Yinglin",,,Enhancing the Context Representation in Similarity-based Word Sense Disambiguation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its global context. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for disambiguation. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system's performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8965,8973,,,,,,,,,,,,,,,,WOS:000860727003008,0
C,"Wang, YW; Hu, J; Levy, R; Qian, P",,,Assoc Computat Linguist,"Wang, Yiwen; Hu, Jennifer; Levy, Roger; Qian, Peng",,,Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models' ability to learn grammatical dependencies in typologically different languages. Here we investigate this question in Mandarin Chinese, which has a logographic, largely syllable-based writing system; different word order; and sparser morphology than English. We train LSTMs, Recurrent Neural Network Grammars, Transformer language models, and Transformer-parameterized generative parsing models on two Mandarin Chinese datasets of different sizes. We evaluate the models' ability to learn different aspects of Mandarin grammar that assess syntactic and semantic relationships. We find suggestive evidence that structural supervision helps with representing syntactic state across intervening content and improves performance in low-data settings, suggesting that the benefits of hierarchical inductive biases in acquiring dependency relationships may extend beyond English.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5604,5620,,,,,,,,,,,,,,,,WOS:000855966305058,0
C,"Wei, J; Garrette, D; Tal, LZ; Pavlick, E",,,Assoc Computat Linguist,"Wei, Jason; Garrette, Dan; Tal Linzen; Pavlick, Ellie",,,Frequency Effects on Syntactic Rule Learning in Transformers,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT's performance on English subject-verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject-verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT's behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,932,948,,,,,,,,,,,,,,,,WOS:000855966301005,0
C,"Wu, CH; Zheng, YH; Mao, XX; Huang, ML",,,Assoc Computat Linguist,"Wu, Chen Henry; Zheng, Yinhe; Mao, Xiaoxi; Huang, Minlie",,,Transferable Persona-Grounded Dialogues via Grounded Minimal Edits,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the grounded minimal editing framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which learns to edit by disentangling and recombining persona-related and persona-agnostic parts of the response. To evaluate persona-grounded minimal editing, we present the PERSONAMI-NEDIT dataset, and experimental results show that GME outperforms competitive baselines by a large margin. To evaluate the transferability, we experiment on the test set of BLEND-EDSKILLTALK and show that GME can edit dialogue models' responses to largely improve their persona consistency while preserving the use of knowledge and empathy.(1)",,,,,,"Zheng, Yinhe/0000-0002-7029-5671",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2368,2382,,,,,,,,,,,,,,,,WOS:000855966302042,0
C,"Yang, YL; Eriguchi, A; Muzio, A; Tadepalli, P; Lee, S; Hassan, H",,,Assoc Computat Linguist,"Yang, Yilin; Eriguchi, Akiko; Muzio, Alexandre; Tadepalli, Prasad; Lee, Stefan; Hassan, Hany",,,Improving Multilingual Translation by Representation and Gradient Regularization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations - commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7266,7279,,,,,,,,,,,,,,,,WOS:000860727001026,0
C,"Yasunaga, M; Leskovec, J; Liang, P",,,Assoc Computat Linguist,"Yasunaga, Michihiro; Leskovec, Jure; Liang, Percy",,,LM-Critic: Language Models for Unsupervised Grammatical Error Correction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if theLMassigns it a higher probability than its local perturbations. We apply this LMCritic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical/grammatical pairs for training a corrector. We evaluate our approach on GEC datasets across multiple domains (CoNLL-2014, BEA2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting (+0.5 F0.5).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7752,7763,,,,,,,,,,,,,,,,WOS:000860727001059,0
C,"Ye, QY; Lin, BYC; Ren, X",,,Assoc Computat Linguist,"Ye, Qinyuan; Lin, Bill Yuchen; Ren, Xiang",,,CROSSFIT : A Few-shot Learning Challenge for Cross-task Generalization in NLP,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CROSSFIT, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CROSSFIT and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from openaccess NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7163,7189,,,,,,,,,,,,,,,,WOS:000860727001020,0
C,"Yuan, CH; Eldardiry, H",,,Assoc Computat Linguist,"Yuan, Chenhan; Eldardiry, Hoda",,,Unsupervised Relation Extraction: A Variational Autoencoder Approach,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Unsupervised relation extraction works by clustering entity pairs that have the same relations in the text. Some existing variational autoencoder (VAE)-based approaches train the relation extraction model as an encoder that generates relation classifications. A decoder is trained along with the encoder to reconstruct the encoder input based on the encoder-generated relation classifications. These classifications are a latent variable so they are required to follow a pre-defined prior distribution which results in unstable training. We propose a VAE-based unsupervised relation extraction technique that overcomes this limitation by using the classifications as an intermediate variable instead of a latent variable. Specifically, classifications are conditioned on sentence input, while the latent variable is conditioned on both the classifications and the sentence input. This allows our model to connect the decoder with the encoder without putting restrictions on the classification distribution; which improves training stability. Our approach is evaluated on the NYT dataset and outperforms state-of-the-art methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1929,1938,,,,,,,,,,,,,,,,WOS:000855966302006,0
C,"Zhou, YX; Liao, LJ; Gao, Y; Jie, ZM; Lu, W",,,Assoc Computat Linguist,"Zhou, Yuxiang; Liao, Lejian; Gao, Yang; Jie, Zhanming; Lu, Wei",,,To be Closer: Learning to Link up Aspects with Opinions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (Huang and Carley, 2019). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between aspects and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such tree structure compared to the standard dependency parse tree. The learning process allows the tree structure to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19% on the standard SemEval Restaurant14 (Pontiki et al., 2014) dataset(1).",,,,,"Lu, Wei/AHA-5606-2022","Lu, Wei/0000-0003-0827-0382",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3899,3909,,,,,,,,,,,,,,,,WOS:000855966304004,0
C,"Barba, E; Procopio, L; Navigli, R",,,Assoc Computat Linguist,"Barba, Edoardo; Procopio, Luigi; Navigli, Roberto",,,ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining assumption that, given a context, each word can be disambiguated individually with no account of the other sense choices. To address this limitation and drop this assumption, we propose CONtinuous SEnse Comprehension (CONSEC), a novel approach to WSD: lever-aging a recent re-framing of this task as a text extraction problem, we adapt it to our formulation and introduce a feedback loop strategy that allows the disambiguation of a target word to be conditioned not only on its context but also on the explicit senses assigned to nearby words. We evaluate CONSEC and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how CONSEC fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/SapienzaNLP/consec.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1492,1503,,,,,,,,,,,,,,,,WOS:000855966301045,0
C,"Benton, A; Li, HY; Malioutov, I",,,Assoc Computat Linguist,"Benton, Adrian; Li, Hanyang; Malioutov, Igor",,,Cross-Register Projection for Headline Part of Speech Tagging,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Part of speech (POS) tagging is a familiar NLP task. State of the art taggers routinely achieve token-level accuracies of over 97% on news body text, evidence that the problem is well-understood. However, the register of English news headlines, headlinese, is very different from the register of long-form text, causing POS tagging models to underperform on headlines. In this work, we automatically annotate news headlines with POS tags by projecting predicted tags from corresponding sentences in news bodies. We train a multi-domain POS tagger on both long-form and headline text and show that joint training on both registers improves over training on just one or naively concatenating training sets. We evaluate on a newly-annotated corpus of over 5,248 English news headlines from the Google sentence compression corpus, and show that our model yields a 23% relative error reduction per token and 19% per headline. In addition, we demonstrate that better headline POS tags can improve the performance of a syntax-based open information extraction system. We make POSH, the POS-tagged Headline corpus, available to encourage research in improved NLP models for news headlines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6475,6490,,,,,,,,,,,,,,,,WOS:000860727000036,0
C,"Chen, JA; Yang, DY",,,Assoc Computat Linguist,"Chen, Jiaao; Yang, Diyi",,,Simple Conversational Data Augmentation for Semi-supervised Abstractive Conversation Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"A Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided insertion to interrupt the development of conversations, and conditional-generation-based substitution to substitute utterances with their paraphrases generated based on the conversation context. To further utilize unlabeled conversations, we combine CODA with two-stage noisy self-training where we first pre-train the summarization model on unlabeled conversations with pseudo summaries and then fine-tune it on labeled conversations. Experiments conducted on the recent conversation summarization datasets demonstrate the effectiveness of our methods over several state-of-the-art data augmentation baselines. We have publicly released our code at https://github.com/GT-SALT/CODA.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6605,6616,,,,,,,,,,,,,,,,WOS:000860727000045,0
C,"Cui, LY; Wu, Y; Liu, SJ; Zhang, Y",,,Assoc Computat Linguist,"Cui, Leyang; Wu, Yu; Liu, Shujie; Zhang, Yue",,,Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world scenario, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2328,2337,,,,,,,,,,,,,,,,WOS:000855966302038,0
C,"Ding, HR; Luo, X",,,Assoc Computat Linguist,"Ding, Haoran; Luo, Xiao",,,AttentionRank: Unsupervised keyphrase Extraction using Self and Cross Attentions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven baselines. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github(1).",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1919,1928,,,,,,,,,,,,,,,,WOS:000855966302005,0
C,"El-Kishky, A; Renduchintala, A; Cross, J; Guzman, F; Koehn, P",,,Assoc Computat Linguist,"El-Kishky, Ahmed; Renduchintala, Adithya; Cross, James; Guzman, Francisco; Koehn, Philipp",,,XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-PhoneticWord Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10424,10430,,,,,,,,,,,,,,,,WOS:000860727004038,0
C,"Gheini, M; Ren, X; May, J",,,Assoc Computat Linguist,"Gheini, Mozhdeh; Ren, Xiang; May, Jonathan",,,Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1754,1765,,,,,,,,,,,,,,,,WOS:000855966301065,0
C,"Ghosal, D; Majumder, N; Mihalcea, R; Poria, S",,,Assoc Computat Linguist,"Ghosal, Deepanway; Majumder, Navonil; Mihalcea, Rada; Poria, Soujanya",,,STaCK: Sentence Ordering with Temporal Commonsense Knowledge,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Sentence order prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence with respect to the chronological sequence of events described in the text. Document-level contextual understanding and commonsense knowledge centered around these events are often essential in uncovering this coherence and predicting the exact chronological order. In this paper, we introduce STaCK - a framework based on graph neural networks and temporal commonsense knowledge to model global information and predict the relative order of sentences. Our graph network accumulates temporal evidence using knowledge of 'past' and 'future' and formulates sentence ordering as a constrained edge classification problem. We report results on five different datasets, and empirically show that the proposed method is naturally suitable for order prediction, thus demonstrating the role of temporal commonsense knowledge.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8676,8686,,,,,,,,,,,,,,,,WOS:000860727002061,0
C,"Huang, JX; Lie, CY; Subudhi, K; Jose, D; Balakrishnan, S; Chen, WZ; Peng, BL; Gao, JF; Han, JW",,,Assoc Computat Linguist,"Huang, Jiaxin; Lie, Chunyuan; Subudhi, Krishan; Jose, Damien; Balakrishnan, Shobana; Chen, Weizhu; Peng, Baolin; Gao, Jianfeng; Han, Jiawei",,,Few-Shot Named Entity Recognition: An Empirical Baseline Study,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve model generalization ability in few-shot settings: (1) metalearning to construct prototypes for different entity types, (2) task-specific supervised pretraining on noisy web data to extract entityrelated representations and (3) self-training to leverage unlabeled in-domain data. On 10 public NER datasets, we perform extensive empirical comparisons over the proposed schemes and their combinations with various proportions of labeled data, our experiments show that (i) in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned using domain labels. (ii) We create new state-of-theart results on both few-shot and training-free settings compared with existing methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10408,10423,,,,,,,,,,,,,,,,WOS:000860727004037,0
C,"Jin, ZJ; von Kugelgen, J; Ni, JW; Vaidhya, T; Kaushal, A; Sachan, M; Scholkopf, B",,,Assoc Computat Linguist,"Jin, Zhijing; von Kuegelgen, Julius; Ni, Jingwei; Vaidhya, Tejas; Kaushal, Ayush; Sachan, Mrinmaya; Schoelkopf, Bernhard",,,Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP community. In this work, we argue that the causal direction of the data collection process bears non-trivial implications that can explain a number of published NLP findings, such as differences in semi-supervised learning (SSL) and domain adaptation (DA) performance across different settings. We categorize common NLP tasks according to their causal direction and empirically assay the validity of the ICM principle for text data using minimum description length. We conduct an extensive meta-analysis of over 100 published SSL and 30 DA studies, and find that the results are consistent with our expectations based on causal insights. This work presents the first attempt to analyze the ICM principle in NLP, and provides constructive suggestions for future modeling choices.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9499,9513,,,,,,,,,,,,,,,,WOS:000860727003049,0
C,"Kim, J; Maddela, M; Kriz, R; Xu, W; Callison-Burch, C",,,Assoc Computat Linguist,"Kim, Joongwon; Maddela, Mounica; Kriz, Reno; Xu, Wei; Callison-Burch, Chris",,,BISECT: Learning to Split and Rephrase Sentences with Bitexts,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this `split and rephrase' task. Our BISECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. BISECT contains higher quality training examples than previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus, and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BISECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6193,6209,,,,,,,,,,,,,,,,WOS:000860727000015,0
C,"Lal, YK; Cao, QQ; Trivedi, H; Singh, R; Balasubramanian, A; Balasubramanian, N",,,Assoc Computat Linguist,"Lal, Yash Kumar; Cao, Qingqing; Trivedi, Harsh; Singh, Reetu; Balasubramanian, Aruna; Balasubramanian, Niranjan",,,IrEne-viz: Visualizing Energy Consumption of Transformer Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"IrEne (Cao et al., 2021) is an energy prediction system that accurately predicts the interpretable inference energy consumption of a wide range of Transformer-based NLP models. We present the IrEne-viz tool, an online platform for visualizing and exploring energy consumption of various Transformer-based models easily. Additionally, we release a public API that can be used to access granular information about energy consumption of transformer models and their components. The live demo is available at http://istonybrooknip github.io/irene/demo/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,251,258,,,,,,,,,,,,,,,,WOS:000855241500029,0
C,"Lee, B; Choi, YS",,,Assoc Computat Linguist,"Lee, Bongseok; Choi, Yong Suk",,,Graph Based Network with Contextualized Representations of Turns in Dialogue,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the state-of-the-art models on most of the benchmark datasets. Our code is available at https://github. com/BlackNoodle/TUCORE-GCN.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,443,455,,,,,,,,,,,,,,,,WOS:000855966300036,0
C,"Lee, J; Wettig, A; Chen, DQ",,,Assoc Computat Linguist,"Lee, Jinhyuk; Wettig, Alexander; Chen, Danqi",,,"Phrase Retrieval Learns Passage Retrieval, Too",2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Dense retrieval methods have shown great promise over sparse retrieval methods in a range of NLP problems. Among them, dense phrase retrieval-the most fine-grained retrieval unit-is appealing because phrases can be directly used as the output for question answering and slot filling tasks.' In this work, we follow the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including passages and documents. We first observe that a dense phrase-retrieval system, without any retraining, already achieves better passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage retrievers, which also helps achieve superior end-to-end QA performance with fewer passages. Then, we provide an interpretation for why phrase-level supervision helps learn better fine-grained entailment compared to passage-level supervision, and also show that phrase retrieval can be improved to achieve competitive performance in document-retrieval tasks such as entity linking and knowledge-grounded dialogue. Finally, we demonstrate how phrase filtering and vector quantization can reduce the size of our index by 4-10x, making dense phrase retrieval a practical and versatile solution in multi-granularity retrieval.(2)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3661,3672,,,,,,,,,,,,,,,,WOS:000855966303069,0
C,"Lhoest, Q; del Moral, AV; Jernite, Y; Thakur, A; von Platen, P; Patil, S; Chaumond, J; Drame, M; Plu, J; Tunstall, L; Davison, J; Sasko, M; Chhablani, G; Malik, B; Brandeis, S; Le Scao, T; Sanh, V; Xu, CW; Patry, N; McMillan-Major, A; Schmid, P; Gugger, S; Delangue, C; Matussiere, T; Debut, L; Bekman, S; Cistac, P; Goehringer, T; Mustar, V; Lagunas, F; Rush, AM; Wolf, T",,,Assoc Computat Linguist,"Lhoest, Quentin; del Moral, Albert Villanova; Jernite, Yacine; Thakur, Abhishek; von Platen, Patrick; Patil, Suraj; Chaumond, Julien; Drame, Mariama; Plu, Julien; Tunstall, Lewis; Davison, Joe; Sasko, Mario; Chhablani, Gunjan; Malik, Bhavitvya; Brandeis, Simon; Le Scao, Teven; Sanh, Victor; Xu, Canwen; Patry, Nicolas; McMillan-Major, Angelina; Schmid, Philipp; Gugger, Sylvain; Delangue, Clement; Matussiere, Theo; Debut, Lysandre; Bekman, Stas; Cistac, Picnic; Goehringer, Thibault; Mustar, Victor; Lagunas, Francois; Rush, Alexander M.; Wolf, Thomas",,,Datasets: A Community Library for Natural Language Processing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021): PROCEEDINGS OF SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https : //github . com/huggingface/datasets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-11-7,,,,2021,,,,,,,175,184,,,,,,,,,,,,,,,,WOS:000855241500021,0
C,"Li, DY; Zhu, XD; Li, Y; Wang, SG; Li, DY; Liao, J; Zheng, JX",,,Assoc Computat Linguist,"Li, Dayu; Zhu, Xiaodan; Li, Yang; Wang, Suge; Li, Deyu; Liao, Jian; Zheng, Jianxing",,,Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Emotion inference in multi-turn conversations aims to predict the participant's emotion in the next upcoming turn without knowing the participant's response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the future feelings of participants, due to the lack of utterance information from the future. Moreover, it is crucial for emotion inference to capture the characteristics of emotional propagation in conversations, such as persistence and contagiousness. In this study, we focus on investigating the task of emotion inference in multi-turn conversations by modeling the propagation of emotional states among participants in the conversation history, and propose an addressee-aware module to automatically learn whether the participant keeps the historical emotional state or is affected by others in the next upcoming turn. In addition, we propose an ensemble strategy to further enhance the model performance. Empirical studies on three different benchmark conversation datasets demonstrate the effectiveness of the proposed model over several strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3935,3941,,,,,,,,,,,,,,,,WOS:000855966304007,0
C,"Li, R; Zhao, WL; Yang, C; Su, S",,,Assoc Computat Linguist,"Li, Rui; Zhao, Wenlin; Yang, Cheng; Su, Sen",,,Treasures Outside Contexts: Improving Event Detection via Global Statistics,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Event detection (ED) aims at identifying event instances of specified types in given texts, which has been formalized as a sequence labeling task. As far as we know, existing neural-based ED models make decisions relying on the contextual semantic features of each word in the input text, which we find is easy to get confused by varied contexts in the test stage. To this end, we come up with the idea of introducing a set of statistical features from word-event co-occurrence frequencies in the entire training set to cooperate with the contextual features. Specifically, we propose a Semantic and Statistic-Joint Discriminative Network (S-2-JDN) consisting of a semantic feature extractor, a statistical feature extractor, and a joint event discriminator. In experiments, S-2-JDN effectively exceeds ten recent state-of-the-art (SOTA) baseline methods on ACE2005 and KBP2015 benchmark datasets. Further, we perform extensive experiments to investigate the effectiveness of S-2-JDN.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2625,2635,,,,,,,,,,,,,,,,WOS:000855966302065,0
C,"Lin, XY; Liu, TY; Jia, WJ; Gong, ZG",,,Assoc Computat Linguist,"Lin, Xiangyu; Liu, Tianyi; Jia, Weijia; Gong, Zhiguo",,,Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the existence of noisy sentences in the sentence bags. In this paper, we propose a novel Multi-Layer Revision Network (MLRN) which alleviates the effects of wordlevel noise by emphasizing inner-sentence correlations before extracting relevant information within sentences. Then, we devise a balanced and noise-resistant Confidence-based Multi-Instance Learning (CMIL) method to filter out noisy sentences as well as assign proper weights to relevant ones. Extensive experiments on two New York Times (NYT) datasets demonstrate that our approach achieves significant improvements over the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,165,174,,,,,,,,,,,,,,,,WOS:000855966300015,0
C,"Liu, FY; Bugliarello, E; Ponti, EM; Reddy, S; Collier, N; Elliott, D",,,Assoc Computat Linguist,"Liu, Fangyu; Bugliarello, Emanuele; Ponti, Edoardo Maria; Reddy, Siva; Collier, Nigel; Elliott, Desmond",,,Visually Grounded Reasoning across Languages and Cultures,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross -lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10467,10485,,,,,,,,,,,,,,,,WOS:000860727004042,0
C,"Luo, G; Darrell, T; Rohrbach, A",,,Assoc Computat Linguist,"Luo, Grace; Darrell, Trevor; Rohrbach, Anna",,,NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting image-text inconsistency generate samples via text manipulation, we propose a dataset where both image and text are unmanipulated but mismatched. We introduce several strategies for automatically retrieving convincing images for a given caption, capturing cases with inconsistent entities or semantic context. Our large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates that machine-driven image repurposing is now a realistic threat, and (2) provides samples that represent challenging instances of mismatch between text and image in news that are able to mislead humans. We benchmark several state-of-the-art multimodal models on our dataset and analyze their performance across different pretraining domains and visual backbones.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6801,6817,,,,,,,,,,,,,,,,WOS:000860727000060,0
C,"Ma, XG; Li, MH; Sun, K; Xin, J; Lin, J",,,Assoc Computat Linguist,"Ma, Xueguang; Li, Minghan; Sun, Kai; Xin, Ji; Lin, Jimmy",,,Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of principal component analysis (PCA), product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracy-space trade-offs, for example, 48x compression with less than 3% drop in top-100 retrieval accuracy on average or 96x compression with less than 4% drop. Code and data are available at http://pyserini.io/.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2854,2859,,,,,,,,,,,,,,,,WOS:000855966302086,0
C,"Mithun, MP; Suntwal, S; Surdeanu, M",,,Assoc Computat Linguist,"Mithun, Mitch Paul; Suntwal, Sandeep; Surdeanu, Mihai",,,Students Who Study Together Learn Better: On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"While neural networks produce state-of-the-art performance in several NLP tasks, they depend heavily on lexicalized information, which transfers poorly between domains. Previous work (Suntwal et al., 2019) proposed delexicalization as a form of knowledge distillation to reduce dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization should be applied? A little helps reduce over-fitting, but too much discards useful information. We propose Group Learning (GL), a knowledge and model distillation approach for fact verification. In our method, while multiple student models have access to different delexicalized data views, they are encouraged to independently learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset and outperforms state-of-theart classifiers that rely on the original data.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6968,6973,,,,,,,,,,,,,,,,WOS:000860727001006,0
C,"Moradi, M; Samwald, M",,,Assoc Computat Linguist,"Moradi, Milad; Samwald, Matthias",,,Evaluating the Robustness of Neural Language Models to Input Perturbations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems' robustness.",,,,,,"Samwald, Matthias/0000-0002-4855-2571",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1558,1570,,,,,,,,,,,,,,,,WOS:000855966301050,0
C,"Provatorova, V; Bhargav, S; Vakulenko, S; Kanoulas, E",,,Assoc Computat Linguist,"Provatorova, Vera; Bhargav, Samarth; Vakulenko, Svitlana; Kanoulas, Evangelos",,,Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was previously shown that performance of EL systems on such datasets is overestimated, since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of several popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between common and uncommon ambiguous entities that require disambiguation, for all of the EL systems under evaluation, demonstrating the effects of prior probability bias and entity overshadowing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10501,10510,,,,,,,,,,,,,,,,WOS:000860727004044,0
C,"Qu, AL; Niu, JW; Mo, SS",,,Assoc Computat Linguist,"Qu, Anlin; Niu, Jianwei; Mo, Shasha",,,Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the Gaussian function to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different RPEs.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2989,2997,,,,,,,,,,,,,,,,WOS:000855966303009,0
C,"Ribeiro, LFR; Pfeiffer, J; Zhang, Y; Gurevych, I",,,Assoc Computat Linguist,"Ribeiro, Leonardo F. R.; Pfeiffer, Jonas; Zhang, Yue; Gurevych, Iryna",,,Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent work on multilingual AMR-to-text generation has exclusively focused on data augmentation strategies that utilize silver AMR. However, this assumes a high quality of generated AMRs, potentially limiting the transferability to the target task. In this paper, we investigate different techniques for automatically generating AMR annotations, where we aim to study which source of information yields better multilingual results. Our models trained on gold AMR with silver (machine translated) sentences outperform approaches which leverage generated silver AMR. We find that combining both complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,742,750,,,,,,,,,,,,,,,,WOS:000855966300057,0
C,"Soto, RAR; Miano, O; Ordonez, J; Chen, B; Khan, A; Bishop, M; Andrews, N",,,Assoc Computat Linguist,"Rivera Soto, Rafael A.; Miano, Olivia; Ordonez, Juanita; Chen, Barry; Khan, Aleem; Bishop, Marcus; Andrews, Nicholas",,,Learning Universal Authorship Representations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains: Amazon reviews, fanfiction short stories, and Reddit comments. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence generalization and propose simple but effective methods to improve transfer.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,913,919,,,,,,,,,,,,,,,,WOS:000855966301003,0
C,"Schlechtweg, D; Tahmasebi, N; Hengchen, S; Dubossarsky, H; McGillivray, B",,,Assoc Computat Linguist,"Schlechtweg, Dominik; Tahmasebi, Nina; Hengchen, Simon; Dubossarsky, Haim; McGillivray, Barbara",,,DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Word meaning is notoriously difficult to capture, both synchronically and diachronically. In this paper, we describe the creation of the largest resource of graded contextualized, diachronic word meaning annotation in four different languages, based on 100,000 human semantic proximity judgments. We describe in detail the multi-round incremental annotation process, the choice for a clustering algorithm to group usages into senses, and possible - diachronic and synchronic - uses for this dataset.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,7079,7091,,,,,,,,,,,,,,,,WOS:000860727001015,0
C,"Ushio, A; Camacho-Collados, J; Schockaert, S",,,Assoc Computat Linguist,"Ushio, Asahi; Camacho-Collados, Jose; Schockaert, Steven",,,Distilling Relation Embeddings from Pre-trained Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9044,9062,,,,,,,,,,,,,,,,WOS:000860727003014,0
C,"Weiss, DB; Roit, P; Klein, A; Ernst, O; Dagan, I",,,Assoc Computat Linguist,"Weiss, Daniela Brook; Roit, Paul; Klein, Ayal; Ernst, Ori; Dagan, Ido",,,QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA-alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9879,9894,,,,,,,,,,,,,,,,WOS:000860727004002,0
C,"Xu, LL; Teng, SJ; Zhao, RY; Guo, JL; Xiao, C; Jiang, DQ; Ren, B",,,Assoc Computat Linguist,"Xu, Linli; Teng, Sijie; Zhao, Ruoyu; Guo, Junliang; Xiao, Chi; Jiang, Deqiang; Ren, Bo",,,Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain representations for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a hybrid algorithm to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed framework with significant improvements over the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2459,2468,,,,,,,,,,,,,,,,WOS:000855966302049,0
C,"Ye, JC; Cai, RJ; Gui, T; Zhang, Q",,,Assoc Computat Linguist,"Ye, Jiacheng; Cai, Ruijian; Gui, Tao; Zhang, Qi",,,Heterogeneous Graph Neural Networks for Keyphrase Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The encoder-decoder framework achieves state-of-the-art results in keyphrase generation (KG) tasks by predicting both present keyphrases that appear in the source document and absent keyphrases that do not. However, relying solely on the source document can result in generating uncontrollable and inaccurate absent keyphrases. To address these problems, we propose a novel graph-based method that can capture explicit knowledge from related references. Our model first retrieves some document-keyphrases pairs similar to the source document from a pre-defined index as references. Then a heterogeneous graph is constructed to capture relationships of different granularities between the source document and its references. To guide the decoding process, a hierarchical attention and copy mechanism is introduced, which directly copies appropriate words from both the source document and its references based on their relevance and significance. The experimental results on multiple KG benchmarks show that the proposed model achieves significant improvements against other baseline models, especially with regard to the absent keyphrase prediction.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2705,2715,,,,,,,,,,,,,,,,WOS:000855966302072,0
C,"Yu, L; Xu, Y",,,Assoc Computat Linguist,"Yu, Lei; Xu, Yang",,,Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from percept, concept, and language to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including metaphor and metonymy.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,920,931,,,,,,,,,,,,,,,,WOS:000855966301004,0
C,"Zhang, SY; Bansal, M",,,Assoc Computat Linguist,"Zhang, Shiyue; Bansal, Mohit",,,Finding a Balanced Degree of Automation for Summary Evaluation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite(2) Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs' presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite(3) Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose in-between metrics, Lite(2:x) Pyramid, where we use a simple regressor to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between automation and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly-collected PyrXSum (with 100/10 XSum examples/systems). It shows that Lite(2) Pyramid consistently has the best summary-level correlations; Lite(3) Pyramid works better than or comparable to other automatic metrics; Lite(2:x) Pyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6617,6632,,,,,,,,,,,,,,,,WOS:000860727000046,0
C,"Zhang, ZR; Zhang, ZJ; Zhou, Y; Wu, LF; Wu, SX; Han, XY; Dou, DJ; Che, TS; Yan, D",,,Assoc Computat Linguist,"Zhang, Zeru; Zhang, Zijie; Zhou, Yang; Wu, Lingfei; Wu, Sixing; Han, Xiaoying; Dou, Dejing; Che, Tianshi; Yan, Da",,,Adversarial Attack against Cross-lingual Knowledge Graph Alignment,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5320,5337,,,,,,,,,,,,,,,,WOS:000855966305036,0
C,"Abbasi, A; Dobolyi, D; Lalor, JP; Netemeyer, R; Smith, K; Yang, Y",,,Assoc Computat Linguist,"Abbasi, Ahmed; Dobolyi, David; Lalor, John P.; Netemeyer, Richard; Smith, Kendall; Yang, Yi",,,Constructing a Psychometric Testbed for Fair Natural Language Processing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this work we construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed, which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education, allowing for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict/categorize users' survey response labels and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3748,3758,,,,,,,,,,,,,,,,WOS:000855966303076,0
C,"Arora, U; Huang, W; He, H",,,Assoc Computat Linguist,"Arora, Udit; Huang, William; He, He",,,Types of Out-of-Distribution Texts and How to Detect Them,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of OOD examples and how to best detect them. We categorize these examples by whether they exhibit a background shift or a semantic shift, and find that the two major approaches to OOD detection, model calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings, while performing worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, highlighting a weak spot for current methods. Since no single method works well across all settings, our results call for an explicit definition of OOD examples when evaluating different detection methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10687,10701,,,,,,,,,,,,,,,,WOS:000860727004059,0
C,"Bau, A; Andreas, J",,,Assoc Computat Linguist,"Bau, Anthony; Andreas, Jacob",,,How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a local context model in which generalization is consistent with the last word observed, and a global context model in which generalization is consistent with the global structure of the input. In experiments in English, Finnish, Mandarin, and random regular languages, we demonstrate that neural language models interpolate between these two forms of generalization: their predictions are well-approximated by a log-linear combination of local and global predictive distributions. We then show that, in some languages, noise mediates the two forms of generalization: noise applied to input tokens encourages global generalization, while noise in history representations encourages local generalization. Finally, we offer a preliminary theoretical explanation of these results by proving that the observed interpolation behavior is expected in log-linear models with a particular feature correlation structure. These results help explain the effectiveness of two popular regularization schemes and show that aspects of sequence model generalization can be understood and controlled.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5513,5526,,,,,,,,,,,,,,,,WOS:000855966305052,0
C,"CH-Wang, S; Jurgens, D",,,Assoc Computat Linguist,"CH-Wang, Sky; Jurgens, David",,,Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable-alternate words used to express the same concept-in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word partner and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demo-graphics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9918,9938,,,,,,,,,,,,,,,,WOS:000860727004006,0
C,"Chrysostomou, G; Aletras, N",,,Assoc Computat Linguist,"Chrysostomou, George; Aletras, Nikolaos",,,Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SALOSS; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SALOSS consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SALOSS models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8189,8200,,,,,,,,,,,,,,,,WOS:000860727002023,0
C,"Dodge, J; Sap, M; Marasovic, A; Agnew, W; Ilharco, G; Groeneveld, D; Mitchell, M; Gardner, M",,,Assoc Computat Linguist,"Dodge, Jesse; Sap, Maarten; Marasovic, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt",,,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1286,1305,,,,,,,,,,,,,,,,WOS:000855966301031,0
C,"EISheikh, A; Bevilacqua, M; Navigli, R",,,Assoc Computat Linguist,"EISheikh, Ahmed; Bevilacqua, Michele; Navigli, Roberto",,,Integrating Personalized PageRank into Neural Word Sense Disambiguation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Neural Word Sense Disambiguation (WSD) has recently been shown to benefit from the incorporation of pre-existing knowledge, such as that coming from the WordNet graph. However, state-of-the-art approaches have been successful in exploiting only the local structure of the graph, with only close neighbors of a given synset influencing the prediction. In this work, we improve a classification model by recomputing logits as a function of both the vanilla independently produced logits and the global WordNet graph. We achieve this by incorporating an online neural approximated PageRank, which enables us to refine edge weights as well. This method exploits the global graph structure while keeping space requirements linear in the number of edges. We obtain strong improvements, matching the current state of the art. Code is available at https://github.com/SapienzaNLP/neural-pagerank-wsd.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9092,9098,,,,,,,,,,,,,,,,WOS:000860727003017,0
C,"Emelin, D; Sennrich, R",,,Assoc Computat Linguist,"Emelin, Denis; Sennrich, Rico",,,Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesireable biases and unable to detect disambiguating information. We quantify biases using established statistical methods and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.(1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8517,8532,,,,,,,,,,,,,,,,WOS:000860727002048,0
C,"Fernandez-Gonzalez, D; Gomez-Rodriguez, C",,,Assoc Computat Linguist,"Fernandez-Gonzalez, Daniel; Gomez-Rodriguez, Carlos",,,Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a bijective function to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.",,,,,,"Fernandez-Gonzalez, Daniel/0000-0002-6733-2371",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10570,10578,,,,,,,,,,,,,,,,WOS:000860727004049,0
C,"He, HF; Zhang, MY; Ning, Q; Roth, D",,,Assoc Computat Linguist,"He, Hangfeng; Zhang, Mingyuan; Ning, Qiang; Roth, Dan",,,Foreseeing the Benefits of Incidental Supervision,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Real-world applications often require improved models by leveraging a range of cheap incidental supervision signals. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations - all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI's effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI's predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,1782,1800,,,,,,,,,,,,,,,,WOS:000855966301067,0
C,"Jhamtani, H; Berg-Kirkpatrick, T",,,Assoc Computat Linguist,"Jhamtani, Harsh; Berg-Kirkpatrick, Taylor",,,Truth-Conditional Captioning of Time Series Data,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical contemporary neural models with attention mechanisms can generate fluent output descriptions for this task, they often generate factually incorrect descriptions. We propose a computational model with a truth-conditional architecture which first runs small learned programs on the input time series, then identifies the programs/patterns which hold true for the given input, and finally conditions on only the chosen valid program (rather than the input time series) to generate the output text description. A program in our model is constructed from modules, which are small neural networks that are designed to capture numerical patterns and temporal information. The modules are shared across multiple programs, enabling compositionality as well as efficient learning of module parameters. The modules, as well as the composition of the modules, are unobserved in data, and we learn them in an end-to-end fashion with the only training signal coming from the accompanying natural language text descriptions. We find that the proposed model is able to generate high-precision captions even though we consider a small and simple space of module types.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,719,733,,,,,,,,,,,,,,,,WOS:000855966300055,0
C,"Liu, ZQ; Wang, SK; Gu, YY; Zhang, RY; Zhang, M; Wang, S",,,Assoc Computat Linguist,"Liu, Zequn; Wang, Shukai; Gu, Yiyang; Zhang, Ruiyi; Zhang, Ming; Wang, Sheng",,,Graphine: A Dataset for Graph-aware Terminology Definition Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware text generation models. We then proposed a novel graph-aware definition generation model Graphex that integrates transformer with graph neural network. Our model outperforms existing text generation models by exploiting the graph structure of terminologies. We further demonstrated how Graphine can be used to evaluate pretrained language models, compare graph representation learning methods and predict sentence granularity. We envision Graphine to be a unique resource for definition generation and many other NLP tasks in biomedicine.(1)",,,,,,"Zhang, Ming/0000-0002-9809-3430",,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3453,3463,,,,,,,,,,,,,,,,WOS:000855966303050,0
C,"Lyu, CC; Cohen, SB; Titov, I",,,Assoc Computat Linguist,"Lyu, Chunchuan; Cohen, Shay B.; Titov, Ivan",,,A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a 'greedy' segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9075,9091,,,,,,,,,,,,,,,,WOS:000860727003016,0
C,"Nan, GS; Zeng, JQ; Qiao, R; Guo, ZJ; Lu, W",,,Assoc Computat Linguist,"Nan, Guoshun; Zeng, Jiaqi; Qiao, Rui; Guo, Zhijiang; Lu, Wei",,,Uncovering Main Causalities for Long-tailed Information Extraction,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset, may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9683,9695,,,,,,,,,,,,,,,,WOS:000860727003064,0
C,"Perez-Beltrachini, L; Lapata, M",,,Assoc Computat Linguist,"Perez-Beltrachini, Laura; Lapata, Mirella",,,Models and Datasets for Cross-Lingual Summarisation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles' bodies from language aligned Wikipedia titles. We analyse the proposed cross-lingual summarisation task with automatic metrics and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pre-trained models in supervised, zero- and few-shot, and out-of-domain scenarios.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9408,9423,,,,,,,,,,,,,,,,WOS:000860727003043,0
C,"Poth, C; Pfeiffer, J; Ruckle, A; Gurevych, I",,,Assoc Computat Linguist,"Poth, Clifton; Pfeiffer, Jonas; Rueckle, Andreas; Gurevych, Iryna",,,What to Pre-Train on? Efficient Intermediate Task Selection,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning. We focus on parameter and computationally efficient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results demonstrate that efficient embedding based methods, which rely solely on the respective datasets, outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of 1% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training. (1)",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10585,10605,,,,,,,,,,,,,,,,WOS:000860727004051,0
C,"Rathi, N; Hahn, M; Futrell, R",,,Assoc Computat Linguist,"Rathi, Neil; Hahn, Michael; Futrell, Richard",,,An Information-Theoretic Characterization of Morphological Fusion,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Linguistic typology generally divides synthetic languages into groups based on their morphological fusion (von Humboldt, 1825). However, this measure has long been thought to be best considered a matter of degree (e.g. Greenberg, 1960). We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,10115,10120,,,,,,,,,,,,,,,,WOS:000860727004017,0
C,"Sawhney, R; Thakkar, M; Agarwal, S; Jin, D; Yang, DY; Flek, L",,,Assoc Computat Linguist,"Sawhney, Ramit; Thakkar, Megh; Agarwal, Shivam; Jin, Di; Yang, Diyi; Flek, Lucie",,,HYPMIX: Hyperbolic Interpolative Data Augmentation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. These methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical geometries. However, these operations are performed in the Euclidean space, simplifying these representations, which may lead to distorted and noisy interpolations. We propose HypMix, a novel model-, data-, and modality-agnostic interpolative data augmentation technique operating in the hyperbolic space, which captures the complex geometry of input and hidden state hierarchies better than its contemporaries. We evaluate HypMix on benchmark and low resource datasets across speech, text, and vision modalities, showing that HypMix consistently outperforms state-of-the-art data augmentation techniques. In addition, we demonstrate the use of HypMix in semi-supervised settings. We further probe into the adversarial robustness and qualitative inferences we draw from HypMix that elucidate the efficacy of the Riemannian hyperbolic manifolds for interpolation-based data augmentation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9858,9868,,,,,,,,,,,,,,,,WOS:000860727003077,0
C,"Thawani, A; Pujara, J; Ilievski, F",,,Assoc Computat Linguist,"Thawani, Avijit; Pujara, Jay; Ilievski, Filip",,,Numeracy enhances the Literacy of Language Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your room but not 500. Does a better grasp of numbers improve a model's understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy for evaluating literacy. To support this investigation, we develop WikiConvert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,6960,6967,,,,,,,,,,,,,,,,WOS:000860727001005,0
C,"Nguyen, T; Luu, AT; Lu, T; Quan, T",,,Assoc Computat Linguist,Thong Nguyen; Anh Tuan Luu; Truc Lu; Tho Quan,,,Enriching and Controlling Global Semantics for Text Summarization,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,9443,9456,,,,,,,,,,,,,,,,WOS:000860727003045,0
C,"Varis, D; Bojar, O",,,Assoc Computat Linguist,"Varis, Dusan; Bojar, Ondrej",,,Sequence Length is a Domain: Length-based Overfitting in Transformer Models,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing task and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8246,8257,,,,,,,,,,,,,,,,WOS:000860727002028,0
C,"Wang, CY; Wang, JN; Qiu, MH; Huang, J; Gao, M",,,Assoc Computat Linguist,"Wang, Chengyu; Wang, Jianing; Qiu, Minghui; Huang, Jun; Gao, Ming",,,TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt embeddings, we propose TransPrompt, a transferable prompting framework for few-shot learning across similar tasks. In TransPrompt, we employ a multitask meta-knowledge acquisition procedure to train a meta-learner that captures cross-task transferable knowledge. Two de-biasing techniques are further designed to make it more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to target tasks with high accuracy. Extensive experiments show that TransPrompt outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance on previously unseen tasks. TransPrompt also outperforms strong fine-tuning baselines when learning with full training sets.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2792,2802,,,,,,,,,,,,,,,,WOS:000855966302080,0
C,"Wang, R; Henao, R",,,Assoc Computat Linguist,"Wang, Rui; Henao, Ricardo",,,Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the replaced positions unchanged. In this paper, we explore the use of paraphrasing as a more principled data augmentation scheme for NER unsupervised consistency training. Specifically, we convert Conditional Random Field (CRF) into a multi-label classification module and encourage consistency on the entity appearance between the original and paraphrased sequences. Experiments show that our method is especially effective when annotations are limited.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5303,5308,,,,,,,,,,,,,,,,WOS:000855966305034,0
C,"Wu, TW; Su, RL; Juang, BH",,,Assoc Computat Linguist,"Wu, Ting-Wei; Su, Ruolin; Juang, Biing-Hwang",,,A Label-Aware BERT Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively accomplish the automation task. The main challenge is that the dialogue often involves complexity in user's intents (or purposes) which are multi-proned, subject to spontaneous change, and difficult to track. Furthermore, public datasets have not considered these complications and the general semantic annotations are lacking which may result in zero-shot problem. Motivated by the above, we propose a Label-Aware BERT Attention Network (LABAN) for zero-shot multi-intent detection. We first encode input utterances with BERT and construct a label embedded space by considering embedded semantics in intent labels. An input utterance is then classified based on its projection weights on each intent embedding in this embedded space. We show that it successfully extends to few/zero-shot setting where part of intent labels are unseen in training data, by also taking account of semantics in these unseen intent labels. Experimental results show that our approach is capable of detecting many unseen intent labels correctly. It also achieves the state-of-the-art performance on five multi-intent datasets in normal cases.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,4884,4896,,,,,,,,,,,,,,,,WOS:000855966305003,0
C,"Xu, CJ; Su, FL; Lehmann, J",,,Assoc Computat Linguist,"Xu, Chengjin; Su, Fenglong; Lehmann, Jens",,,Time-aware Graph Neural Networks for Entity Alignment between Temporal Knowledge Graphs,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of different KGs into a vector space and use GNNs to learn entity representations. To incorporate both relation and time information into the GNN structure of our model, we use a time-aware attention mechanism which assigns different weights to different nodes with orthogonal transformation matrices computed from embeddings of the relevant relations and timestamps in a neighborhood. Experimental results on multiple real-world TKG datasets show that our method significantly outperforms the state-of-the-art methods due to the inclusion of time information. Our datasets and source code are available at https://github.com/soledad921/TEA-GNN",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,8999,9010,,,,,,,,,,,,,,,,WOS:000860727003011,0
C,"Yang, JF; Fancellu, F; Webber, B; Yang, DY",,,Assoc Computat Linguist,"Yang, Jingfeng; Fancellu, Federico; Webber, Bonnie; Yang, Diyi",,,Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"The availability of corpora has led to significant advances in training semantic parsers in English. Unfortunately, for languages other than English, annotated data is limited and so is the performance of the developed parsers. Recently, pretrained multilingual models have been proven useful for zero-shot cross-lingual transfer in many NLP tasks. What else does it require to apply a parser trained in English to other languages for zero-shot cross-lingual semantic parsing? Will simple language-independent features help? To this end, we experiment with six Discourse Representation Structure (DRS) semantic parsers in English, and generalize them to Italian, German and Dutch, where there are only a small number of manually annotated parses available. Extensive experiments show that despite its simplicity, adding Universal Dependency (UD) relations and Universal POS tags (UPOS) as model-agnostic features achieves surprisingly strong improvement on all parsers. We have publicly released our code at https : / / github com/GT- SALT / Multilingual-DRS-Semantic-Parsing.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,5848,5856,,,,,,,,,,,,,,,,WOS:000855966306002,0
C,"Zhan, HL; Shen, L; Chen, HS; Zhang, HN",,,Assoc Computat Linguist,"Zhan, Haolan; Shen, Lei; Chen, Hongshen; Zhang, Hainan",,,CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,2250,2261,,,,,,,,,,,,,,,,WOS:000855966302031,0
C,"Zhang, XF; Sun, HM; Yue, X; Lin, SM; Sun, H",,,Assoc Computat Linguist,"Zhang, Xinliang Frederick; Sun, Heming; Yue, Xiang; Lin, Simon; Sun, Huan",,,COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains similar to 16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains similar to 32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github. com/sunlab-osu/covid-faq.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3759,3769,,,,,,,,,,,,,,,,WOS:000855966303077,0
C,"Zhu, W; Wang, XL; Ni, Y; Xie, GT; Guo, Z; Wu, XM",,,Assoc Computat Linguist,"Zhu, Wei; Wang, Xiaoling; Ni, Yuan; Xie, Guotong; Guo, Zhen; Wu, Xiaoming",,,GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning,2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 07-11, 2021","Punta Cana, DOMINICAN REP",,,,,"In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT's contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT's early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",,,,,,,,,,,,,,,,,,,,,978-1-955917-09-4,,,,2021,,,,,,,3033,3044,,,,,,,,,,,,,,,,WOS:000855966303014,0
C,"Park, Y; Byrd, RJ",,"Lee, L; Harman, D",,"Park, Y; Byrd, RJ",,,Hybrid text mining for finding abbreviations and their definitions,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We present a hybrid text mining method for finding abbreviations and their definitions in free format texts. To deal with the problem this method employs pattern-based abbreviation rules in addition to text markers and cue words. The pattern-based rules describe how abbreviations are formed from definitions. Rules can be generated automatically and/or manually and can be augmented when the system processes new documents. The proposed method has the advantages of high accuracy, high flexibility, wide coverage, and fast recognition.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,126,133,,,,,,,,,,,,,,,,WOS:000223093700016,0
C,"Takamura, H; Matsumoto, Y",,"Lee, L; Harman, D",,"Takamura, H; Matsumoto, Y",,,Feature space restructuring for SVMs with application to text categorization,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"In this paper, we propose a new method of text categorization based on feature space restructuring for SVMs. In our method, independent components of document vectors are extracted using ICA and concatenated with the original vectors. This restructuring makes it possible for SVMs to focus on the latent semantic space without losing information given by the original feature space. Using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data.",,,,,"Takamura, Hiroya/L-6935-2018","Takamura, Hiroya/0000-0002-3244-8294",,,,,,,,,,,,,,,,,,,2001,,,,,,,51,57,,,,,,,,,,,,,,,,WOS:000223093700007,0
C,"Pierce, D; Cardie, C",,"Lee, L; Harman, D",,"Pierce, D; Cardie, C",,,Limitations of co-training for natural language learning from large datasets,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"Co-Training is a weakly supervised learning paradigm in which the redundancy of the learning task is captured by training two classifiers using separate views of the same data. This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data. This study. examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels. Using base noun phrase bracketing as a case study, we find that co-training reduces by 36% the difference in error between co-trained classifers and fully supervised classifiers trained on a labeled version of all available data. However, degradation in the quality of the bootstrapped data arises as an obstacle to further improvement. To address this, we Propose a moderately supervised variant of co-training in which a human corrects the mistakes made during automatic labeling. Our analysis suggests that corrected co-training and similar moderately supervised methods may help co-training scale to large natural language learning tasks.",,,,,,"Cardie, Claire/0000-0002-2061-6094",,,,,,,,,,,,,,,,,,,2001,,,,,,,1,9,,,,,,,,,,,,,,,,WOS:000223093700001,0
C,"Muller, K",,"Lee, L; Harman, D",,"Muller, K",,,Probabilistic context-free grammars for syllabification and grapheme-to-phoneme conversion,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion. The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries. However, our results indicate that the standard probability model does not solve grapheme-to-phoneme conversion sufficiently although, we varied all free parameters of the probabilistic reestimation procedure.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,143,150,,,,,,,,,,,,,,,,WOS:000223093700018,0
C,"Uchimoto, K; Sekine, S; Isahara, H",,"Lee, L; Harman, D",,"Uchimoto, K; Sekine, S; Isahara, H",,,The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,In this paper we describe a morphological analysis method based on a maximum entropy model. This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics. The model has the potential to overcome the unknown word problem.,,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,91,99,,,,,,,,,,,,,,,,WOS:000223093700012,0
C,"Choi, FYY; Wiemer-Hastings, P; Moore, J",,"Lee, L; Harman, D",,"Choi, FYY; Wiemer-Hastings, P; Moore, J",,,Latent semantic analysis for text segmentation,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a). Inter-sentence similarity is estimated by latent semantic analysis (LSA). Boundary locations are discovered by divisive clustering. Test results show LSA is a more accurate similarity measure than the cosine metric (van Rijsbergen, 1979).",,,,,,"Hastings, Peter/0000-0002-0183-001X",,,,,,,,,,,,,,,,,,,2001,,,,,,,109,117,,,,,,,,,,,,,,,,WOS:000223093700014,0
C,"Even-Zohar, Y; Roth, D",,"Lee, L; Harman, D",,"Even-Zohar, Y; Roth, D",,,A sequential model for multi-class classification,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach - a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in part-of-speech tagging.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,10,19,,,,,,,,,,,,,,,,WOS:000223093700002,0
C,"Lyon, C; Malcolm, J; Dickerson, B",,"Lee, L; Harman, D",,"Lyon, C; Malcolm, J; Dickerson, B",,,Detecting short passages of similar text in large document collections,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"This paper presents a statistical method for fingerprinting text. In a large collection of independently written documents each text is associated with a fingerprint which should be different from all the others. If fingerprints are too close then it is suspected that passages of copied or similar text occur in two documents. Our method exploits the characteristic distribution of word trigrams, and measures to determine similarity are based on set theoretic principles. The system was developed using a corpus of broadcast news reports and has been successfully used to detect plagiarism in students' work. It can find small sections that are similar as well as those that are identical. The method is very simple and effective, but seems not to have been used before.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,118,125,,,,,,,,,,,,,,,,WOS:000223093700015,0
C,"Chelba, C; Mahajan, M",,"Lee, L; Harman, D",,"Chelba, C; Mahajan, M",,,Information extraction using the structured language model,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser. The task of template filling is cast as constrained parsing using the SLM. The model is automatically trained from a set of sentences annotated with frame/slot labels and spans. Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. Despite the small amount of 'training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad - personal information management - task.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,74,81,,,,,,,,,,,,,,,,WOS:000223093700010,0
C,"Lee, JS; Kim, B; Lee, GG",,"Lee, L; Harman, D",,"Lee, JS; Kim, B; Lee, GG",,,Automatic Corpus-based Tone using K-TOBI Representation,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"In this paper, we present a prosody generation axchitecture based on K-ToBI (Korean Tone and Break Index) representation. ToBI is, a multi-tier representation system based on linguistic knowledge to transcribe events in an utterance. The TTS system which adopts ToBI as an intermediate representation is known to exhibit higher flexibility, modularity and domain/task portability compared with the direct prosody generation TTS systems. However, the cost of corpus preparation is very expensive for practical-level performance because the TbBI labeled corpus has been manually constructed by many prosody experts and normally requires large amount of data for statistical prosody modeling. Contrary to previous ToBI-based systems, this paper proposes a new method which transcribes the K-ToBI labels completely automatically in Korean speech. We developed automatic corpus-based K-TOBI labeling tools and prediction methods based on several lexico-syntactic linguistic features for decision-tree induction. We demonstrated the performance of F0 generation from automatically predicted K-ToBI labels, and confirmed that the performance is reasonably comparable with state-of-the-art direct prosody generation methods and previous TOBI-based methods.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,134,142,,,,,,,,,,,,,,,,WOS:000223093700017,0
C,"Eneva, E; Hoberman, R; Lita, L",,"Lee, L; Harman, D",,"Eneva, E; Hoberman, R; Lita, L",,,Learning within-sentence semantic coherence,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,In many domains such as speech recognition and machine translation it is extremely useful to be able able to distinguish coherent from non-coherent sentences. We introduce a set of word-based statistical features which measure semantic coherence and can be used to enhance any language application where coherent sentences need to be generated or recognized. We train a decision tree using the constructed feature set to automatically classify sentences as coherent or not. We find that our combination of boosted decision trees and coherence features achieves an accuracy of 80% when distinguishing trigram-generated sentences (non-coherent) from those in the Broadcast News dataset (coherent).,,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,20,26,,,,,,,,,,,,,,,,WOS:000223093700003,0
C,"Koehn, P; Knight, K",,"Lee, L; Harman, D",,"Koehn, P; Knight, K",,,Knowledge sources for word-level translation models,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We present various methods to train word-level translation models for statistical machine translation systems that use widely different knowledge sources ranging from parallel corpora and a bilingual lexicon to only monolingual corpora in two languages. Some novel methods are presented and previously published methods are reviewed. Also, a common evaluation metric enables the first quantitative comparison of these approaches.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,27,35,,,,,,,,,,,,,,,,WOS:000223093700004,0
C,"Sable, C; Church, KW",,"Lee, L; Harman, D",,"Sable, C; Church, KW",,,Using bins to empirically estimate term weights for text categorization,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"This paper introduces a term weighting method for text categorization based on smoothing ideas borrowed from speech recognition. Empirical estimates of weights (likelihood ratios) become unstable when counts are small. Instead of estimating weights for individual words, as Naive Bayes does, words with similar features are grouped into bins, and a single weight is estimated for each bin. This weight is then assigned to all of the words in the bin. The bin-based method is intended for tasks where there is insufficient training data to estimate a separate weight for each word. Experiments show the bin-based method is highly competitive with other current methods. In particular, this method is most similar to Naive Bayes; it generally performs at least as well as Naive Bayes, and sometimes better.",,,,,"Church, Kenneth/AAV-9667-2021; Church, Kenneth/GYR-1624-2022; Church, Kenneth/G-3167-2010","Church, Kenneth/0000-0001-8378-6069; ",,,,,,,,,,,,,,,,,,,2001,,,,,,,58,66,,,,,,,,,,,,,,,,WOS:000223093700008,0
C,"Bangalore, S; Chen, J; Rambow, O",,"Lee, L; Harman, D",,"Bangalore, S; Chen, J; Rambow, O",,,Impact of quality and quantity of corpora on Stochastic generation,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"Recently, there has been some interest in using stochastic approaches in generation. However, there has been little research so far on the question of how the quality, size, and genre of training corpora influence the quality of stochastic generation components. In this paper, we investigate these issues using the FERGUS system. FERGUS uses two distinct stochastic models, a tree model which refers to a grammar, and a linear language model. We use automatic grammar extraction techniques to extract grammars from different-sized tree banks, and then use these extracted grammars to train the tree models. We also investigate the impact of the quality of the annotated, corpus, by using a hand-annotated corpus as well as an automatically annotated corpus. Our results show that automatic grammar extraction is a viable alternative to hand crafted grammars for generation; furthermore, as expected, both quality and size of the training corpus matter",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,159,166,,,,,,,,,,,,,,,,WOS:000223093700020,0
C,"Megyesi, B",,"Lee, L; Harman, D",,"Megyesi, B",,,Comparing data-driven learning algorithms for PoS tagging of Swedish,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"The aim of this study is a systematic evaluation and comparison of four state-of-the-art data-driven learning algorithms applied to part of speech tagging of Swedish. The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning. The systems are evaluated from several aspects. Both the effects of tag set and the effects of the size of training data, are examined. The accuracy is calculated as well as the error rate for known and unknown tokens. The results show differences between the approaches due to the different linguistic information built into the systems.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,151,158,,,,,,,,,,,,,,,,WOS:000223093700019,0
C,"Sakkis, G; Androutsopoulos, I; Paliouras, G; Karkaletsis, V; Spyropoulos, CD; Stamatopoulos, P",,"Lee, L; Harman, D",,"Sakkis, G; Androutsopoulos, I; Paliouras, G; Karkaletsis, V; Spyropoulos, CD; Stamatopoulos, P",,,Stacking classifiers for anti-spam filtering of e-mail,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam. filtering, a novel cost-sensitive application of text categorization. Unsolicited commercial email, or spam, floods mailboxes, causing frustration, wasting bandwidth, and exposing minors to unsuitable content. Using a public corpus, we show that stacking can improve the efficiency of automatically induced anti-spam filters, and that such filters can be used in real-life applications.",,,,,/AAD-7133-2019,/0000-0002-7468-1502,,,,,,,,,,,,,,,,,,,2001,,,,,,,44,50,,,,,,,,,,,,,,,,WOS:000223093700006,0
C,"Rosario, B; Hearst, M",,"Lee, L; Harman, D",,"Rosario, B; Hearst, M",,,Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). in this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,82,90,,,,,,,,,,,,,,,,WOS:000223093700011,0
C,"Ng, HT; Kwan, JLP; Xia, Y",,"Lee, L; Harman, D",,"Ng, HT; Kwan, JLP; Xia, Y",,,Question answering using a large text database: A machine learning approach,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"In this paper, we present a machine learning approach to question answering. The task is answering factual questions, where the answers. axe to be found in documents in a large text database. We trained our system on 398 questions from the Remedia corpus, as well as 138 TREC-8 development questions. We then evaluated our system on 198 questions of the TREC-8 question answering task. Although our learning approach only uses 4 features, we are able to achieve quite competitive accuracy. The results indicate that such a machine learning approach is a promising way to build a state-of-the-art question answering system.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,67,73,,,,,,,,,,,,,,,,WOS:000223093700009,0
C,"Schone, P; Jurafsky, D",,"Lee, L; Harman, D",,"Schone, P; Jurafsky, D",,,Is knowledge-free induction of multiword unit dictionary headwords a solved problem?,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords. We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement. We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,100,108,,,,,,,,,,,,,,,,WOS:000223093700013,0
C,"Gildea, D",,"Lee, L; Harman, D",,"Gildea, D",,,Corpus variation and parser performance,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.",,,,,,"Gildea, Daniel/0000-0002-7858-2624",,,,,,,,,,,,,,,,,,,2001,,,,,,,167,172,,,,,,,,,,,,,,,,WOS:000223093700021,0
C,"Kim, S; Yoon, J; Song, M",,"Lee, L; Harman, D",,"Kim, S; Yoon, J; Song, M",,,Improving lexical mapping model of English-Korean bitext using structural features,PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING,,,,,Conference on Empirical Methods in Natural Language Processing,"JUN 03-04, 2001","Carnegie Mellon Univ, Pittsburgh, PA","Intelligent Informat Syst Inst,USAF Res Lab",Carnegie Mellon Univ,,,"The problem of finding lexical alignments for given sentence pairs is computationally expensive. Furthermore, it. is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of word-for-word correspondences is low. This paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments. We describe how the features improve the performance of the lexical alignment model. The structural features provide the information for the correspondences. of parts-of-speech (POS) sequences. which are useful in translation. Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model. It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge.",,,,,,,,,,,,,,,,,,,,,,,,,2001,,,,,,,36,43,,,,,,,,,,,,,,,,WOS:000223093700005,0
C,"Ainslie, J; Ontanon, S; Alberti, C; Cvicek, V; Fisher, Z; Pham, P; Ravula, A; Sanghai, S; Wang, QF; Yang, L",,,Assoc Computat Linguist,"Ainslie, Joshua; Ontanon, Santiago; Alberti, Chris; Cvicek, Vaclav; Fisher, Zachary; Pham, Philip; Ravula, Anirudh; Sanghai, Sumit; Wang, Qifan; Yang, Li",,,ETC: Encoding Long and Structured Inputs in Transformers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,268,284,,,,,,,,,,,,,,,,WOS:000855160700019,0
C,"Atanasova, P; Simonsen, JG; Lioma, C; Augenstein, I",,,Assoc Computat Linguist,"Atanasova, Pepa; Simonsen, Jakob Grue; Lioma, Christina; Augenstein, Isabelle",,,A Diagnostic Study of Explainability Techniques for Text Classification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3256,3274,,,,,,,,,,,,,,,,WOS:000855160703038,0
C,"Athiwaratkun, B; dos Santos, CN; Krone, J; Xiang, B",,,Assoc Computat Linguist,"Athiwaratkun, Ben; dos Santos, Cicero Nogueira; Krone, Jason; Xiang, Bing",,,Augmented Natural Language for Generative Sequence Labeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework is general purpose, performing well on few-shot, low-resource, and high-resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% -> 90.9%) and 1-shot (70.4% -> 81.0%) state-of-the-art results. Furthermore, our model generates large improvements (46.27% -> 63.83%) in low-resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high-resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,375,385,,,,,,,,,,,,,,,,WOS:000855160700027,0
C,"Bhattacharjee, K; Ballesteros, M; Anubhai, R; Muresan, S; Ma, J; Ladhak, F; Al-Onaizan, Y",,,Assoc Computat Linguist,"Bhattacharjee, Kasturi; Ballesteros, Miguel; Anubhai, Rishita; Muresan, Smaranda; Ma, Jie; Ladhak, Faisal; Al-Onaizan, Yaser",,,To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models can be costly both from an economic and environmental standpoint. In this work, we investigate how to effectively use unlabeled data: by exploring the task-specific semi-supervised approach, Cross-View Training (CVT) and comparing it with task-agnostic BERT in multiple settings that include domain and task relevant English data. CVT uses a much lighter model architecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7927,7934,,,,,,,,,,,,,,,,WOS:000855160708011,0
C,"Chakrabarty, T; Muresan, S; Peng, NY",,,Assoc Computat Linguist,"Chakrabarty, Tuhin; Muresan, Smaranda; Peng, Nanyun",,,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language, such as a simile, goes beyond plain expressions to give readers new insights and inspirations. We tackle the problem of simile generation. Generating a simile requires proper understanding for effective mapping of properties between two concepts. To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge. We then fine-tune a pretrained sequence to sequence model, BART (Lewis et al., 2019), on the literal-simile pairs to generate novel similes given a literal sentence. Experiments show that our approach generates 88% novel similes that do not share properties with the training data. Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37%1 of the times, and three baseline systems including a recent metaphor generation model 71%2 of the times when compared pairwise.3 We also show how replacing literal sentences with similes from our best model in machine generated stories improves evocativeness and leads to better acceptance by human judges.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6455,6469,,,,,,,,,,,,,,,,WOS:000855160706057,0
C,"Dai, ZH; Peng, C; Chen, HJ; Ding, YD",,,Assoc Computat Linguist,"Dai, Zehui; Peng, Cheng; Chen, Huajie; Ding, Yadong",,,A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6955,6965,,,,,,,,,,,,,,,,WOS:000855160707014,0
C,"Ethayarajh, K; Jurafsky, D",,,Assoc Computat Linguist,"Ethayarajh, Kawin; Jurafsky, Dan",,,Utility is in the Eye of the User: A Critique of NLP Leaderboards,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards - in their current form - can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model's utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4846,4853,,,,,,,,,,,,,,,,WOS:000855160705004,0
C,"Gong, CG; Yu, JF; Xia, R",,,Assoc Computat Linguist,"Gong, Chenggong; Yu, Jianfei; Xia, Rui",,,Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation. To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper. Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling. Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7035,7045,,,,,,,,,,,,,,,,WOS:000855160707021,0
C,"Hewitt, J; Hahn, M; Ganguli, S; Liang, P; Manning, CD",,,Assoc Computat Linguist,"Hewitt, John; Hahn, Michael; Ganguli, Surya; Liang, Percy; Manning, Christopher D.",,,RNNs can generate bounded hierarchical languages with optimal memory,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax. We introduce Dyck-(k, m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax. The best known results use O (k(m/2)) memory (hidden units) to generate these languages. We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction. Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1978,2010,,,,,,,,,,,,,,,,WOS:000855160702013,0
C,"Kang, D; Hovy, E",,,Assoc Computat Linguist,"Kang, Dongyeop; Hovy, Eduard",,,Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process. Where can the model learn such high-level coherence? A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on. Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph. However, the task suffers from predicting and selecting appropriate topical content with respect to the given context. To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content. SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation. We also find that a combination of noun and verb types of keywords is the most effective for content selection. As more number of content keywords are provided, overall generation quality also increases.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6533,6543,,,,,,,,,,,,,,,,WOS:000855160706062,0
C,"Li, LJ; Chen, YC; Cheng, Y; Gan, Z; Yu, LC; Liu, JJ",,,Assoc Computat Linguist,"Li, Linjie; Chen, Yen -Chun; Cheng, Yu; Gan, Zhe; Yu, Licheng; Liu, Jingjing",,,HERO: Hierarchical Encoder for Video plus Language Omni-representation Pre-training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2046,2065,,,,,,,,,,,,,,,,WOS:000855160702018,0
C,"Long, WQ; Webbert, B; Xiong, DY",,,Assoc Computat Linguist,"Long, Wanqiu; Webbert, Bonnie; Xiong, Deyi",,,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4way classification is <60%. This is a dramatic drop of 35% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2793,2803,,,,,,,,,,,,,,,,WOS:000855160702080,0
C,"Mallinson, J; Sennrich, R; Lapata, M",,,Assoc Computat Linguist,"Mallinson, Jonathan; Sennrich, Rico; Lapata, Mirella",,,Zero-Shot Crosslingual Sentence Simplification,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks. A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification). Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5109,5126,,,,,,,,,,,,,,,,WOS:000855160705025,0
C,"Rizwan, H; Shakeel, MH; Karim, A",,,Assoc Computat Linguist,"Rizwan, Hammad; Shakeel, Muhammad Haroon; Karim, Asim",,,Hate-Speech and Offensive Language Detection in Roman Urdu,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics. Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task. In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10; 012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hatespeech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4:7 million tweets and make them publicly available. We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2512,2522,,,,,,,,,,,,,,,,WOS:000855160702054,0
C,"Sims, M; Bamman, D",,,Assoc Computat Linguist,"Sims, Matthew; Bamman, David",,,Measuring Information Propagation in Literary Social Networks,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of English fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,642,652,,,,,,,,,,,,,,,,WOS:000855160700047,0
C,"Song, W; Song, ZY; Fu, RJ; Liu, LZ; Cheng, MM; Liu, T",,,Assoc Computat Linguist,"Song, Wei; Song, Ziyao; Fu, Ruiji; Liu, Lizhen; Cheng, Miaomiao; Liu, Ting",,,Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation. We conduct experiments on two datasets: a Chinese dataset and an English dataset. We find that (i) sentence positional encodings can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representation for identifying discourse elements.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2820,2830,,,,,,,,,,,,,,,,WOS:000855160702082,0
C,"Wang, LZ; Jing, L; Zeng, XS; Zhang, HS; Wong, KF",,,Assoc Computat Linguist,"Wang, Lingzhi; Jing Li; Zeng, Xingshan; Zhang, Haisong; Wong, Kam Fai",,,"Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations",PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context. Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn's existing content. Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation. Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models. Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6640,6650,,,,,,,,,,,,,,,,WOS:000855160706071,0
C,"Wang, SY; Wei, ZY; Fan, ZH; Huang, ZF; Sun, WJ; Zhang, Q; Huang, XJ",,,Assoc Computat Linguist,"Wang, Siyuan; Wei, Zhongyu; Fan, Zhihao; Huang, Zengfeng; Sun, Weijian; Zhang, Qi; Huang, Xuanjing",,,PathQG: Neural Question Generation from Facts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9066,9075,,,,,,,,,,,,,,,,WOS:000855160709024,0
C,"Wu, XB; Li, CP; Zhu, Y; Miao, YS",,,Assoc Computat Linguist,"Wu, Xiaobao; Li, Chunping; Zhu, Yan; Miao, Yishu",,,Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts. Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics. We observe that our model can highly improve short text topic modeling performance. Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1772,1782,,,,,,,,,,,,,,,,WOS:000855160701078,0
C,"Zhao, XL; Wu, W; Xu, C; Tao, CY; Zhao, DY; Yan, R",,,Assoc Computat Linguist,"Zhao, Xueliang; Wu, Wei; Xu, Can; Tao, Chongyang; Zhao, Dongyan; Yan, Rui",,,Knowledge-Grounded Dialogue Generation with Pre-trained Language Models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3377,3390,,,,,,,,,,,,,,,,WOS:000855160703047,0
C,"Chen, HJ; Ji, YF",,,Assoc Computat Linguist,"Chen, Hanjie; Ji, Yangfeng",,,Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4236,4251,,,,,,,,,,,,,,,,WOS:000855160704035,0
C,"Chen, KL; Xu, WD; Cheng, XY; Zou, XC; Zhang, YY; Le, S; Wang, TF; Yuan, Q; Wei, C",,,Assoc Computat Linguist,"Chen, Kunlong; Xu, Weidi; Cheng, Xingyi; Zou Xiaochuan; Zhang, Yuyu; Le Song; Wang, Taifeng; Yuan Qi; Wei Chu",,Ant Grp,Question Directed Graph Attention Network for Numerical Reasoning over Text,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP1.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6759,6768,,,,,,,,,,,,,,,,WOS:000855160706082,0
C,"Conia, S; Brignone, F; Zanfardino, D; Navigli, R",,,Assoc Computat Linguist,"Conia, Simone; Brignone, Fabrizio; Zanfardino, Davide; Navigli, Roberto",,,InVeRo: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to a new verb resource, VerbAtlas, and a state-of-the-art pre-trained implementation of a neural, span-based architecture for SRL. Both the resource and the system provide human-readable verb sense and semantic role information, with an easy to use Web interface and RESTful APIs available at http://nlp.uniroma1.it/invero.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,77,84,,,,,,,,,,,,,,,,WOS:000855177700011,0
C,"Dibia, V",,,Assoc Computat Linguist,"Dibia, Victor",,,NeuralQA: A Usable Library for Question Answering (Contextual Query Expansion plus BERT) on Large Datasets,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing tools for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline (query expansion, retrieval, reading, and explanation/sensemaking). To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure (e.g., ElasticSearch instances and reader models trained with the HuggingFace Transformers API) and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion (CQE) using a masked language model (MLM) as well as relevant snippets (RelSnip) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations (e.g., visualization of gradient-based explanations to support qualitative inspection of model behaviour) and large scale search deployment. Code and documentation for NeuralQA is available as open source on Github.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,15,22,,,,,,,,,,,,,,,,WOS:000855177700003,0
C,"Drozdov, A; Rongali, S; Chen, YP; O'Gorman, T; Iyyer, M; McCallum, A",,,Assoc Computat Linguist,"Drozdov, Andrew; Rongali, Subendhu; Chen, Yi-Pei; O'Gorman, Tim; Iyyer, Mohit; McCallum, Andrew",,,Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019a) is a selfsupervised neural model that learns to induce syntactic tree structures for input sentences without access to labeled training data. In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing. To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softlyweighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart. Our experiments show that through fine-tuning a pre-trained DIORA with our new algorithm, we improve the state of the art in unsupervised constituency parsing on the English WSJ Penn Treebank by 2.2 - 6% F1, depending on the data used for fine-tuning.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4832,4845,,,,,,,,,,,,,,,,WOS:000855160705003,0
C,"Feng, YL; Chen, XY; Lin, BY; Wang, PF; Yan, J; Ren, X",,,Assoc Computat Linguist,"Feng, Yanlin; Chen, Xinyue; Lin, Bill Yuchen; Wang, Peifeng; Yan, Jun; Ren, Xiang",,,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Existing work that augment question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks and results in better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released(1).",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1295,1309,,,,,,,,,,,,,,,,WOS:000855160701039,0
C,"He, WW; Yang, M; Yan, R; Li, CM; Shen, Y; Xu, RF",,,Assoc Computat Linguist,"He, Wanwei; Yang, Min; Yan, Rui; Li, Chengming; Shen, Ying; Xu, Ruifeng",,,Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a Two-Teacher One-Student learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously. TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network). Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network. Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student. The usage of discriminators relaxes the rigid coupling between the student and teachers. Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods. For reproducibility, we release the code and data at https://github.com/siat-nlp/TTOS.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3498,3507,,,,,,,,,,,,,,,,WOS:000855160703056,0
C,"Karpukhin, V; Oguz, B; Min, S; Lewis, P; Wu, L; Edunov, S; Chen, D; Yih, WT",,,Assoc Computat Linguist,"Karpukhin, Vladimir; Oguz, Barlas; Min, Sewon; Lewis, Patrick; Wu, Ledell; Edunov, Sergey; Chen, Danqi; Yih, Wen Tau",,,Dense Passage Retrieval for Open-Domain Question Answering,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6769,6781,,,,,,,,,,,,,,,,WOS:000855160706083,0
C,"Li, B; Wang, ZY; Liu, H; Jiang, YF; Du, Q; Xiao, T; Wang, HZ; Zhu, JB",,,Assoc Computat Linguist,"Li, Bei; Wang, Ziyang; Liu, Hui; Jiang, Yufan; Du, Quan; Xiao, Tong; Wang, Huizhen; Zhu, Jingbo",,,Shallow-to-Deep Training for Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is 1.4 x faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at https://github.com/libeineu/SDT-Training.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,995,1005,,,,,,,,,,,,,,,,WOS:000855160701012,0
C,"Mass, Y; Roitman, H",,,Assoc Computat Linguist,"Mass, Yosi; Roitman, Haggai",,,Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision based solely on the documents in the corpus. We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers. We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets. We show that our method outperforms state-ofthe-art methods; this without the need for the expensive process of manually labeling data.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4191,4197,,,,,,,,,,,,,,,,WOS:000855160704031,0
C,"Ng, N; Cho, K; Ghassemi, M",,,Assoc Computat Linguist,"Ng, Nathan; Cho, Kyunghyun; Ghassemi, Marzyeh",,,SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. (1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1268,1283,,,,,,,,,,,,,,,,WOS:000855160701037,0
C,"Papay, S; Klinger, R; Pado, S",,,Assoc Computat Linguist,"Papay, Sean; Klinger, Roman; Pado, Sebastian",,,Dissecting Span Identification Tasks with Performance Prediction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4881,4895,,,,,,,,,,,,,,,,WOS:000855160705007,0
C,"Ren, MC; Geng, X; Qin, T; Huang, HY; Jiang, D",,,Assoc Computat Linguist,"Ren, Mucheng; Geng, Xiubo; Qin, Tao; Huang, Heyan; Jiang, Daxin",,,Towards Interpretable Reasoning over Paragraph Effects in Situation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step black box model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6745,6758,,,,,,,,,,,,,,,,WOS:000855160706081,0
C,"Wang, Y; Li, J; Lyu, MR; King, I",,,Assoc Computat Linguist,"Wang, Yue; Li, Jing; Lyu, Michael R.; King, Irwin",,,Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality Multi-Head Attention ((MH)-H-3-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a novel unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset(1) newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional co-attentions. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3311,3324,,,,,,,,,,,,,,,,WOS:000855160703043,0
C,"Wolf, T; Debut, L; Sanh, V; Chaumond, J; Delangue, C; Moi, A; Cistac, P; Rault, T; Louf, R; Funtowicz, M; Davison, J; Shleifer, S; von Platen, P; Ma, C; Jernite, Y; Plu, JL; Xu, CW; Le Scao, T; Gugger, S; Drame, M; Lhoest, Q; Rush, AM",,,Assoc Computat Linguist,"Wolf, Thomas; Debut, Lysandre; Sanh, Victor; Chaumond, Julien; Delangue, Clement; Moi, Anthony; Cistac, Picnic; Rault, Tim; Louf, Remi; Funtowicz, Morgan; Davison, Joe; Shleifer, Sam; von Platen, Patrick; Ma, Clara; Jernite, Yacine; Plu, Julien; Xu, Canwen; Le Scao, Teven; Gugger, Sylvain; Drame, Mariama; Lhoest, Quentin; Rush, Alexander M.",,,Transformers: State-of-the-Art Natural Language Processing,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS,,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.",,,,,,,,,,,,,,,,,,,,,978-1-952148-62-0,,,,2020,,,,,,,38,45,,,,,,,,,,,,,,,,WOS:000855177700006,0
C,"Yoo, KM; Lee, H; Dernoncourt, F; Bui, T; Chang, W; Lee, SG",,,Assoc Computat Linguist,"Yoo, Kang Min; Lee, Hanbit; Dernoncourt, Franck; Bui, Trung; Chang, Walter; Lee, Sang-Goo",,,Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goal-oriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers' robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs dialog response generation and user simulation, where our model outperforms previous strong baselines.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3406,3425,,,,,,,,,,,,,,,,WOS:000855160703049,0
C,"Yuan, M; Zhang, M; Van Durme, B; Findlater, L; Boyd-Graber, J",,,Assoc Computat Linguist,"Yuan, Michelle; Zhang, Mozhi; Van Durme, Benjamin; Findlater, Leah; Boyd-Graber, Jordan",,,Interactive Refinement of Cross-Lingual Word Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5984,5996,,,,,,,,,,,,,,,,WOS:000855160706015,0
C,"Zeng, S; Xu, RX; Chang, BB; Li, L",,,Assoc Computat Linguist,"Zeng, Shuang; Xu, Runxin; Chang, Baobao; Li, Lei",,,Double Graph Based Reasoning for Document-level Relation Extraction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1630,1640,,,,,,,,,,,,,,,,WOS:000855160701067,0
C,"Zhang, RS; Zheng, YH; Shao, JZ; Mao, XX; Xi, YD; Huang, ML",,,Assoc Computat Linguist,"Zhang, Rongsheng; Zheng, Yinhe; Shao, Jianzhi; Mao, Xiaoxi; Xi, Yadong; Huang, Minlie",,,Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.",,,,,,"Zheng, Yinhe/0000-0002-7029-5671",,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,3449,3460,,,,,,,,,,,,,,,,WOS:000855160703052,0
C,"Zhao, X; Wang, ZH; Wu, H; Yong, Z",,,Assoc Computat Linguist,"Zhao, Xu; Wang, Zihao; Wu, Hao; Yong, Zhang",,,Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semi-supervised methods do not fully utilize the knowledge hidden in annotated and non-annotated data, which hinders further improvement of their performance. In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment. We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively. Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models. Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport. Experimental results on MUSE and VecMap datasets show significant improvement of our models. Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance. Results on distant language pairs further illustrate the advantage and robustness of our proposed method.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,2973,2984,,,,,,,,,,,,,,,,WOS:000855160703013,0
C,"Bisk, Y; Holtzman, A; Thomason, J; Andreas, J; Bengio, Y; Chai, J; Lapata, M; Lazaridou, A; May, J; Nisnevich, A; Pinto, N; Turian, J",,,Assoc Computat Linguist,"Bisk, Yonatan; Holtzman, Ari; Thomason, Jesse; Andreas, Jacob; Bengio, Yoshua; Chai, Joyce; Lapata, Mirella; Lazaridou, Angeliki; May, Jonathan; Nisnevich, Aleksandr; Pinto, Nicolas; Turian, Joseph",,,Experience Grounds Language,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,8718,8735,,,,,,,,,,,,,,,,WOS:000855160708078,0
C,"Chen, A; Stanovsky, G; Singh, S; Gardner, M",,,Assoc Computat Linguist,"Chen, Anthony; Stanovsky, Gabriel; Singh, Sameer; Gardner, Matt",,,MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,6521,6532,,,,,,,,,,,,,,,,WOS:000855160706061,0
C,"Chen, Y; Liu, Y; Chen, G; Jiang, X; Liu, Q",,,Assoc Computat Linguist,"Chen, Yun; Liu, Yang; Chen, Guanhua; Jiang, Xin; Liu, Qun",,,Accurate Word Alignment Induction from Neural Machine Translation,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights DO capture accurate word alignments and propose two novel word alignment induction methods SHIFT-ATT and SHIFT-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. SHIFT-ATT is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. SHIFT-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized SHIFT-ATT alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and SHIFT-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,566,576,,,,,,,,,,,,,,,,WOS:000855160700042,0
C,"Chen, ZH; Song, Y; Chang, TH; Wan, X",,,Assoc Computat Linguist,"Chen, Zhihong; Yan Song; Chang, Tsung-Hui; Xiang Wan",,,Generating Radiology Reports via Memory-driven Transformer,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1439,1449,,,,,,,,,,,,,,,,WOS:000855160701052,0
C,"Dinkar, T; Colombo, P; Labeau, M; Clavel, C",,,Assoc Computat Linguist,"Dinkar, Tanvi; Colombo, Pierre; Labeau, Matthieu; Clavel, Chloe",,,The importance of fillers for text representations of speech transcripts,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"While being an essential component of spoken language, fillers (e.g. um or uh) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two downstream tasks - predicting a speaker's stance and expressed confidence.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,7985,7993,,,,,,,,,,,,,,,,WOS:000855160708016,0
C,"Dong, Y; Wang, SH; Gan, Z; Cheng, Y; Cheung, JCK; Liu, JJ",,,Assoc Computat Linguist,"Dong, Yue; Wang, Shuohang; Gan, Zhe; Cheng, Yu; Cheung, Jackie Chi Kit; Liu, Jingjing",,,Multi-Fact Correction in Abstractive Text Summarization,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose SpanFact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9320,9331,,,,,,,,,,,,,,,,WOS:000855160709044,0
C,"Gao, Y; Li, YF; Lin, Y; Aggarwal, C; Khan, L",,,Assoc Computat Linguist,"Gao, Yang; Li, Yi-Fan; Lin, Yu; Aggarwal, Charu; Khan, Latifur",,,SetConv: A New Approach for Learning from Imbalanced Data,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,1284,1294,,,,,,,,,,,,,,,,WOS:000855160701038,0
C,"Hawkins, RD; Yamakoshi, T; Griffiths, TL; Goldberg, AE",,,Assoc Computat Linguist,"Hawkins, Robert D.; Yamakoshi, Takateru; Griffiths, Thomas L.; Goldberg, Adele E.",,,Investigating representations of verb bias in neural language models,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb - a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4653,4663,,,,,,,,,,,,,,,,WOS:000855160704064,0
C,"Lyu, S; Son, B; Yang, K; Bae, J",,,Assoc Computat Linguist,"Lyu, Sungwon; Son, Bokyung; Yang, Kichang; Bae, Jaekyoung",,,Revisiting Modularized Multilingual NMT to Meet Industrial Demands,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual neural machine translation model that only share modules among the same languages (M2) as a practical alternative to 1-1 to satisfy industrial requirements. Through comprehensive experiments, we identify the benefits of multi-way training and demonstrate that the M2 can enjoy these benefits without suffering from the capacity bottleneck. Furthermore, the interlingual space of the M2 allows convenient modification of the model. By leveraging trained modules, we find that incrementally added modules exhibit better performance than singly trained models. The zero-shot performance of the added modules is even comparable to supervised models. Our findings suggest that the M2 can be a competent candidate for multilingual translation in industries.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,5905,5918,,,,,,,,,,,,,,,,WOS:000855160706009,0
C,"MacAvaney, S; Cohan, A; Goharian, N",,,Assoc Computat Linguist,"MacAvaney, Sean; Cohan, Arman; Goharian, Nazli",,,SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zeroshot ranking algorithm that adapts to COVID-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural reranking model pre-trained on scientific text (SciBERT), and filters the target document collection. This approach ranks top among zeroshot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments. Despite not relying on TREC-COVID data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4171,4179,,,,,,,,,,,,,,,,WOS:000855160704029,0
C,"Nie, YX; Zhou, X; Bansal, M",,,Assoc Computat Linguist,"Nie, Yixin; Zhou, Xiang; Bansal, Mohit",,,What Can We Learn from Collective Human Opinions on Natural Language Inference Data?,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in alpha ALI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,9131,9143,,,,,,,,,,,,,,,,WOS:000855160709029,0
C,"Otani, N; Ozakil, S; Zhao, XY; Li, YC; St Johns, M; Levin, L",,,Assoc Computat Linguist,"Otani, Naoki; Ozakil, Satoru; Zhao, Xingyuan; Li, Yucen; St Johns, Micaelah; Levin, Lori",,,Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs. We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings. CWEs are trained on a word-translation task using the dictionaries that only contain single words. In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language. We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE. We can translate MWEs at a top-10 precision of 30-60%. The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,4451,4464,,,,,,,,,,,,,,,,WOS:000855160704048,0
C,"Quan, J; Zhang, SA; Cao, Q; Li, ZZ; Xiong, DY",,,Assoc Computat Linguist,"Quan, Jun; Zhang, Shian; Cao, Qian; Li, Zizhong; Xiong, Deyi",,,RiSAWOZ: A Large-Scale Multi-DomainWizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling,PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP),,,,,Conference on Empirical Methods in Natural Language Processing (EMNLP),"NOV 16-20, 2020",ELECTR NETWORK,"Bloomberg Engn,Google Res,Apple,Amazon Sci,Baidu,Megagon Labs,Facebook,DeepMind,Grammarly,ByteDance,Zeta Alpha,Babelscape,Naver,Adobe,Hitachi,Salesforce,Univ So Calif, Viterbi Sch Engn, Informat Sci Inst",,,,"In order to alleviate the shortage of multidomain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multidomain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labeled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, we especially provide linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. A series of benchmark models and results are reported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue context-to-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.(1)",,,,,,,,,,,,,,,,,,,,,978-1-952148-60-6,,,,2020,,,,,,,930,940,,,,,,,,,,,,,,,,WOS:000855160701007,0
