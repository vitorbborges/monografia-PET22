Publication Type,Authors,Book Authors,Book Editors,Book Group Authors,Author Full Names,Book Author Full Names,Group Authors,Article Title,Source Title,Book Series Title,Book Series Subtitle,Language,Document Type,Conference Title,Conference Date,Conference Location,Conference Sponsor,Conference Host,Author Keywords,Keywords Plus,Abstract,Addresses,Affiliations,Reprint Addresses,Email Addresses,Researcher Ids,ORCIDs,Funding Orgs,Funding Name Preferred,Funding Text,Cited References,Cited Reference Count,"Times Cited, WoS Core","Times Cited, All Databases",180 Day Usage Count,Since 2013 Usage Count,Publisher,Publisher City,Publisher Address,ISSN,eISSN,ISBN,Journal Abbreviation,Journal ISO Abbreviation,Publication Date,Publication Year,Volume,Issue,Part Number,Supplement,Special Issue,Meeting Abstract,Start Page,End Page,Article Number,DOI,DOI Link,Book DOI,Early Access Date,Number of Pages,WoS Categories,Web of Science Index,Research Areas,IDS Number,Pubmed Id,Open Access Designations,Highly Cited Status,Hot Paper Status,Date of Export,UT (Unique WOS ID),Web of Science Record
J,"Barreto, AMS; Precup, D; Pineau, J",,,,"Barreto, Andre M. S.; Precup, Doina; Pineau, Joelle",,,Practical Kernel-Based Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel-based reinforcement learning (KBRL) stands out among approximate reinforcement learning algorithms for its strong theoretical guarantees. By casting the learning problem as a local kernel approximation, KBRL provides a way of computing a decision policy which converges to a unique solution and is statistically consistent. Unfortunately, the model constructed by KBRL grows with the number of sample transitions, resulting in a computational cost that precludes its application to large-scale or on-line domains. In this paper we introduce an algorithm that turns KBRL into a practical reinforcement learning tool. Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix, potentially much smaller than the original, which retains some fundamental properties of its precursor. KBSF exploits such an insight to compress the information contained in KBRL's model into an approximator of fixed size. This makes it possible to build an approximation considering both the difficulty of the problem and the associated computational cost. KBSF's computational complexity is linear in the number of sample transitions, which is the best one can do without discarding data. Moreover, the algorithm's simple mechanics allow for a fully incremental implementation that makes the amount of memory used independent of the number of sample transitions. The result is a kernel-based reinforcement learning algorithm that can be applied to large-scale problems in both off-line and on-line regimes. We derive upper bounds for the distance between the value functions computed by KBRL and KBSF using the same data. We also prove that it is possible to control the magnitude of the variables appearing in our bounds, which means that, given enough computational resources, we can make KBSF's value function as close as desired to the value function that would be computed by KBRL using the same set of sample transitions. The potential of our algorithm is demonstrated in an extensive empirical study in which KBSF is applied to difficult tasks based on real-world data. Not only does KBSF solve problems that had never been solved before, but it also significantly outperforms other state-of-the-art reinforcement learning algorithms on the tasks studied.",,,,,"Barreto, Andr√© M S/J-5063-2013","Salguero Tejada, Carlos/0000-0003-0930-9277",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,67,,,,,,,,,,,,,,,WOS:000391522700001,0
J,"Escalante-B, AN; Wiskott, L",,,,"Escalante-B, Alberto N.; Wiskott, Laurenz",,,Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a multi-dimensional time series. Graph-based SFA (GSFA) is an extension to SFA for supervised learning that can be used to successfully solve regression problems if combined with a simple supervised post-processing step on a small number of slow features. The objective function of GSFA minimizes the squared output differences between pairs of samples specified by the edges of a structure called training graph. The edges of current training graphs, however, are derived only from the relative order of the labels. Exploiting the exact numerical value of the labels enables further improvements in label estimation accuracy. In this article, we propose the exact label learning (ELL) method to create a more precise training graph that encodes the desired labels explicitly and allows GSFA to extract a normalized version of them directly (i.e., without supervised post-processing). The ELL method is used for three tasks: (1) We estimate gender from artificial images of human faces (regression) and show the advantage of coding additional labels, particularly skin color. (2) We analyze two existing graphs for regression. (3) We extract compact discriminative features to classify traffic sign images. When the number of output features is limited, such compact features provide a higher classification rate compared to a graph that generates features equivalent to those of nonlinear Fisher discriminant analysis. The method is versatile, directly supports multiple labels, and provides higher accuracy compared to current graphs for the problems considered.",,,,,"Wiskott, Laurenz/P-7715-2017","Wiskott, Laurenz/0000-0001-6237-740X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,157,,,,,,,,,,,,,,,WOS:000391666600001,0
J,"Gao, C; Lu, Y; Ma, ZM; Zhou, HH",,,,"Gao, Chao; Lu, Yu; Ma, Zongming; Zhou, Harrison H.",,,Optimal Estimation and Completion of Matrices with Biclustering Structures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Biclustering structures in data matrices were first formalized in a seminal paper by John Hartigan (Hartigan, 1972) where one seeks to cluster cases and variables simultaneously. Such structures are also prevalent in block modeling of networks. In this paper, we develop a theory for the estimation and completion of matrices with biclustering structures, where the data is a partially observed and noise contaminated matrix with a certain underlying biclustering structure. In particular, we show that a constrained least squares estimator achieves minimax rate-optimal performance in several of the most important scenarios. To this end, we derive unified high probability upper bounds for all sub-Gaussian data and also provide matching minimax lower bounds in both Gaussian and binary cases. Due to the close connection of graphon to stochastic block models, an immediate consequence of our general results is a minimax rate-optimal estimator for sparse graphons.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,161,,,,,,,,,,,,,,,WOS:000391668000001,0
J,"Hanneke, S",,,,"Hanneke, Steve",,,Refined Error Bounds for Several Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article studies the achievable guarantees on the error rates of certain learning algorithms, with particular focus on re fining logarithmic factors. Many of the results are based on a general technique for obtaining bounds on the error rates of sample-consistent classifiers with monotonic error regions, in the realizable case. We prove bounds of this type expressed in terms of either the VC dimension or the sample compression size. This general technique also enables us to derive several new bounds on the error rates of general sample-consistent learning algorithms, as well as re fined bounds on the label complexity of the CAL active learning algorithm. Additionally, we establish a simple necessary and sufficient condition for the existence of a distribution-free bound on the error rates of all sample-consistent learning rules, converging at a rate inversely proportional to the sample size. We also study learning in the presence of classification noise, deriving a new excess error rate guarantee for general VC classes under Tsybakov's noise condition, and establishing a simple and general necessary and sufficient condition for the minimax excess risk under bounded noise to converge at a rate inversely proportional to the sample size.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,135,,,,,,,,,,,,,,,WOS:000391658200001,0
J,"Mansinghka, V; Shafto, P; Jonas, E; Petschulat, C; Gasner, M; Tenenbaum, JB",,,,"Mansinghka, Vikash; Shafto, Patrick; Jonas, Eric; Petschulat, Cap; Gasner, Max; Tenenbaum, Joshua B.",,,"CrossCat: A Fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is a widespread need for statistical methods that can analyze high-dimensional datasets without imposing restrictive or opaque modeling assumptions. This paper describes a domain-general data analysis method called CrossCat. CrossCat infers multiple non-overlapping views of the data, each consisting of a subset of the variables, and uses a separate nonparametric mixture to model each view. CrossCat is based on approximately Bayesian inference in a hierarchical, nonparametric model for data tables. This model consists of a Dirichlet process mixture over the columns of a data table in which each mixture component is itself an independent Dirichlet process mixture over the rows; the inner mixture components are simple parametric models whose form depends on the types of data in the table. CrossCat combines strengths of mixture modeling and Bayesian network structure learning. Like mixture modeling, CrossCat can model a broad class of distributions by positing latent variables, and produces representations that can be efficiently conditioned and sampled from for prediction. Like Bayesian networks, CrossCat represents the dependencies and independencies between variables, and thus remains accurate when there are multiple statistical signals. Inference is done via a scalable Gibbs sampling scheme; this paper shows that it works well in practice. This paper also includes empirical results on heterogeneous tabular data of up to 10 million cells, such as hospital cost and quality measures, voting records, unemployment rates, gene expression measurements, and images of handwritten digits. CrossCat infers structure that is consistent with accepted findings and common-sense knowledge in multiple domains and yields predictive accuracy competitive with generative, discriminative, and model-free alternatives.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,138,,,,,,,,,,,,,,,WOS:000391659400001,0
J,"Muandet, K; Sriperumbudur, B; Fukumizu, K; Gretton, A; Scholkopf, B",,,,"Muandet, Krikamol; Sriperumbudur, Bharath; Fukumizu, Kenji; Gretton, Arthur; Schoelkopf, Bernhard",,,Kernel Mean Shrinkage Estimators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A mean function in a reproducing kernel Hilbert space (RKHS), or a kernel mean, is central to kernel methods in that it is used by many classical algorithms such as kernel principal component analysis, and it also forms the core inference step of modern kernel methods that rely on embedding probability distributions in RKHSs. Given a finite sample, an empirical average has been used commonly as a standard estimator of the true kernel mean. Despite a widespread use of this estimator, we show that it can be improved thanks to the well-known Stein phenomenon. We propose a new family of estimators called kernel mean shrinkage estimators (KMSEs), which bene fit from both theoretical justifications and good empirical performance. The results demonstrate that the proposed estimators outperform the standard one, especially in a large d, small n paradigm.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,41,48,,,,,,,,,,,,,,,WOS:000391485700001,0
J,"Niinimaki, T; Parviainen, P; Koivisto, M",,,,"Niinimaki, Teppo; Parviainen, Pekka; Koivisto, Mikko",,,Structure Discovery in Bayesian Networks by Sampling Partial Orders,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present methods based on Metropolis-coupled Markov chain Monte Carlo (MC3) and annealed importance sampling (AIS) for estimating the posterior distribution of Bayesian networks. The methods draw samples from an appropriate distribution of partial orders on the nodes, continued by sampling directed acyclic graphs (DAGs) conditionally on the sampled partial orders. We show that the computations needed for the sampling algorithms are feasible as long as the encountered partial orders have relatively few down-sets. While the algorithms assume suitable modularity properties of the priors, arbitrary priors can be handled by dividing the importance weight of each sampled DAG by the number of topological sorts it has| we give a practical dynamic programming algorithm to compute these numbers. Our empirical results demonstrate that the presented partial-order-based samplers are superior to previous Markov chain Monte Carlo methods, which sample DAGs either directly or via linear orders on the nodes. The results also suggest that the convergence rate of the estimators based on AIS are competitive to those of MC3. Thus AIS is the preferred method, as it enables easier large-scale parallelization and, in addition, supplies good probabilistic lower bound guarantees for the marginal likelihood of the model.",,,,,,"Koivisto, Mikko/0000-0001-9662-3605",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,57,,,,,,,,,,,,,,,WOS:000391489300001,0
J,"Vehtari, A; Mononen, T; Tolvanen, V; Sivula, T; Winther, O",,,,"Vehtari, Aki; Mononen, Tommi; Tolvanen, Ville; Sivula, Tuomas; Winther, Ole",,,Bayesian Leave-One-Out Cross Validation Approximations for Gaussian Latent Variable Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators.That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,103,,,,,,,,,,,,,,,WOS:000391543600001,0
J,"Yuille, A; Mottaghi, R",,,,"Yuille, Alan; Mottaghi, Roozbeh",,,Complexity of Representation and Inference in Compositional Models with Part Sharing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., neurons) for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,11,,,,,,,,,,,,,,,WOS:000391470200001,0
J,"Chazal, F; Glisse, M; Labruere, C; Michel, B",,,,"Chazal, Frederic; Glisse, Marc; Labruere, Catherine; Michel, Bertrand",,,Convergence Rates for Persistence Diagram Estimation in Topological Data Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Computational topology has recently seen an important development toward data analysis, giving birth to the field of topological data analysis. Topological persistence, or persistent homology, appears as a fundamental tool in this field. In this paper, we study topological persistence in general metric spaces, with a statistical approach. We show that the use of persistent homology can be naturally considered in general statistical frameworks and that persistence diagrams can be used as statistics with interesting convergence properties. Some numerical experiments are performed in various contexts to illustrate our results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3603,3635,,,,,,,,,,,,,,,,WOS:000369888000039,0
J,"Hanneke, S; Yang, L",,,,"Hanneke, Steve; Yang, Liu",,,Minimax Analysis of Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work establishes distribution-free upper and lower bounds on the minimax label complexity of active learning with general hypothesis classes, under various noise models. The results reveal a number of surprising facts. In particular, under the noise model of Tsybakov (2004), the minimax label complexity of active learning with a VC class is always asymptotically smaller than that of passive learning, and is typically significantly smaller than the best previously-published upper bounds in the active learning literature. In high noise regimes, it turns out that all active learning problems of a given VC dimension have roughly the same minimax label complexity, which contrasts with well-known results for bounded noise. In low-noise regimes, we find that the label complexity is well-characterized by a simple combinatorial complexity measure we call the star number. Interestingly, we find that almost all of the complexity measures previously explored in the active learning literature have worst-case values exactly equal to the star number. We also propose new active learning strategies that nearly achieve these minimax label complexities.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3487,3602,,,,,,,,,,,,,,,,WOS:000369888000038,0
J,"Heaton, J",,,,"Heaton, Jeff",,,Encog: Library of Interchangeable Machine Learning Models for Java and C#,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces the Encog library for Java and C#, a scalable, adaptable, multi-platform machine learning framework that was first released in 2008. Encog allows a variety of machine learning models to be applied to data sets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors.",,,,,,"Heaton, Jeff/0000-0003-1496-4049",,,,,,,,,,,,,1532-4435,,,,,JUN,2015,16,,,,,,1243,1247,,,,,,,,,,,,,,,,WOS:000369886600004,0
J,"Tacchetti, A; Mallapragada, PK; Santoro, M; Rosasco, L",,,,"Tacchetti, Andrea; Mallapragada, Pavan K.; Santoro, Matteo; Rosasco, Lorenzo",,,GURLS: A Least Squares Library for Supervised Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD license and is available for download at https://github.com/LCSL/GURLS.",,,,,"Santoro, Matteo/ABB-4593-2020; Santoro, Matteo/N-7176-2013","rosasco, lorenzo/0000-0003-3098-383X",,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3201,3205,,,,,,,,,,,,,,,,WOS:000328603600011,0
J,"Chertkov, M; Yedidia, AB",,,,"Chertkov, Michael; Yedidia, Adam B.",,,Approximating the Permanent with Fractional Belief Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discuss schemes for exact and approximate computations of permanents, and compare them with each other. Specifically, we analyze the belief propagation (BP) approach and its fractional belief propagation (FBP) generalization for computing the permanent of a non-negative matrix. Known bounds and Conjectures are verified in experiments, and some new theoretical relations, bounds and Conjectures are proposed. The fractional free energy (FFE) function is parameterized by a scalar parameter gamma is an element of [-1;1], where gamma = -1 corresponds to the BP limit and gamma = 1 corresponds to the exclusion principle (but ignoring perfect matching constraints) mean-field (MF) limit. FFE shows monotonicity and continuity with respect to g. For every non-negative matrix, we define its special value gamma(*) is an element of [-1;0] to be the gamma for which the minimum of the gamma-parameterized FFE function is equal to the permanent of the matrix, where the lower and upper bounds of the g-interval corresponds to respective bounds for the permanent. Our experimental analysis suggests that the distribution of gamma(*) varies for different ensembles but gamma(*) always lies within the [-1;-1/2] interval. Moreover, for all ensembles considered, the behavior of gamma(*) is highly distinctive, offering an empirical practical guidance for estimating permanents of non-negative matrices via the FFE approach.",,,,,"Chertkov, Michael/O-8828-2015","Chertkov, Michael/0000-0002-6758-515X",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,2029,2066,,,,,,,,,,,,,,,,WOS:000323367000011,0
J,"Tulabandhula, T; Rudin, C",,,,"Tulabandhula, Theja; Rudin, Cynthia",,,Machine Learning with Operational Costs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work proposes a way to align statistical modeling with decision making. We provide a method that propagates the uncertainty in predictive modeling to the uncertainty in operational cost, where operational cost is the amount spent by the practitioner in solving the problem. The method allows us to explore the range of operational costs associated with the set of reasonable statistical models, so as to provide a useful way for practitioners to understand uncertainty. To do this, the operational cost is cast as a regularization term in a learning algorithm's objective function, allowing either an optimistic or pessimistic view of possible costs, depending on the regularization parameter. From another perspective, if we have prior knowledge about the operational cost, for instance that it should be low, this knowledge can help to restrict the hypothesis space, and can help with generalization. We provide a theoretical generalization bound for this scenario. We also show that learning with operational costs is related to robust optimization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1989,2028,,,,,,,,,,,,,,,,WOS:000323367000010,0
J,"Roussos, A; Theodorakis, S; Pitsikalis, V; Maragos, P",,,,"Roussos, Anastasios; Theodorakis, Stavros; Pitsikalis, Vassilis; Maragos, Petros",,,Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose the novel approach of dynamic affine-invariant shape-appearance model (Aff-SAM) and employ it for handshape classification and sign recognition in sign language (SL) videos. Aff-SAM offers a compact and descriptive representation of hand configurations as well as regularized model-fitting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand's shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by affine transformations, accounting for 3D hand pose changes and improving model's compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an affine signer adaptation component at the visual level, without requiring training from scratch a new singer-specific model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classification when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1627,1663,,,,,,,,,,,,,,,,WOS:000322506400007,0
J,"Vanhatalo, J; Riihimaki, J; Hartikainen, J; Jylanki, P; Tolvanen, V; Vehtari, A",,,,"Vanhatalo, Jarno; Riihimaki, Jaakko; Hartikainen, Jouni; Jylanki, Pasi; Tolvanen, Ville; Vehtari, Aki",,,GPstuff: Bayesian Modeling with Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods.",,,,,"Vanhatalo, Jarno/H-4435-2012","Vanhatalo, Jarno/0000-0002-6831-0211; Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1175,1179,,,,,,,,,,,,,,,,WOS:000318590500013,0
J,"Statnikov, A; Lytkin, NI; Lemeire, J; Aliferis, CF",,,,"Statnikov, Alexander; Lytkin, Nikita I.; Lemeire, Jan; Aliferis, Constantin F.",,,Algorithms for Discovery of Multiple Markov Boundaries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efficient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efficiency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains.",,,,,"Lemeire, Jan/AAJ-6474-2020","Lemeire, Jan/0000-0002-2106-448X",,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,499,566,,,,,,,,,,,25285052,,,,,WOS:000315981900007,0
J,"Alquier, P; Biau, G",,,,"Alquier, Pierre; Biau, Gerard",,,Sparse Single-Index Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Let (X,Y) be a random pair taking values in R-p x R. In the so-called single-index model, one has Y = f(star)(theta X-star T) + W, where f(star) is an unknown univariate measurable function, theta(star) is an unknown vector in R-d, and W denotes a random noise satisfying E[W vertical bar X] = 0. The single-index model is known to offer a flexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations (p larger than n paradigm). To circumvent this difficulty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures.",,,,,,"Alquier, Pierre/0000-0003-4249-7337",,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,243,280,,,,,,,,,,,,,,,,WOS:000314530200008,0
J,"Kloft, M; Laskov, P",,,,"Kloft, Marius; Laskov, Pavel",,,Security Analysis of Online Centroid Anomaly Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Security issues are crucial in a number of machine learning applications, especially in scenarios dealing with human activity rather than natural phenomena (e. g., information ranking, spam detection, malware detection, etc.). In such cases, learning algorithms may have to cope with manipulated data aimed at hampering decision making. Although some previous work addressed the issue of handling malicious data in the context of supervised learning, very little is known about the behavior of anomaly detection methods in such scenarios. In this contribution,(1) we analyze the performance of a particular method-online centroid anomaly detection-in the presence of adversarial noise. Our analysis addresses the following security-related issues: formalization of learning and attack processes, derivation of an optimal attack, and analysis of attack efficiency and limitations. We derive bounds on the effectiveness of a poisoning attack against centroid anomaly detection under different conditions: attacker's full or limited control over the traffic and bounded false positive rate. Our bounds show that whereas a poisoning attack can be effectively staged in the unconstrained case, it can be made arbitrarily difficult (a strict upper bound on the attacker's gain) if external constraints are properly used. Our experimental evaluation, carried out on real traces of HTTP and exploit traffic, confirms the tightness of our theoretical bounds and the practicality of our protection mechanisms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3681,3724,,,,,,,,,,,,,,,,WOS:000314529000008,0
J,"Do, TMT; Artieres, T",,,,"Trinh-Minh-Tri Do; Artieres, Thierry",,,Regularized Bundle Methods for Convex and Non-Convex Risks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Machine learning is most often cast as an optimization problem. Ideally, one expects a convex objective function to rely on efficient convex optimizers with nice guarantees such as no local optima. Yet, non-convexity is very frequent in practice and it may sometimes be inappropriate to look for convexity at any price. Alternatively one can decide not to limit a priori the modeling expressivity to models whose learning may be solved by convex optimization and rely on non-convex optimization algorithms. The main motivation of this work is to provide efficient and scalable algorithms for non-convex optimization. We focus on regularized unconstrained optimization problems which cover a large number of modern machine learning problems such as logistic regression, conditional random fields, large margin estimation, etc. We propose a novel algorithm for minimizing a regularized objective that is able to handle convex and non-convex, smooth and non-smooth risks. The algorithm is based on the cutting plane technique and on the idea of exploiting the regularization term in the objective function. It may be thought as a limited memory extension of convex regularized bundle methods for dealing with convex and non convex risks. In case the risk is convex the algorithm is proved to converge to a stationary solution with accuracy epsilon with a rate O(1/lambda epsilon) where lambda is the regularization parameter of the objective function under the assumption of a Lipschitz empirical risk. In case the risk is not convex getting such a proof is more difficult and requires a stronger and more disputable assumption. Yet we provide experimental results on artificial test problems, and on five standard and difficult machine learning problems that are cast as convex and non-convex optimization problems that show how our algorithm compares well in practice with state of the art optimization algorithms.",,,,,"arti√®res, thierry/E-9155-2019","arti√®res, thierry/0000-0003-3696-0321",,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3539,3583,,,,,,,,,,,,,,,,WOS:000314529000004,0
J,"Nayak, S; Duncan, K; Sarkar, S; Loeding, B",,,,"Nayak, Sunita; Duncan, Kester; Sarkar, Sudeep; Loeding, Barbara",,,Finding Recurrent Patterns from Continuous Sign Language Sentences for Automated Extraction of Signs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences.",,,,,"Sarkar, Sudeep/ABD-7629-2021; Sarkar, Sudeep/A-8213-2009","Sarkar, Sudeep/0000-0001-7332-4207; Sarkar, Sudeep/0000-0001-7332-4207",,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2589,2615,,,,,,,,,,,,,,,,WOS:000309580600004,0
J,"Brunner, C; Fischer, A; Luig, K; Thies, T",,,,"Brunner, Carl; Fischer, Andreas; Luig, Klaus; Thies, Thorsten",,,Pairwise Support Vector Machines and their Application to Large Scale Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pairwise classification is the task to predict whether the examples a, b of a pair (a, b) belong to the same class or to different classes. In particular, interclass generalization problems can be treated in this way. In pairwise classification, the order of the two input examples should not affect the classification result. To achieve this, particular kernels as well as the use of symmetric training sets in the framework of support vector machines were suggested. The paper discusses both approaches in a general way and establishes a strong connection between them. In addition, an efficient implementation is discussed which allows the training of several millions of pairs. The value of these contributions is confirmed by excellent results on the labeled faces in the wild benchmark.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2279,2292,,,,,,,,,,,,,,,,WOS:000308795200002,0
J,"Hanneke, S",,,,"Hanneke, Steve",,,Activized Learning: Transforming Passive to Active with Improved Label Complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the theoretical advantages of active learning over passive learning. Specifically, we prove that, in noise-free classifier learning for VC classes, any passive learning algorithm can be transformed into an active learning algorithm with asymptotically strictly superior label complexity for all nontrivial target functions and distributions. We further provide a general characterization of the magnitudes of these improvements in terms of a novel generalization of the disagreement coefficient. We also extend these results to active learning in the presence of label noise, and find that even under broad classes of noise distributions, we can typically guarantee strict improvements over the known results for passive learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1469,1587,,,,,,,,,,,,,,,,WOS:000305456600007,0
J,"Lawrence, ND",,,,"Lawrence, Neil D.",,,A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new perspective on spectral dimensionality reduction which views these methods as Gaussian Markov random fields (GRFs). Our unifying perspective is based on the maximum entropy principle which is in turn inspired by maximum variance unfolding. The resulting model, which we call maximum entropy unfolding (MEU) is a nonlinear generalization of principal component analysis. We relate the model to Laplacian eigenmaps and isomap. We show that parameter fitting in the locally linear embedding (LLE) is approximate maximum likelihood MEU. We introduce a variant of LLE that performs maximum likelihood exactly: Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the leading spectral approaches on a robot navigation visualization and a human motion capture data set. Finally the maximum likelihood perspective allows us to introduce a new approach to dimensionality reduction based on L1 regularization of the Gaussian random field via the graphical lasso.",,,,,,"Lawrence, Neil/0000-0001-9258-1030",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1609,1638,,,,,,,,,,,,,,,,WOS:000305456600009,0
J,"Nelson, B; Rubinstein, BIP; Huang, L; Joseph, AD; Lee, SJ; Rao, S; Tygar, JD",,,,"Nelson, Blaine; Rubinstein, Benjamin I. P.; Huang, Ling; Joseph, Anthony D.; Lee, Steven J.; Rao, Satish; Tygar, J. D.",,,Query Strategies for Evading Convex-Inducing Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classifiers are often used to detect miscreant activities. We study how an adversary can systematically query a classifier to elicit information that allows the attacker to evade detection while incurring a near-minimal cost of modifying their intended malfeasance. We generalize the theory of Lowd and Meek (2005) to the family of convex-inducing classifiers that partition their feature space into two sets, one of which is convex. We present query algorithms for this family that construct undetected instances of approximately minimal cost using only polynomially-many queries in the dimension of the space and in the level of approximation. Our results demonstrate that near-optimal evasion can be accomplished for this family without reverse engineering the classifier's decision boundary. We also consider general l(p) costs and show that near-optimal evasion on the family of convex-inducing classifiers is generally efficient for both positive and negative convexity for all levels of approximation if p = 1.",,,,,,"JOSEPH, Anthony D./0000-0002-6798-9664",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1293,1332,,,,,,,,,,,,,,,,WOS:000305456600002,0
J,"Kim, Y; Kwon, S; Choi, H",,,,"Kim, Yongdai; Kwon, Sunghoon; Choi, Hosik",,,Consistent Model Selection Criteria on High Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Asymptotic properties of model selection criteria for high-dimensional regression models are studied where the dimension of covariates is much larger than the sample size. Several sufficient conditions for model selection consistency are provided. Non-Gaussian error distributions are considered and it is shown that the maximal number of covariates for model selection consistency depends on the tail behavior of the error distribution. Also, sufficient conditions for model selection consistency are given when the variance of the noise is neither known nor estimated consistently. Results of simulation studies as well as real data analysis are given to illustrate that finite sample performances of consistent model selection criteria can be quite different.",,,,,,"Kwon, Sunghoon/0000-0003-2722-375X",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1037,1057,,,,,,,,,,,,,,,,WOS:000303773100005,0
J,"Zhao, T; Liu, H; Roeder, K; Lafferty, J; Wasserman, L",,,,"Zhao, Tuo; Liu, Han; Roeder, Kathryn; Lafferty, John; Wasserman, Larry",,,The huge Package for High-dimensional Undirected Graph Estimation in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe an R package named huge which provides easy-to-use functions for estimating high dimensional undirected graphs from data. This package implements recent results in the literature, including Friedman et al. (2007), Liu et al. (2009, 2012) and Liu et al. (2010). Compared with the existing graph estimation package glasso, the huge package provides extra features: (1) instead of using Fortan, it is written in C, which makes the code more portable and easier to modify; (2) besides fitting Gaussian graphical models, it also provides functions for fitting high dimensional semiparametric Gaussian copula models; (3) more functions like data-dependent model selection, data generation and graph visualization; (4) a minor convergence problem of the graphical lasso algorithm is corrected; (5) the package allows the user to apply both lossless and lossy screening rules to scale up large-scale problems, making a tradeoff between computational and statistical efficiency.",,,,,"Liu, Han/P-7105-2018","Roeder, Kathryn/0000-0002-8869-6254",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1059,1062,,,,,,,,,,,26834510,,,,,WOS:000303773100006,0
J,"Jylanki, P; Vanhatalo, J; Vehtari, A",,,,"Jylanki, Pasi; Vanhatalo, Jarno; Vehtari, Aki",,,Robust Gaussian Process Regression with a Student-t Likelihood,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the robust and efficient implementation of Gaussian process regression with a Student-t observation model, which has a non-log-concave likelihood. The challenge with the Student-t model is the analytically intractable inference which is why several approximative methods have been proposed. Expectation propagation (EP) has been found to be a very accurate method in many empirical studies but the convergence of EP is known to be problematic with models containing non-log-concave site functions. In this paper we illustrate the situations where standard EP fails to converge and review different modifications and alternative algorithms for improving the convergence. We demonstrate that convergence problems may occur during the type-II maximum a posteriori (MAP) estimation of the hyperparameters and show that standard EP may not converge in the MAP values with some difficult data sets. We present a robust implementation which relies primarily on parallel EP updates and uses a moment-matching-based double-loop algorithm with adaptively selected step size in difficult cases. The predictive performance of EP is compared with Laplace, variational Bayes, and Markov chain Monte Carlo approximations.",,,,,"Vehtari, Aki/A-7584-2008; Vanhatalo, Jarno/H-4435-2012","Vehtari, Aki/0000-0003-2164-9469; Vanhatalo, Jarno/0000-0002-6831-0211",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3227,3257,,,,,,,,,,,,,,,,WOS:000298103700006,0
J,"Bouckaert, R; Hemmecke, R; Lindner, S; Studeny, M",,,,"Bouckaert, Remco; Hemmecke, Raymond; Lindner, Silvia; Studeny, Milan",,,Efficient Algorithms for Conditional Independence Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The topic of the paper is computer testing of (probabilistic) conditional independence (CI) implications by an algebraic method of structural imsets. The basic idea is to transform (sets of) CI statements into certain integral vectors and to verify by a computer the corresponding algebraic relation between the vectors, called the independence implication. We interpret the previous methods for computer testing of this implication from the point of view of polyhedral geometry. However, the main contribution of the paper is a new method, based on linear programming (LP). The new method overcomes the limitation of former methods to the number of involved variables. We recall/describe the theoretical basis for all four methods involved in our computational experiments, whose aim was to compare the efficiency of the algorithms. The experiments show that the LP method is clearly the fastest one. As an example of possible application of such algorithms we show that testing inclusion of Bayesian network structures or whether a CI statement is encoded in an acyclic directed graph can be done by the algebraic method.",,,,,"Studeny, Milan/H-2719-2014","Studeny, Milan/0000-0001-6038-629X",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3453,3479,,,,,,,,,,,,,,,,WOS:000286637200006,0
J,"Honkela, A; Raiko, T; Kuusela, M; Tornio, M; Karhunen, J",,,,"Honkela, Antti; Raiko, Tapani; Kuusela, Mikael; Tornio, Matti; Karhunen, Juha",,,Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.",,,,,"Raiko, Tapani/E-7237-2012","Raiko, Tapani/0000-0002-0321-304X",,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3235,3268,,,,,,,,,,,,,,,,WOS:000285643600008,0
J,"Rosasco, L; Belkin, M; De Vito, E",,,,"Rosasco, Lorenzo; Belkin, Mikhail; De Vito, Ernesto",,,On Learning with Integral Operators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A large number of learning algorithms, for example, spectral clustering, kernel Principal Components Analysis and many manifold methods are based on estimating eigenvalues and eigenfunctions of operators defined by a similarity function or a kernel, given empirical data. Thus for the analysis of algorithms, it is an important problem to be able to assess the quality of such approximations. The contribution of our paper is two-fold: 1. We use a technique based on a concentration inequality for Hilbert spaces to provide new much simplified proofs for a number of results in spectral approximation. 2. Using these methods we provide several new results for estimating spectral properties of the graph Laplacian operator extending and strengthening results from von Luxburg et al. (2008).",,,,,"De Vito, Ernesto/AAX-5125-2021","De Vito, Ernesto/0000-0002-4320-3292; rosasco, lorenzo/0000-0003-3098-383X",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,905,934,,,,,,,,,,,,,,,,WOS:000277186500018,0
J,"Wang, JH; Shen, XT; Pan, W",,,,"Wang, Junhui; Shen, Xiaotong; Pan, Wei",,,On Efficient Large Margin Semisupervised Learning: Method and Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In classification, semisupervised learning usually involves a large amount of unlabeled data with only a small number of labeled data. This imposes a great challenge in that it is difficult to achieve good classification performance through labeled data alone. To leverage unlabeled data for enhancing classification, this article introduces a large margin semisupervised learning method within the framework of regularization, based on an efficient margin loss for unlabeled data, which seeks efficient extraction of the information from unlabeled data for estimating the Bayes decision boundary for classification. For implementation, an iterative scheme is derived through conditional expectations. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method enables to recover the performance of its supervised counterpart based on complete data in rates of convergence, when possible.",,,,,,"WANG, Junhui/0000-0002-9165-5664",,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,719,742,,,,,,,,,,,24678270,,,,,WOS:000270824500007,0
J,"Sabato, S; Shalev-Shwartz, S",,,,"Sabato, Sivan; Shalev-Shwartz, Shai",,,Ranking categorical features using generalization properties,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,20th Annual Conference on Learning Theory,"JUN 13-15, 2007","San Diego, CA","Google,Machine Learning,IBM",,,,"Feature ranking is a fundamental machine learning task with various applications, including feature selection and decision tree learning. We describe and analyze a new feature ranking method that supports categorical features with a large number of possible values. We show that existing ranking criteria rank a feature according to the training error of a predictor based on the feature. This approach can fail when ranking categorical features with many values. We propose the Ginger ranking criterion, that estimates the generalization error of the predictor associated with the Gini index. We show that for almost all training sets, the Ginger criterion produces an accurate estimation of the true generalization error, regardless of the number of values in a categorical feature. We also address the question of finding the optimal predictor that is based on a single categorical feature. It is shown that the predictor associated with the misclassification error criterion has the minimal expected generalization error. We bound the bias of this predictor with respect to the generalization error of the Bayes optimal predictor, and analyze its concentration properties. We demonstrate the efficiency of our approach for feature selection and for learning decision trees in a series of experiments with synthetic and natural data sets.",,,,,"Sabato, Sivan/C-3209-2011; Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1083,1114,,,,,,,,,,,,,,,,WOS:000258646300005,0
J,"Kuzmin, D; Warmuth, MK",,,,"Kuzmin, Dima; Warmuth, Manfred K.",,,Unlabeled compression schemes for maximum classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum concept classes of VC dimension d over n domain points have size n (<=(n)(d)), and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,2047,2081,,,,,,,,,,,,,,,,WOS:000252744600003,0
J,"Fukumizu, K; Bach, FR; Gretton, A",,,,"Fukumizu, Kenji; Bach, Francis R.; Gretton, Arthur",,,Statistical consistency of kernel canonical correlation analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of finite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justification for the method. The proof uses covariance operators defined on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of finite rank to their population counterparts, which can have infinite rank. The result also gives a sufficient condition for convergence on the regularization coefficient involved in kernel CCA: this should decrease as n(-1/3), where n is the number of data.",,,,,,"Gretton, Arthur/0000-0003-3169-7624; Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,361,383,,,,,,,,,,,,,,,,WOS:000247002600008,0
J,"Bratko, A; Cormack, GV; Filipic, B; Lynam, TR; Zupan, B",,,,"Bratko, Andrej; Cormack, Gordon V.; Filipic, Bogdan; Lynam, Thomas R.; Zupan, Blaz",,,Spam filtering using statistical data compression models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Spam filtering poses a special problem in text categorization, of which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. Since spam evolves continuously and most practical applications are based on online user feedback, the task calls for fast, incremental and robust learning algorithms. In this paper, we investigate a novel approach to spam filtering based on adaptive statistical data compression models. The nature of these models allows them to be employed as probabilistic text classifiers based on character-level or binary sequences. By modeling messages as sequences, tokenization and other error-prone preprocessing steps are omitted altogether, resulting in a method that is very robust. The models are also fast to construct and incrementally updateable. We evaluate the filtering performance of two different compression algorithms; dynamic Markov compression and prediction by partial matching. The results of our empirical evaluation indicate that compression models outperform currently established spam filters, as well as a number of methods proposed in previous studies.",,,,,"Zupan, Blaz/L-1595-2019",,,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2673,2698,,,,,,,,,,,,,,,,WOS:000245390800007,0
J,"Abbeel, P; Koller, D; Ng, AY",,,,"Abbeel, Pieter; Koller, Daphne; Ng, Andrew Y.",,,Learning factor graphs in polynomial time and sample complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the computational and sample complexity of parameter and structure learning in graphical models. Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples, assuming that the data is generated by a network in this class. This result covers both parameter estimation for a known network structure and structure learning. It implies as a corollary that we can learn factor graphs for both Bayesian networks and Markov networks of bounded degree, in polynomial time and sample complexity. Importantly, unlike standard maximum likelihood estimation algorithms, our method does not require inference in the underlying network, and so applies to networks where inference is intractable. We also show that the error of our learned model degrades gracefully when the generating distribution is not a member of the target class of networks. In addition to our main result, we show that the sample complexity of parameter learning in graphical models has an O(1) dependence on the number of variables in the model when using the KL-divergence normalized by the number of variables as the performance criterion.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2006,7,,,,,,1743,1788,,,,,,,,,,,,,,,,WOS:000245389200004,0
J,"Taskar, B; Lacoste-Julien, S; Jordan, MI",,,,"Taskar, Ben; Lacoste-Julien, Simon; Jordan, Michael I.",,,"Structured prediction, dual extragradient and Bregman projections",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex flow, depending on the structure of the problem. We show that this approach provides a memory-efficient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.(1)",,,,,"Jordan, Michael I/C-5253-2013",,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1627,1653,,,,,,,,,,,,,,,,WOS:000245388800019,0
J,"Whiteson, S; Stone, P",,,,"Whiteson, Shimon; Stone, Peter",,,Evolutionary function approximation for reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Temporal difference methods are theoretically grounded and empirically effective methods for addressing reinforcement learning problems. In most real-world reinforcement learning tasks, TD methods require a function approximator to represent the value function. However, using function approximators requires manually making crucial representational decisions. This paper investigates evolutionary function approximation, a novel approach to automatically selecting function approximator representations that enable efficient individual learning. This method evolves individuals that are better able to learn. We present a fully implemented instantiation of evolutionary function approximation which combines NEAT, a neuroevolutionary optimization technique, with Q-learning, a popular TD method. The resulting NEAT+Q algorithm automatically discovers effective representations for neural network function approximators. This paper also presents on-line evolutionary computation, which improves the on-line performance of evolutionary computation by borrowing selection mechanisms used in TD methods to choose individual actions and using them in evolutionary computation to select policies for evaluation. We evaluate these contributions with extended empirical studies in two domains: 1) the mountain car task, a standard reinforcement learning benchmark on which neural network function approximators have previously performed poorly and 2) server job scheduling, a large probabilistic domain drawn from the field of autonomic computing. The results demonstrate that evolutionary function approximation can significantly improve the performance of TD methods and on-line evolutionary computation can significantly improve evolutionary methods. This paper also presents additional tests that offer insight into what factors can make neural network function approximation difficult in practice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,877,917,,,,,,,,,,,,,,,,WOS:000240173400007,0
J,"Gunawardana, A; Byrne, W",,,,"Gunawardana, A; Byrne, W",,,Convergence theorems for generalized alternating minimization procedures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models. In cases where these procedures strictly follow the EM formulation, the convergence properties of the estimation procedures are well understood. In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework. We study EM variants in which the E-step is not performed exactly, either to obtain improved rates of convergence, or due to approximations needed to compute statistics under a model family over which E-steps cannot be realized. Since these variants are not EM procedures, the standard ( G) EM convergence results do not apply to them. We present an information geometric framework for describing such algorithms and analyzing their convergence properties. We apply this framework to analyze the convergence properties of incremental EM and variational EM. For incremental EM, we discuss conditions under these algorithms converge in likelihood. For variational EM, we show how the E-step approximation prevents convergence to local maxima in likelihood.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2049,2073,,,,,,,,,,,,,,,,WOS:000236331100007,0
J,"Boulle, M",,,,"Boulle, M",,,A Bayes optimal approach for partitioning the values of categorical attributes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In supervised machine learning, the partitioning of the values (also called grouping) of a categorical attribute aims at constructing a new synthetic attribute which keeps the information of the initial attribute and reduces the number of its values. In this paper, we propose a new grouping method MODL1 founded on a Bayesian approach. The method relies on a model space of grouping models and on a prior distribution defined on this model space. This results in an evaluation criterion of grouping, which is minimal for the most probable grouping given the data, i.e. the Bayes optimal grouping. We propose new super-linear optimization heuristics that yields near-optimal groupings. Extensive comparative experiments demonstrate that the MODL grouping method builds high quality groupings in terms of predictive quality, robustness and small number of groups.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1431,1452,,,,,,,,,,,,,,,,WOS:000236330100007,0
J,"Ziehe, A; Kawanabe, M; Harmeling, S",,,,"Ziehe, A; Kawanabe, M; Harmeling, S",,,Blind separation of post-nonlinear mixtures using linearizing transformations and temporal decorrelation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose two methods that reduce the post-nonlinear blind source separation problem (PNL-BSS) to a linear BSS problem. The first method is based on the concept of maximal correlation: we apply the alternating conditional expectation (ACE) algorithm-a powerful technique from nonparametric statistics-to approximately invert the componentwise nonlinear functions. The second method is a Gaussianizing transformation, which is motivated by the fact that linearly mixed signals before nonlinear transformation are approximately Gaussian distributed. This heuristic, but simple and efficient procedure works as good as the ACE method. Using the framework provided by ACE, convergence can be proven. The optimal transformations obtained by ACE coincide with the sought-after inverse functions of the nonlinearitics. After equalizing the nonlinearities, temporal decorrelation separation (TDSEP) allows us to recover the source signals. Numerical simulations testing ACE-TD and Gauss-TD on realistic examples are performed with excellent results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1319,1338,,,,,,,,,,,,,,,,WOS:000224808300008,0
J,"Cussens, J; Frisch, AM",,,,"Cussens, J; Frisch, AM",,,Introduction to the special issue on inductive logic programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,"Frisch, Alan Mark/0000-0001-5015-2676; Cussens, James/0000-0002-1363-2336",,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,413,414,,10.1162/153244304773935992,0,,,,,,,,,,,,,WOS:000221345700001,0
J,"Baram, Y; El-Yaniv, R; Luz, K",,,,"Baram, Y; El-Yaniv, R; Luz, K",,,Online choice of active learning algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work is concerned with the question of how to combine online an ensemble of active learners so as to expedite the learning progress in pool-based active learning. We develop an active-learning master algorithm, based on a known competitive algorithm for the multi-armed bandit problem. A major challenge in successfully choosing top performing active learners online is to reliably estimate their progress during the learning session. To this end we propose a simple maximum entropy criterion that provides effective estimates in realistic settings. We study the performance of the proposed master algorithm using an ensemble containing two of the best known active-learning algorithms as well as a new algorithm. The resulting active-learning master algorithm is empirically shown to consistently perform almost as well as and sometimes outperform the best algorithm in the ensemble on a range of classification problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2004,5,,,,,,255,291,,,,,,,,,,,,,,,,WOS:000236327200003,0
J,"Cristianini, N; Shawe-Taylor, J; Williamson, RC",,,,"Cristianini, N; Shawe-Taylor, J; Williamson, RC",,,Introduction to the special issue on Kernel Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,95,96,,10.1162/15324430260185547,0,,,,,,,,,,,,,WOS:000176055300001,0
J,"Birrell, J; Dupuis, P; Katsoulakis, MA; Pantazis, Y; Rey-Bellet, L",,,,"Birrell, Jeremiah; Dupuis, Paul; Katsoulakis, Markos A.; Pantazis, Yannis; Rey-Bellet, Luc",,,"(f, Gamma)-Diyergences: Interpolating between f-Divergences and Integral Probability Metrics",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a rigorous and general framework for constructing information-theoretic divergences that subsume both f-divergences and integral probability metrics (IPMs), such as the 1-Wasserstein distance. We prove under which assumptions these divergences, hereafter referred to as (f, Gamma)-divergences, provide a notion of 'distance' between probability measures and show that they can be expressed as a two-stage mass-redistribution/mass-transport process. The (f, Gamma)-divergences inherit features from IPMs, such as the ability to compare distributions which are not absolutely continuous, as well as from f-divergences, namely the strict concavity of their variational representations and the ability to control heavy-tailed distributions for particular choices of f . When combined, these features establish a divergence with improved properties for estimation, statistical learning, and uncertainty quantification applications. Using statistical learning as an example, we demonstrate their advantage in training generative adversarial networks (GANs) for heavy-tailed, not-absolutely continuous sample distributions. We also show improved performance and stability over gradient-penalized Wasserstein GAN in image generation.",,,,,,"Katsoulakis, Markos/0000-0003-4354-1766",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,70,,,,,,,,,,,,,,,,WOS:000752284100001,0
J,"Lindauer, M; Eggensperger, K; Feurer, M; Biedenkapp, A; Deng, DF; Benjamins, C; Ruhkopf, T; Sass, R; Hutter, F",,,,"Lindauer, Marius; Eggensperger, Katharina; Feurer, Matthias; Biedenkapp, Andre; Deng, Difan; Benjamins, Carolin; Ruhkopf, Tim; Sass, Rene; Hutter, Frank",,,SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3.",,,,,,"Lindauer, Marius/0000-0002-9675-3175",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752288000001,0
J,"Bing, X; Bunea, F; Strimas-Mackey, S; Wegkamp, M",,,,"Bing, Xin; Bunea, Florentina; Strimas-Mackey, Seth; Wegkamp, Marten",,,"Prediction Under Latent Factor Regression: Adaptive PCR, Interpolating Predictors and Beyond",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work is devoted to the finite sample prediction risk analysis of a class of linear predictors of a response Y is an element of R from a high-dimensional random vector X is an element of R-P when (X, Y ) follows a latent factor regression model generated by a unobservable latent vector Z of dimension less than p. Our primary contribution is in establishing finite sample risk bounds for prediction with the ubiquitous Principal Component Regression (PCR) method, under the factor regression model, with the number of principal components adaptively selected from the data-a form of theoretical guarantee that is surprisingly lacking from the PCR literature. To accomplish this, we prove a master theorem that establishes a risk bound for a large class of predictors, including the PCR predictor as a special case. This approach has the benefit of providing a unified framework for the analysis of a wide range of linear prediction methods, under the factor regression setting. In particular, we use our main theorem to recover known risk bounds for the minimum-norm interpolating predictor, which has received renewed attention in the past two years, and a prediction method tailored to a subclass of factor regression models with identifiable parameters. This model-tailored method can be interpreted as prediction via clusters with latent centers. To address the problem of selecting among a set of candidate predictors, we analyze a simple model selection procedure based on data-splitting, providing an oracle inequality under the factor model to prove that the performance of the selected predictor is close to the optimal candidate. We conclude with a detailed simulation study to support and complement our theoretical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700313900001,0
J,"Cucuringu, M; Singh, AV; Sulem, D; Tyagi, H",,,,"Cucuringu, Mihai; Singh, Apoorv Vikram; Sulem, Deborah; Tyagi, Hemant",,,Regularized spectral methods for clustering signed networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of k-way clustering in signed graphs. Considerable attention in recent years has been devoted to analyzing and modeling signed graphs, where the affinity measure between nodes takes either positive or negative values. Recently, Cucuringu et al. (2019) proposed a spectral method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem), which casts the clustering task as a generalized eigenvalue problem optimizing a suitably defined objective function. This approach is motivated by social balance theory, where the clustering task aims to decompose a given network into disjoint groups, such that individuals within the same group are connected by as many positive edges as possible, while individuals from different groups are mainly connected by negative edges. Through extensive numerical experiments, SPONGE was shown to achieve state-of-the-art empirical performance. On the theoretical front, Cucuringu et al. (2019) analyzed SPONGE, as well as the popular Signed Laplacian based spectral method under the setting of a Signed Stochastic Block Model, for k = 2 equal-sized clusters, in the regime where the graph is moderately dense. In this work, we build on the results in Cucuringu et al. (2019) on two fronts for the normalized versions of SPONGE and the Signed Laplacian. Firstly, for both algorithms, we extend the theoretical analysis in Cucuringu et al. (2019) to the general setting of k >= 2 unequal-sized clusters in the moderately dense regime. Secondly, we introduce regularized versions of both methods to handle sparse graphs - a regime where standard spectral methods are known to underperform - and provide theoretical guarantees under the same setting of a Signed Stochastic Block Model. To the best of our knowledge, regularized spectral methods have so far not been considered in the setting of clustering signed graphs. We complement our theoretical results with an extensive set of numerical experiments on synthetic data, and three real world data sets standard in the signed networks literature.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,79,,,,,,,,,,,,,,,,WOS:000726710900001,0
J,"Dedieu, A; Hazimeh, H; Mazumder, R",,,,"Dedieu, Antoine; Hazimeh, Hussein; Mazumder, Rahul",,,Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a discrete optimization formulation for learning sparse classifiers, where the outcome depends upon a linear combination of a small subset of features. Recent work has shown that mixed integer programming (MIP) can be used to solve (to optimality) l(0)-regularized regression problems at scales much larger than what was conventionally considered possible. Despite their usefulness, MIP-based global optimization approaches are significantly slower than the relatively mature algorithms for l(1)-regularization and heuristics for nonconvex regularized problems. We aim to bridge this gap in computation times by developing new MIP-based algorithms for l(0)-regularized classification. We propose two classes of scalable algorithms: an exact algorithm that can handle p approximate to 50, 000 features in a few minutes, and approximate algorithms that can address instances with p approximate to 10(6) in times comparable to the fast l(1)-based algorithms. Our exact algorithm is based on the novel idea of integrality generation, which solves the original problem (with p binary variables) via a sequence of mixed integer programs that involve a small number of binary variables. Our approximate algorithms are based on coordinate descent and local combinatorial search. In addition, we present new estimation error bounds for a class of l(0)-regularized estimators. Experiments on real and synthetic data demonstrate that our approach leads to models with considerably improved statistical performance (especially variable selection) compared to competing methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687037100001,0
J,"Xu, ZQ; Li, P",,,,"Xu, Zhiqiang; Li, Ping",,,On the Riemannian Search for Eigenvector Computation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Eigenvector computation is central to numerical algebra and often critical to many data analysis tasks nowadays. Most research on this problem has been focusing on projection methods like power iterations, such that this category of algorithms can achieve both optimal convergence rates and cheap per-iteration costs. In contrast, search methods belonging to another main category are less understood in this respect. In this work, we consider the leading eigenvector computation as a non-convex optimization problem on the (generalized) Stiefel manifold and covers the cases for both standard and generalized eigenvectors. It is shown that the inexact Riemannian gradient method induced by the shift-and-invert preconditioning is guaranteed to converge to one of the ground-truth eigenvectors at an optimal rate, e.g., O(root kappa(B) lambda(1)/lambda(1)-lambda(p+1) log 1/epsilon) for a pair of real symmetric matrices (A, B) with B being positive definite, where A represents the i-th largest generalized eigenvalue of the matrix pair, p is the multiplicity of lambda(1) , and kappa(B) stands for the condition number of B. The standard eigenvector computation is recovered by setting B to an identity matrix. Our analysis reduces the dependence on the eigengap, making it the first Riemannian eigensolver that achieves the optimal rate. Experiments demonstrate that the proposed search method is able to deliver significantly better performance than projection methods by taking advantages of step-size schemes.",,,,,"Xu, Zhiqiang/AAB-7414-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000726709500001,0
J,"Bertsimas, D; Li, ML",,,,"Bertsimas, Dimitris; Li, Michael Lingzhi",,,Fast Exact Matrix Completion: A Unified Optimization Framework for Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We formulate the problem of matrix completion with and without side information as a non-convex optimization problem. We design fastImpute based on non-convex gradient descent and show it converges to a global minimum that is guaranteed to recover closely the underlying matrix while it scales to matrices of sizes beyond 10(5) x 10(5). We report experiments on both synthetic and real-world datasets that show fastImpute is competitive in both the accuracy of the matrix recovered and the time needed across all cases. Furthermore, when a high number of entries are missing, fastImpute is over 75% lower in MAPE and 15 times faster than current state-of-the-art matrix completion methods in both the case with side information and without.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,231,,,,,,,,,,,,,,,WOS:000605749800001,0
J,"Fehrman, B; Gess, B; Jentzen, A",,,,"Fehrman, Benjamin; Gess, Benjamin; Jentzen, Arnulf",,,Convergence Rates for the Stochastic Gradient Descent Method for Non-Convex Objective Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We prove the convergence to minima and estimates on the rate of convergence for the stochastic gradient descent method in the case of not necessarily locally convex nor contracting objective functions. In particular, the analysis relies on a quantitative use of mini-batches to control the loss of iterates to non-attracted regions. The applicability of the results to simple objective functions arising in machine learning is shown.",,,,,"Jentzen, Arnulf/O-8237-2016","Jentzen, Arnulf/0000-0002-9840-3339",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,136,,,,,,,,,,,,,,,WOS:000556301700001,0
J,"Kamper, F; Steel, SJ; du Preez, JA",,,,"Kamper, Francois; Steel, Sarel J.; du Preez, Johan A.",,,Regularized Gaussian Belief Propagation with Nodes of Arbitrary Size,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian belief propagation (GaBP) is a message-passing algorithm that can be used to perform approximate inference on a pairwise Markov graph (MG) constructed from a multivariate Gaussian distribution in canonical parameterization. The output of GaBP is a set of approximate univariate marginals for each variable in the pairwise MG. An extension of GaBP (labeled GaBP-m), allowing for the approximation of higher-dimensional marginal distributions, was explored by Kamper et al. (2019). The idea is to create an MG in which each node is allowed to receive more than one variable. As in the univariate case, the multivariate extension does not necessarily converge in loopy graphs and, even if convergence occurs, is not guaranteed to provide exact inference. To address the problem of convergence, we consider a multivariate extension of the principle of node regularization proposed by Kamper et al. (2018). We label this algorithm slow GaBP-m (sGaBP-m), where the term slow relates to the damping effect of the regularization on the message passing. We prove that, given sufficient regularization, this algorithm will converge and provide the exact marginal means at convergence, regardless of the way variables are assigned to nodes. The selection of the degree of regularization is addressed through the use of a heuristic, which is based on a tree representation of sGaBP-m. As a further contribution, we extend other GaBP variants in the literature to allow for higher-dimensional marginalization. We show that our algorithm compares favorably with these variants, both in terms of convergence speed and inference quality.",,,,,,"Kamper, Francois/0000-0001-9460-5219",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,101,,,,,,,,,,,,,,,WOS:000546144300001,0
J,"Lehnert, L; Littman, ML",,,,"Lehnert, Lucas; Littman, Michael L.",,,Successor Features Combine Elements of Model-Free and Model-based Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between model-based and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,196,,,,,,,,,,,,,,,WOS:000590002700001,0
J,"Nolan, TH; Menictas, M; Wand, MP",,,,"Nolan, Tui H.; Menictas, Marianne; Wand, Matt P.",,,Streamlined Computing for Variational Inference with Higher Level Random Effects,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive and present explicit algorithms to facilitate streamlined computing for variational inference for models containing higher level random effects. Existing literature, such as Lee and Wand (2016), is such that streamlined variational inference is restricted to mean field variational Bayes algorithms for two-level random effects models. Here we provide the following extensions: (1) explicit Gaussian response mean field variational Bayes algorithms for three-level models, (2) explicit algorithms for the alternative variational message passing approach in the case of two-level and three-level models, and (3) an explanation of how arbitrarily high levels of nesting can be handled based on the recently published matrix algebraic results of the authors. A pay-off from (2) is simple extension to non-Gaussian response models. In summary, we remove barriers for streamlining variational inference algorithms based on either the mean field variational Bayes approach or the variational message passing approach when higher level random effects are present.",,,,,"Wand, Matt/F-9413-2012","Wand, Matt/0000-0003-2555-896X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,157,,,,,,,,,,,,,,,WOS:000570146700001,0
J,"Novikov, A; Izmailov, P; Khrulkov, V; Figurnov, M; Oseledets, I",,,,"Novikov, Alexander; Izmailov, Pavel; Khrulkov, Valentin; Figurnov, Michael; Oseledets, Ivan",,,Tensor Train Decomposition on TensorFlow (T3F),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tensor Train decomposition is used across many branches of machine learning. We present T3F a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000005,0
J,"Bubeck, S; Devanur, NR; Huang, ZY; Niazadeh, R",,,,"Bubeck, Sebastien; Devanur, Nikhil R.; Huang, Zhiyi; Niazadeh, Rad",,,Multi-scale Online Learning: Theory and Applications to Online Auctions and Pricing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider revenue maximization in online auction/pricing problems. A seller sells an identical item in each period to a new buyer, or a new set of buyers. For the online pricing problem, both when the arriving buyer bids or only responds to the posted price, we design algorithms whose regret bounds scale with the best fixed price in-hindsight, rather than the range of the values. Under the bidding model, we further show our algorithms achieve a revenue convergence rate that matches the offline sample complexity of the single-item single-buyer auction. We also show regret bounds that are scale free, and match the offline sample complexity, when comparing to a benchmark that requires a lower bound on the market share. We further expand our results beyond pricing to multi-buyer auctions, and obtain online learning algorithms for auctions, with convergence rates matching the known sample complexity upper bound of online single-item multi-buyer auctions. These results are obtained by generalizing the classical learning from experts and multiarmed bandit problems to their multi-scale versions. In this version, the reward of each action is in a different range, and the regret with respect to a given action scales with its own range, rather than the maximum range. We obtain almost optimal multi-scale regret bounds by introducing a new Online Mirror Descent (OMD) algorithm whose mirror map is the multi-scale version of the negative entropy function. We further generalize to the bandit setting by introducing the stochastic variant of this OMD algorithm.",,,,,"Huang, Zhiyi/I-7976-2013","Huang, Zhiyi/0000-0003-2963-9556",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,1,37,62,,,,,,,,,,,,,,,WOS:000467879600001,0
J,"Ge, AO; Li, XG; Jiang, HM; Liu, H; Zhang, T; Wang, MD; Zhao, T",,,,"Ge, Jason; Li, Xingguo; Jiang, Haoming; Liu, Han; Zhang, Tong; Wang, Mengdi; Zhao, Tuo",,,Picasso: A Sparse Learning Library for High Dimensional Data Analysis in R and Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a new library named picasso (1), which implements a unified framework of pathwise coordinate optimization for a variety of sparse learning problems (e.g., sparse linear regression, sparse logistic regression, sparse Poisson regression and scaled sparse linear regression) combined with efficient active set selection strategies. Besides, the library allows users to choose different sparsity-inducing regularizers, including the convex L-1, nonvoncex MCP and SCAD regularizers. The library is coded in C++ and has user-friendly R and Python wrappers. Numerical experiments demonstrate that picasso can scale up to large problems efficiently.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,44,,,,,,,,,,,,,,,WOS:000463320300001,0
J,"Kaul, A; Jandhyala, VK; Fotopoulos, SB",,,,"Kaul, Abhishek; Jandhyala, Venkata K.; Fotopoulos, Stergios B.",,,An Efficient Two Step Algorithm for High Dimensional Change Point Regression Models Without Grid Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a two step algorithm based on l(1)/l(0) regularization for the detection and estimation of parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the change point as well as the regression parameter estimates. Importantly, the computational cost of our estimator is only 2.Lasso(n, p), where Lasso(n, p) represents the computational burden of one Lasso optimization in a model of size (n, p). In comparison, existing grid search based approaches to this problem require a computational cost of at least n.Lasso(n, p) optimizations. Additionally, the proposed method is shown to be able to consistently detect the case of 'no change', i.e., where no finite change point exists in the model. We allow the true change point parameter tau(0) to possibly move to the boundaries of its parametric space, and the jump size parallel to beta(0) - gamma(0)parallel to(2) to possibly diverge as n increases. We then characterize the corresponding effects on the rates of convergence of the change point and regression estimates. In particular, we show that, while an increasing jump size may have a beneficial effect on the change point estimate, however the optimal rate of regression parameter estimates are preserved only upto a certain rate of the increasing jump size. This behavior in the rate of regression parameter estimates is unique to high dimensional change point regression models only. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.",,,,,"Kaul, Abhishek/AAX-3836-2020",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,111,,,,,,,,,,,,,,,WOS:000476624100001,0
J,"Kralj, J; Robnik-Sikonja, M; Lavrac, N",,,,"Kralj, Jan; Robnik-Sikonja, Marko; Lavrac, Nada",,,NetSDM: Semantic Data Mining with Network Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Semantic data mining (SDM) is a form of relational data mining that uses annotated data together with complex semantic background knowledge to learn rules that can be easily interpreted. The drawback of SDM is a high computational complexity of existing SDM algorithms, resulting in long run times even when applied to relatively small data sets. This paper proposes an effective SDM approach, named NetSDM, which first transforms the available semantic background knowledge into a network format, followed by network analysis based node ranking and pruning to significantly reduce the size of the original background knowledge. The experimental evaluation of the NetSDM methodology on acute lymphoblastic leukemia and breast cancer data demonstrates that NetSDM achieves radical time efficiency improvements and that learned rules are comparable or better than the rules obtained by the original SDM algorithms.",,,,,"Robnik-≈†ikonja, Marko/AAG-6690-2020","Robnik-≈†ikonja, Marko/0000-0002-1232-3320",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,32,,,,,,,,,,,,,,,WOS:000463316600001,0
J,"Livezey, JA; Bujan, AF; Sommer, FT",,,,"Livezey, Jesse A.; Bujan, Alejandro F.; Sommer, Friedrich T.",,,"Learning Overcomplete, Low Coherence Dictionaries with Linear Inference",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finding overcomplete latent representations of data has applications in data analysis, signal processing, machine learning, theoretical neuroscience and many other fields. In an overcomplete representation, the number of latent features exceeds the data dimensionality, which is useful when the data is undersampled by the measurements (compressed sensing or information bottlenecks in neural systems) or composed from multiple complete sets of linear features, each spanning the data space. Independent Components Analysis (ICA) is a linear technique for learning sparse latent representations, which typically has a lower computational cost than sparse coding, a linear generative model which requires an iterative, nonlinear inference step. While well suited for finding complete representations, we show that overcompleteness poses a challenge to existing ICA algorithms. Specifically, the coherence control used in existing ICA and other dictionary learning algorithms, necessary to prevent the formation of duplicate dictionary features, is ill-suited in the overcomplete case. We show that in the overcomplete case, several existing ICA algorithms have undesirable global minima that maximize coherence. We provide a theoretical explanation of these failures and, based on the theory, propose improved coherence control costs for overcomplete ICA algorithms. Further, by comparing ICA algorithms to the computationally more expensive sparse coding on synthetic data, we show that the limited applicability of overcomplete, linear inference can be extended with the proposed cost functions. Finally, when trained on natural images, we show that the coherence control biases the exploration of the data manifold, sometimes yielding suboptimal, coherent solutions. All told, this study contributes new insights into and methods for coherence control for linear ICA, some of which are applicable to many other nonlinear models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,174,,,,,,,,,,,,,,,WOS:000506403100014,0
J,"Soh, D; Tatikonda, S",,,,"Soh, De Wen; Tatikonda, Sekhar",,,Learning Unfaithful K-separable Gaussian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set S graph separates nodes u and v then X-u is conditionally independent of X-v given X-S. The opposite direction need not be true, that is, X-u perpendicular to X-v vertical bar X-S need not imply S is a node separator of u and v. When it does, the relation X-u perpendicular to X-v vertical bar X-S is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form X-i perpendicular to X-j vertical bar X-S. We study two classes of separable Gaussian graphical models, namely, weakly K-separable and strongly K- separable Gaussian graphical models. Using the above test for faithfulness, we introduce algorithms to learn the topologies of weakly K-separable and strongly K-separable Gaussian graphical models with Omega (K log p) sample complexity. For strongly K-separable Gaussian graphical models, we additionally provide a method with error bounds for learning the off-diagonal precision matrix entries.",,,,,"Soh, De Wen/AFR-7227-2022","Soh, De Wen/0000-0001-6490-6467",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,109,,,,,,,,,,,,,,,WOS:000476623700001,0
J,"Sonoda, S; Murata, N",,,,"Sonoda, Sho; Murata, Noboru",,,Transport Analysis of Infinitely Deep Neural Network,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigated the feature map inside deep neural networks (DNNs) by tracking the transport map. We are interested in the role of depth - why do DNNs perform better than shallow models? - and the interpretation of DNNs- what do intermediate layers do? Despite the rapid development in their application, DNNs remain analytically unexplained because the hidden layers are nested and the parameters are not faithful. Inspired by the integral representation of shallow NNs, which is the continuum limit of the width, or the hidden unit number, we developed the flow representation and transport analysis of DNNs. The flow representation is the continuum limit of the depth, or the hidden layer number, and it is specified by an ordinary differential equation ( ODE) with a vector field. We interpret an ordinary DNN as a transport map or an Euler broken line approximation of the flow. Technically speaking, a dynamical system is a natural model for the nested feature maps. In addition, it opens a new way to the coordinate-free treatment of DNNs by avoiding the redundant parametrization of DNNs. Following Wasserstein geometry, we analyze a flow in three aspects: dynamical system, continuity equation, and Wasserstein gradient flow. A key finding is that we specified a series of transport maps of the denoising autoencoder (DAE), which is a cornerstone for the development of deep learning. Starting from the shallow DAE, this paper develops three topics: the transport map of the deep DAE, the equivalence between the stacked DAE and the composition of DAEs, and the development of the double continuum limit or the integral representation of the flow representation. As partial answers to the research questions, we found that deeper DAEs converge faster and the extracted features are better; in addition, a deep Gaussian DAE transports mass to decrease the Shannon entropy of the data distribution. We expect that further investigations on these questions lead to the development of an interpretable and principled alternatives to DNNs.",,,,,"Murata, Noboru/J-3345-2012","Murata, Noboru/0000-0002-4258-6877",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,2,,,,,,,,,,,,,,,WOS:000458661500001,0
J,"Chazal, F; Fasy, B; Lecci, F; Michel, B; Rinaldo, A; Wasserman, L",,,,"Chazal, Frederic; Fasy, Brittany; Lecci, Fabrizio; Michel, Bertrand; Rinaldo, Alessandro; Wasserman, Larry",,,Robust Topological Inference: Distance To a Measure and Kernel Distance,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Let P be a distribution with support S. The salient features of S can be quantified with persistent homology, which summarizes topological features of the sublevel sets of the distance function (the distance of any point x to S). Given a sample from P we can infer the persistent homology using an empirical version of the distance function. However, the empirical distance function is highly non-robust to noise and outliers. Even one outlier is deadly. The distance-to-a-measure (DTM), introduced by Chazal et al. (2011), and the kernel distance, introduced by Phillips et al. (2014), are smooth functions that provide useful topological information but are robust to noise and outliers. Chazal et al. (2015) derived concentration bounds for DTM. Building on these results, we derive limiting distributions and confidence sets, and we propose a method for choosing tuning parameters.",,,,,,"Fasy, Brittany Terese/0000-0003-1908-0154",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,159,,,,,,,,,,,,,,,WOS:000433252400001,0
J,"Filice, S; Castellucci, G; Da San Martino, G; Moschitti, A; Croce, D; Basili, R",,,,"Filice, Simone; Castellucci, Giuseppe; Da San Martino, Giovanni; Moschitti, Alessandro; Croce, Danilo; Basili, Roberto",,,KeLP: a Kernel-based Learning Platform,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"KELP is a Java framework that enables fast and easy implementation of kernel functions over discrete data, such as strings, trees or graphs and their combination with standard vectorial kernels. Additionally, it provides several kernel-based algorithms, e.g., online and batch kernel machines for classification, regression and clustering, and a Java environment for easy implementation of new algorithms. KELP is a versatile toolkit, very appealing both to experts and practitioners of machine learning and Java language programming, who can find extensive documentation, tutorials and examples of increasing complexity on the accompanying website. Interestingly, KELP can be also used without any knowledge of Java programming through command line tools and JSON/XML interfaces enabling the declaration and instantiation of articulated learning models using simple templates. Finally, the extensive use of modularity and interfaces in KELP enables developers to easily extend it with their own kernels and algorithms.",,,,,,"Da San Martino, Giovanni/0000-0002-2609-483X",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,5,,,,,,,,,,,,,,,WOS:000435453000001,0
J,"Giordano, R; Broderick, T; Jordan, MI",,,,"Giordano, Ryan; Broderick, Tamara; Jordan, Michael I.",,,"Covariances, Robustness, and Variational Bayes",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior inference technique that is increasingly popular due to its fast runtimes on large-scale data sets. However, even when MFVB provides accurate posterior means for certain parameters, it often mis-estimates variances and covariances. Furthermore, prior robustness measures have remained undeveloped for MFVB. By deriving a simple formula for the effect of infinitesimal model perturbations on MFVB posterior means, we provide both improved covariance estimates and local robustness measures for MFVB, thus greatly expanding the practical usefulness of MFVB posterior approximations. The estimates for MFVB posterior covariances rely on a result from the classical Bayesian robustness literature that relates derivatives of posterior expectations to posterior covariances and includes the Laplace approximation as a special case. Our key condition is that the MFVB approximation provides good estimates of a select subset of posterior means-an assumption that has been shown to hold in many practical settings. In our experiments, we demonstrate that our methods are simple, general, and fast, providing accurate posterior uncertainty estimates and robustness measures with runtimes that can be an order of magnitude faster than MCMC.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,51,,,,,,,,,,,,,,,WOS:000448376200001,0
J,"Palowitch, J; Bhamidi, S; Nobel, AB",,,,"Palowitch, John; Bhamidi, Shankar; Nobel, Andrew B.",,,Significance-based community detection in weighted networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant background nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro-features of the corresponding systems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,188,,,,,,,,,,30853860,,,,,WOS:000435449900001,0
J,"Yang, ZR; Ning, Y; Liu, H",,,,"Yang, Zhuoran; Ning, Yang; Liu, Han",,,On Semiparametric Exponential Family Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we allow the nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, one advantage of our method is that it is unnecessary to specify the type of each node and the method is more convenient to apply in practice. Under the proposed model, we consider both problems of parameter estimation and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise score test for the presence of a single edge in the graph. Compared to the existing methods for hypothesis tests, our approach takes into account of the symmetry of the parameters, such that the inferential results are invariant with respect to the different parametrizations of the same edge. Thorough numerical simulations and a real data example are provided to back up our theoretical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,57,,,,,,,,,,,,,,,WOS:000452043400001,0
J,"Yu, HZ; Mahmood, AR; Sutton, RS",,,,"Yu, Huizhen; Mahmood, A. Rupam; Sutton, Richard S.",,,On Generalized Bellman Equations and Temporal-Difference Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider off-policy temporal-difference (TD) learning in discounted Markov decision processes, where the goal is to evaluate a policy in a model-free way by using observations of a state process generated without executing the policy. To curb the high variance issue in off-policy TD learning, we propose a new scheme of setting the lambda-parameters of TD, based on generalized Bellman equations. Our scheme is to set lambda according to the eligibility trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded range. Compared with prior work, this scheme is more direct and flexible, and allows much larger lambda values for off-policy TD learning with bounded traces. As to its soundness, using Markov chain theory, we prove the ergodicity of the joint state-trace process under nonrestrictive conditions, and we show that associated with our scheme is a generalized Bellman equation (for the policy to be evaluated) that depends on both the evolution of lambda and the unique invariant probability measure of the state-trace process. These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme, but also prepare the ground for further analysis of gradient-based implementations.",,,,,,"Yu, Huizhen/0000-0002-3673-0094",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,48,,,,,,,,,,,,,,,WOS:000448369400001,0
J,"Ito, N; Takeda, A; Toh, KC",,,,"Ito, Naoki; Takeda, Akiko; Toh, Kim-Chuan",,,A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including C-SVM, l(2)-SVM, nu-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM).",,,,,"Toh, Kim-Chuan/A-6068-2010","Ito, Naoki/0000-0002-1998-0606",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,16,,,,,,,,,,,,,,,WOS:000399838500001,0
J,"McDonald, DJ; Shalizi, CR; Schervish, M",,,,"McDonald, Daniel J.; Shalizi, Cosma Rohila; Schervish, Mark",,,Nonparametric Risk Bounds for Time-Series Forecasting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive generalization error bounds for traditional time-series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,40,,,,,,,,,,,,,,,,WOS:000400516800001,0
J,"Papanikolaou, Y; Foulds, JR; Rubin, TN; Tsoumakas, G",,,,"Papanikolaou, Yannis; Foulds, James R.; Rubin, Timothy N.; Tsoumakas, Grigorios",,,Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVBO) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVBO inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so.",,,,,"Tsoumakas, Grigorios/B-4718-2008","Tsoumakas, Grigorios/0000-0002-7879-669X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,58,62,,,,,,,,,,,,,,,WOS:000405991900001,0
J,"Raymond, J; Ricci-Tersenghi, F",,,,"Raymond, Jack; Ricci-Tersenghi, Federico",,,Improving Variational Methods via Pairwise Linear Response Identities,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inference methods are often formulated as variational approximations: these approximations allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima.",,,,,,"Ricci-Tersenghi, Federico/0000-0003-4970-7376",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,6,,,,,,,,,,,,,,,WOS:000397019200001,0
J,"Serra, P; Mandjes, M",,,,"Serra, Paulo; Mandjes, Michel",,,Dimension Estimation Using Random Connection Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information about intrinsic dimension is crucial to perform dimensionality reduction, compress information, design efficient algorithms, and do statistical adaptation. In this paper we propose an estimator for the intrinsic dimension of a data set. The estimator is based on binary neighbourhood information about the observations in the form of two adjacency matrices, and does not require any explicit distance information. The underlying graph is modelled according to a subset of a specific random connection model, sometimes referred to as the Poisson blob model. Computationally the estimator scales like n log n, and we specify its asymptotic distribution and rate of convergence. A simulation study on both real and simulated data shows that our approach compares favourably with some competing methods from the literature, including approaches that rely on distance information.",,,,,,"de Andrade Serra, Paulo Jorge/0000-0002-4869-4043",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,138,,,,,,,,,,,,,,,WOS:000424547800001,0
J,"Asbeh, N; Lerner, B",,,,"Asbeh, Nuaman; Lerner, Boaz",,,Learning Latent Variable Models by Pairwise Cluster Comparison Part I - Theory and Overview,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Identification of latent variables that govern a problem and the relationships among them, given measurements in the observed world, are important for causal discovery. This identification can be accomplished by analyzing the constraints imposed by the latents in the measurements. We introduce the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and provide a two-stage algorithm called learning PCC (LPCC) that learns a latent variable model (LVM) using PCC. First, LPCC learns exogenous latents and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Since in this first stage LPCC cannot distinguish endogenous latent non-colliders from their exogenous ancestors, a second stage is needed to extract the former, with their observed children, from the latter. If the true graph has no serial connections, LPCC returns the true graph, and if the true graph has a serial connection, LPCC returns a pattern of the true graph. LPCC's most important advantage is that it is not limited to linear or latent-tree models and makes only mild assumptions about the distribution. The paper is divided in two parts: Part I (this paper) provides the necessary preliminaries, theoretical foundation to PCC, and an overview of LPCC; Part II formally introduces the LPCC algorithm and experimentally evaluates its merit in different synthetic and real domains. The code for the LPCC algorithm and data sets used in the experiments reported in Part II are available online.",,,,,"LERNER, BOAZ/AAJ-4064-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,224,,,,,,,,,,,,,,,WOS:000391914500001,0
J,"Doran, G; Ray, S",,,,"Doran, Gary; Ray, Soumya",,,Multiple-Instance Learning from Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new theoretical framework for analyzing the multiple-instance learning (MIL) setting. In MIL, training examples are provided to a learning algorithm in the form of labeled sets, or bags, of instances. Applications of MIL include 3-D quantitative structure activity relationship prediction for drug discovery and content-based image retrieval for web search. The goal of an algorithm is to learn a function that correctly labels new bags or a function that correctly labels new instances. We propose that bags should be treated as latent distributions from which samples are observed. We show that it is possible to learn accurate instance- and bag-labeling functions in this setting as well as functions that correctly rank bags or instances under weak assumptions. Additionally, our theoretical results suggest that it is possible to learn to rank efficiently using traditional, well-studied supervised learning approaches. We perform an extensive empirical evaluation that supports the theoretical predictions entailed by the new framework. The proposed theoretical framework leads to a better understanding of the relationship between the MI and standard supervised learning settings, and it provides new methods for learning from MI data that are more accurate, more efficient, and have better understood theoretical properties than existing MI-specific algorithms.",,,,,"Doran, Gary/AAF-4506-2020","Doran, Gary/0000-0003-1233-2224; Ray, Soumya/0000-0002-6673-6796",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,128,,,,,,,,,,,,,,,WOS:000391655300001,0
J,"Hernandez-Lobato, JM; Gelbart, MA; Adams, RP; Hoffman, MW; Ghahramani, Z",,,,"Hernandez-Lobato, Jose Miguel; Gelbart, Michael A.; Adams, Ryan P.; Hoffman, Matthew W.; Ghahramani, Zoubin",,,A General Framework for Constrained Bayesian Optimization using Information-based Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.",,,,,"Hernandez-Lobato, Jose Miguel/F-2056-2016","Hernandez-Lobato, Jose Miguel/0000-0001-7610-949X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,160,,,,,,,,,,,,,,,WOS:000391667700001,0
J,"Hsu, D; Sabato, S",,,,"Hsu, Daniel; Sabato, Sivan",,,Loss Minimization and Parameter Estimation with Heavy Tails,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work studies applications and generalizations of a simple estimation technique that provides exponential concentration under heavy-tailed distributions, assuming only bounded low-order moments. We show that the technique can be used for approximate minimization of smooth and strongly convex losses, and specifically for least squares linear regression. For instance, our d- dimensional estimator requires just O(d log(1/delta)) random samples to obtain a constant factor approximation to the optimal least squares loss with probability 1 - delta, without requiring the covariates or noise to be bounded or subgaussian. We provide further applications to sparse linear regression and low-rank covariance matrix estimation with similar allowances on the noise and covariate distributions. The core technique is a generalization of the median-of-means estimator to arbitrary metric spaces.",,,,,"Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,18,,,,,,,,,,,,,,,WOS:000391473400001,0
J,"London, B; Huang, B; Getoor, L",,,,"London, Ben; Huang, Bert; Getoor, Lise",,,Stability and Generalization in Structured Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structured prediction models have been found to learn effectively from a few large examples- sometimes even just one. Despite empirical evidence, canonical learning theory cannot guarantee generalization in this setting because the error bounds decrease as a function of the number of examples. We therefore propose new PAC-Bayesian generalization bounds for structured prediction that decrease as a function of both the number of examples and the size of each example. Our analysis hinges on the stability of joint inference and the smoothness of the data distribution. We apply our bounds to several common learning scenarios, including max-margin and soft-max training of Markov random fields. Under certain conditions, the resulting error bounds can be far more optimistic than previous results and can even guarantee generalization from a single large example.",,,,,"Huang, Bert/E-2576-2016","Huang, Bert/0000-0002-8548-7246",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,222,,,,,,,,,,,,,,,WOS:000391914000001,0
J,"Mirzasoleiman, B; Karbasi, A; Sarkar, R; Krause, A",,,,"Mirzasoleiman, Baharan; Karbasi, Amin; Sarkar, Rik; Krause, Andreas",,,Distributed Submodular Maximization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many large-scale machine learning problems-clustering, non-parametric learning, kernel machines, etc.-require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Classical approaches to submodular optimization require centralized access to the full dataset, which is impractical for truly large-scale problems. In this paper, we consider the problem of submodular function maximization in a distributed fashion. We develop a simple, two-stage protocol GREEDI, that is easily implemented using MapReduce style computations. We theoretically analyze our approach, and show that under certain natural conditions, performance close to the centralized approach can be achieved. We begin with monotone submodular maximization subject to a cardinality constraint, and then extend this approach to obtain approximation guarantees for (not necessarily monotone) submodular maximization subject to more general constraints including matroid or knapsack constraints. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse Gaussian process inference and exemplar based clustering on tens of millions of examples using Hadoop.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,238,,,,,,,,,,,,,,,WOS:000391920600001,0
J,"Qin, XJ; Cunningham, P; Salter-Townshend, M",,,,"Qin, Xiangju; Cunningham, Padraig; Salter-Townshend, Michael",,,Online Trans-dimensional von Mises-Fisher Mixture Models for User Profiles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The proliferation of online communities has attracted much attention to modelling user behaviour in terms of social interaction, language adoption and contribution activity. Nevertheless, when applied to large-scale and cross-platform behavioural data, existing approaches generally suffer from expressiveness, scalability and generality issues. This paper proposes trans-dimensional von Mises-Fisher (TvMF) mixture models for L-2 normalised behavioural data, which encapsulate: (1) a Bayesian framework for vMF mixtures that enables prior knowledge and information sharing among clusters, (2) an extended version of reversible jump MCMC algorithm that allows adaptive changes in the number of clusters for vMF mixtures when the model parameters are updated, and (3) an online TvMF mixture model that accommodates the dynamics of clusters for time-varying user behavioural data. We develop efficient collapsed Gibbs sampling techniques for posterior inference, which facilitates parallelism for parameter updates. Empirical results on simulated and real-world data show that the proposed TvMF mixture models can discover more interpretable and intuitive clusters than other widely-used models, such as k-means, non-negative matrix factorization (NMF), Dirichlet process Gaussian mixture models (DP-GMM), and dynamic topic models (DTM). We further evaluate the performance of proposed models in real-world applications, such as the churn prediction task, that shows the usefulness of the features generated.",,,,,"Salter-Townshend, Michael/S-9350-2019","Salter-Townshend, Michael/0000-0001-6232-9109; Cunningham, Padraig/0000-0002-3499-0810",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,200,,,,,,,,,,,,,,,WOS:000391829000001,0
J,"Raskutti, G; Mahoney, MW",,,,"Raskutti, Garvesh; Mahoney, Michael W.",,,A Statistical Perspective on Randomized Sketching for Ordinary Least-Squares,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider statistical as well as algorithmic aspects of solving large-scale least-squares (LS) problems using randomized sketching algorithms. For a LS problem with input data (X, Y) epsilon R-nxp x R-n, sketching algorithms use a sketching matrix, S epsilon R-rxn, Where r << n. Then, rather than solving the LS problem using the full data (X,Y), sketching algorithms solve the LS problem using only the sketched data (SX, SY). Prior work has typically adopted an algorithmic perspective, in that it has made no statistical assumptions on the input X and Y, and instead it has been assumed that the data (X,Y) are fixed and worst-case (WC). Prior results show that, when using sketching matrices such as random projections and leverage-score sampling algorithms, with p less than or similar to r << n the WC error is the same as solving the original problem, up to a small constant. From a statistical perspective, we typically consider the mean-squared error performance of randomized sketching algorithms, when data (X, Y) are generated according to a statistical model Y = X beta + epsilon, where epsilon is a noise process. We provide a rigorous comparison of both perspectives leading to insights on how they differ. To do this, we first develop a framework for assessing algorithmic and statistical aspects of randomized sketching methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and we use our framework to provide upper bounds for several types of random projection and random sampling sketching algorithms. Among other results, we show that the RE can be upper bounded when p less than or similar to r << n while the PE typically requires the sample size r to be substantially larger. Lower bounds developed in subsequent results show that our upper bounds on PE can not be improved.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,31,213,,,,,,,,,,,,,,,WOS:000391836100001,0
J,"Zhu, HX; Strawn, N; Dunson, DB",,,,"Zhu, Hongxiao; Strawn, Nate; Dunson, David B.",,,Bayesian Graphical Models for Multivariate Functional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. By functional data, we refer to data that are realizations of random functions varying over a continuum (e.g., images, signals). We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite co-efficient sequences of the basis expansion, and establish its existence and uniqueness. We also prove the strong hyper Markov property and the conjugacy of this prior under a finite rank condition of the prior kernel parameter. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,204,,,,,,,,,,,,,,,WOS:000391830100001,0
J,"Chen, MM; Weinberger, KQ; Xu, ZX; Sha, F",,,,"Chen, Minmin; Weinberger, Kilian Q.; Xu, Zhixiang (Eddie); Sha, Fei",,,Marginalizing Stacked Linear Denoising Autoencoders,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. They have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized Stacked Linear Denoising Autoencoder (mSLDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSLDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters - in fact, the linear formulation gives rise to a closed-form solution. Consequently, mSLDA, which can be implemented in only 20 lines of MATLAB(TM), is about two orders of magnitude faster than a corresponding SDA. Furthermore, the representations learnt by mSLDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3849,3875,,,,,,,,,,,,,,,,WOS:000369888000045,0
J,"Feldman, V; Kothari, P",,,,"Feldman, Vitaly; Kothari, Pravesh",,,Agnostic Learning of Disjunctions on Symmetric Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of approximating and learning disjunctions (or equivalently, conjunctions) on symmetric distributions over {0,1}(n). Symmetric distributions are distributions whose PDF is invariant under any permutation of the variables. We prove that for every symmetric distribution D, there exists a set of n(O)(log(1/epsilon)) functions S, such that for every disjunction c, there is function p, expressible as a linear combination of functions in S, such that p epsilon-approximates c in l(1) distance on D or E-x similar to D[vertical bar C(x) - p(x)vertical bar] <= epsilon. This implies an agnostic learning algorithm for disjunctions on symmetric distributions that runs in time n(O)(log(1/epsilon)) The best known previous bound is n(O)(1/epsilon(4)) and follows from approximation of the more general class of halfspaces (Wimmer, 2010). We also show that there exists a symmetric distribution D, such that the minimum degree of a polynomial that 1/3-approximates the disjunction of all n variables in l(1) distance on D is Omega(root n). Therefore the learning result above cannot be achieved via 1/3-regression with a polynomial basis used in most other agnostic learning algorithms. Our technique also gives a simple proof that for any product distribution D and every disjunction c, there exists a polynomial p of degree O(log (1/epsilon)) such that p epsilon-approximates c in l(1) distance on D. This was first proved by Blais et al. (2008) via a more involved argument.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3455,3467,,,,,,,,,,,,,,,,WOS:000369888000035,0
J,"Santhanam, N; Anantharam, V",,,,"Santhanam, Narayana; Anantharam, Venkat",,,Agnostic Insurability of Model Classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by problems in insurance, our task is to predict finite upper bounds on a future draw from an unknown distribution p over natural numbers. We can only use past observations generated independently and identically distributed according to p. While p is unknown, it is known to belong to a given collection P of probability distributions on the natural numbers. The support of the distributions p is an element of P may be unbounded, and the prediction game goes on for infinitely many draws. We are allowed to make observations without predicting upper bounds for some time. But we must, with probability 1, start and then continue to predict upper bounds after a finite time irrespective of which p is an element of P governs the data. If it is possible, without knowledge of p and for any prescribed confidence however close to 1, to come up with a sequence of upper bounds that is never violated over an infinite time window with confidence at least as big as prescribed, we say the model class P is insurable. We completely characterize the insurability of any class P of distributions over natural numbers by means of a condition on how the neighborhoods of distributions in P should be, one that is both necessary and sufficient.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2329,2355,,,,,,,,,,,,,,,,WOS:000369887600006,0
J,"Vapnik, V; Izmailov, R",,,,"Vapnik, Vladimir; Izmailov, Rauf",,,Learning Using Privileged Information: Similarity Control and Knowledge Transfer,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.",,,,,,"Izmailov, Rauf/0000-0002-7326-669X",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,2023,2049,,,,,,,,,,,,,,,,WOS:000369887300012,0
J,"Liu, H; Wang, L; Zhao, T",,,,"Liu, Han; Wang, Lie; Zhao, Tuo",,,Calibrated Multivariate Regression with Application to Neural Semantic Basis Discovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a calibrated multivariate regression method named CMR for fitting high dimensional multivariate regression models. Compared with existing methods, CMR calibrates regularization for each regression task with respect to its noise level so that it simultaneously attains improved finite-sample performance and tuning insensitiveness. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rate of convergence in parameter estimation. Computationally, we propose an efficient smoothed proximal gradient algorithm with a worst-case numerical rate of convergence O(1/epsilon), where is a pre-specified accuracy of the objective function value. We conduct thorough numerical simulations to illustrate that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR to solve a brain activity prediction problem and find that it is as competitive as a handcrafted model created by human experts. The R package camel implementing the proposed method is available on the Comprehensive R Archive Network http://cran.r-project.org/web/packages/camel/.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1579,1606,,,,,,,,,,,28316509,,,,,WOS:000369887100009,0
J,"Schnass, K",,,,"Schnass, Karin",,,Local Identification of Overcomplete Dictionaries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the first theoretical results showing that stable identification of over-complete mu-coherent dictionaries Phi is an element of R-dxK is locally possible from training signals with sparsity levels S up to the order O(mu(-2)) and signal to noise ratios up to O (root d). In particular the dictionary is recoverable as the local maximum of a new maximization criterion that generalizes the K-means criterion. For this maximization criterion results for asymptotic exact recovery for sparsity levels up to O (mu(-1)) and stable recovery for sparsity levels up to O (mu(-2)) as well as signal to noise ratios up to O (root d) are provided. These asymptotic results translate to finite sample size recovery results with high probability as long as the sample size N scales as O (K-3 dS (epsilon) over tilde (-2)), where the recovery precision (epsilon) over tilde can go down to the asymptotically achievable precision. Further, to actually find the local maxima of the new criterion, a very simple Iterative Thresholding and K (signed) Means algorithm (ITKM), which has complexity O (dKN) in each iteration, is presented and its local efficiency is demonstrated in several experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2015,16,,,,,,1211,1242,,,,,,,,,,,,,,,,WOS:000369886600003,0
J,"Barbero, A; Takeda, A; Lopez, J",,,,"Barbero, Alvaro; Takeda, Akiko; Lopez, Jorge",,,Geometric Intuition and Algorithms for Ev-SVM,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this work we address the Ev-SVM model proposed by Perez-Cruz et al. as an extension of the traditional v support vector classification model (v-SVM). Through an enhancement of the range of admissible values for the regularization parameter v, the Ev-SVM has been shown to be able to produce a wider variety of decision functions, giving rise to a better adaptability to the data. However, while a clear and intuitive geometric interpretation can be given for the v-SVM model as a nearest-point problem in reduced convex hulls (RCH-NPP), no previous work has been made in developing such intuition for the Ev-SVM model. In this paper we show how Ev-SVM can be reformulated as a geometrical problem that generalizes RCH-NPP, providing new insights into this model. Under this novel point of view, we propose the RAPMINOS algorithm, able to solve Ev-SVM more efficiently than the current methods. Furthermore, we show how RAPMINOS is able to address the Ev-SVM model for any choice of regularization norm l(p)>= 1, seamlessly, which further extends the SVM model flexibility beyond the usual Ev-SVM models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,323,369,,,,,,,,,,,,,,,,WOS:000369886000001,0
J,"Peters, J; Mooij, JM; Janzing, D; Scholkopf, B",,,,"Peters, Jonas; Mooij, Joris M.; Janzing, Dominik; Schoelkopf, Bernhard",,,Causal Discovery with Continuous Additive Noise Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Peters, Jonas/0000-0002-1487-7511",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2009,2053,,,,,,,,,,,,,,,,WOS:000344638300004,0
J,"Stadler, N; Stekhoven, DJ; Buhlmann, P",,,,"Stadler, Nicolas; Stekhoven, Daniel J.; Buehlmann, Peter",,,Pattern Alternating Maximization Algorithm for Missing Data in High-Dimensional Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel and efficient algorithm for maximizing the observed log-likelihood of a multivariate normal data matrix with missing values. We show that our procedure, based on iteratively regressing the missing on the observed variables, generalizes the standard EM algorithm by alternating between different complete data spaces and performing the E-Step incrementally. In this non-standard setup we prove numerical convergence to a stationary point of the observed log-likelihood. For high-dimensional data, where the number of variables may greatly exceed sample size, we perform regularization using a Lasso-type penalty. This introduces sparsity in the regression coefficients used for imputation, permits fast computation and warrants competitive performance in terms of estimating the missing entries. We show on simulated and real data that the new method often improves upon other modern imputation techniques such as k-nearest neighbors imputation, nuclear norm minimization or a penalized likelihood approach with an I1-penalty on the concentration matrix.",,,,,"B√ºhlmann, Peter/A-2107-2013; Stekhoven, Daniel J./K-1887-2019","B√ºhlmann, Peter/0000-0002-1782-6015; ",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,1903,1928,,,,,,,,,,,,,,,,WOS:000344638300001,0
J,"van den Oord, A; Schrauwen, B",,,,"van den Oord, Aaron; Schrauwen, Benjamin",,,The Student-t Mixture as a Natural Image Patch Prior with Application to Image Compression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent results have shown that Gaussian mixture models (GMMs) are remarkably good at density modeling of natural image patches, especially given their simplicity. In terms of log likelihood on real-valued data they are comparable with the best performing techniques published, easily outperforming more advanced ones, such as deep belief networks. They can be applied to various image processing tasks, such as image denoising, deblurring and inpainting, where they improve on other generic prior methods, such as sparse coding and field of experts. Based on this we propose the use of another, even richer mixture model based image prior: the Student-t mixture model (STM). We demonstrate that it convincingly surpasses GMMs in terms of log likelihood, achieving performance competitive with the state of the art in image patch modeling. We apply both the GMM and STM to the task of lossy and lossless image compression, and propose efficient coding schemes that can easily be extended to other unsupervised machine learning models. Finally, we show that the suggested techniques outperform JPEG, with results comparable to or better than JPEG 2000.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2061,2086,,,,,,,,,,,,,,,,WOS:000344638300006,0
J,"Xu, Z; Kusner, MJ; Weinberger, KQ; Chen, M; Chapelle, O",,,,"Xu, Zhixiang (Eddie); Kusner, Matt J.; Weinberger, Kilian Q.; Chen, Minmin; Chapelle, Olivier",,,Classifier Cascades and Trees for Minimizing Feature Evaluation Cost,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Machine learning algorithms have successfully entered industry through many real-world applications (e.g., search engines and product recommendations). In these applications, the test-time CPU cost must be budgeted and accounted for. In this paper, we examine two main components of the test-time CPU cost, classifier evaluation cost and feature extraction cost, and show how to balance these costs with the classifier accuracy. Since the computation required for feature extraction dominates the test-time cost of a classifier in these settings, we develop two algorithms to efficiently balance the performance with the test-time cost. Our first contribution describes how to construct and optimize a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. Our second contribution is a natural reduction of the tree of classifiers into a cascade. The cascade is particularly useful for class-imbalanced data sets as the majority of instances can be early-exited out of the cascade when the algorithm is sufficiently confident in its prediction. Because both approaches only compute features for inputs that benefit from them the most, we find our trained classifiers lead to high accuracies at a small fraction of the computational cost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2113,2144,,,,,,,,,,,,,,,,WOS:000344638300008,0
J,"Volkovs, MN; Zemel, RS",,,,"Volkovs, Maksims N.; Zemel, Richard S.",,,New Learning Methods for Supervised and Unsupervised Preference Aggregation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we present a general treatment of the preference aggregation problem, in which multiple preferences over objects must be combined into a single consensus ranking. We consider two instances of this problem: unsupervised aggregation where no information about a target ranking is available, and supervised aggregation where ground truth preferences are provided. For each problem class we develop novel learning methods that are applicable to a wide range of preference types.(1) Specifically, for unsupervised aggregation we introduce the Multinomial Preference model (MPM) which uses a multinomial generative process to model the observed preferences. For the supervised problem we develop a supervised extension for MPM and then propose two fully supervised models. The first model employs SVD factorization to derive effective item features, transforming the aggregation problems into a learning-to-rank one. The second model aims to eliminate the costly SVD factorization and instantiates a probabilistic CRF framework, deriving unary and pairwise potentials directly from the observed preferences. Using a probabilistic framework allows us to directly optimize the expectation of any target metric, such as NDCG or ERR. All the proposed models operate on pairwise preferences and can thus be applied to a wide range of preference types. We empirically validate the models on rank aggregation and collaborative filtering data sets and demonstrate superior empirical accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1135,1176,,,,,,,,,,,,,,,,WOS:000335458100010,0
J,"Arias-Castro, E; Pelletier, B",,,,"Arias-Castro, Ery; Pelletier, Bruno",,,On the Convergence of Maximum Variance Unfolding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing specific rates of convergence under standard assumptions. We find that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1747,1770,,,,,,,,,,,,,,,,WOS:000323367000003,0
J,"Kanamori, T; Takeda, A; Suzuki, T",,,,"Kanamori, Takafumi; Takeda, Akiko; Suzuki, Taiji",,,Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There are two main approaches to binary classification problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is defined for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufficiently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1461,1504,,,,,,,,,,,,,,,,WOS:000322506400001,0
J,"Hall, R; Rinaldo, A; Wasserman, L",,,,"Hall, Rob; Rinaldo, Alessandro; Wasserman, Larry",,,Differential Privacy for Functions and Functional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Differential privacy is a rigorous cryptographically-motivated characterization of data privacy which may be applied when releasing summaries of a database. Previous work has focused mainly on methods for which the output is a finite dimensional vector, or an element of some discrete set. We develop methods for releasing functions while preserving differential privacy. Specifically, we show that adding an appropriate Gaussian process to the function of interest yields differential privacy. When the functions lie in the reproducing kernel Hilbert space (RKHS) generated by the covariance kernel of the Gaussian process, then the correct noise level is established by measuring the sensitivity of the function in the RKHS norm. As examples we consider kernel density estimation, kernel support vector machines, and functions in RKHSs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,703,727,,,,,,,,,,,,,,,,WOS:000315981900013,0
J,"Shalev-Shwartz, S; Zhang, T",,,,"Shalev-Shwartz, Shai; Zhang, Tong",,,Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,567,599,,,,,,,,,,,,,,,,WOS:000315981900008,0
J,"Zhang, ZH; Wang, SS; Liu, DH; Jordan, MI",,,,"Zhang, Zhihua; Wang, Shusen; Liu, Dehua; Jordan, Michael I.",,,EP-GIG Priors and Applications in Bayesian Sparse Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose a novel framework for the construction of sparsity-inducing priors. In particular, we define such priors as a mixture of exponential power distributions with a generalized inverse Gaussian density (EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and the special cases include Gaussian scale mixtures and Laplace scale mixtures. Furthermore, Laplace scale mixtures can subserve a Bayesian framework for sparse learning with nonconvex penalization. The densities of EP-GIG can be explicitly expressed. Moreover, the corresponding posterior distribution also follows a generalized inverse Gaussian distribution. We exploit these properties to develop EM algorithms for sparse empirical Bayesian learning. We also show that these algorithms bear an interesting resemblance to iteratively reweighted l(2) or l(1) methods. Finally, we present two extensions for grouped variable selection and logistic regression.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X; Wang, Shusen/0000-0003-3928-6782",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,2031,2061,,,,,,,,,,,,,,,,WOS:000307020700011,0
J,"Song, L; Smola, A; Gretton, A; Bedo, J; Borgwardt, K",,,,"Song, Le; Smola, Alex; Gretton, Arthur; Bedo, Justin; Borgwardt, Karsten",,,Feature Selection via Dependence Maximization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice.",,,,,,"Gretton, Arthur/0000-0003-3169-7624; Borgwardt, Karsten/0000-0001-7221-2393; Bedo, Justin/0000-0001-5704-0212",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1393,1434,,,,,,,,,,,,,,,,WOS:000305456600005,0
J,"Petrik, M; Zilberstein, S",,,,"Petrik, Marek; Zilberstein, Shlomo",,,Robust Approximate Bilinear Programming for Value Function Approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Value function approximation methods have been successfully used in many applications, but the prevailing techniques often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this worst-case complexity is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze the formulation as well as a simple approximate algorithm for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.",,,,,,"Zilberstein, Shlomo/0000-0001-9817-7848; Petrik, Marek/0000-0002-4568-7948",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,3027,3063,,,,,,,,,,,,,,,,WOS:000298103200010,0
J,"Gonen, M; Alpaydin, E",,,,"Gonen, Mehmet; Alpaydin, Ethem",,,Multiple Kernel Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.",,,,,"G√∂nen, Mehmet/E-8270-2012; G√∂nen, Mehmet/O-7322-2015; ALPAYDIN, ETHEM/E-6127-2013; xiankai, chen/A-4624-2009","G√∂nen, Mehmet/0000-0002-2483-075X; ALPAYDIN, ETHEM/0000-0001-7506-0321; ",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2211,2268,,,,,,,,,,,,,,,,WOS:000293757900004,0
J,"Ozertem, U; Erdogmus, D",,,,"Ozertem, Umut; Erdogmus, Deniz",,,Locally Defined Principal Curves and Surfaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Principal curves are defined as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redefine principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve fitting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous first and second derivatives. Therefore, we first present our principal curve/surface definition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems.",,,,,"Erdogmus, Deniz/A-8170-2009",,,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1249,1286,,,,,,,,,,,,,,,,WOS:000290096100003,0
J,"Dhillon, PS; Foster, DP; Ungar, LH",,,,"Dhillon, Paramveer S.; Foster, Dean P.; Ungar, Lyle H.",,,Minimum Description Length Penalization for Group and Multi-Task Sparse Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a framework MIC (Multiple Inclusion Criterion) for learning sparse models based on the information theoretic Minimum Description Length (MDL) principle. MIC provides an elegant way of incorporating arbitrary sparsity patterns in the feature space by using two-part MDL coding schemes. We present MIC based models for the problems of grouped feature selection (MIC-GROUP) and multi-task feature selection (MIC-MULTI). MIC-GROUP assumes that the features are divided into groups and induces two level sparsity, selecting a subset of the feature groups, and also selecting features within each selected group. MIC-MULTI applies when there are multiple related tasks that share the same set of potentially predictive features. It also induces two level sparsity, selecting a subset of the features, and then selecting which of the tasks each feature should be added to. Lastly, we propose a model, TRANSFEAT, that can be used to transfer knowledge from a set of previously learned tasks to a new task that is expected to share similar features. All three methods are designed for selecting a small set of predictive features from a large pool of candidate features. We demonstrate the effectiveness of our approach with experimental results on data from genomics and from word sense disambiguation problems.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,525,564,,,,,,,,,,,,,,,,WOS:000288896800006,0
J,"Gupta, R; Sarawagi, S; Diwan, AA",,,,"Gupta, Rahul; Sarawagi, Sunita; Diwan, Ajit A.",,,Collective Inference for Extraction MRFs Coupled with Symmetric Clique Potentials,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many structured information extraction tasks employ collective graphical models that capture inter-instance associativity by coupling them with various clique potentials. We propose tractable families of such potentials that are invariant under permutations of their arguments, and call them symmetric clique potentials. We present three families of symmetric potentials-MAX, SUM, and MAJORITY. We propose cluster message passing for collective inference with symmetric clique potentials, and present message computation algorithms tailored to such potentials. Our first message computation algorithm, called alpha-pass, is sub-quadratic in the clique size, outputs exact messages for MAX, and computes 13/15-approximate messages for Potts, a popular member of the SUM family. Empirically, it is upto two orders of magnitude faster than existing algorithms based on graph-cuts or belief propagation. Our second algorithm, based on Lagrangian relaxation, operates on MAJORITY potentials and provides close to exact solutions while being two orders of magnitude faster. We show that the cluster message passing framework is more principled, accurate and converges faster than competing approaches. We extend our collective inference framework to exploit associativity of more general intra-domain properties of instance labelings, which opens up interesting applications in domain adaptation. Our approach leads to significant error reduction on unseen domains without incurring any overhead of model retraining.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3097,3135,,,,,,,,,,,,,,,,WOS:000285643600005,0
J,"Mooij, JM",,,,"Mooij, Joris M.",,,libDAI: A Free and Open Source C plus plus Library for Discrete Approximate Inference in Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes the software package libDAI, a free & open source C++ library that provides implementations of various exact and approximate inference methods for graphical models with discrete-valued variables. libDAI supports directed graphical models (Bayesian networks) as well as undirected ones (Markov random fields and factor graphs). It offers various approximations of the partition sum, marginal probability distributions and maximum probability states. Parameter learning is also supported. A feature comparison with other open source software packages for approximate inference is given. libDAI is licensed under the GPL v2+ license and is available at http://www.libdai.org.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2169,2173,,,,,,,,,,,,,,,,WOS:000282523300004,0
J,"Ojala, M; Garriga, GC",,,,"Ojala, Markus; Garriga, Gemma C.",,,Permutation Tests for Studying Classifier Performance,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We explore the framework of permutation-based p-values for assessing the performance of classifiers. In this paper we study two simple permutation tests. The first test assess whether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2010,11,,,,,,1833,1863,,,,,,,,,,,,,,,,WOS:000282522400003,0
J,"Krause, A",,,,"Krause, Andreas",,,SFO: A Toolbox for Submodular Function Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, a fundamental problem structure has emerged as very useful in a variety of machine learning applications: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions for large problems. We present SFO, a toolbox for use in MATLAB or Octave that implements algorithms for minimization and maximization of submodular functions. A tutorial script illustrates the application of submodularity to machine learning and AI problems such as feature selection, clustering, inference and optimized information gathering.",,,,,"Krause, Andreas/A-5888-2008","Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1141,1144,,,,,,,,,,,,,,,,WOS:000277186600005,0
J,"Ben-Haim, Y; Tom-Tov, E",,,,"Ben-Haim, Yael; Tom-Tov, Elad",,,A Streaming Parallel Decision Tree Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new algorithm for building decision tree classifiers. The algorithm is executed in a distributed environment and is especially designed for classifying large data sets and streaming data. It is empirically shown to be as accurate as a standard decision tree classifier, while being scalable for processing of streaming data on multiple processors. These findings are supported by a rigorous analysis of the algorithm's accuracy. The essence of the algorithm is to quickly construct histograms at the processors, which compress the data to a fixed amount of memory. A master processor uses this information to find near-optimal split points to terminal tree nodes. Our analysis shows that guarantees on the local accuracy of split points imply guarantees on the overall tree accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,849,872,,,,,,,,,,,,,,,,WOS:000277186500016,0
J,"Newman, D; Asuncion, A; Smyth, P; Welling, M",,,,"Newman, David; Asuncion, Arthur; Smyth, Padhraic; Welling, Max",,,Distributed Algorithms for Topic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.",,,,,,"Smyth, Padhraic/0000-0001-9971-8378",,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1801,1828,,,,,,,,,,,,,,,,WOS:000270825200002,0
J,"Woodsend, K; Gondzio, J",,,,"Woodsend, Kristian; Gondzio, Jacek",,,Hybrid MPI/OpenMP Parallel Linear Support Vector Machine Training,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines are a powerful machine learning technology, but the training process involves a dense quadratic optimization problem and is computationally challenging. A parallel implementation of linear Support Vector Machine training has been developed, using a combination of MPI and OpenMP. Using an interior point method for the optimization and a reformulation that avoids the dense Hessian matrix, the structure of the augmented system matrix is exploited to partition data and computations amongst parallel processors efficiently. The new implementation has been applied to solve problems from the PASCAL Challenge on Large-scale Learning. We show that our approach is competitive, and is able to solve problems in the Challenge many times faster than other parallel approaches. We also demonstrate that the hybrid version performs more efficiently than the version using pure MPI.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1937,1953,,,,,,,,,,,,,,,,WOS:000270825200007,0
J,"Xu, H; Caramanis, C; Mannor, S",,,,"Xu, Huan; Caramanis, Constantine; Mannor, Shie",,,Robustness and Regularization of Support Vector Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classification that explicitly build in protection to noise, and at the same time control overfitting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well.",,,,,"xu, huan/R-5436-2016; Xu, Huan/M-5155-2014","xu, huan/0000-0002-5712-0308; Caramanis, Constantine/0000-0001-9939-8378; Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1485,1510,,,,,,,,,,,,,,,,WOS:000270825000007,0
J,"Escalante, HJ; Montes, M; Sucar, LE",,,,"Jair Escalante, Hugo; Montes, Manuel; Enrique Sucar, Luis",,,Particle Swarm Model Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes the application of particle swarm optimization (PSO) to the problem of full model selection, FMS, for classification tasks. FMS is defined as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classification error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. FMS can be applied to any classification domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with FMS. We adopt PSO for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows PSO to avoid overfitting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with PSO, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge.",,,,,,"Sucar, Luis Enrique/0000-0002-3685-5567",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,405,440,,,,,,,,,,,,,,,,WOS:000270824200011,0
J,"Igel, C; Heidrich-Meisner, V; Glasmachers, T",,,,"Igel, Christian; Heidrich-Meisner, Verena; Glasmachers, Tobias",,,Shark,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"SHARK is an object-oriented library for the design of adaptive systems. It comprises methods for single-and multi-objective optimization (e. g., evolutionary and gradient-based algorithms) as well as kernel-based methods, neural networks, and other machine learning techniques.",,,,,"Igel, Christian/B-4091-2009","Igel, Christian/0000-0003-2868-0856",,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,993,996,,,,,,,,,,,,,,,,WOS:000258646300001,0
J,"Clemencon, S; Vayatis, N",,,,"Clemencon, Stephan; Vayatis, Nicolas",,,Ranking the best instances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We first state the problem of finding the best instances which can be cast as a classification problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where first, the best instances would be tentatively identified and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2007,8,,,,,,2671,2699,,,,,,,,,,,,,,,,WOS:000252745100001,0
J,"Elidan, G; Nachman, I; Friedman, N",,,,"Elidan, Gal; Nachman, Iftach; Friedman, Nir",,,Ideal Parent structure learning for continuous variable Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian networks in general, and continuous variable networks in particular, have become increasingly popular in recent years, largely due to advances in methods that facilitate automatic learning from data. Yet, despite these advances, the key task of learning the structure of such models remains a computationally intensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks in the presence of missing values or hidden variables, a scenario that is part of many real-life problems. In this work we present a general method for speeding structure search for continuous variable networks with common parametric distributions. We efficiently evaluate the approximate merit of candidate structure modifications and apply time consuming (exact) computations only to the most promising ones, thereby achieving significant improvement in the running time of the search algorithm. Our method also naturally and efficiently facilitates the addition of useful new hidden variables into the network structure, a task that is typically considered both conceptually difficult and computationally prohibitive. We demonstrate our method on synthetic and real-life data sets, both for learning structure on fully and partially observable data, and for introducing new hidden variables during structure search.",,,,,"Nachman, Iftach/E-6750-2010; Friedman, Nir/H-9692-2012; Elidan, GAl/A-7380-2009; Nachman, Iftach/AAK-2051-2020","Nachman, Iftach/0000-0002-4667-0398; Friedman, Nir/0000-0002-9678-3550; Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1799,1833,,,,,,,,,,,,,,,,WOS:000252744400005,0
J,"Blum, A; Mansour, Y",,,,"Blum, Avrim; Mansour, Yishay",,,From external to internal regret,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"External regret compares the performance of an online algorithm, selecting among N actions, to the performance of the best of those actions in hindsight. Internal regret compares the loss of an online algorithm to the loss of a modified online algorithm, which consistently replaces one action by another. In this paper we give a simple generic reduction that, given an algorithm for the external regret problem, converts it to an efficient online algorithm for the internal regret problem. We provide methods that work both in the full information model, in which the loss of every action is observed at each time step, and the partial information ( bandit) model, where at each time step only the loss of the selected action is observed. The importance of internal regret in game theory is due to the fact that in a general game, if each player has sublinear internal regret, then the empirical frequencies converge to a correlated equilibrium. For external regret we also derive a quantitative regret bound for a very general setting of regret, which includes an arbitrary set of modification rules ( that possibly modify the online algorithm) and an arbitrary set of time selection functions ( each giving different weight to each time step). The regret for a given time selection and modification rule is the difference between the cost of the online algorithm and the cost of the modified online algorithm, where the costs are weighted by the time selection function. This can be viewed as a generalization of the previously-studied sleeping experts setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2007,8,,,,,,1307,1324,,,,,,,,,,,,,,,,WOS:000248351800004,0
J,"Mooij, JM; Kappen, HJ",,,,"Mooij, Joris M.; Kappen, Hilbert J.",,,Loop corrections for approximate inference on factor graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a method to improve approximate inference methods by correcting for the influence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i. e., probability distributions on the Markov blanket of a variable for a modified graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives significantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including real world networks, and conclude that the LC method generally obtains the most accurate results.",,,,,"Kappen, H.J./L-4425-2015",,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1113,1143,,,,,,,,,,,,,,,,WOS:000248351700008,0
J,"Elbaz, A; Lee, HK; Servedio, RA; Wan, A",,,,"Elbaz, Ariel; Lee, Homin K.; Servedio, Rocco A.; Wan, Andrew",,,Separating models of learning from correlated and uncorrelated data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efficiently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efficient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efficiently in the Random Walk setting but not in the standard setting where all examples are independent.",,,,,"El-Baz, Ayman/AAC-6689-2019","El-Baz, Ayman/0000-0001-7264-1323",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,277,290,,,,,,,,,,,,,,,,WOS:000247002600004,0
J,"Micchelli, CA; Xu, YS; Zhang, HZ",,,,"Micchelli, Charles A.; Xu, Yuesheng; Zhang, Haizhang",,,Universal kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,In this paper we investigate conditions on the features of a continuous kernel so that it may approximate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property.,,,,,"Zhang, Haizhang/GPX-1222-2022; Xu, yue/HGE-1737-2022","Zhang, Haizhang/0000-0002-8241-3145; ",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2651,2667,,,,,,,,,,,,,,,,WOS:000245390800005,0
J,"Rousu, J; Saunders, C; Szedmak, S; Shawe-Taylor, J",,,,"Rousu, Juho; Saunders, Craig; Szedmak, Sandor; Shawe-Taylor, John",,,Kernel-based learning of hierarchical multilabel classification models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a kernel-based algorithm for hierarchical text classification where the documents are allowed to belong to more than one category at a time. The classification model is a variant of the Maximum Margin Markov Network framework, where the classification hierarchy is represented as a Markov tree equipped with an exponential family defined on the edges. We present an efficient optimization algorithm based on incremental conditional gradient ascent in single-example subspaces spanned by the marginal dual variables. The optimization is facilitated with a dynamic programming based algorithm that computes best update directions in the feasible set. Experiments show that the algorithm can feasibly optimize training sets of thousands of examples and classification hierarchies consisting of hundreds of nodes. Training of the full hierarchical model is as efficient as training independent SVM-light classifiers for each node. The algorithm's predictive accuracy was found to be competitive with other recently introduced hierarchical multi-category or multilabel classification learning algorithms.",,,,,"Rousu, Juho/E-8195-2012","Rousu, Juho/0000-0002-0705-4314; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1601,1626,,,,,,,,,,,,,,,,WOS:000245388800018,0
J,"Bickel, PJ; Ritov, Y; Zakai, A",,,,"Bickel, Peter J.; Ritov, Ya'acov; Zakai, Alon",,,Some theory for generalized boosting algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We give a review of various aspects of boosting, clarifying the issues through a few simple results, and relate our work and that of others to the minimax paradigm of statistics. We consider the population version of the boosting algorithm and prove its convergence to the Bayes classifier as a corollary of a general result about Gauss-Southwell optimization in Hilbert space. We then investigate the algorithmic convergence of the sample version, and give bounds to the time until perfect separation of the sample. We conclude by some results on the statistical optimality of the L-2 boosting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,705,732,,,,,,,,,,,,,,,,WOS:000240173400001,0
J,"Spratling, MW",,,,"Spratling, Michael W.",,,Learning image components for object recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In order to perform object recognition it is necessary to learn representations of the underlying components of images. Such components correspond to objects, object-parts, or features. Non-negative matrix factorisation is a generative model that has been specifically proposed for finding such meaningful representations of image data, through the use of non-negativity constraints on the factors. This article reports on an empirical investigation of the performance of non-negative matrix factorisation algorithms. It is found that such algorithms need to impose additional constraints on the sparseness of the factors in order to successfully deal with occlusion. However, these constraints can themselves result in these algorithms failing to identify image components under certain conditions. In contrast, a recognition model ( a competitive learning neural network algorithm) reliably and accurately learns representations of elementary image features without such constraints.",,,,,"Spratling, Michael W/G-7689-2011","Spratling, Michael W/0000-0001-9531-2813",,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,793,815,,,,,,,,,,,,,,,,WOS:000240173400004,0
J,"Drukh, E; Mansour, Y",,,,"Drukh, E; Mansour, Y",,,Concentration bounds for unigram language models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show several high-probability concentration bounds for learning unigram language models. One interesting quantity is the probability of all words appearing exactly k times in a sample of size m. A standard estimator for this quantity is the Good-Turing estimator. The existing analysis on its error shows a high-probability bound of approximately O(k/root m). We improve its dependency on k to O((4)root k/root m + k/m). We also analyze the empirical frequencies estimator, showing that with high probability its error is bounded by approximately O(1/k + root k/m). We derive a combined estimator, which has an error of approximately O(m(-2/5)), for any k. A standard measure for the quality of a learning algorithm is its expected per-word log-loss. The leave-one-out method can be used for estimating the log-loss of the unigram model. We show that its error has a high-probability bound of approximately O(1/root m), for any underlying distribution. We also bound the log-loss a priori, as a function of various parameters of the distribution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2005,6,,,,,,1231,1264,,,,,,,,,,,,,,,,WOS:000236330000001,0
J,"Sarela, J; Valpola, H",,,,"Sarela, J; Valpola, H",,,Denoising source separation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new algorithmic framework called denoising source separation (DSS) is introduced. The main benefit of this framework is that it allows for the easy development of new source separation algorithms which can be optimised for specific problems. In this framework, source separation algorithms are constructed around denoising procedures. The resulting algorithms can range from almost blind to highly specialised source separation algorithms. Both simple linear and more complex nonlinear or adaptive denoising schemes are considered. Some existing independent component analysis algorithms are reinterpreted within the DSS framework and new, robust blind source separation algorithms are suggested. The framework is derived as a one-unit equivalent to an EM algorithm for source separation. However, in the DSS framework it is easy to utilise various kinds of denoising procedures which need not be based on generative models. In the experimental section, various DSS schemes are applied extensively to artificial data, to real magnetoencephalograms and to simulated CDMA mobile network signals. Finally, various extensions to the proposed DSS algorithms are considered. These include nonlinear observation mappings, hierarchical models and over-complete, nonorthogonal feature spaces. With these extensions, DSS appears to have relevance to many existing models of neural information processing.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2005,6,,,,,,233,272,,,,,,,,,,,,,,,,WOS:000236329400001,0
J,"Almeida, LB",,,,"Almeida, LB",,,MISEP - Linear and nonlinear ICA based on mutual information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear Independent Components Analysis (ICA) has become an important signal processing and data analysis technique, the typical application being blind source separation in a wide range of signals, such as biomedical, acoustical and astrophysical ones. Nonlinear ICA is less developed, but has the potential to become at least as powerful. This paper presents MISEP, an ICA technique for linear and nonlinear mixtures, which is based on the minimization of the mutual information of the estimated components. MISEP is a generalization of the popular INFOMAX technique, which is extended in two ways: (1) to deal with nonlinear mixtures, and (2) to be able to adapt to the actual statistical distributions of the sources, by dynamically estimating the nonlinearities to be used at the outputs. The resulting MISEP method optimizes a network with a specialized architecture, with a single objective function: the output entropy. The paper also briefly discusses the issue of nonlinear source separation. Examples of linear and nonlinear source separation performed by MISEP are presented.",,,,,"Almeida, Luis/A-7651-2012","Almeida, Luis/0000-0002-1324-0068",,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1297,1318,,,,,,,,,,,,,,,,WOS:000224808300007,0
J,"Christmann, A; Steinwart, I",,,,"Christmann, A; Steinwart, I",,,On robustness properties of convex risk minimization methods for pattern recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have besides other good properties also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the influence function of the classifiers and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.",,,,,"Christmann, Andreas/R-3542-2019","Christmann, Andreas/0000-0002-8408-3549",,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,1007,1034,,,,,,,,,,,,,,,,WOS:000236328000006,0
J,"Mesterharm, C",,,,"Mesterharm, C",,,Tracking linear-threshold concepts with winnow,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,"In this paper, we give a mistake-bound for learning arbitrary linear-threshold concepts that are allowed to change over time in the on-line model of learning. We use a variation of the Winnow algorithm and show that the bounds for learning shifting linear-threshold functions have many of the same advantages that the traditional Winnow algorithm has on fixed concepts. These benefits include a weak dependence on the number of irrelevant attributes, inexpensive runtime, and robust behavior against noise. In fact, we show that the bound for tracking Winnow has even better performance with respect to irrelevant attributes. Let X e [0, 1](n) be an instance of the learning problem. In the previous bounds, the number of mistakes depends on In n. In this paper, the shifting concept bound depends on max In (\\X\\(1)). We show that this behavior is a result of certain parameter choices in the tracking version of Winnow, and we show how to use related parameters to get a similar mistake bound for the traditional fixed concept version of Winnow.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,819,838,,10.1162/1532443041424274,0,,,,,,,,,,,,,WOS:000223238800004,0
J,"Blei, DM; Ng, AY; Jordan, MI",,,,"Blei, DM; Ng, AY; Jordan, MI",,,Latent Dirichlet allocation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.",,,,,"Jordan, Michael I/C-5253-2013; Lobo, Diele/I-9106-2012",,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,993,1022,,10.1162/jmlr.2003.3.4-5.993,0,,,,,,,,,,,,,WOS:000184926200015,0
J,"Zhang, T; Damerau, L; Johnson, D",,,,"Zhang, T; Damerau, L; Johnson, D",,,Text chunking based on a generalization of Winnow,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes a text chunking system based on a generalization of the Winnow algorithm. We propose a general statistical model for text chunking which we then convert into a classification problem. We argue that the Winnow family of algorithms is particularly suitable for solving classification problems arising from NLP applications, due to their robustness to irrelevant features. However in theory, Winnow may not converge for linearly non-separable data. To remedy this problem, we employ a generalization of the original Winnow method. An additional advantage of the new algorithm is that it provides reliable confidence estimates for its classification predictions. This property is required in our statistical modeling approach. We show that our system achieves state of the art performance in text chunking with less computational cost then previous systems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,615,638,,10.1162/153244302320884560,0,,,,,,,,,,,,,WOS:000179542800004,0
J,"Lodhi, H; Saunders, C; Shawe-Taylor, J; Cristianini, N; Watkins, C",,,,"Lodhi, H; Saunders, C; Shawe-Taylor, J; Cristianini, N; Watkins, C",,,Text classification using string kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique.",,,,,"Watkins, Christopher JCH/E-5811-2013","Watkins, Christopher JCH/0000-0001-9020-4530; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,419,444,,10.1162/153244302760200687,0,,,,,,,,,,,,,WOS:000178101500005,0
J,"Zhang, T; Iyengar, VS",,,,"Zhang, T; Iyengar, VS",,,Recommender systems using linear classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recommender systems use historical data on user preferences and other available data on users (for example, demographics) and items (for example, taxonomy) to predict items a new user might like. Applications of these methods include recommending items for purchase and personalizing the browsing experience on a web-site. Collaborative filtering methods have focused on using just the history of user preferences to make the recommendations. These methods have been categorized as memory-based if they operate over the entire data to make predictions and as model-based if they use the data to build a model which is then used for predictions. In this paper, we propose the use of linear classifiers in a model-based recommender system. We compare our method with another model-based method using decision trees and with memory-based methods using data from various domains. Our experimental results indicate that these linear models are well suited for this application. They outperform a commonly proposed memory-based method in accuracy and also have a better tradeoff between off-line and on-line computational requirements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,313,334,,10.1162/153244302760200641,0,,,,,,,,,,,,,WOS:000178101500001,0
J,"Cannings, TI; Fan, YY",,,,"Cannings, Timothy, I; Fan, Yingying",,,The correlation-assisted missing data estimator,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a novel approach to estimation problems in settings with missing data. Our proposal - the Correlation-Assisted Missing data (CAM) estimator - works by exploiting the relationship between the observations with missing features and those without missing features in order to obtain improved prediction accuracy. In particular, our theoretical results elucidate general conditions under which the proposed CAM estimator has lower mean squared error than the widely used complete-case approach in a range of estimation problems. We showcase in detail how the CAM estimator can be applied to U-Statistics to obtain an unbiased, asymptotically Gaussian estimator that has lower variance than the complete-case U-Statistic. Further, in nonparametric density estimation and regression problems, we construct our CAM estimator using kernel functions, and show it has lower asymptotic mean squared error than the corresponding complete-case kernel estimator. We also include practical demonstrations throughout the paper using simulated data and the Terneuzen birth cohort and Brandsma datasets available from CRAN.",,,,,,"Cannings, Timothy I/0000-0002-2111-4168",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,49,,,,,,,,,,,,,,,,WOS:000766878900001,0
J,"Sharma, U; Kaplan, J",,,,"Sharma, Utkarsh; Kaplan, Jared",,,Scaling Laws from the Data Manifold Dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law L proportional to N-alpha in the number of network parameters N. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension d. This simple theory predicts that the scaling exponents alpha approximate to 4/d for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of d and alpha by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,34,,,,,,,,,,,,,,,,WOS:000752266600001,0
J,"Fornasier, M; Huang, H; Pareschi, L; Sunnen, P",,,,"Fornasier, Massimo; Huang, Hui; Pareschi, Lorenzo; Suennen, Philippe",,,Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the implementation of a new stochastic Kuramoto-Vicsek-type model for global optimization of nonconvex functions on the sphere. This model belongs to the class of Consensus-Based Optimization. In fact, particles move on the sphere driven by a drift towards an instantaneous consensus point, which is computed as a convex combination of particle locations, weighted by the cost function according to Laplace's principle, and it represents an approximation to a global minimizer. The dynamics is further perturbed by a random vector field to favor exploration, whose variance is a function of the distance of the particles to the consensus point. In particular, as soon as the consensus is reached the stochastic component vanishes. The main results of this paper are about the proof of convergence of the numerical scheme to global minimizers provided conditions of well-preparation of the initial datum. The proof combines previous results of mean-field limit with a novel asymptotic analysis, and classical convergence results of numerical methods for SDE. We present several numerical experiments, which show that the algorithm proposed in the present paper scales well with the dimension and is extremely versatile. To quantify the performances of the new approach, we show that the algorithm is able to perform essentially as good as ad hoc state of the art methods in challenging problems in signal processing and machine learning, namely the phase retrieval problem and the robust subspace detection.",,,,,"Pareschi, Lorenzo/HHZ-8140-2022; Pareschi, Lorenzo/D-2036-2012","Pareschi, Lorenzo/0000-0002-8706-6939; Pareschi, Lorenzo/0000-0002-8706-6939",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706451700001,0
J,"Fujita, Y; Nagarajan, P; Kataoka, T; Ishikawa, T",,,,"Fujita, Yasuhiro; Nagarajan, Prabhat; Kataoka, Toshiki; Ishikawa, Takahiro",,,ChainerRL: A Deep Reinforcement Learning Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: https://github.com/chainer/chainerrl.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,14,,,,,,,,,,,,,,,,WOS:000656390000001,0
J,"Luo, ZT; Sang, HY; Mallick, B",,,,"Luo, Zhao Tang; Sang, Huiyan; Mallick, Bani",,,A Bayesian Contiguous Partitioning Method for Learning Clustered Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article develops a Bayesian partitioning prior model from spanning trees of a graph, by first assigning priors on spanning trees, and then the number and the positions of removed edges given a spanning tree. The proposed method guarantees contiguity in clustering and allows to detect clusters with arbitrary shapes and sizes, whereas most existing partition models such as binary trees and Voronoi tessellations do not possess such properties. We embed this partition model within a hierarchical modeling framework to detect a clustered pattern in latent variables. We focus on illustrating the method through a clustered regression coefficient model for spatial data and propose extensions to other hierarchical models. We prove Bayesian posterior concentration results under an asymptotic framework with random graphs. We design an efficient collapsed Reversible Jump Markov chain Monte Carlo (RJ-MCMC) algorithm to estimate the clustered coefficient values and their uncertainty measures. Finally, we illustrate the performance of the model with simulation studies and a real data analysis of detecting the temperature-salinity relationship from water masses in the Atlantic Ocean.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500037,0
J,"Perez-Ortiz, M; Rivasplata, O; Shawe-Taylor, J; Szepesvari, C",,,,"Perez-Ortiz, Maria; Rivasplata, Omar; Shawe-Taylor, John; Szepesvari, Csaba",,,Tighter Risk Certificates for Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.",,,,,,"Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000706885200001,0
J,"Rosciszewski, P; Martyniak, M; Schodowski, F",,,,"Rosciszewski, Pawel; Martyniak, Michal; Schodowski, Filip",,,TensorHive: Management of Exclusive GPU Access for Distributed Machine Learning Workloads,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"TENSORHIVE is a tool for organizing work of research and engineering teams that use servers with GPUs for machine learning workloads. In a comprehensive web interface, it supports reservation of GPUs for exclusive usage, hardware monitoring, as well as configuring, executing and queuing distributed computational jobs. Focusing on easy installation and simple configuration, the tool automatically detects the available computing resources and monitors their utilization. Reservations granted on the basis of flexible access control settings are protected by pluggable violation hooks. The job execution module includes auto-configuration templates for distributed neural network training jobs in frameworks such as TensorFlow and PyTorch. Documentation, source code, usage examples and issue tracking are available at the project page: https://github.com/roscisz/TensorHive/",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706878700001,0
J,"Tian, Y; Feng, Y",,,,"Tian, Ye; Feng, Yang",,,RaSE: Random Subspace Ensemble Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a flexible ensemble classification framework, Random Subspace Ensemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate many weak learners, where each weak learner is a base classifier trained in a subspace optimally selected from a collection of random subspaces. To conduct subspace selection, we propose a new criterion, ratio information criterion (RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis includes the risk and Monte-Carlo variance of the RaSE classifier, establishing the screening consistency and weak consistency of RIC, and providing an upper bound for the misclassification rate of the RaSE classifier. In addition, we show that in a high-dimensional framework, the number of random subspaces needs to be very large to guarantee that a subspace covering signals is selected. Therefore, we propose an iterative version of the RaSE algorithm and prove that under some specific conditions, a smaller number of generated random subspaces are needed to find a desirable subspace through iteration. An array of simulations under various models and real-data applications demonstrate the effectiveness and robustness of the RaSE classifier and its iterative version in terms of low misclassification rate and accurate feature ranking. The RaSE algorithm is implemented in the R package RaSEn on CRAN.",,,,,"Feng, Yang/Y-7374-2019","Feng, Yang/0000-0001-7746-7598",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500045,0
J,"Wang, R; Xu, YS",,,,"Wang, Rui; Xu, Yuesheng",,,"Representer Theorems in Banach Spaces: Minimum Norm Interpolation, Regularized Learning and Semi-Discrete Inverse Problems",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning a function from a finite number of sampled data points (measurements) is a fundamental problem in science and engineering. This is often formulated as a minimum norm interpolation (MNI) problem, a regularized learning problem or, in general, a semi discrete inverse problem (SDIP), in either Hilbert spaces or Banach spaces. The goal of this paper is to systematically study solutions of these problems in Banach spaces. We aim at obtaining explicit representer theorems for their solutions, on which convenient solution methods can then be developed. For the MNI problem, the explicit representer theorems enable us to express the infimum in terms of the norm of the linear combination of the interpolation functionals. For the purpose of developing efficient computational algorithms, we establish the fixed-point equation formulation of solutions of these problems. We reveal that unlike in a Hilbert space, in general, solutions of these problems in a Banach space may not be able to be reduced to truly finite dimensional problems (with certain infinite dimensional components hidden). We demonstrate how this obstacle can be removed, reducing the original problem to a truly finite dimensional one, in the special case when the Banach space is l(1)(N).",,,,,"Xu, yue/HGE-1737-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706443900001,0
J,"Wilson, AC; Recht, B; Jordan, MI",,,,"Wilson, Ashia C.; Recht, Ben; Jordan, Michael, I",,,A Lyapunov Analysis of Accelerated Methods in Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Accelerated optimization methods, such as Nesterov's accelerated gradient method, play a significant role in optimization. Several accelerated methods are provably optimal under standard oracle models. Such optimality results are obtained using a technique known as estimate sequences which yields upper bounds on convergence properties. The technique of estimate sequences has long been considered difficult to understand and deploy, leading many researchers to generate alternative, more intuitive methods and analyses. We show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This connection allows us to develop a unified analysis of many existing accelerated algorithms, introduce new algorithms, and strengthen the connection between accelerated algorithms and continuous-time dynamical systems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,113,,,,,,,,,,,,,,,WOS:000663168400001,0
J,"Geoffrey, C; Guillaume, L; Matthieu, L",,,,"Geoffrey, Chinot; Guillaume, Lecue; Matthieu, Lerasle",,,Robust high dimensional learning for Lipschitz and convex losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We establish risk bounds for Regularized Empirical Risk Minimizers (RERM) when the loss is Lipschitz and convex and the regularization function is a norm. In a first part, we obtain these results in the i.i.d. setup under subgaussian assumptions on the design. In a second part, a more general framework where the design might have heavier tails and data may be corrupted by outliers both in the design and the response variables is considered. In this situation, RERM performs poorly in general. We analyse an alternative procedure based on median-of-means principles and called minmax MOM. We show optimal subgaussian deviation rates for these estimators in the relaxed setting. The main results are meta-theorems allowing a wide-range of applications to various problems in learning theory. To show a non-exhaustive sample of these potential applications, it is applied to classification problems with logistic loss functions regularized by LASSO and SLOPE, to regression problems with Huber loss regularized by Group LASSO and Total Variation. Another advantage of the minmax MOM formulation is that it suggests a systematic way to slightly modify descent based algorithms used in high-dimensional statistics to make them robust to outliers Lecu ' e and Lerasle (2017b). We illustrate this principle in a Simulations section where a minmax MOM version of classical proximal descent algorithms are turned into robust to outliers algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,233,,,,,,,,,,,,,,,WOS:000608903000001,0
J,"Kossaifi, J; Lipton, ZC; Kolbeinsson, A; Khanna, A; Furlanello, T; Anandkumar, A",,,,"Kossaifi, Jean; Lipton, Zachary C.; Kolbeinsson, Arinbjorn; Khanna, Aran; Furlanello, Tommaso; Anandkumar, Anima",,,Tensor Regression Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",,,,,"Kossaifi, Jean/AAW-8519-2021","Kossaifi, Jean/0000-0002-4445-3429; Kolbeinsson, Arinbjorn/0000-0002-2444-7048",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,123,,,,,,,,,,,,,,,WOS:000556191300001,0
J,"Krishnamurthy, A; Langford, J; Slivkins, A; Zhang, CC",,,,"Krishnamurthy, Akshay; Langford, John; Slivkins, Aleksandrs; Zhang, Chicheng",,,"Contextual Bandits with Continuous Actions: Smoothing, Zooming, and Adapting",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study contextual bandit learning with an abstract policy class and continuous action space. We obtain two qualitatively different regret bounds: one competes with a smoothed version of the policy class under no continuity assumptions, while the other requires standard Lipschitz assumptions. Both bounds exhibit data-dependent zooming behavior and, with no tuning, yield improved guarantees for benign problems. We also study adapting to unknown smoothness parameters, establishing a price-of-adaptivity and deriving optimal adaptive algorithms that require no additional information.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,137,,,,,,,,,,,,,,,WOS:000558790900001,0
J,"Lin, SB; Wang, D; Zhou, DX",,,,"Lin, Shao-Bo; Wang, Di; Zhou, Ding-Xuan",,,Distributed Kernel Ridge Regression with Communications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper focuses on generalization performance analysis for distributed algorithms in the framework of learning theory. Taking distributed kernel ridge regression (DKRR) for example, we succeed in deriving its optimal learning rates in expectation and providing theoretically optimal ranges of the number of local processors. Due to the gap between theory and experiments, we also deduce optimal learning rates for DKRR in probability to essentially reflect the generalization performance and limitations of DKRR. Furthermore, we propose a communication strategy to improve the learning performance of DKRR and demonstrate the power of communications in DKRR via both theoretical assessments and numerical experiments.",,,,,"Zhou, Ding-Xuan/B-3160-2013","Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,93,,,,,,,,,,,,,,,WOS:000545027700001,0
J,"Ma, Z; Ma, ZM; Yuan, HS",,,,"Ma, Zhuang; Ma, Zongming; Yuan, Hongsong",,,Universal Latent Space Model Fitting for Large Networks with Edge Covariates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent space models are effective tools for statistical modeling and visualization of network data. Due to their close connection to generalized linear models, it is also natural to incorporate covariate information in them. The current paper presents two universal fitting algorithms for networks with edge covariates: one based on nuclear norm penalization and the other based on projected gradient descent. Both algorithms are motivated by maximizing the likelihood function for an existing class of inner-product models, and we establish their statistical rates of convergence for these models. In addition, the theory informs us that both methods work simultaneously for a wide range of different latent space models that allow latent positions to affect edge formation in flexible ways, such as distance models. Furthermore, the effectiveness of the methods is demonstrated on a number of real world network data sets for different statistical tasks, including community detection with and without edge covariates, and network assisted learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300004,0
J,"Martens, J",,,,"Martens, James",,,New Insights and Perspectives on the Natural Gradient Method,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used empirical approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature matrices, but notably not the Hessian).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,146,,,,,,,,,,,,,,,WOS:000570102500001,0
J,"Tuo, R; Wang, WJ",,,,"Tuo, Rui; Wang, Wenjia",,,Kriging Prediction with Isotropic Matern Correlations: Robustness and Experimental Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work investigates the prediction performance of the kriging predictors. We derive some error bounds for the prediction error in terms of non-asymptotic probability under the uniform metric and L-p metrics when the spectral densities of both the true and the imposed correlation functions decay algebraically. The Matern family is a prominent class of correlation functions of this kind. Our analysis shows that, when the smoothness of the imposed correlation function exceeds that of the true correlation function, the prediction error becomes more sensitive to the space-filling property of the design points. In particular, the kriging predictor can still reach the optimal rate of convergence, if the experimental design scheme is quasi-uniform. Lower bounds of the kriging prediction error are also derived under the uniform metric and Lp metrics. An accurate characterization of this error is obtained, when an oversmoothed correlation function and a space-filling design is used.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,187,,,,,,,,,,,,,,,WOS:000570236900001,0
J,"Wang, YZ; Wu, S",,,,"Wang, Yazhen; Wu, Shang",,,Asymptotic Analysis via Stochastic Differential Equations of Gradient Descent Algorithms in Statistical and Computational Paradigms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates the asymptotic behaviors of gradient descent algorithms (particularly accelerated gradient descent and stochastic gradient descent) in the context of stochastic optimization arising in statistics and machine learning, where objective functions are estimated from available data. We show that these algorithms can be computationally modeled by continuous-time ordinary or stochastic differential equations. We establish gradient flow central limit theorems to describe the limiting dynamic behaviors of these computational algorithms and the large-sample performances of the related statistical procedures, as the number of algorithm iterations and data size both go to infinity, where the gradient flow central limit theorems are governed by some linear ordinary or stochastic differential equations, like time-dependent Ornstein-Uhlenbeck processes. We illustrate that our study can provide a novel unified framework for a joint computational and statistical asymptotic analysis, where the computational asymptotic analysis studies the dynamic behaviors of these algorithms with time (or the number of iterations in the algorithms), the statistical asymptotic analysis investigates the large-sample behaviors of the statistical procedures (like estimators and classifiers) that are computed by applying the algorithms; in fact, the statistical procedures are equal to the limits of the random sequences generated from these iterative algorithms, as the number of iterations goes to infinity. The joint analysis results based on the obtained gradient flow central limit theorems lead to the identification of four factors-learning rate, batch size, gradient covariance, and Hessian-to derive new theories regarding the local minima found by stochastic gradient descent for solving non-convex optimization problems.",,,,,"Wu, Shang/AAF-6284-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,199,,,,,,,,,,,,,,,WOS:000590004500001,0
J,"Yang, P; Chen, JB; Hsieh, CJ; Wang, JL; Jordan, MI",,,,"Yang, Puyudi; Chen, Jianbo; Hsieh, Cho-Jui; Wang, Jane-Ling; Jordan, Michael, I",,,Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000018,0
J,"Chen, J; Li, X",,,,"Chen, Ji; Li, Xiaodong",,,Model-free Nonconvex Matrix Completion: Local Minima Analysis and Applications in Memory-efficient Kernel PCA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work studies low-rank approximation of a positive semidefinite matrix from partial entries via nonconvex optimization. We characterized how well local-minimum based low-rank factorization approximates a fixed positive semidefinite matrix without any assumptions on the rank-matching, the condition number or eigenspace incoherence parameter. Furthermore, under certain assumptions on rank-matching and well-boundedness of condition numbers and eigenspace incoherence parameters, a corollary of our main theorem improves the state-of-the-art sampling rate results for nonconvex matrix completion with no spurious local minima in Ge et al. (2016, 2017). In addition, we have investigated when the proposed nonconvex optimization results in accurate low-rank approximations even in presence of large condition numbers, large incoherence parameters, or rank mismatching. We also propose to apply the nonconvex optimization to memory-efficient kernel PCA. Compared to the well-known Nystrom methods, numerical experiments indicate that the proposed nonconvex optimization approach yields more stable results in both low-rank approximation and clustering.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,142,,,,,,,,,,,,,,,WOS:000491132200006,0
J,"Keys, KL; Zhou, H; Lange, K",,,,"Keys, Kevin L.; Zhou, Hua; Lange, Kenneth",,,Proximal Distance Algorithms: Theory and Practice,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Proximal distance algorithms combine the classical penalty method of constrained minimization with distance majorization. If f(x) is the loss function, and C is the constraint set in a constrained minimization problem, then the proximal distance principle mandates minimizing the penalized loss f (x) + rho/2 dist(x, C)(2) and following the solution x(rho) to its limit as rho tends to infinity. At each iteration the squared Euclidean distance dist(x,C)(2) is majorized by the spherical quadratic parallel to x-P-c(x(k))parallel to(2), where P-c(x(k)) denotes the projection of the current iterate x(k) onto C. The minimum of the surrogate function f (x) + rho/2 parallel to x-P-c(x(k))parallel to(2) is given by the proximal map prox(rho-1) (f)[P-c(x(k))]. The next iterate x(k+1) automatically decreases the original penalized loss for fixed rho. Since many explicit projections and proximal maps are known, it is straightforward to derive and implement novel optimization algorithms in this setting. These algorithms can take hundreds if not thousands of iterations to converge, but the simple nature of each iteration makes proximal distance algorithms competitive with traditional algorithms. For convex problems, proximal distance algorithms reduce to proximal gradient algorithms and therefore enjoy well understood convergence properties. For nonconvex problems, one can attack convergence by invoking Zangwill's theorem. Our numerical examples demonstrate the utility of proximal distance algorithms in various high-dimensional settings, including a) linear programming, b) constrained least squares, c) projection to the closest kinship matrix, d) projection onto a second-order cone constraint, e) calculation of Horn's copositive matrix index, f) linear complementarity programming, and g) sparse principal components analysis. The proximal distance algorithm in each case is competitive or superior in speed to traditional methods such as the interior point method and the alternating direction method of multipliers (ADMM). Source code for the numerical examples can be found at https://github.com/klkeys/proxdist.",,,,,"Keys, Kevin Lawrence/I-8292-2019","Keys, Kevin Lawrence/0000-0003-4886-6666; Zhou, Hua/0000-0003-1320-7118",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,66,,,,,,,,,,31649491,,,,,WOS:000467894300001,0
J,"Koppel, A; Warnell, G; Stump, E; Ribeiro, A",,,,"Koppel, Alec; Warnell, Garrett; Stump, Ethan; Ribeiro, Alejandro",,,Parsimonious Online Learning with Kernels via Sparse Projections in Function Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Despite their attractiveness, popular perception is that techniques for nonparametric function approximation do not scale to streaming data due to an intractable growth in the amount of storage they require. To solve this problem in a memory-affordable way, we propose an online technique based on functional stochastic gradient descent in tandem with supervised sparsification based on greedy function subspace projections. The method, called parsimonious online learning with kernels (POLK), provides a controllable tradeoff between its solution accuracy and the amount of memory it requires. We derive conditions under which the generated function sequence converges almost surely to the optimal function, and we establish that the memory requirement remains finite. We evaluate POLK for kernel multi-class logistic regression and kernel hinge-loss classification on three canonical data sets: a synthetic Gaussian mixture model, the MNIST hand-written digits, and the Brodatz texture database. On all three tasks, we observe a favorable trade-off of objective function evaluation, classification performance, and complexity of the nonparametric regressor extracted by the proposed method.",,,,,"Koppel, Alec/ABC-5438-2020","Koppel, Alec/0000-0003-2447-2873; Ribeiro, Alejandro/0000-0003-4230-9906",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,3,,,,,,,,,,,,,,,WOS:000458661800001,0
J,"May, A; Garakani, AB; Lu, ZY; Guo, D; Liu, K; Bellet, A; Fan, LX; Collins, M; Hsu, D; Kingsbury, B; Picheny, M; Sha, F",,,,"May, Avner; Garakani, Alireza Bagheri; Lu, Zhiyun; Guo, Dong; Liu, Kuan; Bellet, Aurelien; Fan, Linxi; Collins, Michael; Hsu, Daniel; Kingsbury, Brian; Picheny, Michael; Sha, Fei",,,Kernel Approximation Methods for Speech Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the performance of kernel methods on the acoustic modeling task for automatic speech recognition, and compare their performance to deep neural networks (DNNs). To scale the kernel methods to large data sets, we use the random Fourier feature method of Rahimi and Recht (2007). We propose two novel techniques for improving the performance of kernel acoustic models. First, we propose a simple but effective feature selection method which reduces the number of random features required to attain a fixed level of performance. Second, we present a number of metrics which correlate strongly with speech recognition performance when computed on the heldout set; we attain improved performance by using these metrics to decide when to stop training. Additionally, we show that the linear bottleneck method of Sainath et al. (2013a) improves the performance of our kernel models significantly, in addition to speeding up training and making the models more compact. Leveraging these three methods, the kernel methods attain token error rates between 0.5% better and 0.1% worse than fully-connected DNNs across four speech recognition data sets, including the TIMIT and Broadcast News benchmark tasks.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,59,,,,,,,,,,,,,,,WOS:000467878400001,0
J,"Bacry, E; Bompaire, M; Deegan, P; Gaiffas, S; Poulsen, SV",,,,"Bacry, Emmanuel; Bompaire, Martin; Deegan, Philip; Gaiffas, Stephane; Poulsen, Soren V.",,,"tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces tick, is a statistical learning library for Python 3, with a particular emphasis on time-dependent models, such as point processes, tools for generalized linear models and survival analysis. The core of the library provides model computational classes, solvers and proximal operators for regularization. It relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https://github.com/X-DataInitiative/tick.",,,,,"Bompaire, Martin/ABF-7984-2020",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,214,,,,,,,,,,,,,,,WOS:000435628800001,0
J,"Bybee, L; Atchade, Y",,,,"Bybee, Leland; Atchade, Yves",,,Change-Point Computation for Large Graphical Models: A Scalable Algorithm for Gaussian Graphical Models with Change-Points,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphical models with change-points are computationally challenging to fit, particularly in cases where the number of observation points and the number of nodes in the graph are large. Focusing on Gaussian graphical models, we introduce an approximate majorize-minimize (MM) algorithm that can be useful for computing change-points in large graphical models. The proposed algorithm is an order of magnitude faster than a brute force search. Under some regularity conditions on the data generating process, we show that with high probability, the algorithm converges to a value that is within statistical error of the true change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is also introduced. The performances of the proposed algorithms are evaluated on synthetic data sets and the algorithm is also used to analyze structural changes in the S&P 500 over the period 2000-2016.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,11,,,,,,,,,,,,,,,WOS:000443225400001,0
J,"Celisse, A; Mary-Huard, T",,,,"Celisse, Alain; Mary-Huard, Tristan",,,Theoretical Analysis of Cross-Validation for Estimating the Risk of the k-Nearest Neighbor Classifier,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The present work aims at deriving theoretical guaranties on the behavior of some cross-validation procedures applied to the k-nearest neighbors (kNN) rule in the context of binary classification. Here we focus on the leave-p-out cross-validation (LpO) used to assess the performance of the kNN classifier. Remarkably this LpO estimator can be efficiently computed in this context using closed-form formulas derived by Celisse and Mary-Huard (2011). We describe a general strategy to derive moment and exponential concentration inequalities for the LpO estimator applied to the kNN classifier. Such results are obtained first by exploiting the connection between the LpO estimator and U-statistics, and second by making an intensive use of the generalized Efron-Stein inequality applied to the L1O estimator. One other important contribution is made by deriving new quantifications of the discrepancy between the LpO estimator and the classification error/risk of the kNN classifier. The optimality of these bounds is discussed by means of several lower bounds as well as simulation experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,58,,,,,,,,,,,,,,,WOS:000452043700001,0
J,"Grigoryeva, L; Ortega, JP",,,,"Grigoryeva, Lyudmila; Ortega, Juan-Pablo",,,Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new class of non-homogeneous state-affine systems is introduced for use in reservoir computing. Sufficient conditions are identified that guarantee first, that the associated reservoir computers with linear readouts are causal, time-invariant, and satisfy the fading memory property and second, that a subset of this class is universal in the category of fading memory filters with stochastic almost surely uniformly bounded inputs. This means that any discrete-time filter that satisfies the fading memory property with random inputs of that type can be uniformly approximated by elements in the non-homogeneous state-affine family.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,24,,,,,,,,,,,,,,,WOS:000443229200001,0
J,"Helmbold, DP; Long, PM",,,,"Helmbold, David P.; Long, Philip M.",,,Surprising properties of dropout in deep networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,200,,,,,,,,,,,,,,,WOS:000435456100001,0
J,"Miasojedow, B; Rejchel, W",,,,"Miasojedow, Blazej; Rejchel, Wojciech",,,Sparse Estimation in Ising Model via Penalized Monte Carlo Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a model selection problem in high-dimensional binary Markov random fields. The usefulness of the Ising model in studying systems of complex interactions has been confirmed in many papers. The main drawback of this model is the intractable norming constant that makes estimation of parameters very challenging. In the paper we propose a Lasso penalized version of the Monte Carlo maximum likelihood method. We prove that our algorithm, under mild regularity conditions, recognizes the true dependence structure of the graph with high probability. The efficiency of the proposed method is also investigated via numerical studies.",,,,,,"Miasojedow, Blazej/0000-0002-3691-9372; Rejchel, Wojciech/0000-0003-1148-1439",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,75,,,,,,,,,,,,,,,WOS:000454478200001,0
J,"Vitelli, V; Sorensen, O; Crispino, M; Frigessi, A; Arjas, E",,,,"Vitelli, Valeria; Sorensen, Oystein; Crispino, Marta; Frigessi, Arnoldo; Arjas, Elja",,,Probabilistic preference learning with the Mallows rank model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-k items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quanti fi cations of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets.",,,,,"S√Érensen, √É¬òystein/AAW-6186-2020; Vitelli, Valeria/AAE-9152-2019","S√Érensen, √É¬òystein/0000-0003-0724-3542; Vitelli, Valeria/0000-0002-6746-0453; Frigessi, Arnoldo/0000-0001-7103-7589",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,49,,,,,,,,,,,,,,,,WOS:000433252300001,0
J,"Yao, QM; Kwok, JT",,,,"Yao, Quanming; Kwok, James T.",,,Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, a popular subclass of l(1) -based nonconvex sparsity-inducing and low-rank regularizers is considered. This includes nonconvex variants of lasso, sparse group lasso, tree-structured lasso, nuclear norm and total variation regularizers. We propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex one, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). This is further extended to consider cases where the convexified regularizer does not have a closed-form proximal step, and when the loss function is nonconvex nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem.",,,,,"Yao, Quanming/Y-6095-2019","Yao, Quanming/0000-0001-8944-8618",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,179,,,,,,,,,,,,,,,WOS:000435443800001,0
J,"Hamm, J",,,,"Hamm, Jihun",,,Minimax Filter: Learning to Preserve Privacy from Inference Attacks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differentially privacy, which uses a more rigorous definition of privacy loss, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform minimax optimization. In addition, the paper proposes a noisy minimax filter which combines minimax filter and differentially-private mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or higher target task accuracy and lower inference accuracy, often significantly lower than previous methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,129,,,,,,,,,,,,,,,WOS:000424545500001,0
J,"Minsker, S; Srivastava, S; Lin, LZ; Dunson, DB",,,,"Minsker, Stanislav; Srivastava, Sanvesh; Lin, Lizhen; Dunson, David B.",,,Robust and Scalable Bayes via a Median of Subset Posterior Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,124,,,,,,,,,,,,,,,WOS:000424542000001,0
J,"Schmitt, M; Schuller, B",,,,"Schmitt, Maximilian; Schuller, Bjoern",,,openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce oPENXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual descriptors, introducing a prior step of vector quantisation. The oPENXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed sub-bags to a final bag. It provides a variety of extensions and options. To our knowledge, oPENXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool have been exemplified in different scenarios: sentiment analysis in tweets, classification of snore sounds, and time-dependent emotion recognition based on acoustic, linguistic, and visual information, where improved results over other feature representations were observed.",,,,,"Schmitt, Maximilian/ABD-4551-2020","Schmitt, Maximilian/0000-0001-7453-5612",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,96,,,,,,,,,,,,,,,WOS:000412485700001,0
J,"Dutta, S; Sarkar, S; Ghosh, AK",,,,"Dutta, Subhajit; Sarkar, Soham; Ghosh, Anil K.",,,Multi-scale Classification using Localized Spatial Depth,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we develop and investigate a new classifier based on features extracted using spatial depth. Our construction is based on fitting a generalized additive model to posterior probabilities of different competing classes. To cope with possible multi-modal as well as non-elliptic nature of the population distribution, we also develop a localized version of spatial depth and use that with varying degrees of localization to build the classifier. Final classification is done by aggregating several posterior probability estimates, each of which is obtained using this localized spatial depth with a fixed scale of localization. The proposed classifier can be conveniently used even when the dimension of the data is larger than the sample size, and its good discriminatory power for such data has been established using theoretical as well as numerical results.",,,,,,"Sarkar, Soham/0000-0003-0412-6826",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,218,,,,,,,,,,,,,,,WOS:000391838000001,0
J,"Hernandez-Lobato, D; Morales-Mombiela, P; Lopez-Paz, D; Suarez, A",,,,"Hernandez-Lobato, Daniel; Morales-Mombiela, Pablo; Lopez-Paz, David; Suarez, Alberto",,,Non-linear Causal Inference using Gaussianity Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference.",,,,,"Su√°rez, Alberto/D-6293-2011","Su√°rez, Alberto/0000-0003-4534-0909",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,28,,,,,,,,,,,,,,,WOS:000391477400001,0
J,"Juba, B",,,,"Juba, Brendan",,,Integrated Common Sense Learning and Planning in POMDPs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We formulate a new variant of the problem of planning in an unknown environment, for which we can provide algorithms with reasonable theoretical guarantees in spite of large state spaces and time horizons, partial observability, and complex dynamics. In this variant, an agent is given a collection of example traces produced by a reference policy, which may, for example, capture the agent's past behavior. The agent is (only) asked to find policies that are supported by regularities in the dynamics that are observable on these example traces. We describe an efficient algorithm that uses such common sense knowledge reflected in the example traces to construct decision tree policies for goal-oriented factored POMDPs. More precisely, our algorithm (provably) succeeds at finding a policy for a given input goal when (1) there is a CNF that is almost always observed satisfied on the traces of the POMDP, capturing a sufficient approximation of its dynamics and (2) for a decision tree policy of bounded complexity, there exist small-space resolution proofs that the goal is achieved on each branch using the aforementioned CNF capturing the common sense rules. Such a CNF always exists for noisy STRIPS domains, for example. Our results thus essentially establish that the possession of a suitable exploration policy for collecting the necessary examples is the fundamental obstacle to learning to act in such environments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,96,,,,,,,,,,,,,,,WOS:000391539900001,0
J,"Lazaric, A; Ghavamzadeh, M; Munos, R",,,,"Lazaric, Alessandro; Ghavamzadeh, Mohammad; Munos, Remi",,,Analysis of Classification-based Policy Iteration Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a variant of the classification-based approach to policy iteration which uses a cost-sensitive loss function weighting each classification mistake by its actual regret, that is, the difference between the action-value of the greedy action and of the action chosen by the classifier. For this algorithm, we provide a full finite-sample analysis. Our results state a performance bound in terms of the number of policy improvement steps, the number of rollouts used in each iteration, the capacity of the considered policy space (classifier), and a capacity measure which indicates how well the policy space can approximate policies that are greedy with respect to any of its members. The analysis reveals a tradeoff between the estimation and approximation errors in this classification-based policy iteration setting. Furthermore it con firms the intuition that classification-based policy iteration algorithms could be favorably compared to value-based approaches when the policies can be approximated more easily than their corresponding value functions. We also study the consistency of the algorithm when there exists a sequence of policy spaces with increasing capacity.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,19,,,,,,,,,,,,,,,WOS:000391473700001,0
J,"Martinez, AM; Webb, GI; Chen, SL; Zaidi, NA",,,,"Martinez, Ana M.; Webb, Geoffrey I.; Chen, Shenglei; Zaidi, Nayyar A.",,,Scalable Learning of Bayesian Network Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the k-dependence Bayesian classifier (KDB) that discriminatively selects a sub-model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on 16 large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression.",,,,,"Webb, Geoff i/R-9967-2017; Webb, Geoffrey/Q-7270-2019","Webb, Geoff i/0000-0001-9963-5169; Webb, Geoffrey/0000-0001-9963-5169; Zaidi, Nayyar/0000-0003-4024-2517; Martinez, Ana M./0000-0002-4220-8358",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,44,,,,,,,,,,,,,,,WOS:000391484200001,0
J,"Misra, N; Kuruoglu, EE",,,,"Misra, Navodit; Kuruoglu, Ercan E.",,,Stable Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stable random variables are motivated by the central limit theorem for densities with (potentially) unbounded variance and can be thought of as natural generalizations of the Gaussian distribution to skewed and heavy-tailed phenomenon. In this paper, we introduce alpha stable graphical (alpha-SG) models, a class of multivariate stable densities that can also be represented as Bayesian networks whose edges encode linear dependencies between random variables. One major hurdle to the extensive use of stable distributions is the lack of a closed-form analytical expression for their densities. This makes penalized maximum likelihood based learning computationally demanding. We establish theoretically that the Bayesian information criterion (BIC) can asymptotically be reduced to the computationally more tractable minimum dispersion criterion (MDC) and develop StabLe, a structure learning algorithm based on MDC. We use simulated datasets for five benchmark network topologies to empirically demonstrate how StabLe improves upon ordinary least squares (OLS) regression. We also apply StabLe to microarray gene expression data for lymphoblastoid cells from 727 individuals belonging to eight global population groups. We establish that StabLe improves test set performance relative to OLS via ten-fold cross-validation. Finally, we develop SGEX, a method for quantifying differential expression of genes between different population groups.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,36,168,,,,,,,,,,,,,,,WOS:000391673300001,0
J,"Su, WJ; Boyd, S; Candes, EJ",,,,"Su, Weijie; Boyd, Stephen; Candes, Emmanuel J.",,,A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,153,,,,,,,,,,,,,,,WOS:000391664200001,0
J,"Vapnik, V; Izmailov, R",,,,"Vapnik, Vladimir; Izmailov, Rauf",,,Synergy of Monotonic Rules,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article describes a method for constructing a special rule (we call it synergy rule) that uses as its input information the outputs (scores) of several monotonic rules which solve the same pattern recognition problem. As an example of scores of such monotonic rules we consider here scores of SVM classifiers. In order to construct the optimal synergy rule, we estimate the conditional probability function based on the direct problem setting, which requires solving a Fredholm integral equation. Generally, solving a Fredholm equation is an ill-posed problem. However, in our model, we look for the solution of the equation in the set of monotonic and bounded functions, which makes the problem well-posed. This allows us to solve the equation accurately even with training data sets of limited size. In order to construct a monotonic solution, we use the set of functions that belong to Reproducing Kernel Hilbert Space (RKHS) associated with the INK-spline kernel (splines with In finite Numbers of Knots) of degree zero. The paper provides details of the methods for finding multidimensional conditional probability in a set of monotonic functions to obtain the corresponding synergy rules. We demonstrate effectiveness of such rules for 1) solving standard pattern recognition problems, 2) constructing multi-class classification rules, 3) constructing a method for knowledge transfer from multiple intelligent teachers in the LUPI paradigm.",,,,,,"Izmailov, Rauf/0000-0002-7326-669X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,136,,,,,,,,,,,,,,,WOS:000391658600001,0
J,"Lopez-Paz, D; Muandet, K; Recht, B",,,,"Lopez-Paz, David; Muandet, Krikamol; Recht, Benjamin",,,The Randomized Causation Coefficient,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We are interested in learning causal relationships between pairs of random variables, purely from observational data. To effectively address this task, the state-of-the-art relies on strong assumptions on the mechanisms mapping causes to effects, such as invertibility or the existence of additive noise, which only hold in limited situations. On the contrary, this short paper proposes to learn how to perform causal inference directly from data, without the need of feature engineering. In particular, we pose causality as a kernel mean embedding classification problem, where inputs are samples from arbitrary probability distributions on pairs of random variables, and labels are types of causal relationships. We validate the performance of our method on synthetic and real-world data against the state-of-the-art. Moreover, we submitted our algorithm to the ChaLearn's Fast Causation Coefficient Challenge competition, with which we won the fastest code prize and ranked third in the overall leaderboard.",,,,,,"Muandet, Krikamol/0000-0002-4182-5282",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2901,2907,,,,,,,,,,,,,,,,WOS:000369888000019,0
J,"Montufar, G; Ay, N; Ghazi-Zahedi, K",,,,"Montufar, Guido; Ay, Nihat; Ghazi-Zahedi, Keyan",,,Geometry and Expressive Power of Conditional Restricted Boltzmann Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parameterized by interaction weights and biases. We address the representational power of these models, proving results on their ability to represent conditional Markov random fields and conditional distributions with restricted supports, the minimal size of universal approximators, the maximal model approximation errors, and on the dimension of the set of representable conditional distributions. We contribute new tools for investigating conditional probability models, which allow us to improve the results that can be derived from existing work on restricted Boltzmann machine probability models.",,,,,,"Montufar Cuartas, Guido F./0000-0002-0131-2669",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2405,2436,,,,,,,,,,,,,,,,WOS:000369888000002,0
J,"Pokarowski, P; Mielniczuk, J",,,,"Pokarowski, Piotr; Mielniczuk, Jan",,,Combined l(1) and Greedy l(0) Penalized Least Squares for Linear Model Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening-ordering-selection (SOS). Screening of predictors is based on the thresholded Lasso that is l(1) penalized least squares. The screened predictors are then fitted using least squares (LS) and ordered with respect to their vertical bar t vertical bar statistics. Finally, a model is selected using greedy generalized information criterion (GIC) that is l(0) penalized LS in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the SOS algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. Our error bounds and numerical experiments show that SOS is worth considering alternative for multi-stage convex relaxation, the latest quasiconvex penalized LS. For the traditional setting (n > p) we give Sanov-type bounds on the error probabilities of the ordering-selection algorithm. It is surprising consequence of our bounds that the selection error of greedy GIC is asymptotically not larger than of exhaustive GIC.",,,,,,"Mielniczuk, Jan/0000-0003-2621-2303",,,,,,,,,,,,,1532-4435,,,,,MAY,2015,16,,,,,,961,992,,,,,,,,,,,,,,,,WOS:000369886400001,0
J,"Cano, A; Luna, JM; Zafra, A; Ventura, S",,,,"Cano, Alberto; Maria Luna, Jose; Zafra, Amelia; Ventura, Sebastian",,,A Classification Module for Genetic Programming Algorithms in JCLEC,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"JCLEC-Classification is a usable and extensible open source library for genetic programming classification algorithms. It houses implementations of rule-based methods for classification based on genetic programming, supporting multiple model representations and providing to users the tools to implement any classifier easily. The software is written in Java and it is available from http : // jclec. sourceforge. net/classification under the GPL license.",,,,,"Cano, Alberto/K-4889-2014; Zafra, Amelia/H-5714-2019; Ventura, Sebastian/A-7753-2008; Luna, Jose Maria/K-4893-2014","Cano, Alberto/0000-0001-9027-298X; Zafra, Amelia/0000-0003-3868-6143; Ventura, Sebastian/0000-0003-4216-6378; Luna, Jose Maria/0000-0003-3537-2931",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,491,494,,,,,,,,,,,,,,,,WOS:000369886000005,0
J,"Qiu, Q; Sapiro, G",,,,"Qiu, Qiang; Sapiro, Guillermo",,,Learning Transformations for Clustering and Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A low-rank transformation learning framework for subspace clustering and classification is proposed here. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. Low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using nuclear norm as the modeling and optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a maximally separated structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separation between the subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results presented here help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public data sets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification. The learned low cost transform is also applicable to other classification frameworks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2015,16,,,,,,187,226,,,,,,,,,,,,,,,,WOS:000369885800002,0
J,"Doliwa, T; Fan, GJ; Simon, HU; Zilles, S",,,,"Doliwa, Thorsten; Fan, Gaojian; Simon, Hans Ulrich; Zilles, Sandra",,,"Recursive Teaching Dimension, VC-Dimension and Sample Compression",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with various combinatorial parameters of classes that can be learned from a small set of examples. We show that the recursive teaching dimension, recently introduced by Zilles et al. (2008), is strongly connected to known complexity notions in machine learning, e.g., the self-directed learning complexity and the VC-dimension. To the best of our knowledge these are the first results unveiling such relations between teaching and query learning as well as between teaching and the VC-dimension. It will turn out that for many natural classes the RTD is upper-bounded by the VCD, e.g., classes of VCdimension 1, intersection-closed classes and finite maximum classes. However, we will also show that there are certain (but rare) classes for which the recursive teaching dimension exceeds the VC-dimension. Moreover, for maximum classes, the combinatorial structure induced by the RTD, called teaching plan, is highly similar to the structure of sample compression schemes. Indeed one can transform any repetition-free teaching plan for a maximum class C into an unlabeled sample compression scheme for C and vice versa, while the latter is produced by (i) the corner-peeling algorithm of Rubinstein and Rubinstein (2012) and (ii) the tail matching algorithm of Kuzmin and Warmuth (2007).",,,,,,"Simon, Hans/0000-0002-1587-0944",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3107,3131,,,,,,,,,,,,,,,,WOS:000344638800009,0
J,"Lowd, D; Davis, J",,,,"Lowd, Daniel; Davis, Jesse",,,Improving Markov Network Structure Learning Using Decision Trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most existing algorithms for learning Markov network structure either are limited to learning interactions among few variables or are very slow, due to the large space of possible structures. In this paper, we propose three new methods for using decision trees to learn Markov network structures. The advantage of using decision trees is that they are very fast to learn and can represent complex interactions among many variables. The first method, DTSL, learns a decision tree to predict each variable and converts each tree into a set of conjunctive features that define the Markov network structure. The second, DT-BLM, builds on DTSL by using it to initialize a search-based Markov network learning algorithm recently proposed by DTSL with those learned by an L1-regularized logistic regression method (L1) proposed by Ravikumar et al. (2009). In an extensive empirical evaluation on 20 data sets, DTSL is comparable to L1 and significantly faster and more accurate than two other baselines. DT-BLM is slower than DTSL, but obtains slightly higher accuracy. DT+L1 combines the strengths of DTSL and L1 to perform significantly better than either of them with only a modest increase in training time.",,,,,"Davis, Jesse/A-3596-2015","Davis, Jesse/0000-0002-3748-9263",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,501,532,,,,,,,,,,,,,,,,WOS:000335457700005,0
J,"Hyttinen, A; Eberhardt, F; Hoyer, PO",,,,"Hyttinen, Antti; Eberhardt, Frederick; Hoyer, Patrik O.",,,Experiment Selection for Causal Discovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufficient) set of variables. Recent results in the causal discovery literature have explored various identifiability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection.",,,,,,"Hyttinen, Antti/0000-0002-6649-3229",,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3041,3071,,,,,,,,,,,,,,,,WOS:000328603600004,0
J,"Neuvial, P",,,,"Neuvial, Pierre",,,Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is defined as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the Benjamini-Hochberg procedure achieves FDR control at any pre-specified level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to infinity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m(-k/(2k+1)), where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one-and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model.",,,,,"Neuvial, Pierre/ABG-7649-2020","Neuvial, Pierre/0000-0003-3584-9998",,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1423,1459,,,,,,,,,,,,,,,,WOS:000320709300008,0
J,"Lee, J; Sun, MX; Lebanon, G",,,,"Lee, Joonseok; Sun, Mingxuan; Lebanon, Guy",,,PREA: Personalized Recommendation Algorithms Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recommendation systems are important business applications with significant economic impact. In recent years, a large number of algorithms have been proposed for recommendation systems. In this paper, we describe an open-source toolkit implementing many recommendation algorithms as well as popular evaluation metrics. In contrast to other packages, our toolkit implements recent state-of-the-art algorithms as well as most classic algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2699,2703,,,,,,,,,,,,,,,,WOS:000309580600007,0
J,"Fortin, FA; De Rainville, FM; Gardner, MA; Parizeau, M; Gagne, C",,,,"Fortin, Felix-Antoine; De Rainville, Francois-Michel; Gardner, Marc-Andre; Parizeau, Marc; Gagne, Christian",,,DEAP: Evolutionary Algorithms Made Easy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black-box frameworks. Freely available with extensive documentation at http://deap.gel.ulaval.ca, DEAP is an open source project under an LGPL license.",,,,,,"Parizeau, Marc/0000-0001-9929-646X; Gagne, Christian/0000-0003-3697-4184",,,,,,,,,,,,,1532-4435,,,,,JUL,2012,13,,,,,,2171,2175,,,,,,,,,,,,,,,,WOS:000307496000002,0
J,"Zeng, J",,,,"Zeng, Jia",,,A Topic Modeling Toolbox Using Belief Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2012,13,,,,,,2233,2236,,,,,,,,,,,,,,,,WOS:000307496000005,0
J,"Hennig, P; Schuler, CJ",,,,"Hennig, Philipp; Schuler, Christian J.",,,Entropy Search for Information-Efficient Global Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1809,1837,,,,,,,,,,,,,,,,WOS:000307020700003,0
J,"Tsamardinos, I; Triantafillou, S; Lagani, V",,,,"Tsamardinos, Ioannis; Triantafillou, Sofia; Lagani, Vincenzo",,,Towards Integrative Causal Analysis of Heterogeneous Data Sets and Studies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the field of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (fit) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causally-inspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org.",,,,,"Lagani, Vincenzo/H-2820-2019","Lagani, Vincenzo/0000-0002-6552-6076",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1097,1157,,,,,,,,,,,,,,,,WOS:000303773100008,0
J,"Bergstra, J; Bengio, Y",,,,"Bergstra, James; Bengio, Yoshua",,,Random Search for Hyper-Parameter Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent High Throughput methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,281,305,,,,,,,,,,,,,,,,WOS:000303046000003,0
J,"Hannah, LA; Blei, DM; Powell, WB",,,,"Hannah, Lauren A.; Blei, David M.; Powell, Warren B.",,,Dirichlet Process Mixtures of Generalized Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new class of methods for nonparametric regression. Given a data set of input-response pairs, the DP-GLM produces a global model of the joint distribution through a mixture of local generalized linear models. DP-GLMs allow both continuous and categorical inputs, and can model the same class of responses that can be modeled with a generalized linear model. We study the properties of the DP-GLM, and show why it provides better predictions and density estimates than existing Dirichlet process mixture regression models. We give conditions for weak consistency of the joint distribution and pointwise consistency of the regression estimate.",,,,,"Powell, Warren/N-8263-2019",,,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1923,1953,,,,,,,,,,,,,,,,WOS:000293757200005,0
J,"Wang, LW; Sugiyama, M; Jing, ZX; Yang, C; Zhou, ZH; Feng, JF",,,,"Wang, Liwei; Sugiyama, Masashi; Jing, Zhaoxiang; Yang, Cheng; Zhou, Zhi-Hua; Feng, Jufu",,,A Refined Margin Analysis for Boosting Algorithms via Equilibrium Margin,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Much attention has been paid to the theoretical explanation of the empirical success of AdaBoost. The most influential work is the margin theory, which is essentially an upper bound for the generalization error of any voting classifier in terms of the margin distribution over the training data. However, important questions were raised about the margin explanation. Breiman (1999) proved a bound in terms of the minimum margin, which is sharper than the margin distribution bound. He argued that the minimum margin would be better in predicting the generalization error. Grove and Schuurmans (1998) developed an algorithm called LP-AdaBoost which maximizes the minimum margin while keeping all other factors the same as AdaBoost. In experiments however, LP-AdaBoost usually performs worse than AdaBoost, putting the margin explanation into serious doubt. In this paper, we make a refined analysis of the margin theory. We prove a bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound is uniformly sharper than Breiman's minimum margin bound. Thus our result suggests that the minimum margin may be not crucial for the generalization error. We also show that a large Emargin and a small empirical error at Emargin imply a smaller bound of the generalization error. Experimental results on benchmark data sets demonstrate that AdaBoost usually has a larger Emargin and a smaller test error than LP-AdaBoost, which agrees well with our theory.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1835,1863,,,,,,,,,,,,,,,,WOS:000293757200002,0
J,"Shimizu, S; Inazumi, T; Sogawa, Y; Hyvarinen, A; Kawahara, Y; Washio, T; Hoyer, PO; Bollen, K",,,,"Shimizu, Shohei; Inazumi, Takanori; Sogawa, Yasuhiro; Hyvarinen, Aapo; Kawahara, Yoshinobu; Washio, Takashi; Hoyer, Patrik O.; Bollen, Kenneth",,,DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite.",,,,,"Hyvarinen, Aapo/E-9006-2012; Shimizu, Shohei/B-4425-2010; KAWAHARA, Yoshinobu/AAM-7540-2020; Kawahara, Yoshinobu/J-2462-2014","Shimizu, Shohei/0000-0002-1931-0733; KAWAHARA, Yoshinobu/0000-0001-7789-4709; Kawahara, Yoshinobu/0000-0001-7789-4709; Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1225,1248,,,,,,,,,,,,,,,,WOS:000290096100002,0
J,"Henderson, J; Titov, I",,,,"Henderson, James; Titov, Ivan",,,Incremental Sigmoid Belief Networks for Grammar Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a class of Bayesian networks appropriate for structured prediction problems where the Bayesian network's model structure is a function of the predicted output structure. These incremental sigmoid belief networks (ISBNs) make decoding possible because inference with partial output structures does not require summing over the unboundedly many compatible model structures, due to their directed edges and incrementally specified model structure. ISBNs are specifically targeted at challenging structured prediction problems such as natural language parsing, where learning the domain's complex statistical dependencies benefits from large numbers of latent variables. While exact inference in ISBNs with large numbers of latent variables is not tractable, we propose two efficient approximations. First, we demonstrate that a previous neural network parsing model can be viewed as a coarse mean-field approximation to inference with ISBNs. We then derive a more accurate but still tractable variational approximation, which proves effective in artificial experiments. We compare the effectiveness of these models on a benchmark natural language parsing task, where they achieve accuracy competitive with the state-of-the-art. The model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of natural language grammar learning.",,,,,,"Henderson, James/0000-0003-3714-4799",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3541,3570,,,,,,,,,,,,,,,,WOS:000286637200009,0
J,"Vinh, NX; Epps, J; Bailey, J",,,,"Vinh, Nguyen Xuan; Epps, Julien; Bailey, James",,,"Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0,1] range better than other normalized variants.",,,,,"Nguyen, Xuan Vinh/E-9861-2011; Kovalev, Grigoriy/V-8768-2018","Epps, Julien/0000-0001-6624-5551",,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2837,2854,,,,,,,,,,,,,,,,WOS:000284040000008,0
J,"Forero, PA; Cano, A; Giannakis, GB",,,,"Forero, Pedro A.; Cano, Alfonso; Giannakis, Georgios B.",,,Consensus-Based Distributed Support Vector Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops algorithms to train support vector machines when training data are distributed across different nodes, and their communication to a centralized processing unit is prohibited due to, for example, communication complexity, scalability, or privacy reasons. To accomplish this goal, the centralized linear SVM problem is cast as a set of decentralized convex optimization sub-problems (one per node) with consensus constraints on the wanted classifier parameters. Using the alternating direction method of multipliers, fully distributed training algorithms are obtained without exchanging training data among nodes. Different from existing incremental approaches, the overhead associated with inter-node communications is fixed and solely dependent on the network topology rather than the size of the training sets available per node. Important generalizations to train nonlinear SVMs in a distributed fashion are also developed along with sequential variants capable of online processing. Simulated tests illustrate the performance of the novel algorithms.(1)",,,,,"Giannakis, Georgios/Z-4413-2019","Giannakis, Georgios/0000-0002-0196-0260",,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1663,1707,,,,,,,,,,,,,,,,WOS:000282522000004,0
J,"Escalera, S; Pujol, O; Radeva, P",,,,"Escalera, Sergio; Pujol, Oriol; Radeva, Petia",,,Error-Correcting Ouput Codes Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we present an open source Error-Correcting Output Codes (ECOC) library. The ECOC framework is a powerful tool to deal with multi-class categorization problems. This library contains both state-of-the-art coding (one-versus-one, one-versus-all, dense random, sparse random, DECOC, forest-ECOC, and ECOC-ONE) and decoding designs (hamming, euclidean, inverse hamming, laplacian, beta-density, attenuated, loss-based, probabilistic kernel-based, and loss-weighted) with the parameters defined by the authors, as well as the option to include your own coding, decoding, and base classifier.",,,,,"Pujol, Oriol/F-7146-2016; Escalera, Sergio/L-2998-2015; Radeva, Petia/I-3385-2015","Pujol, Oriol/0000-0001-7573-009X; Escalera, Sergio/0000-0003-0617-8873; Radeva, Petia/0000-0003-0047-5172",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,661,664,,,,,,,,,,,,,,,,WOS:000277186500008,0
J,"Journee, M; Nesterov, Y; Richtarik, P; Sepulchre, R",,,,"Journee, Michel; Nesterov, Yurii; Richtarik, Peter; Sepulchre, Rodolphe",,,Generalized Power Method for Sparse Principal Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we develop a new approach to sparse principal component analysis (sparse PCA). We propose two single-unit and two block optimization formulations of the sparse PCA problem, aimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are therefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is decreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best convergence properties in the case when either the objective function or the feasible set are strongly convex, which is the case with our single-unit formulations and can be enforced in the block case. Finally, we demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in computational speed.",,,,,"Richtarik, Peter/O-5797-2018; Nesterov, Yurii/C-4373-2008","Sepulchre, Rodolphe/0000-0002-7047-3124",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,517,553,,,,,,,,,,,,,,,,WOS:000277186500003,0
J,"Guyon, I; Saffari, A; Dror, G; Cawley, G",,,,"Guyon, Isabelle; Saffari, Amir; Dror, Gideon; Cawley, Gavin",,,Model Selection: Beyond the Bayesian/Frequentist Divide,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The principle of parsimony also known as Ockham's razor has inspired many theories of model selection. Yet such theories, all making arguments in favor of parsimony, are based on very different premises and have developed distinct methodologies to derive algorithms. We have organized challenges and edited a special issue of JMLR and several conference proceedings around the theme of model selection. In this editorial, we revisit the problem of avoiding overfitting in light of the latest results. We note the remarkable convergence of theories as different as Bayesian theory, Minimum Description Length, bias/variance tradeoff, Structural Risk Minimization, and regularization, in some approaches. We also present new and interesting examples of the complementarity of theories leading to hybrid algorithms, neither frequentist, nor Bayesian, or perhaps both frequentist and Bayesian!",,,,,,"Cawley, Gavin/0000-0002-4118-9095",,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,61,87,,,,,,,,,,,,,,,,WOS:000277186400003,0
J,"Slobodianik, N; Zaporozhets, D; Madras, N",,,,"Slobodianik, Nikolai; Zaporozhets, Dmitry; Madras, Neal",,,Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we refine this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion.",,,,,"Zaporozhets, Dmitry/U-8827-2017",,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1511,1526,,,,,,,,,,,,,,,,WOS:000270825000008,0
J,"Agarwal, S; Niyogi, P",,,,"Agarwal, Shivani; Niyogi, Partha",,,Generalization Bounds for Ranking Algorithms via Algorithmic Stability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained much attention in machine learning. We study generalization properties of ranking algorithms using the notion of algorithmic stability; in particular, we derive generalization bounds for ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with generalization bounds based on uniform convergence, which in many cases cannot be applied to these algorithms. Our results generalize earlier results that were derived in the special setting of bipartite ranking (Agarwal and Niyogi, 2005) to a more general setting of the ranking problem that arises frequently in applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,441,474,,,,,,,,,,,,,,,,WOS:000270824200012,0
J,"Kang, CS; Tian, J",,,,"Kang, Changsung; Tian, Jin",,,Markov Properties for Linear Causal Models with Correlated Errors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A linear causal model with correlated errors, represented by a DAG with bi-directed edges, can be tested by the set of conditional independence relations implied by the model. A global Markov property specifies, by the d-separation criterion, the set of all conditional independence relations holding in any model associated with a graph. A local Markov property specifies a much smaller set of conditional independence relations which will imply all other conditional independence relations which hold under the global Markov property. For DAGs with bi-directed edges associated with arbitrary probability distributions, a local Markov property is given in Richardson (2003) which may invoke an exponential number of conditional independencies. In this paper, we show that for a class of linear structural equation models with correlated errors, there is a local Markov property which will invoke only a linear number of conditional independence relations. For general linear models, we provide a local Markov property that often invokes far fewer conditional independencies than that in Richardson (2003). The results have applications in testing linear structural equation models with correlated errors.",,,,,"Tian, Jin/Q-9736-2019; Tian, Jin/GZM-3191-2022","Tian, Jin/0000-0001-5313-1600; Tian, Jin/0000-0001-5313-1600",,,,,,,,,,,,,1532-4435,,,,,JAN,2009,10,,,,,,41,70,,,,,,,,,,,,,,,,WOS:000270824100002,0
J,"Zhang, K; Chan, LW",,,,"Zhang, Kun; Chan, Laiwan",,,Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is well known that solutions to the nonlinear independent component analysis (ICA) problem are highly non-unique. In this paper we propose the minimal nonlinear distortion (MND) principle for tackling the ill-posedness of nonlinear ICA problems. MND prefers the nonlinear ICA solution with the estimated mixing procedure as close as possible to linear, among all possible solutions. It also helps to avoid local optima in the solutions. To achieve MND, we exploit a regularization term to minimize the mean square error between the nonlinear mixing mapping and the best-fitting linear one. The effect of MND on the inherent trivial and non-trivial indeterminacies in nonlinear ICA solutions is investigated. Moreover, we show that local MND is closely related to the smoothness regularizer penalizing large curvature, which provides another useful regularization condition for nonlinear ICA. Experiments on synthetic data show the usefulness of the MND principle for separating various nonlinear mixtures. Finally, as an application, we use nonlinear ICA with MND to separate daily returns of a set of stocks in Hong Kong, and the linear causal relations among them are successfully discovered. The resulting causal relations give some interesting insights into the stock market. Such a result can not be achieved by linear ICA. Simulation studies also verify that when doing causality discovery, sometimes one should not ignore the nonlinear distortion in the data generation procedure, even if it is weak.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2455,2487,,,,,,,,,,,,,,,,WOS:000262637600002,0
J,"Loustau, S",,,,"Loustau, Sebastien",,,Aggregation of SVM classifiers using Sobolev spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classifier with no tuning parameter. It is a combination of SVM classifiers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1559,1582,,,,,,,,,,,,,,,,WOS:000258646800009,0
J,"Zhao, P; Yu, B",,,,"Zhao, Peng; Yu, Bin",,,Stagewise Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many statistical machine learning algorithms minimize either an empirical loss function as in Ad-aBoost, or a penalized empirical loss as in Lasso or SVM. A single regularization tuning parameter controls the trade-off between fidelity to the data and generalizability, or equivalently between bias and variance. When this tuning parameter changes, a regularization path of solutions to the minimization problem is generated, and the whole path is needed to select a tuning parameter to optimize the prediction or interpretation performance. Algorithms such as homotopy-Lasso or LARS-Lasso and Forward Stagewise Fitting (FSF) (aka e-Boosting) are of great interest because of their resulted sparse models for interpretation in addition to prediction. In this paper, we propose the BLasso algorithm that ties the FSF (e-Boosting) algorithm with the Lasso method that minimizes the L-1 penalized L-2 loss. BLasso is derived as a coordinate descent method with a fixed stepsize applied to the general Lasso loss function (L1 penalized convex loss). It consists of both a forward step and a backward step. The forward step is similar to e-Boosting or FSF, but the backward step is new and revises the FSF (or e-Boosting) path to approximate the Lasso path. In the cases of a finite number of base learners and a bounded Hessian of the loss function, the BLasso path is shown to converge to the Lasso path when the stepsize goes to zero. For cases with a larger number of base learners than the sample size and when the true model is sparse, our simulations indicate that the BLasso model estimates are sparser than those from FSF with comparable or slightly better prediction performance, and that the the discrete stepsize of BLasso and FSF has an additional regularization effect in terms of prediction and sparsity. Moreover, we introduce the Generalized BLasso algorithm to minimize a general convex loss penalized by a general convex function. Since the (Generalized) BLasso relies only on differences not derivatives, we conclude that it provides a class of simple and easy-to-implement algorithms for tracing the regularization or solution paths of penalized minimization problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2007,8,,,,,,2701,2726,,,,,,,,,,,,,,,,WOS:000252745100002,0
J,"Ghavamzadeh, M; Mahadevan, S",,,,"Ghavamzadeh, Mohammad; Mahadevan, Sridhar",,,Hierarchical average reward reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. The average reward optimality criterion has been recognized to be more appropriate for a wide class of continuing tasks than the discounted framework. Although average reward RL has been studied for decades, prior work has been largely limited to flat policy representations. In this paper, we develop a framework for HRL based on the average reward optimality criterion. We investigate two formulations of HRL based on the average reward SMDP model, both for discrete-time and continuous-time. These formulations correspond to two notions of optimality that have been previously explored in HRL: hierarchical optimality and recursive optimality. We present algorithms that learn to find hierarchically and recursively optimal average reward policies under discrete-time and continuous-time average reward SMDP models. We use two automated guided vehicle (AGV) scheduling tasks as experimental testbeds to study the empirical performance of the proposed algorithms. The first problem is a relatively simple AGV scheduling task, in which the hierarchically and recursively optimal policies are different. We compare the proposed algorithms with three other HRL methods, including a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm on this problem. The second problem is a larger AGV scheduling task. We model this problem using both discrete-time and continuous-time models. We use a hierarchical task decomposition in which the hierarchically and recursively optimal policies are the same for this problem. We compare the performance of the proposed algorithms with a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm, as well as a non-hierarchical average reward algorithm. The results show that the proposed hierarchical average reward algorithms converge to the same performance as their discounted reward counterparts.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2007,8,,,,,,2629,2669,,,,,,,,,,,,,,,,WOS:000252744900004,0
J,"Hamsici, OC; Martinez, AM",,,,"Hamsici, Onur C.; Martinez, Aleix M.",,,Spherical-homoscedastic distributions: The equivalency of spherical and normal distributions in classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere Sp-1. Such representations should then be modeled using spherical distributions. However, the difficulty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead-as if the data were represented in R p rather than Sp-1. This opens the question to whether the classification results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases ( which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to define optimal classifiers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classifiers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classification of images of objects, gene expression sequences, and text data.",,,,,"Martinez, Aleix M/A-2380-2008",,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1583,1623,,,,,,,,,,,,,,,,WOS:000249353700008,0
J,"Olsson, RK; Hansen, LK",,,,"Olsson, Rasmus Kongsgaard; Hansen, Lars Kai",,,Linear state-space models for blind source separation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We apply a type of generative modelling to the problem of blind source separation in which prior knowledge about the latent source signals, such as time-varying auto-correlation and quasi-periodicity, are incorporated into a linear state-space model. In simulations, we show that in terms of signal-to-error ratio, the sources are inferred more accurately as a result of the inclusion of strong prior knowledge. We explore different schemes of maximum-likelihood optimization for the purpose of learning the model parameters. The Expectation Maximization algorithm, which is often considered the standard optimization method in this context, results in slow convergence when the noise variance is small. In such scenarios, quasi-Newton optimization yields substantial improvements in a range of signal to noise ratios. We analyze the performance of the methods on convolutive mixtures of speech signals.",,,,,"Hansen, Lars/E-3174-2013","Hansen, Lars Kai/0000-0003-0442-5877",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2585,2602,,,,,,,,,,,,,,,,WOS:000245390800002,0
J,"Munos, R",,,,"Munos, R",,,Geometric variance reduction in Markov chains: Application to value function and gradient estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a variance reduction technique for Monte Carlo estimation of functionals in Markov chains. The method is based on designing sequential control variates using successive approximations of the function of interest V. Regular Monte Carlo estimates have a variance of O(1/N), where N is the number of sample trajectories of the Markov chain. Here, we obtain a geometric variance reduction O(rho(N)) (with rho < 1) up to a threshold that depends on the approximation error V - AV, where A is an approximation operator linear in the values. Thus, if V belongs to the right approximation space (i.e. AV = V), the variance decreases geometrically to zero. An immediate application is value function estimation in Markov chains, which may be used for policy evaluation in a policy iteration algorithm for solving Markov Decision Processes. Another important domain, for which variance reduction is highly needed, is gradient estimation, that is computing the sensitivity partial derivative alpha V of the performance measure V with respect to some parameter a of the transition probabilities. For example, in policy parametric optimization, computing an estimate of the policy gradient is required to perform a gradient optimization method. We show that, using two approximations for the value function and the gradient, a geometric variance reduction is also achieved, up to a threshold that depends on the approximation errors of both of those representations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,413,427,,,,,,,,,,,,,,,,WOS:000236331700008,0
J,"Huang, TK; Weng, RC; Lin, CJ",,,,"Huang, TK; Weng, RC; Lin, CJ",,,Generalized Bradley-Terry models and multi-class probability estimates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Bradley-Terry model for obtaining individual skill from paired comparisons has been popular in many areas. In machine learning, this model is related to multi-class probability estimates by coupling all pairwise classification results. Error correcting output codes (ECOC) are a general framework to decompose a multi-class problem to several binary problems. To obtain probability estimates under this framework, this paper introduces a generalized Bradley-Terry model in which paired individual comparisons are extended to paired team comparisons. We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill. Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi-class probability estimates. Moreover, we discuss four extensions of the proposed model: 1) weighted individual skill, 2) home-field advantage, 3) ties, and 4) comparisons with more than two teams.",,,,,,"WENG, CHIU-HSING/0000-0001-5495-3817; Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,85,115,,,,,,,,,,,,,,,,WOS:000236331400004,0
J,"Rakotomamonjy, A; Canu, S",,,,"Rakotomamonjy, A; Canu, S",,,"Frames, reproducing kernels, regularization and learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work deals with a method for building a reproducing kernel Hilbert space (RKHS) from a Hilbert space with frame elements having special properties. Conditions on existence and a method of construction are given. Then, these RKHS are used within the framework of regularization theory for function approximation. Implications on semiparametric estimation are discussed and a multiscale scheme of regularization is also proposed. Results on toy and real-world approximation problems illustrate the effectiveness of such methods.",,,,,,"Canu, Stephane/0000-0002-7602-4557",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1485,1515,,,,,,,,,,,,,,,,WOS:000236330100009,0
J,"Ihler, AT; Fisher, JW; Willsky, AS",,,,"Ihler, AT; Fisher, JW; Willsky, AS",,,Loopy belief propagation: Convergence and effects of message errors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Belief propagation (BP) is an increasingly popular method of performing approximate inference on arbitrary graphical models. At times, even further approximations are required, whether due to quantization of the messages or model parameters, from other simplified message or model representations, or from stochastic approximation methods. The introduction of such errors into the BP message computations has the potential to affect the solution obtained adversely. We analyze the effect resulting from message approximation under two particular measures of error, and show bounds on the accumulation of errors in the system. This analysis leads to convergence conditions for traditional BP message passing, and both strict bounds and estimates of the resulting error in systems of approximate BP message passing.",,,,,,"Ihler, Alexander/0000-0002-4331-1015",,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,905,936,,,,,,,,,,,,,,,,WOS:000236329700007,0
J,"Hyvarinen, A",,,,"Hyvarinen, A",,,Estimation of non-normalized statistical models by score matching,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.",,,,,"Hyvarinen, Aapo/E-9006-2012","Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,695,709,,,,,,,,,,,,,,,,WOS:000236329600012,0
J,"Bhattacharyya, C",,,,"Bhattacharyya, C",,,Second order cone programming formulations for feature selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper addresses the issue of feature selection for linear classifiers given the moments of the class conditional densities. The problem is posed as finding a minimal set of features such that the resulting classifier has a low misclassification error. Using a bound on the misclassification error involving the mean and covariance of class conditional densities and minimizing an L-1 norm as an approximate criterion for feature selection, a second order programming formulation is derived. To handle errors in estimation of mean and covariances, a tractable robust formulation is also discussed. In a slightly different setting the Fisher discriminant is derived. Feature selection for Fisher discriminant is also discussed. Experimental results on synthetic data sets and on real life microarray data show that the proposed formulations are competitive with the state of the art linear programming formulation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2004,5,,,,,,1417,1433,,,,,,,,,,,,,,,,WOS:000236328400001,0
J,"Wolf, L; Shashua, A",,,,"Wolf, L; Shashua, A",,,Learning over sets using kernel principal angles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning with instances defined over a space of sets of vectors. We derive a new positive definite kernel f (A, B) defined over pairs of matrices A, B based on the concept of principal angles between two linear subspaces. We show that the principal angles can be recovered using only inner-products between pairs of column vectors of the input matrices thereby allowing the original column vectors of A, B to be mapped onto arbitrarily high-dimensional feature spaces. We demonstrate the usage of the matrix-based kernel function f (A, B) with experiments on two visual tasks. The first task is the discrimination of irregular motion trajectory of an individual or a group of individuals in a video sequence. We use the SVM approach using f (A, B) where an input matrix represents the motion trajectory of a group of individuals over a certain (fixed) time frame. We show that the classification (irregular versus regular) greatly outperforms the conventional representation where all the trajectories form a single vector. The second application is the visual recognition of faces from input video sequences representing head motion and facial expressions where f (A, B) is used to compare two image sequences.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,913,931,,10.1162/1532443041827934,0,,,,,,,,,,,,,WOS:000231002600001,0
J,"Chen, YX; Wang, JZ",,,,"Chen, YX; Wang, JZ",,,Image categorization by learning and reasoning with regions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Designing computer programs to automatically categorize images using low-level features is a challenging research topic in computer vision. In this paper, we present a new learning technique, which extends Multiple-Instance Learning (MIL), and its application to the problem of region-based image categorization. Images are viewed as bags, each of which contains a number of instances corresponding to regions obtained from image segmentation. The standard MIL problem assumes that a bag is labeled positive if at least one of its instances is positive; otherwise, the bag is negative. In the proposed MIL framework, DD-SVM, a bag label is determined by some number of instances satisfying various properties. DD-SVM first learns a collection of instance prototypes according to a Diverse Density (DD) function. Each instance prototype represents a class of instances that is more likely to appear in bags with the specific label than in the other bags. A nonlinear mapping is then defined using the instance prototypes and maps every bag to a point in a new feature space, named the bag feature space. Finally, standard support vector machines are trained in the bag feature space. We provide experimental results on an image categorization problem and a drug activity prediction problem.",,,,,,"Wang, James/0000-0003-4379-4173",,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,913,939,,,,,,,,,,,,,,,,WOS:000236328000003,0
J,"Wu, TF; Lin, CJ; Weng, RC",,,,"Wu, TF; Lin, CJ; Weng, RC",,,Probability estimates for multi-class classification by pairwise coupling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Pairwise coupling is a popular multi-class classification method that combines all comparisons for each pair of classes. This paper presents two approaches for obtaining class probabilities. Both methods can be reduced to linear systems and are easy to implement. We show conceptually and experimentally that the proposed approaches are more stable than the two existing popular methods: voting and the method by Hastie and Tibshirani (1998).,,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,975,1005,,,,,,,,,,,,,,,,WOS:000236328000005,0
J,"Markovitch, S; Shatil, A",,,,"Markovitch, S; Shatil, A",,,Speedup learning for repair-based search by identifying redundant steps,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Repair-based search algorithms start with an initial solution and attempt to improve it by iteratively applying repair operators. Such algorithms can often handle large-scale problems that may be difficult for systematic search algorithms. Nevertheless, the computational cost of solving such problems is still very high. We observed that many of the repair steps applied by such algorithms are redundant in the sense that they do not eventually contribute to finding a solution. Such redundant steps are particularly harmful in repair-based search, where each step carries high cost due to the very high branching factor typically associated with it. Accurately identifying and avoiding such redundant steps would result in faster local search without harming the algorithm's problem-solving ability. In this paper we propose a speed-up learning methodology for attaining this goal. It consists of the following steps: defining the concept of a redundant step; acquiring this concept during off-line learning by analyzing solution paths for training problems, tagging all the steps along the paths according to the redundancy definition and using an induction algorithm to infer a classifier based on the tagged examples; and using the acquired classifier to filter out redundant steps while solving unseen problems. Our algorithm was empirically tested on instances of real-world employee timetabling problems (ETP). The problem solver to be improved is based on one of the best methods for solving some large ETP instances. Our results show a significant improvement in speed for test problems that are similar to the given example problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,649,681,,10.1162/153244304773936081,0,,,,,,,,,,,,,WOS:000221345700010,0
J,"Fukumizu, K; Bach, FR; Jordan, MI",,,,"Fukumizu, K; Bach, FR; Jordan, MI",,,Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel method of dimensionality reduction for supervised learning problems. Given a regression or classification problem in which we wish to predict a response variable Y from an explanatory variable X, we treat the problem of dimensionality reduction as that of finding a low-dimensional effective subspace for X which retains the statistical relationship between X and Y. We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem we establish a general nonparmetric characterization of conditional independence using covariance operators on reproducing kernel Hilbert spaces. This characterization allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods for dimensionality reduction in supervised learning, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y. We present experiments that compare the performance of the method with conventional methods.",,,,,"Jordan, Michael I/C-5253-2013","Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,JAN,2004,5,,,,,,73,99,,,,,,,,,,,,,,,,WOS:000236326900002,0
J,"Rifkin, R; Klautau, A",,,,"Rifkin, R; Klautau, A",,,In defense of one-vs-all classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of multiclass classification. Our main thesis is that a simple one-vs-all scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.",,,,,"Klautau, Aldebaro/A-8696-2008","Klautau, Aldebaro/0000-0001-7773-2080",,,,,,,,,,,,,1532-4435,,,,,JAN,2004,5,,,,,,101,141,,,,,,,,,,,,,,,,WOS:000236326900003,0
J,"Bengio, Y; Ducharme, R; Vincent, P; Jauvin, C",,,,"Bengio, Y; Ducharme, R; Vincent, P; Jauvin, C",,,A neural probabilistic language model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Machine Learning Methods for Text and Images,2001,"VANCOUVER, CANADA",,,,,"A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1137,1155,,10.1162/153244303322533223,0,,,,,,,,,,,,,WOS:000186002400006,0
J,"Brodley, CE; Danyluk, AP",,,,"Brodley, CE; Danyluk, AP",,,Special Issue on the Eighteenth International Conference on Machine Learning (ICML2001),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,619,619,,10.1162/jmlr.2003.3.4-5.619,0,,,,,,,,,,,,,WOS:000184926200001,0
J,"Mitchell, R; Cooper, J; Frank, E; Holmes, G",,,,"Mitchell, Rory; Cooper, Joshua; Frank, Eibe; Holmes, Geoffrey",,,Sampling Permutations for Shapley Value Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Game-theoretic attribution techniques based on Shapley values are used to interpret blackbox machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi-Monte Carlo methods have not yet been applied to the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel in combination with kernel herding and sequential Bayesian quadrature. The RKHS perspective also leads to quasi-Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere Sd-2 and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000766879700001,0
J,"Altschuler, JM; Boix-Adsera, E",,,,"Altschuler, Jason M.; Boix-Adsera, Enric",,,Wasserstein Barycenters can be Computed in Polynomial Time in Fixed Dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Computing Wasserstein barycenters is a fundamental geometric problem with widespread applications in machine learning, statistics, and computer graphics. However, it is unknown whether Wasserstein barycenters can be computed in polynomial time, either exactly or to high precision (i.e., with polylog(1/epsilon) runtime dependence). This paper answers these questions in the affirmative for any fixed dimension. Our approach is to solve an exponential-size linear programming formulation by efficiently implementing the corresponding separation oracle using techniques from computational geometry.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500044,0
J,"Balabdaoui, F; Doss, CR; Durot, C",,,,"Balabdaoui, Fadoua; Doss, Charles R.; Durot, Cecile",,,Unlinked Monotone Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider so-called univariate unlinked (sometimes decoupled, or shuffled) regression when the unknown regression curve is monotone. In standard monotone regression, one observes a pair (X; Y) where a response Y is linked to a covariate X through the model Y = m(0) (X) + epsilon, with m(0) the (unknown) monotone regression function and epsilon the unobserved error (assumed to be independent of X). In the unlinked regression setting one gets only to observe a vector of realizations from both the response Y and from the covariate X where now Y d = m(0) (X) + epsilon. There is no (observed) pairing of X and Y. Despite this, it is actually still possible to derive a consistent non-parametric estimator of m0 under the assumption of monotonicity of m(0) and knowledge of the distribution of the noise epsilon. In this paper, we establish an upper bound on the rate of convergence of such an estimator under minimal assumption on the distribution of the covariate X. We discuss extensions to the case in which the distribution of the noise is unknown. We develop a second order algorithm for its computation, and we demonstrate its use on synthetic data. Finally, we apply our method (in a fully data driven way, without knowledge of the error distribution) on longitudinal data from the US Consumer Expenditure Survey.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687283400001,0
J,"Dall'Amico, L; Couillet, R; Tremblay, N",,,,"Dall'Amico, Lorenzo; Couillet, Romain; Tremblay, Nicolas",,,A Unified Framework for Spectral Clustering in Sparse Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article considers spectral community detection in the regime of sparse networks with heterogeneous degree distributions, for which we devise an algorithm to efficiently retrieve communities. Specifically, we demonstrate that a well parametrized form of regularized Laplacian matrices can be used to perform spectral clustering in sparse networks without suffering from its degree heterogeneity. Besides, we exhibit important connections between this proposed matrix and the now popular non-backtracking matrix, the Bethe-Hessian matrix, as well as the standard Laplacian matrix. Interestingly, as opposed to competitive methods, our proposed improved parametrization inherently accounts for the hardness of the classification problem. These findings are summarized under the form of an algorithm capable of both estimating the number of communities and achieving high-quality community reconstruction.",,,,,"Dall'Amico, Lorenzo/AAE-4319-2022","Dall'Amico, Lorenzo/0000-0002-7493-6421",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706886000001,0
J,"Hanneke, S",,,,"Hanneke, Steve",,,Learning Whenever Learning is Possible: Universal Learning under General Stochastic Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work initiates a general study of learning and generalization without the i.i.d. assumption, starting from first principles. While the traditional approach to statistical learning theory typically relies on standard assumptions from probability theory (e.g., i.i.d. or stationary ergodic), in this work we are interested in developing a theory of learning based only on the most fundamental and necessary assumptions implicit in the requirements of the learning problem itself. We specifically study universally consistent function learning, where the objective is to obtain low long-run average loss for any target function, when the data follow a given stochastic process. We are then interested in the question of whether there exist learning rules guaranteed to be universally consistent given only the assumption that universally consistent learning is possible for the given data process. The reasoning that motivates this criterion emanates from a kind of optimist's decision theory, and so we refer to such learning rules as being optimistically universal. We study this question in three natural learning settings: inductive, self-adaptive, and online. Remarkably, as our strongest positive result, we find that optimistically universal learning rules do indeed exist in the self-adaptive learning setting. Establishing this fact requires us to develop new approaches to the design of learning algorithms. Along the way, we also identify concise characterizations of the family of processes under which universally consistent learning is possible in the inductive and self-adaptive settings. We additionally pose a number of enticing open problems, particularly for the online learning setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687300400001,0
J,"Hodgkinson, L; Salomone, R; Roosta, F",,,,"Hodgkinson, Liam; Salomone, Robert; Roosta, Fred",,,Implicit Langevin Algorithms for Sampling From Log-concave Densities,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For sampling from a log-concave density, we study implicit integrators resulting from theta-method discretization of the overdamped Langevin diffusion stochastic differential equation. Theoretical and algorithmic properties of the resulting sampling methods for theta is an element of[0,1] and a range of step sizes are established. Our results generalize and extend prior works in several directions. In particular, for theta >= 1/2, we prove geometric ergodicity and stability of the resulting methods for all step sizes. We show that obtaining subsequent samples amounts to solving a strongly-convex optimization problem, which is readily achievable using one of numerous existing methods. Numerical examples supporting our theoretical analysis are also presented.",,,,,,"Hodgkinson, Liam/0000-0002-4595-0347",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700308500001,0
J,"Koepernik, P; Pfaff, F",,,,"Koepernik, Peter; Pfaff, Florian",,,Consistency of Gaussian Process Regression in Metric Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian process (GP) regressors are used in a wide variety of regression tasks, and many recent applications feature domains that are non-Euclidean manifolds or other metric spaces. In this paper, we examine formal consistency of GP regression on general metric spaces. Specifically, we consider a GP prior on an unknown real-valued function with a metric domain space and examine consistency of the resulting posterior distribution. If the kernel is continuous and the sequence of sampling points lies sufficiently dense, then the variance of the posterior GP is shown to converge to zero almost surely monotonically and in L-P for all p > 1, uniformly on compact sets. Moreover, we prove that if the difference between the observed function and the mean function of the prior lies in the reproducing kernel Hilbert space of the prior's kernel, then the posterior mean converges pointwise in L-2 to the unknown function, and, under an additional assumption on the kernel, uniformly on compacts in L-1. This paper provides an important step towards the theoretical legitimization of GP regression on manifolds and other non-Euclidean metric spaces.",,,,,,"Pfaff, Florian/0000-0003-2987-7685",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706442200001,0
J,"Lin, LC; Dobriban, E",,,,"Lin, Licong; Dobriban, Edgar",,,What Causes the Test Error? Going Beyond Bias-Variance via ANOVA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modern machine learning methods are often overparametrized, allowing adaptation to the data at a fine level. This can seem puzzling; in the worst case, such models do not need to generalize. This puzzle inspired a great amount of work, arguing when overparametrization reduces test error, in a phenomenon called double descent. Recent work aimed to under-stand in greater depth why overparametrization is helpful for generalization. This lead to discovering the unimodality of variance as a function of the level of parametrization, and to decomposing the variance into that arising from label noise, initialization, and randomness in the training data to understand the sources of the error. In this work we develop a deeper understanding of this area. Specifically, we propose using the analysis of variance (ANOVA) to decompose the variance in the test error in a symmetric way, for studying the generalization performance of certain two-layer linear and non-linear networks. The advantage of the analysis of variance is that it reveals the effects of initialization, label noise, and training data more clearly than prior approaches. Moreover, we also study the monotonicity and unimodality of the variance components. While prior work studied the unimodality of the overall variance, we study the properties of each term in the variance decomposition. One of our key insights is that often, the interaction between training samples and initialization can dominate the variance; surprisingly being larger than their marginal effect. Also, we characterize phase transitions where the variance changes from unimodal to monotone. On a technical level, we leverage advanced deterministic equivalent techniques for Haar random matrices, that-to our knowledge-have not yet been used in the area. We verify our results in numerical simulations and on empirical data examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687053500001,0
J,"Liu, MR; Rafique, H; Lin, QH; Yang, TB",,,,"Liu, Mingrui; Rafique, Hassan; Lin, Qihang; Yang, Tianbao",,,First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider first-order convergence theory and algorithms for solving a class of non-convex non-concave min-max saddle-point problems, whose objective function is weakly convex in the variables of minimization and weakly concave in the variables of maximization. It has many important applications in machine learning including training Generative Adversarial Nets (GANs). We propose an algorithmic framework motivated by the inexact proximal point method, where the weakly monotone variational inequality (VI) corresponding to the original min-max problem is solved through approximately solving a sequence of strongly monotone VIs constructed by adding a strongly monotone mapping to the original gradient mapping. We prove first-order convergence to a nearly stationary solution of the original min-max problem of the generic algorithmic framework and establish different rates by employing different algorithms for solving each strongly monotone VI. Experiments verify the convergence theory and also demonstrate the effectiveness of the proposed methods on training GANs.",,,,,"Khan, Adnan/GQH-6131-2022","Liu, Mingrui/0000-0002-5181-3429",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687036900001,0
J,"Stelmakh, I; Shah, N; Singh, A",,,,"Stelmakh, Ivan; Shah, Nihar; Singh, Aarti",,,Fair and Accurate Reviewer Assignment in Peer Review,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of automated assignment of papers to reviewers in conference peer review, with a focus on fairness and statistical accuracy. Our fairness objective is to maximize the review quality of the most disadvantaged paper, in contrast to the commonly used objective of maximizing the total quality over all papers. We design an assignment algorithm based on an incremental max-flow procedure that we prove is near-optimally fair. Our statistical accuracy objective is to ensure correct recovery of the papers that should be accepted. We provide a sharp minimax analysis of the accuracy of the peer-review process for a popular objective-score model as well as for a novel subjective-score model that we propose in the paper. Our analysis proves that our proposed assignment algorithm also leads to a near-optimal statistical accuracy. Finally, we design a novel experiment that allows for an objective comparison of various assignment algorithms, and overcomes the inherent difficulty posed by the absence of a ground truth in experiments on peer-review. The results of this experiment as well as of other experiments on synthetic and real data corroborate the theoretical guarantees of our algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687138500001,0
J,"Tang, TM; Allen, GI",,,,"Tang, Tiffany M.; Allen, Genevera, I",,,Integrated Principal Components Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Data integration, or the strategic analysis of multiple sources of data simultaneously, can often lead to discoveries that may be hidden in individualistic analyses of a single data source. We develop a new unsupervised data integration method named Integrated Principal Components Analysis (iPCA), which is a model-based generalization of PCA and serves as a practical tool to find and visualize common patterns that occur in multiple data sets. The key idea driving iPCA is the matrix-variate normal model, whose Kronecker product covariance structure captures both individual patterns within each data set and joint patterns shared by multiple data sets. Building upon this model, we develop several penalized (sparse and non-sparse) covariance estimators for iPCA, and using geodesic convexity, we prove that our non-sparse iPCA estimator converges to the global solution of a non-convex problem. We also demonstrate the practical advantages of iPCA through extensive simulations and a case study application to integrative genomics for Alzheimer's disease. In particular, we show that the joint patterns extracted via iPCA are highly predictive of a patient's cognition and Alzheimer's diagnosis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706866000001,0
J,"Villacampa-Calvo, C; Zaldivar, B; Garrido-Merchan, EC; Hernandez-Lobato, D",,,,"Villacampa-Calvo, Carlos; Zaldivar, Bryan; Garrido-Merchan, Eduardo C.; Hernandez-Lobato, Daniel",,,Multi-class Gaussian Process Classification with Noisy Inputs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is a common practice in the machine learning community to assume that the observed data are noise-free in the input attributes. Nevertheless, scenarios with input noise are common in real problems, as measurements are never perfectly accurate. If this input noise is not taken into account, a supervised machine learning method is expected to perform sub-optimally. In this paper, we focus on multi-class classification problems and use Gaussian processes (GPs) as the underlying classifier. Motivated by a data set coming from the astrophysics domain, we hypothesize that the observed data may contain noise in the inputs. Therefore, we devise several multi-class GP classifiers that can account for input noise. Such classifiers can be efficiently trained using variational inference to approximate the posterior distribution of the latent variables of the model. Moreover, in some situations, the amount of noise can be known before-hand. If this is the case, it can be readily introduced in the proposed methods. This prior information is expected to lead to better performance results. We have evaluated the proposed methods by carrying out several experiments, involving synthetic and real data. These include several data sets from the UCI repository, the MNIST data set and a data set coming from astrophysics. The results obtained show that, although the classification error is similar across methods, the predictive distribution of the proposed methods is better, in terms of the test log-likelihood, than the predictive distribution of a classifier based on GPs that ignores input noise.",,,,,,"Villacampa Calvo, Carlos/0000-0002-5732-9101",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500036,0
J,"Wang, MJ; Allen, GI",,,,"Wang, Minjie; Allen, Genevera, I",,,Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In mixed multi-view data, multiple sets of diverse features are measured on the same set of samples. By integrating all available data sources, we seek to discover common group structure among the samples that may be hidden in individualistic cluster analyses of a single data view. While several techniques for such integrative clustering have been explored, we propose and develop a convex formalization that enjoys strong empirical performance and inherits the mathematical properties of increasingly popular convex clustering methods. Specifically, our Integrative Generalized Convex Clustering Optimization (iGecco) method employs different convex distances, losses, or divergences for each of the different data views with a joint convex fusion penalty that leads to common groups. Additionally, integrating mixed multi-view data is often challenging when each data source is high-dimensional. To perform feature selection in such scenarios, we develop an adaptive shifted grouplasso penalty that selects features by shrinking them towards their loss-specific centers. Our so-called iGecco+ approach selects features from each data view that are best for determining the groups, often leading to improved integrative clustering. To solve our problem, we develop a new type of generalized multi-block ADMM algorithm using subproblem approximations that more efficiently fits our model for big data sets. Through a series of numerical experiments and real data examples on text mining and genomics, we show that iGecco+ achieves superior empirical performance for high-dimensional mixed multi-view data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,55,,,,,,,,,,34744522,,,,,WOS:000656355100001,0
J,"Abbe, E; Kulkarni, S; Lee, EJ",,,,"Abbe, Emmanuel; Kulkarni, Sanjeev; Lee, Eun Jee",,,Generalized Nonbacktracking Bounds on the Influence in Independent Cascade Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops deterministic upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit r-nonbacktracking walks and Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing algorithms. Further, we provide parameterized versions of the bounds that control the trade-off between efficiency and accuracy. Finally, the tightness of the bounds is illustrated on various network models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000006,0
J,"Bahmani, S; Romberg, J",,,,"Bahmani, Sohail; Romberg, Justin",,,Convex Programming for Estimation in Nonlinear Recurrent Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a formulation for nonlinear recurrent models that includes simple parametric models of recurrent neural networks as a special case. The proposed formulation leads to a natural estimator in the form of a convex program. We provide a sample complexity for this estimator in the case of stable dynamics, where the nonlinear recursion has a certain contraction property, and under certain regularity conditions on the input distribution. We evaluate the performance of the estimator by simulation on synthetic data. These numerical experiments also suggest the extent at which the imposed theoretical assumptions may be relaxed.",,,,,,"Bahmani, Sohail/0000-0002-8316-8313",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,235,,,,,,,,,,,,,,,WOS:000608906900001,0
J,"Barnes, LP; Han, YJ; Ozgur, A",,,,"Barnes, Leighton Pate; Han, Yanjun; Ozgur, Ayfer",,,Lower Bounds for Learning Distributions under Communication Constraints via Fisher Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning high-dimensional, nonparametric and structured (e.g., Gaussian) distributions in distributed networks, where each node in the network observes an independent sample from the underlying distribution and can use k bits to communicate its sample to a central processor. We consider three different models for communication. Under the independent model, each node communicates its sample to a central processor by independently encoding it into k bits. Under the more general sequential or blackboard communication models, nodes can share information interactively but each node is restricted to write at most k bits on the final transcript. We characterize the impact of the communication constraint k on the minimax risk of estimating the underlying distribution under `2 loss. We develop minimax lower bounds that apply in a unified way to many common statistical models and reveal that the impact of the communication constraint can be qualitatively different depending on the tail behavior of the score function associated with each model. A key ingredient in our proofs is a geometric characterization of Fisher information from quantized samples.",,,,,,"Ozgur, Ayfer/0000-0002-4455-4692",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,236,,,,,,,,,,,,,,,WOS:000608907700001,0
J,"Belchi, F; Brodzki, J; Burfitt, M; Niranjan, M",,,,"Belchi, Francisco; Brodzki, Jacek; Burfitt, Matthew; Niranjan, Mahesan",,,A Numerical Measure of the Instability of Mapper-Type Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mapper is an unsupervised machine learning algorithm generalising the notion of clustering to obtain a geometric description of a dataset. The procedure splits the data into possibly overlapping bins which are then clustered. The output of the algorithm is a graph where nodes represent clusters and edges represent the sharing of data points between two clusters. However, several parameters must be selected before applying Mapper and the resulting graph may vary dramatically with the choice of parameters. We define an intrinsic notion of Mapper instability that measures the variability of the output as a function of the choice of parameters required to construct a Mapper output. Our results and discussion are general and apply to all Mapper-type algorithms. We derive theoretical results that provide estimates for the instability and suggest practical ways to control it. We provide also experiments to illustrate our results and in particular we demonstrate that a reliable candidate Mapper output can be identified as a local minimum of instability regarded as a function of Mapper input parameters.",,,,,,"Niranjan, Mahesan/0000-0001-7021-140X; Burfitt, Matthew/0000-0001-7176-3476; Belchi Guillamon, Francisco/0000-0001-5863-3343",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,202,,,,,,,,,,,,,,,WOS:000590006500001,0
J,"Dobriban, E; Sheng, Y",,,,"Dobriban, Edgar; Sheng, Yue",,,WONDER: Weighted One-shot Distributed Ridge Regression in High Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many areas, practitioners need to analyze large data sets that challenge conventional single-machine computing. To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental and highly important problem in this area: How to do ridge regression in a distributed computing environment? Ridge regression is an extremely popular method for supervised learning, and has several optimality properties, thus it is important to study. We study one-shot methods that construct weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high-dimensional random-effects model where each predictor has a small effect, we discover several new phenomena. Infinite-worker limit: The distributed estimator works well for very large numbers of machines, a phenomenon we call infinite-worker limit. Optimal weights: The optimal weights for combining local estimators sum to more than unity, due to the downward bias of ridge. Thus, all averaging methods are suboptimal. We also propose a new Weighted ONe-shot DistributEd Ridge regression algorithm (WONDER). We test WONDER in simulation studies and using the Million Song Dataset as an example. There it can save at least 100x in computation time, while nearly preserving test accuracy.",,,,,"Sheng, Yue/AAN-3368-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000018,0
J,"Eldardiry, H; Neville, J; Rossi, RA",,,,"Eldardiry, Hoda; Neville, Jennifer; Rossi, Ryan A.",,,Ensemble Learning for Relational Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a theoretical analysis framework for relational ensemble models. We show that ensembles of collective classifiers can improve predictions for graph data by reducing errors due to variance in both learning and inference. In addition, we propose a relational ensemble framework that combines a relational ensemble learning approach with a relational ensemble inference approach for collective classification. The proposed ensemble techniques are applicable for both single and multiple graph settings. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed framework. Finally, our experimental results support the theoretical analysis and confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000002,0
J,"Shi, CC; Lu, WB; Song, R",,,,"Shi, Chengchun; Lu, Wenbin; Song, Rui",,,Breaking the Curse of Nonregularity with Subagging - Inference of the Mean Outcome under Optimal Treatment Regimes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Precision medicine is an emerging medical approach that allows physicians to select the treatment options based on individual patient information. The goal of precision medicine is to identify the optimal treatment regime (OTR) that yields the most favorable clinical outcome. Prior to adopting any OTR in clinical practice, it is crucial to know the impact of implementing such a policy. Although considerable research has been devoted to estimating the OTR in the literature, less attention has been paid to statistical inference of the OTR. Challenges arise in the nonregular cases where the OTR is not uniquely defined. To deal with nonregularity, we develop a novel inference method for the mean outcome under an OTR (the optimal value function) based on subsample aggregating (subagging). The proposed method can be applied to multi-stage studies where treatments are sequentially assigned over time. Bootstrap aggregating (bagging) and subagging have been recognized as effective variance reduction techniques to improve unstable estimators or classifiers (Buhlmann and Yu, 2002). However, it remains unknown whether these approaches can yield valid inference results. We show the proposed confidence interval (CI) for the optimal value function achieves nominal coverage. In addition, due to the variance reduction effect of subagging, our method enjoys certain statistical optimality. Specifically, we show that the mean squared error of the proposed value estimator is strictly smaller than that based on the simple sample-splitting estimator in the nonregular cases. Moreover, under certain conditions, the length of our proposed CI is shown to be on average shorter than CIs constructed based on the existing state-of-the-art method (Luedtke and van der Laan, 2016) and the oracle method which works as well as if an OTR were known. Extensive numerical studies are conducted to back up our theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,176,,,,,,,,,,,,,,,WOS:000570207500001,0
J,"Spiridonoff, A; Olshevsky, A; Paschalidis, IC",,,,"Spiridonoff, Artin; Olshevsky, Alex; Paschalidis, Ioannis Ch",,,Robust Asynchronous Stochastic Gradient-Push: Asymptotically Optimal and Network-Independent Performance for Strongly Convex Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the standard model of distributed optimization of a sum of functions F(z) = Sigma(n)(i=1) f(i)(z), where node i in a network holds the function f(i)(z). We allow for a harsh network model characterized by asynchronous updates, message delays, unpredictable message losses, and directed communication among nodes. In this setting, we analyze a modification of the Gradient-Push method for distributed optimization, assuming that (i) node i is capable of generating gradients of its function f(i)(z) corrupted by zero-mean bounded{support additive noise at each step, (ii) F(z) is strongly convex, and (iii) each fi(z) has Lipschitz gradients. We show that our proposed method asymptotically performs as well as the best bounds on centralized gradient descent that takes steps in the direction of the sum of the noisy gradients of all the functions f(1)(z), ..., f(n)(z) at each step.",,,,,"Spiridonoff, Artin/AAR-5212-2021","Olshevsky, Alex/0000-0002-5852-9789",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,32989377,,,,,WOS:000529405000010,0
J,"Yu, M; Gupta, V; Kolar, M",,,,"Yu, Ming; Gupta, Varun; Kolar, Mladen",,,Simultaneous Inference for Pairwise Graphical Models with Generalized Score Matching,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic graphical models provide a flexible yet parsimonious framework for modeling dependencies among nodes in networks. There is a vast literature on parameter estimation and consistent model selection for graphical models. However, in many of the applications, scientists are also interested in quantifying the uncertainty associated with the estimated parameters and selected models, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for statistical inference on edge parameters in pairwise graphical models based on generalized Hyvarinen scoring rule. Hyvarinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form, which is a common problem for graphical models, including Ising models and truncated Gaussian graphical models. Our estimator allows us to perform statistical inference for general graphical models whereas the existing works mostly focus on statistical inference for Gaussian graphical models where finding normalizing constant is computationally tractable. Under mild conditions that are typically assumed in the literature for consistent estimation, we prove that our proposed estimator is root n-consistent and asymptotically normal, which allows us to construct confidence intervals and build hypothesis tests for edge parameters. Moreover, we show how our proposed method can be applied to test hypotheses that involve a large number of model parameters simultaneously. We illustrate validity of our estimator through extensive simulation studies on a diverse collection of data-generating processes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,91,,,,,,,,,,,,,,,WOS:000545027100001,0
J,"Yuan, XM; Zeng, SZ; Zhang, J",,,,"Yuan, Xiaoming; Zeng, Shangzhi; Zhang, Jin",,,Discerning the Linear Convergence of ADMM for Structured Convex Optimization through the Lens of Variational Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Despite the rich literature, the linear convergence of alternating direction method of multipliers (ADMM) has not been fully understood even for the convex case. For example, the linear convergence of ADMM can be empirically observed in a wide range of applications arising in statistics, machine learning, and related areas, while existing theoretical results seem to be too stringent to be satisfied or too ambiguous to be checked and thus why the ADMM performs linear convergence for these applications still seems to be unclear. In this paper, we systematically study the local linear convergence of ADMM in the context of convex optimization through the lens of variational analysis. We show that the local linear convergence of ADMM can be guaranteed without the strong convexity of objective functions together with the full rank assumption of the coefficient matrices, or the full polyhedricity assumption of their subdifferential; and it is possible to discern the local linear convergence for various concrete applications, especially for some representative models arising in statistical learning. We use some variational analysis techniques sophisticatedly; and our analysis is conducted in the most general proximal version of ADMM with Fortin and Glowinski's larger step size so that all major variants of the ADMM known in the literature are covered.",,,,,,"Yuan, Xue-Ming/0000-0003-1575-0130; Zeng, Shangzhi/0000-0002-6950-7825",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000542194600006,0
J,"Chew, R; Wenger, M; Kery, C; Nance, J; Richards, K; Hadley, E; Baumgartner, P",,,,"Chew, Rob; Wenger, Michael; Kery, Caroline; Nance, Jason; Richards, Keith; Hadley, Emily; Baumgartner, Peter",,,SMART: An Open Source Data Labeling Platform for Supervised Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"SMART is an open source web application designed to help data scientists and research teams efficiently build labeled training data sets for supervised machine learning tasks. SMART provides users with an intuitive interface for creating labeled data sets, supports active learning to help reduce the required amount of labeled data, and incorporates inter-rater reliability statistics to provide insight into label quality. SMART is designed to be platform agnostic and easily deployable to meet the needs of as many different research teams as possible. The project website(1) contains links to the code repository and extensive user documentation.",,,,,"Hessabi, Shaahin/ABB-5246-2020; Chew, Rob/AAI-6054-2020","Hessabi, Shaahin/0000-0003-3193-2567; Chew, Rob/0000-0002-6979-1766; Hadley, Emily/0000-0003-0074-4344; Kery, Caroline/0000-0002-1794-3369; Wenger, Michael/0000-0001-8098-0683",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,1,5,82,,,,,,,,,,,,,,,WOS:000467897600001,0
J,"Khetan, A; Oh, S",,,,"Khetan, Ashish; Oh, Sewoong",,,Spectrum Estimation from a Few Entries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Singular values of a data in a matrix form provide insights on the structure of the data, the e ff ective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative fi ltering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. In this paper, we address the problem of directly recovering the spectrum, which is the set of singular values, and also in sample-e ffi cient approaches for recovering a spectral sum function, which is an aggregate sum of a fi xed function applied to each of the singular values. Our approach is to fi rst estimate the Schatten k -norms of a matrix for a small set of values of k, and then apply Chebyshev approximation when estimating a spectral sum function or apply moment matching in Wasserstein distance when estimating the singular values directly. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures called network motifs in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we signi fi cantly improve upon a competing approach of using matrix completion methods, below the matrix completion threshold, above which matrix completion algorithms recover the underlying low-rank matrix exactly.",,,,,"Jeong, Yongwook/N-7413-2016",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,21,,,,,,,,,,,,,,,WOS:000458667800001,0
J,"Pirinen, A; Ames, B",,,,"Pirinen, Aleksis; Ames, Brendan",,,Exact Clustering of Weighted Graphs via Semidefinite Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As a model problem for clustering, we consider the densest k-disjoint-clique problem of partitioning a weighted complete graph into k disjoint subgraphs such that the sum of the densities of these subgraphs is maximized. We establish that such subgraphs can be recovered from the solution of a particular semidefinite relaxation with high probability if the input graph is sampled from a distribution of clusterable graphs. Specifically, the semidefinite relaxation is exact if the graph consists of k large disjoint subgraphs, corresponding to clusters, with weight concentrated within these subgraphs, plus a moderate number of nodes not belonging to any cluster. Further, we establish that if noise is weakly obscuring these clusters, i.e, the between-cluster edges are assigned very small weights, then we can recover significantly smaller clusters. For example, we show that in approximately sparse graphs, where the between-cluster weights tend to zero as the size n of the graph tends to infinity, we can recover clusters of size polylogarithmic in n under certain conditions on the distribution of edge weights. Empirical evidence from numerical simulations is also provided to support these theoretical phase transitions to perfect recovery of the cluster structure.",,,,,"Ames, Brendan/AAT-7018-2020","Ames, Brendan/0000-0003-0810-7956",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,30,,,,,,,,,,,,,,,WOS:000463316400001,0
J,"Wang, J; Zhang, ZQ; Ye, JP",,,,"Wang, Jie; Zhang, Zhanqiu; Ye, Jieping",,,Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l(1) and l(2) norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable to sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method-called DPC (decomposition of convex set)-for non-negative Lasso. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,163,,,,,,,,,,,,,,,WOS:000506403100003,0
J,"Chen, RD; Paschalidis, IC",,,,"Chen, Ruidi; Paschalidis, Ioannis Ch.",,,A Robust Learning Approach for Regression Models Based on Distributionally Robust Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a Distributionally Robust Optimization (DRO) approach to estimate a robustified regression plane in a linear regression setting, when the observed samples are potentially contaminated with adversarially corrupted outliers. Our approach mitigates the impact of outliers by hedging against a family of probability distributions on the observed data, some of which assign very low probabilities to the outliers. The set of distributions under consideration are close to the empirical distribution in the sense of the Wasserstein metric. We show that this DRO formulation can be relaxed to a convex optimization problem which encompasses a class of models. By selecting proper norm spaces for the Wasserstein metric, we are able to recover several commonly used regularized regression models. We provide new insights into the regularization term and give guidance on the selection of the regularization coefficient from the standpoint of a confidence region. We establish two types of performance guarantees for the solution to our formulation under mild conditions. One is related to its out-of-sample behavior (prediction bias), and the other concerns the discrepancy between the estimated and true regression planes (estimation bias). Extensive numerical results demonstrate the superiority of our approach to a host of regression models, in terms of the prediction and estimation accuracies. We also consider the application of our robust learning procedure to outlier detection, and show that our approach achieves a much higher AUC (Area Under the ROC Curve) than M-estimation (Huber, 1964, 1973).",,,,,"Chen, Ruidi/AAZ-3412-2021",,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,13,,,,,,,,,,34421397,,,,,WOS:000443225700001,0
J,"Tikka, S; Karvanen, J",,,,"Tikka, Santtu; Karvanen, Juha",,,Enhancing Identification of Causal Effects by Pruning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Causal models communicate our assumptions about causes and effects in real-world phenomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria.,,,,,,"Tikka, Santtu/0000-0003-4039-4342",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,194,,,,,,,,,,,,,,,WOS:000435453700001,0
J,"Wang, SS; Gittens, A; Mahoney, MW",,,,"Wang, Shusen; Gittens, Alex; Mahoney, Michael W.",,,"Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the mass in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR; these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.",,,,,,"Wang, Shusen/0000-0003-3928-6782",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,218,,,,,,,,,,,,,,,WOS:000435629300001,0
J,"Chaudhuri, S; Tewari, A",,,,"Chaudhuri, Sougata; Tewari, Ambuj",,,Online Learning to Rank with Top-k Feedback,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over T rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top k ranked items, where k << m. The first setting is non - contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve O (T-2/3) regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. k = 1. We empirically demonstrate the performance of our algorithms on simulated and real world data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,103,,,,,,,,,,,,,,,WOS:000413449100001,0
J,"Kpotufe, S; Verma, N",,,,"Kpotufe, Samory; Verma, Nakul",,,Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel regression or classification also referred to as weighted is an element of-NN methods in Machine Learning) are appealing for their simplicity and therefore ubiquitous in data analysis. However, practical implementations of kernel regression or classification consist of quantizing or sub-sampling data for improving time efficiency, often at the cost of prediction quality. While such tradeoffs are necessary in practice, their statistical implications are generally not well understood, hence practical implementations come with few performance guarantees. In particular, it is unclear whether it is possible to maintain the statistical accuracy of kernel prediction-crucial in some applications-while improving prediction time. The present work provides guiding principles for combining kernel prediction with data-quantization so as to guarantee good tradeoffs between prediction time and accuracy, and in particular so as to approximately maintain the good accuracy of vanilla kernel prediction. Furthermore, our tradeoff guarantees are worked out explicitly in terms of a tuning parameter which acts as a knob that favors either time or accuracy depending on practical needs. On one end of the knob, prediction time is of the same order as that of single-nearest-neighbor prediction which is statistically inconsistent) while maintaining consistency; on the other end of the knob, the prediction risk is nearly minimax-optimal in terms of the original data size) while still reducing time complexity. The analysis thus reveals the interaction between the data-quantization approach and the kernel prediction method, and most importantly gives explicit control of the tradeoff to the practitioner rather than fixing the tradeoff in advance or leaving it opaque. The theoretical results are validated on data from a range of real-world application domains; in particular we demonstrate that the theoretical knob performs as expected.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,44,,,,,,,,,,,,,,,WOS:000405963500001,0
J,"Mahsereci, M; Hennig, P",,,,"Mahsereci, Maren; Hennig, Philipp",,,Probabilistic Line Searches for Stochastic Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,119,,,,,,,,,,,,,,,WOS:000424540900001,0
J,"Martinez, D; Alenya, G; Ribeiro, T; Inoue, K; Torras, C",,,,"Martinez, David; Alenya, Guillem; Ribeiro, Tony; Inoue, Katsumi; Torras, Carme",,,Relational Reinforcement Learning for Planning with Exogenous Effects,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multi-valued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way.",,,,,"Torras, Carme/M-1794-2014; Torras, Carme/AAG-3027-2019; Alenya, Guillem/ABH-1090-2020; Aleny√†, Guillem/A-5087-2010","Torras, Carme/0000-0002-2933-398X; Torras, Carme/0000-0002-2933-398X; Alenya, Guillem/0000-0002-6018-154X; Aleny√†, Guillem/0000-0002-6018-154X; Inoue, Katsumi/0000-0002-2717-9122",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,78,,,,,,,,,,,,,,,WOS:000412061800001,0
J,"Trouillon, T; Dance, CR; Gaussier, E; Welbl, J; Riedel, S; Bouchard, G",,,,"Trouillon, Theo; Dance, Christopher R.; Gaussier, Eric; Welbl, Johannes; Riedel, Sebastian; Bouchard, Guillaume",,,Knowledge Graph Completion via Complex Tensor Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs-labeled directed graphs|and predicting missing relationships-labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices-thus all possible relation/adjacency matrices-are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,130,,,,,,,,,,,,,,,WOS:000424545800001,0
J,"Kumar, MP; Dokania, PK",,,,"Kumar, M. Pawan; Dokania, Puneet K.",,,Rounding-based Moves for Semi-Metric Labeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Semi-metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given semi-metric distance function over the label set. Popular methods for solving semi-metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,91,,,,,,,,,,,,,,,WOS:000391538500001,0
J,"Lee, J; Kim, S; Lebanon, G; Singer, Y; Bengio, S",,,,"Lee, Joonseok; Kim, Seungyeon; Lebanon, Guy; Singer, Yoram; Bengio, Samy",,,LLORMA : Local Low-Rank Matrix Approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank. In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations. The two approaches approximate the observed matrix as a weighted sum of low-rank matrices. These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,15,,,,,,,,,,,,,,,WOS:000391471500001,0
J,"Lu, J; Hoi, SCH; Wang, JL; Zhao, PL; Liu, ZY",,,,"Lu, Jing; Hoi, Steven C. H.; Wang, Jialei; Zhao, Peilin; Liu, Zhi-Yong",,,Large Scale Online Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) Nystrom Online Gradient Descent (NOGD) algorithm that applies the Nystrom method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches.",,,,,,"Hoi, Steven/0000-0002-4584-3453",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,47,,,,,,,,,,,,,,,WOS:000391485400001,0
J,"Luttinen, J",,,,"Luttinen, Jaakko",,,BayesPy: Variational Bayesian Inference in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,41,,,,,,,,,,,,,,,WOS:000391482800001,0
J,"McDonald, AM; Pontil, M; Stamos, D",,,,"McDonald, Andrew M.; Pontil, Massimiliano; Stamos, Dimitris",,,New Perspectives on k-Support and Cluster Norms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a regularizer which is defined as a parameterized infimum of quadratics, and which we call the box-norm. We show that the k-support norm, a regularizer proposed by Argyriou et al. (2012) for sparse vector prediction problems, belongs to this family, and the box-norm can be generated as a perturbation of the former. We derive an improved algorithm to compute the proximity operator of the squared box-norm, and we provide a method to compute the norm. We extend the norms to matrices, introducing the spectral k-support norm and spectral box-norm. We note that the spectral box-norm is essentially equivalent to the cluster norm, a multitask learning regularizer introduced by Jacob et al. (2009a), and which in turn can be interpreted as a perturbation of the spectral k-support norm. Centering the norm is important for multitask learning and we also provide a method to use centered versions of the norms as regularizers. Numerical experiments indicate that the spectral k-support and box-norms and their centered variants provide state of the art performance in matrix completion and multitask learning problems respectively.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,155,,,,,,,,,,,,,,,WOS:000391665200001,0
J,"McQueen, J; Meila, M; VanderPlas, J; Zhang, ZY",,,,"McQueen, James; Meila, Marina; VanderPlas, Jacob; Zhang, Zhongyue",,,Megaman: Scalable Manifold Learning in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Manifold Learning (M L) is a class of algorithms seeking a low-dimensional non-linear representation of high-dimensional data. Thus, ML algorithms are most applicable to highdimensional data and require large sample sizes to accurately estimate the manifold. Despite this, most existing manifold learning implementations are not particularly scalable. Here we present a Python package that implements a variety of manifold learning algorithms in a modular and scalable fashion, using fast approximate neighbors searches and fast sparse eigendecompositions. The package incorporates theoretical advances in manifold learning, such as the unbiased Laplacian estimator introduced by Coifman and Lafon (2006) and the estimation of the embedding distortion by the Riemannian metric method introduced by Perrault-Joncas and Meila (2013). In benchmarks, even on a single-core desktop computer, our code embeds millions of data points in minutes, and takes just 200 minutes to embed the main sample of galaxy spectra from the Sloan Digital Sky Survey consisting of 0.6 million samples in 3750-dimensions a task which has not previously been possible.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,148,,,,,,,,,,,,,,,WOS:000391662500001,0
J,"Melchior, J; Fischer, A; Wiskott, L",,,,"Melchior, Jan; Fischer, Asja; Wiskott, Laurenz",,,How to Center Deep Boltzmann Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work analyzes centered Restricted Boltzmann Machines (RBMs) and centered Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. We show analytically that (i)centered and normal Boltzmann Machines (BMs) and thus RBMs and DBMs are different parameterizations of the same model class, such that any normal BM/RBM/DBM can be transformed to an equivalent centered BM/RBM/DBM and vice versa, and that this equivalence generalizes to artificial neural networks in general, (ii) the expected performance of centered binary BMs/RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any offset value in the range of zero to one, (iii) centering can be reformulated as a different update rule for normal BMs/RBMs/DBMs, and (iv) using the enhanced gradient is equivalent to setting the offset values to the average over model and data mean. Furthermore, we present numerical simulations suggesting that (i) optimal generative performance is achieved by subtracting mean values from visible as well as hidden variables, (ii) centered binary RBMs/DBMs reach significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii) centering variants whose offsets depend on the model mean, like the enhanced gradient, suffer from severe divergence problems, (iv) learning is stabilized if an exponentially moving average over the batch means is used for the offset values instead of the current batch mean, which also prevents the enhanced gradient from severe divergence, (v) on a similar level of log-likelihood values centered binary RBMs/DBMs have smaller weights and bigger bias parameters than normal binary RBMs/DBMs, (vi) centering leads to an update direction that is closer to the natural gradient, which is extremely efficient for training as we show for small binary RBMs, (vii) centering eliminates the need for greedy layer-wise pre-training of DBMs, which often even deteriorates the results independently of whether centering is used or not, and (ix) centering is also beneficial for auto encoders.",,,,,"Wiskott, Laurenz/P-7715-2017","Wiskott, Laurenz/0000-0001-6237-740X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,99,,,,,,,,,,,,,,,WOS:000391540600001,0
J,"Pfeuffer, J; Serang, O",,,,"Pfeuffer, Julianus; Serang, Oliver",,,A Bounded p-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm parallel to center dot parallel to (infinity), and use this approach to derive two numerically stable methods based on the idea of computing p-norms via fast convolution: The first method proposed, with runtime in O(klog(k)log(log(k))) (which is less than 18klog(k) for any vectors that can be practically realized), uses the p-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in O(klog(k)) (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of p-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The p-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from O(nk(2)) steps to O(nklog(k)) steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,36,,,,,,,,,,,,,,,WOS:000391481300001,0
J,"Read, J; Reutemann, P; Pfahringer, B; Holmes, G",,,,"Read, Jesse; Reutemann, Peter; Pfahringer, Bernhard; Holmes, Geoff",,,MEKA: A Multi-label/Multi-target Extension to WEKA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present MEKA : an open-source Java framework based on the well-known WEKA library. MEKA provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi-supervised contexts.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,21,,,,,,,,,,,,,,,WOS:000391474200001,0
J,"Sun, L; Nikolaev, AG",,,,"Sun, Lei; Nikolaev, Alexander G.",,,Mutual Information Based Matching for Causal Inference with Observational Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an information theory-driven matching methodology for making causal inference from observational data. The paper adopts a potential outcomes framework view on evaluating the strength of cause-effect relationships: the population-wide average effects of binary treatments are estimated by comparing two groups of units {the treated and untreated (control). To reduce the bias in such treatment effect estimation, one has to compose a control group in such a way that across the compared groups of units, treatment is independent of the units' covariates. This requirement gives rise to a subset selection / matching problem. This paper presents the models and algorithms that solve the matching problem by minimizing the mutual information (MI) between the covariates and the treatment variable. Such a formulation becomes tractable thanks to the derived optimality conditions that tackle the non-linearity of the sample-based MI function. Computational experiments with mixed integer-programming formulations and four matching algorithms demonstrate the utility of MI based matching for causal inference studies. The algorithmic developments culminate in a matching heuristic that allows for balancing the compared groups in polynomial (close to linear) time, thus allowing for treatment effect estimation with large data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,199,,,,,,,,,,,,,,,WOS:000391828500001,0
J,"Trillos, NG; Slepcev, D; von Brecht, J; Laurent, T; Bresson, X",,,,"Trillos, Nicolas Garcia; Slepcev, Dejan; von Brecht, James; Laurent, Thomas; Bresson, Xavier",,,Consistency of Cheeger and Ratio Graph Cuts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper establishes the consistency of a family of graph-cut-based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,181,,,,,,,,,,,,,,,WOS:000391822100001,0
J,"Valenzuela, ML; Rozenblit, JW",,,,"Valenzuela, Michael L.; Rozenblit, Jerzy W.",,,Learning Using Anti-Training with Sacrificial Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Traditionally the machine-learning community has viewed the No Free Lunch (NFL) theorems for search and optimization as a limitation. We review, analyze, and unify the NFL theorem with the perspectives of blind search and meta-learning to arrive at necessary conditions for improving black-box optimization. We survey meta-learning literature to determine when and how meta-learning can benefit machine learning. Then, we generalize meta-learning in the context of the NFL theorems, to arrive at a novel technique called anti-training with sacrificial data (ATSD). Our technique applies at the meta level to arrive at domain specific algorithms. We also show how to generate sacrificial data. An extensive case study is presented along with simulated annealing results to demonstrate the efficacy of the ATSD method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,24,,,,,,,,,,,,,,,WOS:000391475000001,0
J,"Wallace, BC; Kuiper, J; Sharma, A; Zhu, MX; Marshall, IJ",,,,"Wallace, Byron C.; Kuiper, Joel; Sharma, Aakash; Zhu, Mingxi (Brian); Marshall, Iain J.",,,Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations.To this end, we propose a novel method {supervised distant supervision (SDS) - {that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction.",,,,,,"Marshall, Iain/0000-0003-2594-2654",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,132,,,,,,,,,,27746703,,,,,WOS:000391657100001,0
J,"Aragam, B; Zhou, Q",,,,"Aragam, Bryon; Zhou, Qing",,,Concave Penalized Estimation of Sparse Gaussian Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular l(0) (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2273,2328,,,,,,,,,,,,,,,,WOS:000369887600005,0
J,"van Erven, T; Grunwald, PD; Mehta, NA; Reid, MD; Williamson, RC",,,,"van Erven, Tim; Grunwald, Peter D.; Mehta, Nishant A.; Reid, Mark D.; Williamson, Robert C.",,,Fast Rates in Statistical and Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning - a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.",,,,,,"Mehta, Nishant/0000-0002-9639-0124",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1793,1861,,,,,,,,,,,,,,,,WOS:000369887300005,0
J,"Tran-Dinh, Q; Kyrillidis, A; Cevher, V",,,,"Tran-Dinh, Quoc; Kyrillidis, Anastasios; Cevher, Volkan",,,Composite Self-Concordant Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a variable metric framework for minimizing the sum of a self-concordant function and a possibly non-smooth convex function, endowed with an easily computable proximal operator. We theoretically establish the convergence of our framework without relying on the usual Lipschitz gradient assumption on the smooth part. An important highlight of our work is a new set of analytic step-size selection and correction procedures based on the structure of the problem. We describe concrete algorithmic instances of our framework for several interesting applications and demonstrate them numerically on both synthetic and real data.",,,,,"Tran-Dinh, Quoc/AAX-8950-2020","Tran-Dinh, Quoc/0000-0002-1077-2579",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,371,416,,,,,,,,,,,,,,,,WOS:000369886000002,0
J,"Yan, Q; Ye, JP; Shen, XT",,,,"Yan, Qi; Ye, Jieping; Shen, Xiaotong",,,Simultaneous Pursuit of Sparseness and Rank Structures for Matrix Decomposition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"multi-response regression, pursuit of two different types of structures is essential to battle the curse of dimensionality. In this paper, we seek a sparsest decomposition representation of a parameter matrix in terms of a sum of sparse and low rank matrices, among many overcomplete decompositions. On this basis, we propose a constrained method subject to two nonconvex constraints, respectively for sparseness and low-rank properties. Computationally, obtaining an exact global optimizer is rather challenging. To overcome the difficulty, we use an alternating directions method solving a low-rank subproblem and a sparseness subproblem alternatively, where we derive an exact solution to the low-rank subproblem, as well as an exact solution in a special case and an approximated solution generally through a surrogate of the Lo-constraint and difference convex programming, for the sparse subproblem. Theoretically, we establish convergence rates of a global minimizer in the Hellinger-distance, providing an insight into why pursuit of two different types of decomposed structures is expected to deliver higher estimation accuracy than its counterparts based on either sparseness alone or low-rank approximation alone. Numerical examples are given to illustrate these aspects, in addition to an application to facial imagine recognition and multiple time series analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2015,16,,,,,,47,75,,,,,,,,,,,,,,,,WOS:000369885500002,0
J,"Balcan, MF; Liang, YY; Gupta, P",,,,"Balcan, Maria-Florina; Liang, Yingyu; Gupta, Pramod",,,Robust Hierarchical Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the most widely used techniques for data clustering is agglomerative clustering. Such algorithms have been long used across many different fields ranging from computational biology to social sciences to computer vision in part because their output is easy to interpret. Unfortunately, it is well known, however, that many of the classic agglomerative clustering algorithms are not robust to noise. In this paper we propose and analyze a new robust algorithm for bottom-up agglomerative clustering. We show that our algorithm can be used to cluster accurately in cases where the data satisfies a number of natural properties and where the traditional agglomerative algorithms fail. We also show how to adapt our algorithm to the inductive setting where our given data is only a small random sample of the entire data set. Experimental evaluations on synthetic and real world data sets show that our algorithm achieves better performance than other hierarchical algorithms in the presence of noise.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2014,15,,,,,,3831,3871,,,,,,,,,,,,,,,,WOS:000354999700002,0
J,"Muller, AC; Behnke, S",,,,"Mueller, Andreas C.; Behnke, Sven",,,PyStruct - Learning Structured Prediction in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structured prediction methods have become a central tool for many machine learning applications. While more and more algorithms are developed, only very few implementations are available. PYSTRUCT aims at providing a general purpose implementation of standard structured prediction methods, both for practitioners and as a baseline for researchers. It is written in Python and adapts paradigms and types from the scientific Python community for seamless integration with other projects.",,,,,"Behnke, Sven/B-5509-2013","Behnke, Sven/0000-0002-5040-7525",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2055,2060,,,,,,,,,,,,,,,,WOS:000344638300005,0
J,"Wager, S; Hastie, T; Efron, B",,,,"Wager, Stefan; Hastie, Trevor; Efron, Bradley",,,Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the variability of predictions made by bagged learners and random forests, and show how to estimate standard errors for these methods. Our work builds on variance estimates for bagging proposed by Efron (1992, 2013) that are based on the jackknife and the infinitesimal jackknife (IJ). In practice, bagged predictors are computed using a finite number B of bootstrap replicates, and working with a large B can be computationally expensive. Direct applications of jackknife and IJ estimators to bagging require B = Theta (n(1.5)) bootstrap replicates to converge, where n is the size of the training set. We propose improved versions that only require B = Theta (n) replicates. Moreover, we show that the IJ estimator requires 1.7 times less bootstrap replicates than the jackknife to achieve a given accuracy. Finally, we study the sampling distributions of the jackknife and IJ variance estimates themselves. We illustrate our findings with multiple experiments and simulation studies.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1625,1651,,,,,,,,,,,25580094,,,,,WOS:000344638100001,0
J,"Wang, Z; Paterlini, S; Gao, FC; Yang, YH",,,,"Wang, Zhan; Paterlini, Sandra; Gao, Fuchang; Yang, Yuhong",,,Adaptive Minimax Regression Estimation over Sparse l(q)-Hulls,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a dictionary of M-n predictors, in a random design regression setting with n observations, we construct estimators that target the best performance among all the linear combinations of the predictors under a sparse l(q) - norm (0 <= q <= 1) constraint on the linear coe ffi cients. Besides identifying the optimal rates of convergence, our universal aggregation strategies by model mixing achieve the optimal rates simultaneously over the full range of 0 <= q <= 1 for any M n and without knowledge of the l(q)-norm of the best linear coe ffi cients to represent the regression function. To allow model misspeci fi cation, our upper bound results are obtained in a framework of aggregation of estimates. A striking feature is that no speci fi c relationship among the predictors is needed to achieve the upper rates of convergence (hence permitting basically arbitrary correlations between the predictors). Therefore, whatever the true regression function (assumed to be uniformly bounded), our estimators automatically exploit any sparse representation of the regression function (if any), to the best extent possible within the l(q)-constrained linear combinations for any 0 <= q <= 1. A sparse approximation result in the l(q)-hulls turns out to be crucial to adaptively achieve minimax rate optimal aggregation. It precisely characterizes the number of terms needed to achieve a prescribed accuracy of approximation to the best linear combination in an l(q)-hull for 0 <= q <= 1. It o ff ers the insight that the minimax rate of l(q)-aggregation is basically determined by an e ff ective model size, which is a sparsity index that depends on q, M-n, n, and the l(q)-norm bound in an easily interpretable way based on a classical model selection theory that deals with a large number of models.",,,,,"Yang, Yuhong/J-6596-2019; Paterlini, Sandra/B-3770-2016","Yang, Yuhong/0000-0003-3618-3083; Paterlini, Sandra/0000-0003-4269-4496; Gao, Fuchang/0000-0001-7699-4433",,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1675,1711,,,,,,,,,,,,,,,,WOS:000344638100003,0
J,"Gupta, MR; Bengio, S; Weston, J",,,,"Gupta, Maya R.; Bengio, Samy; Weston, Jason",,,Training Highly Multiclass Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classification problems with thousands or more classes often have a large range of class-confusabilities, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training. We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent, can also be profitably applied to the non-convex joint training of supervised dimensionality reduction and linear classifiers as done in Wsabie. Experiments on Image Net benchmark data sets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs-all linear SVMs and Wsabie.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1461,1492,,,,,,,,,,,,,,,,WOS:000338420000009,0
J,"Coviello, E; Chan, AB; Lanckriet, GRG",,,,"Coviello, Emanuele; Chan, Antoni B.; Lanckriet, Gert R. G.",,,Clustering Hidden Markov Models with Variational HEM,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a cluster center, that is, a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand-writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization.",,,,,"CHAN, Antoni B./D-7858-2013","CHAN, Antoni B./0000-0002-2886-2513",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,697,747,,,,,,,,,,,,,,,,WOS:000335457700011,0
J,"Mukherjee, I; Rudin, C; Schapire, RE",,,,"Mukherjee, Indraneel; Rudin, Cynthia; Schapire, Robert E.",,,The Rate of Convergence of AdaBoost,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The AdaBoost algorithm was designed to combine many weak hypotheses that perform slightly better than random guessing into a strong hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the exponential loss. Unlike previous work, our proofs do not require a weak learning assumption, nor do they require that minimizers of the exponential loss are finite. Our first result shows that the exponential loss of AdaBoost's computed parameter vector will be at most epsilon more than that of any parameter vector of l (1)-norm bounded by B in a number of rounds that is at most a polynomial in B and 1/epsilon. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/epsilon iterations, AdaBoost achieves a value of the exponential loss that is at most epsilon more than the best possible value, where C depends on the data set. We show that this dependence of the rate on epsilon is optimal up to constant factors, that is, at least Omega(1/epsilon) rounds are necessary to achieve within epsilon of the optimal exponential loss.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2315,2347,,,,,,,,,,,,,,,,WOS:000324799600004,0
J,"Hoffman, MD; Blei, DM; Wang, C; Paisley, J",,,,"Hoffman, Matthew D.; Blei, David M.; Wang, Chong; Paisley, John",,,Stochastic Variational Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topicmodel outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.",,,,,"Paisley, John/AAF-8586-2019",,,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1303,1347,,,,,,,,,,,,,,,,WOS:000320709300004,0
J,"Wang, Y; Tran, D; Liao, ZC; Forsyth, D",,,,"Wang, Yang; Duan Tran; Liao, Zicheng; Forsyth, David",,,Discriminative Hierarchical Part-based Models for Human Parsing and Action Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of parsing human poses and recognizing their actions in static images with part-based models. Most previous work in part-based models only considers rigid parts (e. g., torso, head, half limbs) guided by human anatomy. We argue that this representation of parts is not necessarily appropriate. In this paper, we introduce hierarchical poselets-a new representation for modeling the pose configuration of human bodies. Hierarchical poselets can be rigid parts, but they can also be parts that cover large portions of human bodies (e.g., torso + left arm). In the extreme case, they can be the whole bodies. The hierarchical poselets are organized in a hierarchical way via a structured model. Human parsing can be achieved by inferring the optimal labeling of this hierarchical model. The pose information captured by this hierarchical model can also be used as a intermediate representation for other high-level tasks. We demonstrate it in action recognition from static images.",,,,,"Cataldi, Antonio/AAM-7411-2021",,,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,3075,3102,,,,,,,,,,,,,,,,WOS:000313200000009,0
J,"Aho, T; Zenko, B; Dzeroski, S; Elomaa, T",,,,"Aho, Timo; Zenko, Bernard; Dzeroski, Saso; Elomaa, Tapio",,,Multi-Target Regression with Rule Ensembles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Methods for learning decision rules are being successfully applied to many problem domains, in particular when understanding and interpretation of the learned model is necessary. In many real life problems, we would like to predict multiple related (nominal or numeric) target attributes simultaneously. While several methods for learning rules that predict multiple targets at once exist, they are all based on the covering algorithm, which does not work well for regression problems. A better solution for regression is the rule ensemble approach that transcribes an ensemble of decision trees into a large collection of rules. An optimization procedure is then used to select the best (and much smaller) subset of these rules and to determine their respective weights. We introduce the FIRE algorithm for solving multi-target regression problems, which employs the rule ensembles approach. We improve the accuracy of the algorithm by adding simple linear functions to the ensemble. We also extensively evaluate the algorithm with and without linear functions. The results show that the accuracy of multi-target regression rule ensembles is high. They are more accurate than, for instance, multi-target regression trees, but not quite as accurate as multi-target random forests. The rule ensembles are significantly more concise than random forests, and it is also possible to create compact rule sets that are smaller than a single regression tree but still comparable in accuracy.",,,,,"Elomaa, Tapio/AAE-6936-2020; Dzeroski, Saso/ABH-2758-2021; Elomaa, Tapio P/G-4233-2014","Elomaa, Tapio/0000-0002-5230-0027; Dzeroski, Saso/0000-0003-2363-712X; ",,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2367,2407,,,,,,,,,,,,,,,,WOS:000308795200005,0
J,"Rinaldo, A; Singh, A; Nugent, R; Wasserman, L",,,,"Rinaldo, Alessandro; Singh, Aarti; Nugent, Rebecca; Wasserman, Larry",,,Stability of Density-Based Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High density clusters can be characterized by the connected components of a level set L(lambda) = {x : p(x) > lambda} of the underlying probability density function p generating the data, at some appropriate level lambda >= 0. The complete hierarchical clustering can be characterized by a cluster tree T = boolean OR L-lambda(lambda). In this paper, we study the behavior of a density level set estimate (L) over cap(lambda) and cluster tree estimate (T) over cap based on a kernel density estimator with kernel bandwidth h. We define two notions of instability to measure the variability of (L) over cap(lambda) and (T) over cap as a function of h, and investigate the theoretical properties of these instability measures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,905,948,,,,,,,,,,,,,,,,WOS:000303773100001,0
J,"Orabona, F; Jie, L; Caputo, B",,,,"Orabona, Francesco; Jie, Luo; Caputo, Barbara",,,Multi Kernel Learning with Online-Batch Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years there has been a lot of interest in designing principled classification algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve the MKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and experimentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very few iterations are sufficient to reach good solutions. Experiments on standard benchmark databases support our claims.",,,,,"Caputo, Barbara/J-8976-2015; Orabona, Francesco/G-6758-2012; Caputo, Barbara/F-3928-2011","Caputo, Barbara/0000-0001-7169-0158; orabona, francesco/0000-0001-8523-6845",,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,227,253,,,,,,,,,,,,,,,,WOS:000303046000001,0
J,"Glowacka, D; Shawe-Taylor, J; Clark, A; de la Higuera, C; Johnson, M",,,,"Glowacka, Dorota; Shawe-Taylor, John; Clark, Alexander; de la Higuera, Colin; Johnson, Mark",,,"Introduction to the Special Topic on Grammar Induction, Representation of Language and Language Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Grammar induction refers to the process of learning grammars and languages from data; this finds a variety of applications in syntactic pattern recognition, the modeling of natural language acquisition, data mining and machine translation. This special topic contains several papers presenting some of recent developments in the area of grammar induction and language learning, as applied to various problems in Natural Language Processing, including supervised and unsupervised parsing and statistical machine translation.",,,,,,", Colin/0000-0002-1703-9572; Shawe-Taylor, John/0000-0002-2030-0073; Johnson, Mark/0000-0003-4809-8441",,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1425,1428,,,,,,,,,,,,,,,,WOS:000290096100008,0
J,"Cavallanti, G; Cesa-Bianchi, N; Gentile, C",,,,"Cavallanti, Giovanni; Cesa-Bianchi, Nicolo; Gentile, Claudio",,,Linear Algorithms for Online Multitask Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce new Perceptron-based algorithms for the online multitask binary classification problem. Under suitable regularity conditions, our algorithms are shown to improve on their baselines by a factor proportional to the number of tasks. We achieve these improvements using various types of regularization that bias our algorithms towards specific notions of task relatedness. More specifically, similarity among tasks is either measured in terms of the geometric closeness of the task reference vectors or as a function of the dimension of their spanned subspace. In addition to adapting to the online setting a mix of known techniques, such as the multitask kernels of Evgeniou et al., our analysis also introduces a matrix-based multitask extension of the p-norm Perceptron, which is used to implement spectral co-regularization. Experiments on real-world data sets complement and support our theoretical findings.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2901,2934,,,,,,,,,,,,,,,,WOS:000284040000010,0
J,"Aoyagi, M",,,,"Aoyagi, Miki",,,Stochastic Complexity and Generalization Error of a Restricted Boltzmann Machine in Bayesian Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the asymptotic form of the generalization error for the restricted Boltzmann machine in Bayesian estimation. It has been shown that obtaining the maximum pole of zeta functions is related to the asymptotic form of the generalization error for hierarchical learning models (Watanabe, 2001a,b). The zeta function is defined by using a Kullback function. We use two methods to obtain the maximum pole: a new eigenvalue analysis method and a recursive blowing up process. We show that these methods are effective for obtaining the asymptotic form of the generalization error of hierarchical learning models.",,,,,"„Ç¢„Ç™„É§„ÇÆ, „Éü„Ç≠/HHN-7243-2022",,,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1243,1272,,,,,,,,,,,,,,,,WOS:000282521500002,0
J,"Venna, J; Peltonen, J; Nybo, K; Aidos, H; Kaski, S",,,,"Venna, Jarkko; Peltonen, Jaakko; Nybo, Kristian; Aidos, Helena; Kaski, Samuel",,,Information Retrieval Perspective to Nonlinear Dimensionality Reduction for Data Visualization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonlinear dimensionality reduction methods are often used to visualize high-dimensional data, although the existing methods have been designed for other related tasks such as manifold learning. It has been difficult to assess the quality of visualizations since the task has not been well-defined. We give a rigorous definition for a specific visualization task, resulting in quantifiable goodness measures and new visualization methods. The task is information retrieval given the visualization: to find similar data based on the similarities shown on the display. The fundamental tradeoff between precision and recall of information retrieval can then be quantified in visualizations as well. The user needs to give the relative cost of missing similar points vs. retrieving dissimilar points, after which the total cost can be measured. We then introduce a new method NeRV (neighbor retrieval visualizer) which produces an optimal visualization by minimizing the cost. We further derive a variant for supervised visualization; class information is taken rigorously into account when computing the similarity relationships. We show empirically that the unsupervised version outperforms existing unsupervised dimensionality reduction methods in the visualization task, and the supervised version outperforms existing supervised methods.",,,,,"Kaski, Samuel/B-6684-2008; Peltonen, Jaakko/ABD-1698-2020; Aidos, Helena/ABC-1421-2020; Peltonen, Jaakko T/O-5172-2016","Kaski, Samuel/0000-0003-1925-9154; Peltonen, Jaakko/0000-0003-3485-8585; Aidos, Helena/0000-0001-6827-4217; Peltonen, Jaakko T/0000-0003-3485-8585",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,451,490,,,,,,,,,,,,,,,,WOS:000277186500001,0
J,"Vovk, V; Zhdanov, F",,,,"Vovk, Vladimir; Zhdanov, Fedor",,,Prediction With Expert Advice For The Brier Game,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches, with well-known bookmakers playing the role of experts. The theoretical performance guarantee is not excessively loose on the football data set and is rather tight on the tennis data set.",,,,,,"Vovk, Vladimir/0000-0003-2602-6877",,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2445,2471,,,,,,,,,,,,,,,,WOS:000272346600002,0
J,"Drton, M; Eichler, M; Richardson, TS",,,,"Drton, Mathias; Eichler, Michael; Richardson, Thomas S.",,,Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models' long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of 'bow-free' recursive linear models. The term 'bow-free' refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the first iteration whenever the MLE is available in closed form.",,,,,,"Drton, Mathias/0000-0001-5614-3025",,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2329,2348,,,,,,,,,,,,,,,,WOS:000272346400006,0
J,"Boulle, M",,,,"Boulle, Marc",,,A Parameter-Free Classification Method for Large Scale Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper, we present a parameter-free scalable classification method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classification enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with data sets that are far larger than the available central memory. We finally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1367,1385,,,,,,,,,,,,,,,,WOS:000270825000002,0
J,"Taylor, ME; Stone, P",,,,"Taylor, Matthew E.; Stone, Peter",,,Transfer Learning for Reinforcement Learning Domains: A Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.",,,,,,"Taylor, Matthew/0000-0001-8946-0211",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1633,1685,,,,,,,,,,,,,,,,WOS:000270825000012,0
J,"Maes, F",,,,"Maes, Francis",,,Nieme: Large-Scale Energy-Based Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce NIEME, (1) a machine learning library for large-scale classification, regression and ranking. NIEME relies on the framework of energy-based models (LeCun et al., 2006) which unifies several learning algorithms ranging from simple perceptrons to recent models such as the pegasos support vector machine or l1-regularized maximum entropy models. This framework also unifies batch and stochastic learning which are both seen as energy minimization problems. NIEME can hence be used in a wide range of situations, but is particularly interesting for large-scale learning tasks where both the examples and the features are processed incrementally. Being able to deal with new incoming features at any time within the learning process is another original feature of the NIEME toolbox. NIEME is released under the GPL license. It is efficiently implemented in C++, it works on Linux, Mac OS X and Windows and provides interfaces for C++, Java and Python.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,743,746,,,,,,,,,,,,,,,,WOS:000270824500008,0
J,"Li, J; Moore, AW",,,,"Li, Jia; Moore, Andrew W.",,,Forecasting Web Page Views: Methods and Observations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for long-term prediction, ESSF improves accuracy significantly over other methods that ignore the yearly seasonality.",,,,,"Moore, Andrew W/A-4589-2009",,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2217,2250,,,,,,,,,,,,,,,,WOS:000262637300007,0
J,"Nickisch, H; Rasmussen, CE",,,,"Nickisch, Hannes; Rasmussen, Carl Edward",,,Approximations for Binary Gaussian Process Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection ( selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.",,,,,"Nickisch, Hannes/I-7049-2017","Nickisch, Hannes/0000-0003-1604-6647; Rasmussen, Carl Edward/0000-0001-8899-7850",,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2035,2078,,,,,,,,,,,,,,,,WOS:000262637300001,0
J,"Caponnetto, A; Micchelli, CA; Pontil, M; Ying, YM",,,,"Caponnetto, Andrea; Micchelli, Charles A.; Pontil, Massimiliano; Ying, Yiming",,,Universal multi-task kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we are concerned with reproducing kernel Hilbert spaces H-K of functions from an input space into a Hilbert space Y, an environment appropriate for multi-task learning. The reproducing kernel K associated to H-K has its values as operators on Y. Our primary goal here is to derive conditions which ensure that the kernel K is universal. This means that on every compact subset of the input space, every continuous function with values in Y can be uniformly approximated by sections of the kernel. We provide various characterizations of universal kernels and highlight them with several concrete examples of some practical importance. Our analysis uses basic principles of functional analysis and especially the useful notion of vector measures which we describe in sufficient detail to clarify our results.",,,,,"Ying, Yiming/AGD-7246-2022; Ying, Yiming/A-4196-2013","Ying, Yiming/0000-0001-7345-6672; Ying, Yiming/0000-0001-7345-6672; CAPONNETTO, Andrea/0000-0002-6311-0667",,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1615,1646,,,,,,,,,,,,,,,,WOS:000258646800011,0
J,"Gyorgy, A; Linder, T; Lugosi, G; Ottucsak, G",,,,"Gyorgy, Andras; Linder, Tamas; Lugosi, Gabor; Ottucsak, Gyoergy",,,The on-line shortest path problem under partial monitoring,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path ( defined as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is proportional to 1/root n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efficient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m << n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented.",,,,,,"Lugosi, Gabor/0000-0003-1614-5901; Gyorgy, Andras/0000-0003-0586-4337",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2369,2403,,,,,,,,,,,,,,,,WOS:000252744800006,0
J,"Koh, KM; Kim, SJ; Boyd, S",,,,"Koh, Kwangmoo; Kim, Seung-Jean; Boyd, Stephen",,,An interior-point method for large-scale l(1)-regularized logistic regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Logistic regression with l(1) regularization has been proposed as a promising method for feature selection in classification problems. In this paper we describe an efficient interior-point method for solving large-scale l(1)-regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e. g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efficiently than by solving a family of problems independently.",,,,,,"Koh, Kwangmoo/0000-0002-5126-2946",,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1519,1555,,,,,,,,,,,,,,,,WOS:000249353700006,0
J,"Buhlmann, P; Yu, B",,,,"Buehlmann, Peter; Yu, Bin",,,Sparse boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose Sparse Boosting (the SparseL(2)Boost algorithm), a variant on boosting with the squared error loss. SparseL(2)Boost yields sparser solutions than the previously proposed L(2)Boosting by minimizing some penalized L-2-loss functions, the FPE model selection criteria, through small-step gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL(2)Boost. We prove an equivalence of SparseL(2)Boost to Breiman's nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL(2)Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL(2)Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L(2)Boosting. Consequently, we can select between SparseL(2)Boost and L(2)Boosting by comparing their gMDL scores.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1001,1024,,,,,,,,,,,,,,,,WOS:000245388400005,0
J,"Liu, T; Moore, AW; Gray, A",,,,"Liu, Ting; Moore, Andrew W.; Gray, Alexander",,,New algorithms for efficient high-dimensional nonparametric classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is about non-approximate acceleration of high-dimensional nonparametric operations such as k nearest neighbor classifiers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the data points close to the query, but merely need to answer questions about the properties of that set of data points. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. This is applicable to many algorithms in nonparametric statistics, memory-based learning and kernel-based learning. But for clarity, this paper concentrates on pure k-NN classification. We introduce new ball-tree algorithms that on real-world data sets give accelerations from 2-fold to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include data sets with up to 106 dimensions and 105 records, and demonstrate non-trivial speed-ups while giving exact answers.",,,,,,"Cardie, Claire/0000-0002-2061-6094",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1135,1158,,,,,,,,,,,,,,,,WOS:000245388400010,0
J,"Passerini, A; Frasconi, P; De Raedt, L",,,,"Passerini, A; Frasconi, P; De Raedt, L",,,Kernels on prolog proof trees: Statistical learning in the ILP setting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop kernels for measuring the similarity between relational instances using background knowledge expressed in first-order logic. The method allows us to bridge the gap between traditional inductive logic programming (ILP) representations and statistical approaches to supervised learning. Logic programs are first used to generate proofs of given visitor programs that use predicates declared in the available background knowledge. A kernel is then defined over pairs of proof trees. The method can be used for supervised learning tasks and is suitable for classification as well as regression. We report positive empirical results on Bongard-like and M-of-N problems that are difficult or impossible to solve with traditional ILP techniques, as well as on real bioinformatics and chemoinformatics data sets.",,,,,"De Raedt, Luc/AAX-1544-2021; Frasconi, Paolo/G-2944-2010","De Raedt, Luc/0000-0002-6860-6303; passerini, andrea/0000-0002-2765-5395",,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,307,342,,,,,,,,,,,,,,,,WOS:000236331700005,0
J,"Ratsch, G; Warmuth, MK",,,,"Ratsch, G; Warmuth, MK",,,Efficient margin maximizing with boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"AdaBoost produces a linear combination of base hypotheses and predicts with the sign of this linear combination. The linear combination may be viewed as a hyperplane in feature space where the base hypotheses form the features. It has been observed that the generalization error of the algorithm continues to improve even after all examples are on the correct side of the current hyperplane. The improvement is attributed to the experimental observation that the distances ( margins) of the examples to the separating hyperplane are increasing even after all examples are on the correct side. We introduce a new version of AdaBoost, called AdaBoost(v)*, that explicitly maximizes the minimum margin of the examples up to a given precision. The algorithm incorporates a current estimate of the achievable margin into its calculation of the linear coefficients of the base hypotheses. The bound on the number of iterations needed by the new algorithms is the same as the number needed by a known version of AdaBoost that must have an explicit estimate of the achievable margin as a parameter. We also illustrate experimentally that our algorithm requires considerably fewer iterations than other algorithms that aim to maximize the margin.",,,,,"R√§tsch, Gunnar/B-8182-2009; R√§tsch, Gunnar/O-5914-2017","R√§tsch, Gunnar/0000-0001-5486-8532",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2131,2152,,,,,,,,,,,,,,,,WOS:000236331100009,0
J,"Binev, P; Cohen, A; Dahmen, W; DeVore, R; Temlyakov, V",,,,"Binev, P; Cohen, A; Dahmen, W; DeVore, R; Temlyakov, V",,,Universal algorithms for learning theory part I : piecewise constant functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with the construction and analysis of a universal estimator for the regression problem in supervised learning. Universal means that the estimator does not depend on any a priori assumptions about the regression function to be estimated. The universal estimator studied in this paper consists of a least-square fitting procedure using piecewise constant functions on a partition which depends adaptively on the data. The partition is generated by a splitting procedure which differs from those used in CART algorithms. It is proven that this estimator performs at the optimal convergence rate for a wide class of priors on the regression function. Namely, as will be made precise in the text, if the regression function is in any one of a certain class of approximation spaces (or smoothness spaces of order not exceeding one - a limitation resulting because the estimator uses piecewise constants) measured relative to the marginal measure, then the estimator converges to the regression function (in the least squares sense) with an optimal rate of convergence in terms of the number of samples. The estimator is also numerically feasible and can be implemented on-line.",,,,,"Temlyakov, Vladimir N/I-2872-2017","Temlyakov, Vladimir N/0000-0002-7945-9513",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1297,1321,,,,,,,,,,,,,,,,WOS:000236330100002,0
J,"Hutter, M; Poland, J",,,,"Hutter, M; Poland, J",,,Adaptive online prediction by following the perturbed leader,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When applying aggregating strategies to Prediction with Expert Advice ( PEA), the learning rate must be adaptively tuned. The natural choice of root complexity/current loss renders the analysis of Weighted Majority (WM) derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative Follow the Perturbed Leader (FPL) algorithm from Kalai and Vempala (2003) based on Hannan's algorithm is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are new.",,,,,,"Hutter, Marcus/0000-0002-3263-4097",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,639,660,,,,,,,,,,,,,,,,WOS:000236329600010,0
J,"Stainvas, I; Lowe, D",,,,"Stainvas, I; Lowe, D",,,A generative model for separating illumination and reflectance from images,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is well known that even slight changes in nonuniform illumination lead to a large image variability and are crucial for many visual tasks. This paper presents a new ICA related probabilistic model where the number of sources exceeds the number of sensors to perform an image segmentation and illumination removal, simultaneously. We model illumination and reflectance in log space by a generalized autoregressive process and Hidden Gaussian Markov random field, respectively. The model ability to deal with segmentation of illuminated images is compared with a Canny edge detector and homomorphic filtering. We apply the model to two problems: synthetic image segmentation and sea surface pollution detection from intensity images.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1499,1519,,,,,,,,,,,,,,,,WOS:000224808300015,0
J,"Zhang, NL",,,,"Zhang, NL",,,Hierarchical latent class models for cluster analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent class models are used for cluster analysis of categorical data. Underlying such a model is the assumption that the observed variables are mutually independent given the class variable. A serious problem with the use of latent class models, known as local dependence, is that this assumption is often untrue. In this paper we propose hierarchical latent class models as a framework where the local dependence problem can be addressed in a principled manner. We develop a search-based algorithm for learning hierarchical latent class models from data. The algorithm is evaluated using both synthetic and real-world data.",,,,,,"Zhang, Nevin Lianwen/0000-0002-4662-3217",,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,697,723,,,,,,,,,,,,,,,,WOS:000236327700006,0
J,"d'Avignon, C; Geman, D",,,,"d'Avignon, C; Geman, D",,,Tree-structured neural decoding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose adaptive testing as a general mechanism for extracting information about stimuli from spike trains. Each test or question corresponds to choosing a neuron and a time interval and checking for a given number of spikes. No assumptions are made about the distribution of spikes or any other aspect of neural encoding. The chosen questions are those which most reduce the uncertainty about the stimulus, as measured by entropy and estimated from stimulus-response data. Our experiments are based on accurate simulations of responses to pure tones in the auditory nerve and are meant to illustrate the ideas rather than investigate the auditory system. The results cohere nicely with well-understood encoding of amplitude and frequency in the auditory nerve, suggesting that adaptive testing might provide a powerful tool for investigating complex and poorly understood neural structures.",,,,,"Geman, Donald/A-3325-2010",,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,743,754,,10.1162/153244304773936117,0,,,,,,,,,,,,,WOS:000221345700013,0
J,"Getoor, L; Friedman, N; Koller, D; Taskar, B",,,,"Getoor, L; Friedman, N; Koller, D; Taskar, B",,,Learning probabilistic models of link structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"Most real-world data is heterogeneous and richly interconnected. Examples include the Web, hypertext, bibliometric data and social networks. In contrast, most statistical learning methods work with flat data representations, forcing us to convert our data into a form that loses much of the link structure. The recently introduced framework of probabilistic relational models (PRMs) embraces the object-relational nature of structured data by capturing probabilistic interactions between attributes of related entities. In this paper, we extend this framework by modeling interactions between the attributes and the link structure itself. An advantage of our approach is a unified generative model for both content and relational structure. We propose two mechanisms for representing a probabilistic distribution over link structures: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict link structure and, moreover, the observed link structure can be used to provide better predictions for the attributes in the model.",,,,,"Friedman, Nir/H-9692-2012","Friedman, Nir/0000-0002-9678-3550",,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,679,707,,10.1162/jmlr.2003.3.4-5.679,0,,,,,,,,,,,,,WOS:000184926200004,0
J,"Herbrich, R; Williamson, RC",,,,"Herbrich, R; Williamson, RC",,,Algorithmic luckiness,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classical statistical learning theory studies the generalisation performance of machine learning algorithms rather indirectly. One of the main detours is that algorithms are studied in terms of the hypothesis class that they draw their hypotheses from. In this paper, motivated by the luckiness framework of Shawe-Taylor et al. (1998), we study learning algorithms more directly and in a way that allows us to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits the margin, the sparsity of the resultant weight vector, and the degree of clustering of the training data in feature space.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,175,212,,10.1162/153244303765208368,0,,,,,,,,,,,,,WOS:000182488500001,0
J,"Tax, DMJ; Duin, RPW",,,,"Tax, DMJ; Duin, RPW",,,Uniform object generation for optimizing one-class classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, COLORADO",,,,,"In one-class classification, one class of data, called the target class, has to be distinguished from the rest of the feature space. It is assumed that only examples of the target class are available. This classifier has to be constructed such that objects not originating from the target set, by definition outlier objects, are not classified as target objects. In previous research the support vector data description (SVDD) is proposed to solve the problem of one-class classification. It models a hypersphere around the target set, and by the introduction of kernel functions, more flexible descriptions are obtained. In the original optimization of the SVDD, two parameters have to be given beforehand by the user. To automatically optimize the values for these parameters, the error on both the target and outlier data has to be estimated. Because no outlier examples are available, we propose a method for generating artificial outliers, uniformly distributed in a hypersphere. An (relative) efficient estimate for the volume covered by the one-class classifiers is obtained, and so an estimate for the outlier error. Results are shown for artificial data and for real world data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,155,173,,10.1162/15324430260185583,0,,,,,,,,,,,,,WOS:000176055300005,0
J,"Drefs, J; Guiraud, E; Lucke, J",,,,"Drefs, Jakob; Guiraud, Enrico; Luecke, Joerg",,,Evolutionary Variational Optimization of Generative Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We combine two popular optimization approaches to derive learning algorithms for generative models: variational optimization and evolutionary algorithms. The combination is realized for generative models with discrete latents by using truncated posteriors as the family of variational distributions. The variational parameters of truncated posteriors are sets of latent states. By interpreting these states as genomes of individuals and by using the variational lower bound to define a fitness, we can apply evolutionary algorithms to realize the variational loop. The used variational distributions are very flexible and we show that evolutionary algorithms can effectively and efficiently optimize the variational bound. Furthermore, the variational loop is generally applicable (black box) with no analytical derivations required. To show general applicability, we apply the approach to three generative models (we use Noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse Coding). To demonstrate effectiveness and efficiency of the novel variational approach, we use the standard competitive benchmarks of image denoising and inpainting. The benchmarks allow quantitative comparisons to a wide range of methods including probabilistic approaches, deep deterministic and generative networks, and non-local image processing methods. In the category of zero-shot learning (when only the corrupted image is used for training), we observed the evolutionary variational algorithm to significantly improve the state-of-the-art in many benchmark settings. For one well-known inpainting benchmark, we also observed state-of-the-art performance across all categories of algorithms although we only train on the corrupted image. In general, our investigations highlight the importance of research on optimization methods for generative models to achieve performance improvements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752326600001,0
J,"Arachie, C; Huang, B",,,,"Arachie, Chidubem; Huang, Bert",,,A General Framework for Adversarial Label Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider the task of training classifiers without fully labeled data. We propose a weakly supervised method-adversarial label learning-that trains classifiers to perform well when noisy and possibly correlated labels are provided. Our framework allows users to provide different weak labels and multiple constraints on these labels. Our model then attempts to learn parameters for the data by solving a zero-sum game for the binary problems and a non-zero sum game optimization for multi-class problems. The game is between an adversary that chooses labels for the data and a model that minimizes the error made by the adversarial labels. The weak supervision constrains what labels the adversary can choose. The method therefore minimizes an upper bound of the classifier's error rate using projected primal-dual subgradient descent. Minimizing this bound protects against bias and dependencies in the weak supervision. We first show the performance of our framework on binary classification tasks then we extend our algorithm to show its performance on multiclass datasets. Our experiments show that our method can train without labels and outperforms other approaches for weakly supervised learning.,,,,,"Huang, Bert/E-2576-2016","Huang, Bert/0000-0002-8548-7246",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,33,118,,,,,,,,,,,,,,,WOS:000663171200001,0
J,"Blanca, A; Chen, ZC; Stefankovic, D; Vigoda, E",,,,"Blanca, Antonio; Chen, Zongchen; Stefankovic, Daniel; Vigoda, Eric",,,Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the identity testing problem for restricted Boltzmann machines (RBMs), and more generally, for undirected graphical models. In this problem, given sample access to the Gibbs distribution corresponding to an unknown or hidden model M* and given an explicit model M, the goal is to distinguish if either M = M* or if the models are (statistically) far apart. We establish the computational hardness of identity testing for RBMs (i.e., mixed Ising models on bipartite graphs), even when there are no latent variables or an external field. Specifically, we show that unless RP = NP, there is no polynomial-time identity testing algorithm for RBMs when beta d = omega(log n), where d is the maximum degree of the visible graph and beta is the largest edge weight (in absolute value); when beta d = O(log n) there is an efficient identity testing algorithm that utilizes the structure learning algorithm of Klivans and Meka (2017). We prove similar lower bounds for purely ferromagnetic RBMs with inconsistent external fields and for the ferromagnetic Potts model. To prove our results, we introduce a novel methodology to reduce the corresponding approximate counting problem to testing utilizing the phase transition exhibited by these models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700314900001,0
J,"Castera, C; Bolte, J; Fevotte, C; Pauwels, E",,,,"Castera, Camille; Bolte, Jerome; Fevotte, Cedric; Pauwels, Edouard",,,An Inertial Newton Algorithm for Deep Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new second-order inertial optimization method for machine learning called INNA. It exploits the geometry of the loss function while only requiring stochastic ap-proximations of the function values and the generalized gradients. This makes INNA fully implementable and adapted to large-scale optimization problems such as the training of deep neural networks. The algorithm combines both gradient-descent and Newton-like behaviors as well as inertia. We prove the convergence of INNA for most deep learning problems. To do so, we provide a well-suited framework to analyze deep learning loss functions involving tame optimization in which we study a continuous dynamical system together with its discrete stochastic approximations. We prove sublinear convergence for the continuous-time differential inclusion which underlies our algorithm. Additionally, we also show how standard optimization mini-batch methods applied to non-smooth non-convex problems can yield a certain type of spurious stationary points never discussed before. We address this issue by providing a theoretical framework around the new idea of D-criticality; we then give a simple asymptotic analysis of INNA. Our algorithm allows for using an aggressive learning rate of o(1/ log k). From an empirical viewpoint, we show that INNA returns competitive results with respect to state of the art (stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687130800001,0
J,"Hang, HY; Lin, ZC; Liu, XY; Wen, HW",,,,"Hang, Hanyuan; Lin, Zhouchen; Liu, Xiaoyu; Wen, Hongwei",,,Histogram Transform Ensembles for Large-scale Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a novel algorithm for large-scale regression problems named Histogram Transform Ensembles (HTE), composed of random rotations, stretchings, and translations. Our HTE method first implements a histogram transformed partition to the random affine mapped data, then adaptively leverages constant functions or SVMs to obtain the individual regression estimates, and eventually builds the ensemble predictor through an average strategy. First of all, in this paper, we investigate the theoretical properties of HTE when the regression function lies in the Holder space C-k,C-alpha, k is an element of N-0, alpha is an element of (0, 1]. In the case that k = 0, 1, we adopt the constant regressors and develop the naive his-togram transforms (NHT). Within the space C-0,C-alpha although almost optimal convergence rates can be derived for both single and ensemble NHT, we fail to show the benefits of ensembles over single estimators theoretically. In contrast, in the subspace C-1,C-alpha, we prove that if d >= 2(1 + alpha)/alpha, the lower bound of the convergence rates for single NHT turns out to be worse than the upper bound of the convergence rates for ensemble NHT. In the other case when k >= 2, the NHT may no longer be appropriate in predicting smoother regression functions. Instead, we circumvent this issue by applying kernel histogram transforms (KHT) equipped with smoother regressors, such as support vector machines (SVMs). Accordingly, it turns out that both single and ensemble KHT enjoy almost optimal convergence rates. Then, we validate the above theoretical results with extensive numerical experiments. On the one hand, simulations are conducted to elucidate that ensemble NHT outperforms single NHT. On the other hand, the effects of bin sizes on the accuracy of both NHT and KHT are also in accord with the theoretical analysis. Last but not least, in the real-data experiments, comparisons between the ensemble KHT, equipped with adaptive histogram transforms, and other state-of-the-art large-scale regression estimators verify the effectiveness and precision of the proposed algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,95,,,,,,,,,,,,,,,WOS:000663148600001,0
J,"Liu, FH; Shi, L; Huang, XL; Yang, J; Suykens, JAK",,,,"Liu, Fanghui; Shi, Lei; Huang, Xiaolin; Yang, Jie; Suykens, Johan A. K.",,,Generalization Properties of hyper-RKHS and its Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper generalizes regularized regression problems in a hyper-reproducing kernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning and out-of-sample extensions, and proves asymptotic convergence results for the introduced regression models in an approximation theory view. Algorithmically, we consider two regularized regression models with bivariate forms in this space, including kernel ridge regression (KRR) and support vector regression (SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with Nystrum approximation for scalability in large sample cases. This framework is general: the underlying kernel is learned from a broad class, and can be positive definite or not, which adapts to various requirements in kernel learning. Theoretically, we study the convergence behavior of regularized regression algorithms in hyper-RKHS and derive the learning rates, which goes beyond the classical analysis on RKHS due to the non-trivial independence of pairwise samples and the characterisation of hyper-RKHS. Experimentally, results on several benchmarks suggest that the employed framework is able to learn a general kernel function form an arbitrary similarity matrix, and thus achieves a satisfactory performance on classification tasks.",,,,,"Shi, Lei/P-1989-2018; Suykens, Johan/C-9781-2014","Shi, Lei/0000-0002-9512-5273; Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700312700001,0
J,"Sanz-Serna, JM; Zygalakis, KC",,,,"Maria Sanz-Serna, Jesus; Zygalakis, Konstantinos C.",,,Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a framework that allows for the non-asymptotic study of the 2-Wasserstein distance between the invariant distribution of an ergodic stochastic differential equation and the distribution of its numerical approximation in the strongly log-concave case. This allows us to study in a unified way a number of different integrators proposed in the literature for the overdamped and under damped Langevin dynamics. In addition, we analyse a novel splitting method for the underdamped Langevin dynamics which only requires one gradient evaluation per time step. Under an additional ber kappa, the algorithm is shown to produce with an O(kappa(5/4)d(1/4)epsilon(-1/2)) complexity samples from a smoothness assumption on a d-dimensional strongly log-concave distribution with condition numdistribution that, in Wasserstein distance, is at most epsilon > 0 away from the target distribution.",,,,,,"Sanz-Serna, JM/0000-0001-7284-0971",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706442400001,0
J,"Park, G; Moon, SJ; Park, S; Jeon, JJ",,,,"Park, Gunwoong; Moon, Sang Jun; Park, Sion; Jeon, Jong-June",,,Learning a High-dimensional Linear Structural Equation Model via l(1)-Regularized Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops a new approach to learning high-dimensional linear structural equation models (SEMs) without the commonly assumed faithfulness, Gaussian error distribution, and equal error distribution conditions. A key component of the algorithm is componentwise ordering and parent estimations, where both problems can be efficiently addressed using l(1)-regularized regression. This paper proves that sample sizes n = Omega(d(2) log p) and n = Omega(d(2)p(2/m)) are sufficient for the proposed algorithm to recover linear SEMs with subGaussian and (4m)-th bounded-moment error distributions, respectively, where p is the number of nodes and d is the maximum degree of the moralized graph. Further shown is the worst-case computational complexity O(n(p(3) + p(2d2))), and hence, the proposed algorithm is statistically consistent and computationally feasible for learning a high-dimensional linear SEM when its moralized graph is sparse. Through simulations, we verify that the proposed algorithm is statistically consistent and computationally feasible, and it performs well compared to the state-of-the-art US, GDS, LISTEN and TD algorithms with our settings. We also demonstrate through real COVID-19 data that the proposed algorithm is well-suited to estimating a virus-spread map in China.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,102,,,,,,,,,,,,,,,WOS:000663155400001,0
J,"Shi, CC; Luo, SK; Zhu, HT; Song, R",,,,"Shi, Chengchun; Luo, Shikai; Zhu, Hongtu; Song, Rui",,,An Online Sequential Test for Qualitative Treatment Effects,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tech companies (e.g., Google or Facebook) often use randomized online experiments and/or A/B testing primarily based on the average treatment effects to compare their new product with an old one. However, it is also critically important to detect qualitative treatment effects such that the new one may significantly outperform the existing one only under some specific circumstances. The aim of this paper is to develop a powerful testing procedure to efficiently detect such qualitative treatment effects. We propose a scalable online updating algorithm to implement our test procedure. It has three novelties including adaptive randomization, sequential monitoring, and online updating with guaranteed type-I error control. We also thoroughly examine the theoretical properties of our testing procedure including the limiting distribution of test statistics and the justification of an efficient bootstrap method. Extensive empirical studies are conducted to examine the finite sample performance of our test procedure.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000726695300001,0
J,"Stemmer, U",,,,"Stemmer, Uri",,,Locally Private k-Means Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We design a new algorithm for the Euclidean k-means problem that operates in the local model of differential privacy. Unlike in the non-private literature, differentially private algorithms for the k-means objective incur both additive and multiplicative errors. Our algorithm significantly reduces the additive error while keeping the multiplicative error the same as in previous state-of-the-art results. Specifically, on a database of size n, our algorithm guarantees O (1) multiplicative error and approximate to n(1/2+a) additive error for an arbitrarily small constant a > 0. All previous algorithms in the local model had additive error approximate to n(2/3+a). Our techniques extend to k-median clustering. We show that the additive error we obtain is almost optimal in terms of its dependency on the database size n. Specifically, we give a simple lower bound showing that every locally-private algorithm for the k-means objective must have additive error at least approximate to root n.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687198800001,0
J,"Wang, F; Zhou, L; Tang, L; Song, PXK",,,,"Wang, Fei; Zhou, Ling; Tang, Lu; Song, Peter X. K.",,,Method of Contraction-Expansion (MOCE) for Simultaneous Inference in Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Simultaneous inference after model selection is of critical importance to address scientific hypotheses involving a set of parameters. In this paper, we consider a high-dimensional linear regression model in which a regularization procedure such as LASSO is applied to yield a sparse model. To establish a simultaneous post-model selection inference, we propose a method of contraction and expansion (MOCE) along the line of debiasing estimation in that we investigate a desirable trade-off between model selection variability and sample variability by the means of forward screening. We establish key theoretical results for the inference from the proposed MOCE procedure. Once the expanded model is properly selected, the theoretical guarantees and simultaneous confidence regions can be constructed by the joint asymptotic normal distribution. In comparison with existing methods, our proposed method exhibits stable and reliable coverage at a nominal significance level and enjoys substantially less computational burden. Thus, our MOCE approach is trustworthy in solving real-world problems.",,,,,,"Tang, Lu/0000-0001-6143-9314",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706444400001,0
J,"Anil, R; Capan, G; Drost-Fromm, I; Dunning, T; Friedman, E; Grant, T; Quinn, S; Ranjan, P; Schelter, S; Yilmazel, O",,,,"Anil, Robin; Capan, Gokhan; Drost-Fromm, Isabel; Dunning, Ted; Friedman, Ellen; Grant, Trevor; Quinn, Shannon; Ranjan, Paritosh; Schelter, Sebastian; Yilmazel, Ozgur",,,Apache Mahout: Machine Learning on Distributed Dataflow Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"APACHE MAHOUT is a library for scalable machine learning (ML) on distributed dataflow systems, offering various implementations of classification, clustering, dimensionality reduction and recommendation algorithms. Mahout was a pioneer in large-scale machine learning in 2008, when it started and targeted MapReduce, which was the predominant abstraction for scalable computing in industry at that time. Mahout has been widely used by leading web companies and is part of several commercial cloud offerings. In recent years, Mahout migrated to a general framework enabling a mix of dataflow programming and linear algebraic computations on backends such as APACHE SPARK and APACHE FLINK. This design allows users to execute data preprocessing and model training in a single, unified dataflow system, instead of requiring a complex integration of several specialized systems. Mahout is maintained as a community-driven open source project at the Apache Software Foundation, and is available under https://mahout.apache.org.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,127,,,,,,,,,,,,,,,WOS:000556295000001,0
J,"Elman, MR; Minnier, J; Chang, XH; Choi, D",,,,"Elman, Miriam R.; Minnier, Jessica; Chang, Xiaohui; Choi, Dongseok",,,Noise Accumulation in High Dimensional Classification and Total Signal Index,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Great attention has been paid to Big Data in recent years. Such data hold promise for scientific discoveries but also pose challenges to analyses. One potential challenge is noise accumulation. In this paper, we explore noise accumulation in high dimensional two-group classification. First, we revisit a previous assessment of noise accumulation with principal component analyses, which yields a different threshold for discriminative ability than originally identified. Then we extend our scope to its impact on classifiers developed with three common machine learning approaches- random forest, support vector machine, and boosted classification trees. We simulate four scenarios with differing amounts of signal strength to evaluate each method. After determining noise accumulation may affect the performance of these classifiers, we assess factors that impact it. We conduct simulations by varying sample size, signal strength, signal strength proportional to the number predictors, and signal magnitude with random forest classifiers. These simulations suggest that noise accumulation affects the discriminative ability of high-dimensional classifiers developed using common machine learning methods, which can be modified by sample size, signal strength, and signal magnitude. We developed the measure total signal index (TSI) to track the trends of total signal and noise accumulation.",,,,,"Minnier, Jessica/I-8788-2019","Minnier, Jessica/0000-0002-3527-2757",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000011,0
J,"Gonon, L; Grigoryeva, L; Ortega, JP",,,,"Gonon, Lukas; Grigoryeva, Lyudmila; Ortega, Juan-Pablo",,,Risk Bounds for Reservoir Computing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the practices of reservoir computing in the framework of statistical learning theory. In particular, we derive finite sample upper bounds for the generalization error committed by specific families of reservoir computing systems when processing discrete-time inputs under various hypotheses on their dependence structure. Non-asymptotic bounds are explicitly written down in terms of the multivariate Rademacher complexities of the reservoir systems and the weak dependence structure of the signals that are being handled. This allows, in particular, to determine the minimal number of observations needed in order to guarantee a prescribed estimation accuracy with high probability for a given reservoir family. At the same time, the asymptotic behavior of the devised bounds guarantees the consistency of the empirical risk minimization procedure for various hypothesis classes of reservoir functionals.",,,,,,"Gonon, Lukas/0000-0003-3367-2455",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,240,,,,,,,,,,,,,,,WOS:000608911700001,0
J,"He, BH; Liu, YY; Wu, YS; Yin, GS; Zhao, XQ",,,,"He, Baihua; Liu, Yanyan; Wu, Yuanshan; Yin, Guosheng; Zhao, Xingqiu",,,Functional Martingale Residual Process for High-Dimensional Cox Regression with Model Averaging,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularization methods for the Cox proportional hazards regression with high-dimensional survival data have been studied extensively in the literature. However, if the model is mis-specified, this would result in misleading statistical inference and prediction. To enhance the prediction accuracy for the relative risk and the survival probability, we propose three model averaging approaches for the high-dimensional Cox proportional hazards regression. Based on the martingale residual process, we define the delete-one cross-validation (CV) process, and further propose three novel CV functionals, including the end-time CV, integrated CV, and supremum CV, to achieve more accurate prediction for the risk quantities of clinical interest. The optimal weights for candidate models, without the constraint of summing up to one, can be obtained by minimizing these functionals, respectively. The proposed model averaging approach can attain the lowest possible prediction loss asymptotically. Furthermore, we develop a greedy model averaging algorithm to overcome the computational obstacle when the dimension is high. The performances of the proposed model averaging procedures are evaluated via extensive simulation studies, demonstrating that our methods achieve superior prediction accuracy over the existing regularization methods. As an illustration, we apply the proposed methods to the mantle cell lymphoma study.",,,,,"Yin, Guosheng/D-3214-2009","Yin, Guosheng/0000-0003-3276-1392",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,207,,,,,,,,,,,,,,,WOS:000590010100001,0
J,"Kazemi, SM; Goel, R; Jain, K; Kobyzev, I; Sethi, A; Forsyth, P; Poupart, P",,,,"Kazemi, Seyed Mehran; Goel, Rishab; Jain, Kshitij; Kobyzev, Ivan; Sethi, Akshay; Forsyth, Peter; Poupart, Pascal",,,Representation Learning for Dynamic Graphs: A Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000022,0
J,"Lin, QH; Nadarajah, S; Soheili, N; Yang, TB",,,,"Lin, Qihang; Nadarajah, Selvaprabu; Soheili, Negar; Yang, Tianbao",,,A Data Efficient and Feasible Level Set Method for Stochastic Convex Optimization with Expectation Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic convex optimization problems with expectation constraints (SOECs) are encountered in statistics and machine learning, business, and engineering. The SOEC objective and constraints contain expectations defined with respect to complex distributions or large data sets, leading to high computational complexity when solved by the algorithms that use exact functions and their gradients. Recent stochastic first order methods exhibit low computational complexity when handling SOECs but guarantee near-feasibility and near-optimality only at convergence. These methods may thus return highly infeasible solutions when heuristically terminated, as is often the case, due to theoretical convergence criteria being highly conservative. This issue limits the use of first order methods in several applications where the SOEC constraints encode implementation requirements. We design a stochastic feasible level set method (SFLS) for SOECs that has low complexity and emphasizes feasibility before convergence. Specifically, our level-set method solves a root-finding problem by calling a novel first order oracle that computes a stochastic upper bound on the level-set function by extending mirror descent and online validation techniques. We establish that SFLS maintains a high-probability feasible solution at each root-finding iteration and exhibits favorable complexity compared to state-of-the-art deterministic feasible level set and stochastic subgradient methods. Numerical experiments on three diverse applications highlight how SFLS finds feasible solutions with small optimality gaps with lower complexity than the former approaches.",,,,,"Nadarajah, Selvaprabu/ABG-8391-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,143,,,,,,,,,,,,,,,WOS:000558805000001,0
J,"Lyu, H; Needell, D; Balzano, L",,,,"Lyu, Hanbaek; Needell, Deanna; Balzano, Laura",,,Online matrix factorization for Markovian data and applications to Network Dictionary Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Online Matrix Factorization (OMF) is a fundamental tool for dictionary learning problems, giving an approximate representation of complex data sets in terms of a reduced number of extracted features. Convergence guarantees for most of the OMF algorithms in the literature assume independence between data matrices, and the case of dependent data streams remains largely unexplored. In this paper, we show that a non-convex generalization of the well-known OMF algorithm for i.i.d. stream of data in (Mairal et al., 2010) converges almost surely to the set of critical points of the expected loss function, even when the data matrices are functions of some underlying Markov chain satisfying a mild mixing condition. This allows one to extract features more efficiently from dependent data streams, as there is no need to subsample the data sequence to approximately satisfy the independence assumption. As the main application, by combining online non-negative matrix factorization and a recent MCMC algorithm for sampling motifs from networks, we propose a novel framework of Network Dictionary Learning, which extracts network dictionary patches from a given network in an online manner that encodes main features of the network. We demonstrate this technique and its application to network denoising problems on real-world network",,,,,,"Balzano, Laura/0000-0003-2914-123X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,251,,,,,,,,,,,,,,,WOS:000608920100001,0
J,"Ma, F; Meng, DY; Dong, XY; Yang, Y",,,,"Ma, Fan; Meng, Deyu; Dong, Xuanyi; Yang, Yi",,,Self-paced Multi-view Co-training,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a draw without replacement strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.",,,,,"yang, yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; yang, yang/GVT-5210-2022; Yang, Yi/B-9273-2017; Dong, Xuanyi/Q-5434-2019; yang, yang/GWB-9426-2022","Yang, Yi/0000-0002-0512-880X; Dong, Xuanyi/0000-0001-9272-1590; ",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000009,0
J,"Nakada, R; Imaizumi, M",,,,"Nakada, Ryumei; Imaizumi, Masaaki",,,Adaptive Approximation and Generalization of Deep Neural Network with Intrinsic Dimensionality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this study, we prove that an intrinsic low dimensionality of covariates is the main factor that determines the performance of deep neural networks (DNNs). DNNs generally provide outstanding empirical performance. Hence, numerous studies have actively investigated the theoretical properties of DNNs to understand their underlying mechanisms. In particular, the behavior of DNNs in terms of high-dimensional data is one of the most critical questions. However, this issue has not been sufficiently investigated from the aspect of covariates, although high-dimensional data have practically low intrinsic dimensionality. In this study, we derive bounds for an approximation error and a generalization error regarding DNNs with intrinsically low dimensional covariates. We apply the notion of the Minkowski dimension and develop a novel proof technique. Consequently, we show that convergence rates of the errors by DNNs do not depend on the nominal high dimensionality of data, but on its lower intrinsic dimension. We further prove that the rate is optimal in the minimax sense. We identify an advantage of DNNs by showing that DNNs can handle a broader class of intrinsic low dimensional data than other adaptive estimators. Finally, we conduct a numerical simulation to validate the theoretical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,174,,,,,,,,,,,,,,,WOS:000570206800001,0
J,"Yu, M; Gupta, V; Kolar, M",,,,"Yu, Ming; Gupta, Varun; Kolar, Mladen",,,Estimation of a Low-rank Topic-Based Model for Information Cascades,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000023,0
J,"Alaya, MZ; Klopp, O",,,,"Alaya, Mokhtar Z.; Klopp, Olga",,,Collective Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Matrix completion aims to reconstruct a data matrix based on observations of a small number of its entries. Usually in matrix completion a single matrix is considered, which can be, for example, a rating matrix in recommendation system. However, in practical situations, data is often obtained from multiple sources which results in a collection of matrices rather than a single one. In this work, we consider the problem of collective matrix completion with multiple and heterogeneous matrices, which can be count, binary, continuous, etc. We first investigate the setting where, for each source, the matrix entries are sampled from an exponential family distribution. Then, we relax the assumption of exponential family distribution for the noise. In this setting, we do not assume any specific model for the observations. The estimation procedures are based on minimizing the sum of a goodness-of-fit term and the nuclear norm penalization of the whole collective matrix. We prove that the proposed estimators achieve fast rates of convergence under the two considered settings and we corroborate our results with numerical experiments.",,,,,"Alaya, Mokhtar Z./AAM-9242-2021","Alaya, Mokhtar Z./0000-0002-1103-6944",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,148,,,,,,,,,,,,,,,WOS:000491132200012,0
J,"Csikos, M; Mustafa, NH; Kupayskii, A",,,,"Csikos, Monika; Mustafa, Nabil H.; Kupayskii, Audrey",,,Tight Lower Bounds on the VC-dimension of Geometric Set Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The VC-dimension of a set system is a way to capture its complexity and has been a key parameter studied extensively in machine learning and geometry communities. In this paper, we resolve two longstanding open problems on bounding the VC-dimension of two fundamental set systems: k-fold unions/intersections of half-spaces and the simplices set system. Among other implications, it settles an open question in machine learning that was first studied in the foundational paper of Blumer et al. (1989) as well as by Eisenstat and Angluin (2007) and Johnson (2008).",,,,,,"Kupavskii, Andrey/0000-0002-8313-9598",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,81,,,,,,,,,,,,,,,WOS:000467897300001,0
J,"Erdogdu, MA; Bayati, M; Dicker, LH",,,,"Erdogdu, Murat A.; Bayati, Mohsen; Dicker, Lee H.",,,Scalable Approximations for Generalized Linear Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In stochastic optimization, the population risk is generally approximated by the empirical risk which is in turn minimized by an iterative algorithm. However, in the large-scale setting, empirical risk minimization may be computationally restrictive. In this paper, we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models. We focus on large-scale problems where the iterative minimization of the empirical risk is computationally intractable, i.e., the number of observations n is much larger than the dimension of the parameter p (n >> p >> 1). We show that under random sub-Gaussian design, the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares (OLS) estimator. Using this relation, we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a quadratic convergence rate, and that are computationally cheaper than any batch optimization algorithm by at least a factor of O (p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm on well-known classification and regression problems, through extensive numerical studies on large-scale datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.",,,,,,"Bayati, Mohsen/0000-0002-7280-912X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,7,,,,,,,,,,,,,,,WOS:000458663200001,0
J,"Fan, YY; Demirkaya, E; Lv, JC",,,,"Fan, Yingying; Demirkaya, Emre; Lv, Jinchi",,,Nonuniformity of P-values Can Occur Early in Diverging Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Evaluating the joint significance of covariates is of fundamental importance in a wide range of applications. To this end, p-values are frequently employed and produced by algorithms that are powered by classical large-sample asymptotic theory. It is well known that the conventional p-values in Gaussian linear model are valid even when the dimensionality is a non-vanishing fraction of the sample size, but can break down when the design matrix becomes singular in higher dimensions or when the error distribution deviates from Gaussianity. A natural question is when the conventional p-values in generalized linear models become invalid in diverging dimensions. We establish that such a breakdown can occur early in nonlinear models. Our theoretical characterizations are confirmed by simulation studies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,77,,,,,,,,,,32190012,,,,,WOS:000467896200001,0
J,"Ferrer, L; McLaren, M",,,,"Ferrer, Luciana; McLaren, Mitchell",,,Joint PLDA for Simultaneous Modeling of Two Factors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic linear discriminant analysis (PLDA) is a method used for biometric problems like speaker or face recognition that models the variability of the samples using two latent variables, one that depends on the class of the sample and another one that is assumed independent across samples and models the within-class variability. In this work, we propose a generalization of PLDA that enables joint modeling of two sample-dependent factors: the class of interest and a nuisance condition. The approach does not change the basic form of PLDA but rather modifies the training procedure to consider the dependency across samples of the latent variable that models within-class variability. While the identity of the nuisance condition is needed during training, it is not needed during testing since we propose a scoring procedure that marginalizes over the corresponding latent variable. We show results on a multilingual speaker-verification task, where the language spoken is considered a nuisance condition. The proposed joint PLDA approach leads to significant performance gains in this task for two different data sets, in particular when the training data contains mostly or only monolingual speakers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,24,,,,,,,,,,,,,,,WOS:000458668500001,0
J,"Huttenrauch, M; Sosic, A; Neumann, G",,,,"Huettenrauch, Maximilian; Sosic, Adrian; Neumann, Gerhard",,,Deep Reinforcement Learning for Swarm Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, the observation vector for decentralized decision making is represented by a concatenation of the (local) information an agent gathers about other agents. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions, where we treat the agents as samples and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and neural networks trained end-to-end. We evaluate the representation on two well-known problems from the swarm literature - rendezvous and pursuit evasion in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents, facilitating the development of complex collective strategies.",,,,,,"Sosic, Adrian/0000-0003-2845-6635",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,54,,,,,,,,,,,,,,,WOS:000463324000001,0
J,"Zafar, MB; Valera, I; Gomez-Rodriguez, M; Gummadi, KP",,,,"Zafar, Muhammad Bilal; Valera, Isabel; Gomez-Rodriguez, Manuel; Gummadi, Krishna P.",,,Fairness Constraints: A Flexible Approach for Fair Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithmic decision making is employed in an increasing number of real-world applications to aid human decision making. While it has shown considerable promise in terms of improved decision accuracy, in some scenarios, its outcomes have been also shown to impose an unfair (dis) advantage on people from certain social groups (e.g., women, blacks). In this context, there is a need for computational techniques to limit unfairness in algorithmic decision making. In this work, we take a step forward to ful fi ll that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classi fi ers. The main technical innovation of our framework is a general and intuitive measure of decision boundary unfairness, which serves as a tractable proxy to several of the most popular computational de fi nitions of unfairness from the literature. Leveraging our measure, we can reduce the design of fair margin-based classi fi ers to adding tractable constraints on their decision boundaries. Experiments on multiple synthetic and real-world datasets show that our framework is able to successfully limit unfairness, often at a small cost in terms of accuracy.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,75,,,,,,,,,,,,,,,WOS:000467895700001,0
J,"Burns, DM; Whyne, CM",,,,"Burns, David M.; Whyne, Cari M.",,,Seglearn: A Python Package for Learning Sequences and Time Series,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"seglearn is an open-source Python package for performing machine learning on time series or sequences. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. Sequences and series may be learned directly with deep learning models or via feature representation with classical machine learning estimators. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage. Source code and documentation can be downloaded from https://github.com/dmbee/seglearn.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454481100001,0
J,"Li, XG; Zhao, T; Arora, R; Liu, H; Hong, MY",,,,"Li, Xingguo; Zhao, Tuo; Arora, Raman; Liu, Han; Hong, Mingyi",,,On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The cyclic block coordinate descent-type (CBCD-type) methods, which perform iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of O(p log(1/epsilon)), where E is a pre-specified accuracy of the objective value, and p is the number of blocks. However, such iteration complexity explicitly depends on p, and therefore is at least p times worse than the complexity O(log(1/epsilon)) of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity O(log(2) (p).log(1/epsilon)) of the CBCD-type methods matches that of the GD methods in term of dependency on p, up to a log(2)p factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of p/log(2)(p). We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a log(2)(p) factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with p. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones.",,,,,"Liu, Han/P-7105-2018; Hong, Mingyi/H-6274-2013",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,184,,,,,,,,,,,,,,,WOS:000435449200001,0
J,"Wen, ZY; Shi, JS; Li, QB; He, BS; Chen, J",,,,"Wen, Zeyi; Shi, Jiashuai; Li, Qinbin; He, Bingsheng; Chen, Jian",,,ThunderSVM: A Fast SVM Library on GPUs and CPUs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support Vector Machines (SVMs) are classic supervised learning models for classification, regression and distribution estimation. A survey conducted by Kaggle in 2017 shows that 26% of the data mining and machine learning practitioners are users of SVMs. However, SVM training and prediction are very expensive computationally for large and complex problems. This paper presents an efficient and open source SVM software toolkit called ThunderSVM which exploits the high-performance of Graphics Processing Units (GPUs) and multi-core CPUs. ThunderSVM supports all the functionalities-including classification (SVC), regression (SVR) and one-class SVMs-of LibSVM and uses identical command line options, such that existing LibSVM users can easily apply our toolkit. ThunderSVM can be used through multiple language interfaces including C/C++, Python, R and MATLAB. Our experimental results show that ThunderSVM is generally an order of magnitude faster than LibSVM while producing identical SVMs. In addition to the high efficiency, we design our convex optimization solver in a general way such that SVC, SVR, and one-class SVMs share the same solver for the ease of maintenance. Documentation, examples, and more about ThunderSVM are available at https ://github. com/zeyiwen/thundersvm .",,,,,,"Wen, Zeyi/0000-0003-3370-6053; Li, Qinbin/0000-0002-6539-6443; He, Bingsheng/0000-0001-8618-4581",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,21,,,,,,,,,,,,,,,WOS:000443228400001,0
J,"Benavoli, A; Corani, G; Demsar, J; Zaffalon, M",,,,"Benavoli, Alessio; Corani, Giorgio; Demsar, Janez; Zaffalon, Marco",,,Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better more sound and useful alternatives for it.",,,,,"Dem≈°ar, Janez/AAS-2762-2020; Zaffalon, Marco/M-7035-2017","Zaffalon, Marco/0000-0001-8908-1502; corani, giorgio/0000-0002-1541-8384; benavoli, alessio/0000-0002-2522-7178",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,77,,,,,,,,,,,,,,,WOS:000412061400001,0
J,"Liu, WW; Tsang, IW",,,,"Liu, Weiwei; Tsang, Ivor W.",,,Making Decision Trees Feasible in Ultrahigh Feature and Label Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data-dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions.",,,,,,"Tsang, Ivor/0000-0001-8095-4637",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,81,,,,,,,,,,,,,,,WOS:000412062300001,0
J,"Takenouchi, T; Kanamori, T",,,,"Takenouchi, Takashi; Kanamori, Takafumi",,,Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we focus on parameters estimation of probabilistic models in discrete space. A naive calculation of the normalization constant of the probabilistic model on discrete space is often infeasible and statistical inference based on such probabilistic models has di ffi culty. In this paper, we propose a novel estimator for probabilistic models on discrete space, which is derived from an empirically localized homogeneous divergence. The idea of the empirical localization makes it possible to ignore an unobserved domain on sample space, and the homogeneous divergence is a discrepancy measure between two positive measures and has a weak coincidence axiom. The proposed estimator can be constructed without calculating the normalization constant and is asymptotically consistent and Fisher e ffi cient. We investigate statistical properties of the proposed estimator and reveal a relationship between the empirically localized homogeneous divergence and a mixture of the ff - divergence. The ff - divergence is a non- homogeneous discrepancy measure that is frequently discussed in the context of information geometry. Using the relationship, we also propose an asymptotically consistent estimator of the normalization constant. Experiments showed that the proposed estimator comparably performs to the maximum likelihood estimator but with drastically lower computational cost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,26,,,,,,,,,,,,,,,,WOS:000405991100001,0
J,"Alquier, P; Ridgway, J; Chopin, N",,,,"Alquier, Pierre; Ridgway, James; Chopin, Nicolas",,,On the properties of variational approximations of Gibbs posteriors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately often intractable. One may sample from it using Markov chain Monte Carlo, but this is usually too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. In addition, we show that, when the risk function is convex, a variational approximation can be obtained in polynomial time using a convex solver. We give finite sample oracle inequalities for the corresponding estimator. We specialize our results to several learning tasks (classification, ranking, matrix completion), discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.",,,,,,"Alquier, Pierre/0000-0003-4249-7337; Chopin, Nicolas/0000-0002-0628-5815",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,239,,,,,,,,,,,,,,,WOS:000391920700001,0
J,"Erdogdu, MA",,,,"Erdogdu, Murat A.",,,Newton-Stein Method: An Optimization Method for GLMs via Stein's Lemma,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (n >> p >> 1). In this regime, optimization algorithms can immensely bene fit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the general case where the rows of the design matrix are samples from a sub-Gaussian distribution. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various optimization algorithms on several data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,216,,,,,,,,,,,,,,,WOS:000391837400001,0
J,"Fogel, F; d'Aspremont, A; Vojnovic, M",,,,"Fogel, Fajwel; d'Aspremont, Alexandra; Vojnovic, Milan",,,Spectral Ranking using Seriation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a seriation algorithm for ranking a set of items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than classical scoring methods. Finally, we bound the ranking error when only a random subset of the comparions are observed. An additional bene fit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,45,88,,,,,,,,,,,,,,,WOS:000391531700001,0
J,"Hanneke, S",,,,"Hanneke, Steve",,,The Optimal Sample Complexity of PAC Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,38,,,,,,,,,,,,,,,WOS:000391481900001,0
J,"Ivanoff, S; Picard, F; Rivoirard, V",,,,"Ivanoff, Stephane; Picard, Franck; Rivoirard, Vincent",,,Adaptive Lasso and group-Lasso for functional Poisson regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High dimensional Poisson regression has become a standard framework for the analysis of massive counts datasets. In this work we estimate the intensity function of the Poisson regression model by using a dictionary approach, which generalizes the classical basis approach, combined with a Lasso or a group-Lasso procedure. Selection depends on penalty weights that need to be calibrated. Standard methodologies developed in the Gaussian framework can not be directly applied to Poisson models due to heteroscedasticity. Here we provide data-driven weights for the Lasso and the group-Lasso derived from concentration inequalities adapted to the Poisson case. We show that the associated Lasso and group-Lasso procedures satisfy fast and slow oracle inequalities. Simulations are used to assess the empirical performance of our procedure, and an original application to the analysis of Next Generation Sequencing data is provided.",,,,,,"Picard, Franck/0000-0001-8084-5481",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,55,,,,,,,,,,,,,,,WOS:000391488600001,0
J,"Leifert, G; Strauss, T; Gruning, T; Wustlich, W; Labahn, R",,,,"Leifert, Gundram; Strauss, Tobias; Gruening, Tobias; Wustlich, Welf; Labahn, Roger",,,Cells in Multidimensional Recurrent Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multidimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,97,,,,,,,,,,,,,,,WOS:000391540100001,0
J,"Pavlidis, NG; Hofmeyr, DP; Tasoulis, SK",,,,"Pavlidis, Nicos G.; Hofmeyr, David P.; Tasoulis, Sotiris K.",,,Minimum Density Hyperplanes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Associating distinct groups of objects (clusters) with contiguous regions of high probability density (high-density clusters), is central to many statistical and machine learning approaches to the classification of unlabelled data. We propose a novel hyperplane classifier for clustering and semi-supervised classification which is motivated by this objective. The proposed minimum density hyperplane minimises the integral of the empirical probability density function along it, thereby avoiding intersection with high density clusters. We show that the minimum density and the maximum margin hyperplanes are asymptotically equivalent, thus linking this approach to maximum margin clustering and semi-supervised support vector classifiers. We propose a projection pursuit formulation of the associated optimisation problem which allows us to find minimum density hyperplanes efficiently in practice, and evaluate its performance on a range of benchmark data sets. The proposed approach is found to be very competitive with state of the art methods for clustering and semi-supervised classification.",,,,,"Hofmeyr, David/AAC-4042-2021","Hofmeyr, David/0000-0003-3068-8128; Pavlidis, Nicos/0000-0002-0301-5350",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,156,,,,,,,,,,,,,,,WOS:000391665900001,0
J,"Rieck, K; Wressnegger, C",,,,"Rieck, Konrad; Wressnegger, Christian",,,Harry: A Tool for Measuring String Similarity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.",,,,,,"Rieck, Konrad/0000-0002-5054-8758",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,9,,,,,,,,,,,,,,,WOS:000391467200001,0
J,"Sojoudi, S",,,,"Sojoudi, Somayeh",,,Equivalence of Graphical Lasso and Thresholding for Sparse Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with the problem of finding a sparse graph capturing the conditional dependence between the entries of a Gaussian random vector, where the only available information is a sample correlation matrix. A popular approach to address this problem is the graphical lasso technique, which employs a sparsity-promoting regularization term. This paper derives a simple condition under which the computationally-expensive graphical lasso behaves the same as the simple heuristic method of thresholding. This condition depends only on the solution of graphical lasso and makes no direct use of the sample correlation matrix or the regularization coefficient. It is proved that this condition is always satisfied if the solution of graphical lasso is close to its first-order Taylor approximation or equivalently the regularization term is relatively large. This condition is tested on several random problems, and it is shown that graphical lasso and the thresholding method lead to highly similar results in the case where a sparse graph is sought. We also conduct two case studies on brain connectivity networks of twenty subjects based on fMRI data and the topology identification of electrical circuits to support the findings of this work on the similarity of graphical lasso and thresholding.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,115,,,,,,,,,,,,,,,WOS:000391551400001,0
J,"Su, CW; Borsuk, ME",,,,"Su, Chengwei; Borsuk, Mark E.",,,Improving Structure MCMC for Bayesian Networks through Markov Blanket Resampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithms for inferring the structure of Bayesian networks from data have become an increasingly popular method for uncovering the direct and indirect influences among variables in complex systems. A Bayesian approach to structure learning uses posterior probabilities to quantify the strength with which the data and prior knowledge jointly support each possible graph feature. Existing Markov Chain Monte Carlo (MCMC) algorithms for estimating these posterior probabilities are slow in mixing and convergence, especially for large networks. We present a novel Markov blanket resampling (MBR) scheme that intermittently reconstructs the Markov blanket of nodes, thus allowing the sampler to more effectively traverse low-probability regions between local maxima. As we can derive the complementary forward and backward directions of the MBR proposal distribution, the Metropolis-Hastings algorithm can be used to account for any asymmetries in these proposals. Experiments across a range of network sizes show that the MBR scheme outperforms other state-of-the-art algorithms, both in terms of learning performance and convergence rate. In particular, MBR achieves better learning performance than the other algorithms when the number of observations is relatively small and faster convergence when the number of variables in the network is large.",,,,,"Borsuk, Mark/AAV-1663-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,118,,,,,,,,,,,,,,,WOS:000391552500001,0
J,"Wu, C; Kwon, S; Shen, XT; Pan, W",,,,"Wu, Chong; Kwon, Sunghoon; Shen, Xiaotong; Pan, Wei",,,A New Algorithm and Theory for Penalized Regression-based Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering is unsupervised and exploratory in nature. Yet, it can be performed through penalized regression with grouping pursuit, as demonstrated in Pan et al. (2013). In this paper, we develop a more efficient algorithm for scalable computation and a new theory of clustering consistency for the method. This algorithm, called DC-ADMM, combines difference of convex (DC) programming with the alternating direction method of multipliers (ADMM). This algorithm is shown to be more computationally efficient than the quadratic penalty based algorithm of Pan et al. (2013) because of the former's closed-form updating formulas. Numerically, we compare the DC-ADMM algorithm with the quadratic penalty algorithm to demonstrate its utility and scalability. Theoretically, we establish a finitesample mis-clustering error bound for penalized regression based clustering with the L-0 constrained regularization in a general setting. On this ground, we provide conditions for clustering consistency of the penalized clustering method. As an end product, we put R package prclust implementing PRclust with various loss and grouping penalty functions available on GitHub and CRAN.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,25,188,,,,,,,,,,,,,,,WOS:000391825700001,0
J,"Yang, D; Ma, ZM; Buja, A",,,,"Yang, Dan; Ma, Zongming; Buja, Andreas",,,Rate Optimal Denoising of Simultaneously Sparse and Low Rank Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study minimax rates for denoising simultaneously sparse and low rank matrices in high dimensions. We show that an iterative thresholding algorithm achieves (near) optimal rates adaptively under mild conditions for a large class of loss functions. Numerical experiments on synthetic datasets also demonstrate the competitive performance of the proposed method.,,,,,"YANG, Dan/HHD-2733-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,92,,,,,,,,,,,,,,,WOS:000391538700001,0
J,"Zhao, AQ; Feng, Y; Wang, L; Tong, X",,,,"Zhao, Anqi; Feng, Yang; Wang, Lie; Tong, Xin",,,Neyman-Pearson Classification under High-Dimensional Settings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most existing binary classification methods target on the optimization of the overall classification risk and may fail to serve some real-world applications such as cancer diagnosis, where users are more concerned with the risk of misclassifying one specific class than the other. Neyman-Pearson (NP) paradigm was introduced in this context as a novel statistical framework for handling asymmetric type I/II error priorities. It seeks classifiers with a minimal type II error and a constrained type I error under a user specified level. This article is the first attempt to construct classifiers with guaranteed theoretical performance under the NP paradigm in high-dimensional settings. Based on the fundamental Neyman-Pearson Lemma, we used a plug-in approach to construct NP-type classifiers for Naive Bayes models. The proposed classifiers satisfy the NP oracle inequalities, which are natural NP paradigm counterparts of the oracle inequalities in classical binary classification. Besides their desirable theoretical properties, we also demonstrated their numerical advantages in prioritized error control via both simulation and real data studies.",,,,,"Feng, Yang/M-1994-2019; Feng, Yang/Y-7374-2019","Feng, Yang/0000-0001-7746-7598",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,39,212,,,,,,,,,,,,,,,WOS:000391835600001,0
J,"Thomann, P; Steinwart, I; Schmid, N",,,,"Thomann, Philipp; Steinwart, Ingo; Schmid, Nico",,,Towards an Axiomatic Approach to Hierarchical Clustering of Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose some axioms for hierarchical clustering of probability measures and investigate their ramifications. The basic idea is to let the user stipulate the clusters for some elementary measures. This is done without the need of any notion of metric, similarity or dissimilarity. Our main results then show that for each suitable choice of user-defined clustering on elementary measures we obtain a unique notion of clustering on a large set of distributions satisfying a set of additivity and continuity axioms. We illustrate the developed theory by numerous examples including some with and some without a density.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1949,2002,,,,,,,,,,,,,,,,WOS:000369887300010,0
J,"Bubenik, P",,,,"Bubenik, Peter",,,Statistical Topological Data Analysis using Persistence Landscapes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We define a new topological summary for data that we call the persistence landscape. Since this summary lies in a vector space, it is easy to combine with tools from statistics and machine learning, in contrast to the standard topological summaries. Viewed as a random variable with values in a Banach space, this summary obeys a strong law of large numbers and a central limit theorem. We show how a number of standard statistical tests can be used for statistical inference using this summary. We also prove that this summary is stable and that it can be used to provide lower bounds for the bottleneck and Wasserstein distances.",,,,,,"Bubenik, Peter/0000-0001-5262-2133",,,,,,,,,,,,,1532-4435,,,,,JAN,2015,16,,,,,,77,102,,,,,,,,,,,,,,,,WOS:000369885500003,0
J,"Chiang, KY; Hsieh, CJ; Natarajan, N; Dhillon, IS; Tewari, A",,,,"Chiang, Kai-Yang; Hsieh, Cho-Jui; Natarajan, Nagarajan; Dhillon, Inderjit S.; Tewari, Ambuj",,,Prediction and Clustering in Signed Networks: A Local to Global Perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The study of social networks is a burgeoning research area. However, most existing work is on networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive (like, trust) or negative (dislike, distrust). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Local patterns of social balance have been used in the past for sign prediction. We define more general measures of social imbalance (MOIs) based on l-cycles in the network and give a simple sign prediction rule. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, also has a balance theoretic interpretation when applied to signed networks. Motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. We provide theoretical performance guarantees for our low-rank matrix completion approach via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of social balance, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1177,1213,,,,,,,,,,,,,,,,WOS:000335458100011,0
J,"Bottou, L; Peters, J; Quinonero-Candela, J; Charles, DX; Chickering, DM; Portugaly, E; Ray, D; Simard, P; Snelson, E",,,,"Bottou, Leon; Peters, Jonas; Quinonero-Candela, Joaquin; Charles, Denis X.; Chickering, D. Max; Portugaly, Elon; Ray, Dipankar; Simard, Patrice; Snelson, Ed",,,Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.,,,,,,"Peters, Jonas/0000-0002-1487-7511",,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3207,3260,,,,,,,,,,,,,,,,WOS:000329786900001,0
J,"Forghani, Y; Sadoghi, H",,,,"Forghani, Yahya; Sadoghi, Hadi",,,"Comment on Robustness and Regularization of Support Vector Machines by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper comments on the published work dealing with robustness and regularization of support vector machines ( Journal of Machine Learning Research, Vol. 10, pp. 1485-1510, 2009) by H. Xu et al. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem.",,,,,"sadoghiyazdi, hadi/T-9515-2019; forghani, yahya/AAO-7692-2021","forghani, yahya/0000-0002-3218-5666",,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3493,3494,,,,,,,,,,,,,,,,WOS:000329786900009,0
J,"Ryabko, D; Mary, J",,,,"Ryabko, Daniil; Mary, Jeremie",,,A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A metric between time-series distributions is proposed that can be evaluated using binary classification methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classification and concern highly dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2837,2856,,,,,,,,,,,,,,,,WOS:000327007400011,0
J,"Chen, LS; Buja, A",,,,"Chen, Lisha; Buja, Andreas",,,"Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called stress functions. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following benefits and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs.",,,,,"Chen, Lisha/HGC-6247-2022",,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1145,1173,,,,,,,,,,,,,,,,WOS:000318590500012,0
J,"Hyvarinen, A; Smith, SM",,,,"Hyvarinen, Aapo; Smith, Stephen M.",,,Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present new measures of the causal direction, or direction of effect, between two non-Gaussian random variables. They are based on the likelihood ratio under the linear non-Gaussian acyclic model (LiNGAM). We also develop simple first-order approximations of the likelihood ratio and analyze them based on related cumulant-based measures, which can be shown to find the correct causal directions. We show how to apply these measures to estimate LiNGAM for more than two variables, and even in the case of more variables than observations. We further extend the method to cyclic and nonlinear models. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data, and it is computationally and conceptually very simple. Results on simulated fMRI data indicate that the method may be useful in neuroimaging where the number of time points is typically quite small.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432; Smith, Stephen/0000-0001-8166-069X",,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,111,152,,,,,,,,,,,31695580,,,,,WOS:000314530200004,0
J,"Azar, MG; Gomez, V; Kappen, HJ",,,,"Azar, Mohammad Gheshlaghi; Gomez, Vicenc; Kappen, Hilbert J.",,,Dynamic Policy Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a novel policy iteration method, called dynamic policy programming (DPP), to estimate the optimal policy in the infinite-horizon Markov decision processes. DPP is an incremental algorithm that forces a gradual change in policy update. This allows us to prove finite-iteration and asymptotic l(infinity)-norm performance-loss bounds in the presence of approximation/estimation error which depend on the average accumulated error as opposed to the standard bounds which are expressed in terms of the supremum of the errors. The dependency on the average error is important in problems with limited number of samples per iteration, for which the average of the errors can be significantly smaller in size than the supremum of the errors. Based on these theoretical results, we prove that a sampling-based variant of DPP (DPP-RL) asymptotically converges to the optimal policy. Finally, we illustrate numerically the applicability of these results on some benchmark problems and compare the performance of the approximate variants of DPP with some existing reinforcement learning (RL) methods.",,,,,"G√≥mez, Vicen√ß/D-1984-2009; Kappen, H.J./L-4425-2015","G√≥mez, Vicen√ß/0000-0001-5146-7645; ",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3207,3245,,,,,,,,,,,,,,,,WOS:000313200200003,0
J,"Lazaric, A; Ghavamzadeh, M; Munos, R",,,,"Lazaric, Alessandro; Ghavamzadeh, Mohammad; Munos, Remi",,,Finite-Sample Analysis of Least-Squares Policy Iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We first consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a fixed policy, using the least-squares temporal-difference (LSTD) learning method, and report finite-sample analysis for this algorithm. To do so, we first derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is beta-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,3041,3074,,,,,,,,,,,,,,,,WOS:000313200000008,0
J,"Helmbold, DP; Long, PM",,,,"Helmbold, David P.; Long, Philip M.",,,On the Necessity of Irrelevant Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2012,13,,,,,,2145,2170,,,,,,,,,,,,,,,,WOS:000307496000001,0
J,"Biau, G",,,,"Biau, Gerard",,,Analysis of a Random Forests Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Random forests are a scheme proposed by Leo Breiman in the 2000's for building a predictor ensemble with a set of decision trees that grow in randomly selected subspaces of data. Despite growing interest and practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical forces driving the algorithm. In this paper, we offer an in-depth analysis of a random forests model suggested by Breiman (2004), which is very close to the original algorithm. We show in particular that the procedure is consistent and adapts to sparsity, in the sense that its rate of convergence depends only on the number of strong features and not on how many noise variables are present.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1063,1095,,,,,,,,,,,,,,,,WOS:000303773100007,0
J,"Yan, F; Kittler, J; Mikolajczyk, K; Tahir, A",,,,"Yan, Fei; Kittler, Josef; Mikolajczyk, Krystian; Tahir, Atif",,,Non-Sparse Multiple Kernel Fisher Discriminant Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparsity-inducing multiple kernel Fisher discriminant analysis (MK-FDA) has been studied in the literature. Building on recent advances in non-sparse multiple kernel learning (MKL), we propose a non-sparse version of MK-FDA, which imposes a general l(p) norm regularisation on the kernel weights. We formulate the associated optimisation problem as a semi-infinite program (SIP), and adapt an iterative wrapper algorithm to solve it. We then discuss, in light of latest advances in MKL optimisation techniques, several reformulations and optimisation strategies that can potentially lead to significant improvements in the efficiency and scalability of MK-FDA. We carry out extensive experiments on six datasets from various application areas, and compare closely the performance of l(p) MK-FDA, fixed norm MK-FDA, and several variants of SVM-based MKL (MK-SVM). Our results demonstrate that l(p) MK-FDA improves upon sparse MK-FDA in many practical situations. The results also show that on image categorisation problems, l(p) MK-FDA tends to outperform its SVM counterpart. Finally, we also discuss the connection between (MK-)FDA and (MK-)SVM, under the unified framework of regularised kernel machines.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,607,642,,,,,,,,,,,,,,,,WOS:000303772100005,0
J,"Jenatton, R; Audibert, JY; Bach, F",,,,"Jenatton, Rodolphe; Audibert, Jean-Yves; Bach, Francis",,,Structured Variable Selection with Sparsity-Inducing Norms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms. These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual l(1)-norm and the group l(1)-norm by allowing the subsets to overlap. This leads to a specific set of allowed nonzero patterns for the solutions of such problems. We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns. This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns. We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2777,2824,,,,,,,,,,,,,,,,WOS:000298103200002,0
J,"van Seijen, H; Whiteson, S; van Hasselt, H; Wiering, M",,,,"van Seijen, Harm; Whiteson, Shimon; van Hasselt, Hado; Wiering, Marco",,,Exploiting Best-Match Equations for Efficient Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article presents and evaluates best-match learning, a new approach to reinforcement learning that trades off the sample efficiency of model-based methods with the space efficiency of model-free methods. Best-match learning works by approximating the solution to a set of best-match equations, which combine a sparse model with a model-free Q-value function constructed from samples not used by the model. We prove that, unlike regular sparse model-based methods, best-match learning is guaranteed to converge to the optimal Q-values in the tabular case. Empirical results demonstrate that best-match learning can substantially outperform regular sparse model-based methods, as well as several model-free methods that strive to improve the sample efficiency of temporal-difference methods. In addition, we demonstrate that best-match learning can be successfully combined with function approximation.",,,,,"Wiering, Marco/C-5909-2012","Wiering, Marco/0000-0003-4331-7537",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,2045,2094,,,,,,,,,,,,,,,,WOS:000293757200010,0
J,"Park, C; Huang, JHZ; Ding, Y",,,,"Park, Chiwoo; Huang, Jianhua Z.; Ding, Yu",,,Domain Decomposition Approach for Fast Gaussian Process Regression of Large Spatial Data Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian process regression is a flexible and powerful tool for machine learning, but the high computational complexity hinders its broader applications. In this paper, we propose a new approach for fast computation of Gaussian process regression with a focus on large spatial data sets. The approach decomposes the domain of a regression function into small subdomains and infers a local piece of the regression function for each subdomain. We explicitly address the mismatch problem of the local pieces on the boundaries of neighboring subdomains by imposing continuity constraints. The new approach has comparable or better computation complexity as other competing methods, but it is easier to be parallelized for faster computation. Moreover, the method can be adaptive to non-stationary features because of its local nature and, in particular, its use of different hyperparameters of the covariance function for different local regions. We illustrate application of the method and demonstrate its advantages over existing methods using two synthetic data sets and two real spatial data sets.",,,,,"Park, Chiwoo/ABA-4876-2021","Park, Chiwoo/0000-0002-2463-8901",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1697,1728,,,,,,,,,,,,,,,,WOS:000292304000008,0
J,"Tan, VYF; Anandkumar, A; Willsky, AS",,,,"Tan, Vincent Y. F.; Anandkumar, Animashree; Willsky, Alan S.",,,Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed. It is shown that this algorithm is both structurally consistent and risk consistent and the error probability of structure learning decays faster than any polynomial in the number of samples under fixed model size. For the high-dimensional scenario where the size of the model d and the number of edges k scale with the number of samples n, sufficient conditions on (n, d, k) are given for the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identified; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1617,1653,,,,,,,,,,,,,,,,WOS:000292304000006,0
J,"Aflalo, J; Ben-Tal, A; Bhattacharyya, C; Nath, JS; Raman, S",,,,"Aflalo, Jonathan; Ben-Tal, Aharon; Bhattacharyya, Chiranjib; Nath, Jagarlapudi Saketha; Raman, Sankaran",,,Variable Sparsity Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper(1) presents novel algorithms and applications for a particular class of mixed-norm regularization based Multiple Kernel Learning (MKL) formulations. The formulations assume that the given kernels are grouped and employ l(1) norm regularization for promoting sparsity within RKHS norms of each group and l(s), s >= 2 norm regularization for promoting non-sparse combinations across groups. Various sparsity levels in combining the kernels can be achieved by varying the grouping of kernels-hence we name the formulations as Variable Sparsity Kernel Learning (VSKL) formulations. While previous attempts have a non-convex formulation, here we present a convex formulation which admits efficient Mirror-Descent (MD) based solving techniques. The proposed MD based algorithm optimizes over product of simplices and has a computational complexity of O (m(2)n(tot) log n(max)/epsilon(2)) where m is no. training data points, n(max), n(tot) are the maximum no. kernels in any group, total no. kernels respectively and epsilon is the error in approximating the objective. A detailed proof of convergence of the algorithm is also presented. Experimental results show that the VSKL formulations are well-suited for multi-modal learning tasks like object categorization. Results also show that the MD based algorithm outperforms state-of-the-art MKL solvers in terms of computational efficiency.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,565,592,,,,,,,,,,,,,,,,WOS:000288896800007,0
J,"Wang, CP; Liao, XJ; Carin, L; Dunson, DB",,,,"Wang, Chunping; Liao, Xuejun; Carin, Lawrence; Dunson, David B.",,,Classification with Incomplete Data Using Dirichlet Process Priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A non-parametric hierarchical Bayesian framework is developed for designing a classifier, based on a mixture of simple (linear) classifiers. Each simple classifier is termed a local expert, and the number of experts and their construction are manifested via a Dirichlet process formulation. The simple form of the experts allows analytical handling of incomplete data. The model is extended to allow simultaneous design of classifiers on multiple data sets, termed multi-task learning, with this also performed non-parametrically via the Dirichlet process. Fast inference is performed using variational Bayesian (VB) analysis, and example results are presented for several data sets. We also perform inference via Gibbs sampling, to which we compare the VB results.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3269,3311,,,,,,,,,,,23990757,,,,,WOS:000286637200001,0
J,"Yu, GQ; Feng, YJ; Miller, DJ; Xuan, JH; Hoffman, EP; Clarke, R; Davidson, B; Shih, IM; Wang, Y",,,,"Yu, Guoqiang; Feng, Yuanjian; Miller, David J.; Xuan, Jianhua; Hoffman, Eric P.; Clarke, Robert; Davidson, Ben; Shih, Ie-Ming; Wang, Yue",,,Matched Gene Selection and Committee Classifier for Molecular Classification of Heterogeneous Diseases,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Microarray gene expressions provide new opportunities for molecular classification of heterogeneous diseases. Although various reported classification schemes show impressive performance, most existing gene selection methods are suboptimal and are not well-matched to the unique characteristics of the multicategory classification problem. Matched design of the gene selection method and a committee classifier is needed for identifying a small set of gene markers that achieve accurate multicategory classification while being both statistically reproducible and biologically plausible. We report a simpler and yet more accurate strategy than previous works for multicategory classification of heterogeneous diseases. Our method selects the union of one-versus-everyone (OVE) phenotypic up-regulated genes (PUGs) and matches this gene selection with a one-versus-rest support vector machine (OVRSVM). Our approach provides even-handed gene resources for discriminating both neighboring and well-separated classes. Consistent with the OVRSVM structure, we evaluated the fold changes of OVE gene expressions and found that only a small number of high-ranked genes were required to achieve superior accuracy for multicategory classification. We tested the proposed PUG-OVRSVM method on six real microarray gene expression data sets (five public benchmarks and one in-house data set) and two simulation data sets, observing significantly improved performance with lower error rates, fewer marker genes, and higher performance sustainability, as compared to several widely-adopted gene selection and classification methods. The MATLAB toolbox, experiment data and supplement files are available at http://www.cbil.ece.vt.edu/software.htm.",,,,,"sdsd, shihieih/ABE-6190-2020; Clarke, Robert/A-6485-2008","Clarke, Robert/0000-0002-9278-0854",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2141,2167,,,,,,,,,,,,,,,,WOS:000282523300003,0
J,"Yoshida, R; West, M",,,,"Yoshida, Ryo; West, Mike",,,Bayesian Learning in Sparse Graphical Factor Models via Variational Mean-Field Annealing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a class of sparse latent factor models, called graphical factor models (GFMs), and relevant sparse learning algorithms for posterior mode estimation. Linear, Gaussian GFMs have sparse, orthogonal factor loadings matrices, that, in addition to sparsity of the implied covariance matrices, also induce conditional independence structures via zeros in the implied precision matrices. We describe the models and their use for robust estimation of sparse latent factor structure and data/signal reconstruction. We develop computational algorithms for model exploration and posterior mode search, addressing the hard combinatorial optimization involved in the search over a huge space of potential sparse configurations. A mean-field variational technique coupled with annealing is developed to successively generate artificial posterior distributions that, at the limiting temperature in the annealing schedule, define required posterior modes in the GFM parameter space. Several detailed empirical studies and comparisons to related approaches are discussed, including analyses of handwritten digit image and cancer gene expression data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1771,1798,,,,,,,,,,,20890391,,,,,WOS:000282522000008,0
J,"Ravikumar, P; Agarwal, A; Wainwright, MJ",,,,"Ravikumar, Pradeep; Agarwal, Alekh; Wainwright, Martin J.",,,Message-passing for Graph-structured Linear Programs: Proximal Methods and Rounding Schemes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of computing a maximum a posteriori (MAP) configuration is a central computational challenge associated with Markov random fields. There has been some focus on tree-based linear programming (LP) relaxations for the MAP problem. This paper develops a family of super-linearly convergent algorithms for solving these LPs, based on proximal minimization schemes using Bregman divergences. As with standard message-passing on graphs, the algorithms are distributed and exploit the underlying graphical structure, and so scale well to large problems. Our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman projections used to compute each proximal update. We establish convergence guarantees for our algorithms, and illustrate their performance via some simulations. We also develop two classes of rounding schemes, deterministic and randomized, for obtaining integral configurations from the LP solutions. Our deterministic rounding schemes use a re-parameterization property of our algorithms so that when the LP solution is integral, the MAP solution can be obtained even before the LP-solver converges to the optimum. We also propose graph-structured randomized rounding schemes applicable to iterative LP-solving algorithms in general. We analyze the performance of and report simulations comparing these rounding schemes.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1043,1080,,,,,,,,,,,,,,,,WOS:000277186600001,0
J,"Kojima, K; Perrier, E; Imoto, S; Miyano, S",,,,"Kojima, Kaname; Perrier, Eric; Imoto, Seiya; Miyano, Satoru",,,Optimal Search on Clustered Structural Constraint for Learning Bayesian Network Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning an optimal Bayesian network in a constrained search space; skeletons are compelled to be subgraphs of a given undirected graph called the super-structure. The previously derived constrained optimal search (COS) remains limited even for sparse superstructures. To extend its feasibility, we propose to divide the super-structure into several clusters and perform an optimal search on each of them. Further, to ensure acyclicity, we introduce the concept of ancestral constraints (ACs) and derive an optimal algorithm satisfying a given set of ACs. Finally, we theoretically derive the necessary and sufficient sets of ACs to be considered for finding an optimal constrained graph. Empirical evaluations demonstrate that our algorithm can learn optimal Bayesian networks for some graphs containing several hundreds of vertices, and even for super-structures having a high average degree (up to four), which is a drastic improvement in feasibility over the previous optimal algorithm. Learnt networks are shown to largely outperform state-of-the-art heuristic algorithms both in terms of score and structural hamming distance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,285,310,,,,,,,,,,,,,,,,WOS:000277186400009,0
J,"Rudin, C",,,,"Rudin, Cynthia",,,The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We are interested in supervised ranking algorithms that perform especially well near the top of the ranked list, and are only required to perform sufficiently well on the rest of the list. In this work, we provide a general form of convex objective that gives high-scoring examples more importance. This push near the top of the list can be chosen arbitrarily large or small, based on the preference of the user. We choose l(p)-norms to provide a specific type of push; if the user sets p larger, the objective concentrates harder on the top of the list. We derive a generalization bound based on the p-norm objective, working around the natural asymmetry of the problem. We then derive a boosting- style algorithm for the problem of ranking with a push at the top. The usefulness of the algorithm is illustrated through experiments on repository data. We prove that the minimizer of the algorithm's objective is unique in a specific sense. Furthermore, we illustrate how our objective is related to quality measurements for information retrieval.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2233,2271,,,,,,,,,,,,,,,,WOS:000272346400003,0
J,"Brunskill, E; Leffler, BR; Li, L; Littman, ML; Roy, N",,,,"Brunskill, Emma; Leffler, Bethany R.; Li, Lihong; Littman, Michael L.; Roy, Nicholas",,,Provably Efficient Learning with Typed Parametric Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufficiently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of significant, real-world domains, such as robot navigation across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-specific quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufficient manner to produce good performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1955,1988,,,,,,,,,,,,,,,,WOS:000270825200008,0
J,"Foster, L; Waagen, A; Aijaz, N; Hurley, M; Luis, A; Rinsky, J; Satyavolu, C; Way, MJ; Gazis, P; Srivastava, A",,,,"Foster, Leslie; Waagen, Alex; Aijaz, Nabeela; Hurley, Michael; Luis, Apolonio; Rinsky, Joel; Satyavolu, Chandrika; Way, Michael J.; Gazis, Paul; Srivastava, Ashok",,,Stable and Efficient Gaussian Process Calculations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The use of Gaussian processes can be an effective approach to prediction in a supervised learning environment. For large data sets, the standard Gaussian process approach requires solving very large systems of linear equations and approximations are required for the calculations to be practical. We will focus on the subset of regressors approximation technique. We will demonstrate that there can be numerical instabilities in a well known implementation of the technique. We discuss alternate implementations that have better numerical stability properties and can lead to better predictions. Our results will be illustrated by looking at an application involving prediction of galaxy redshift from broadband spectrum data.",,,,,"Way, Michael/P-5815-2019; Way, Michael/D-5254-2012","Way, Michael/0000-0003-3728-0475; ",,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,857,882,,,,,,,,,,,,,,,,WOS:000270824600002,0
J,"Ghanty, P; Paul, S; Pal, NR",,,,"Ghanty, Pradip; Paul, Samrat; Pal, Nikhil R.",,,NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose a new multilayer classifier architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classification module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classification module we use support vector machines (SVMs)-here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classification module classifies the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classification module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVMis found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classification module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been verified with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed.",,,,,"Ghanty, Pradip/AGG-1334-2022",,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,591,622,,,,,,,,,,,,,,,,WOS:000270824500003,0
J,"Feldman, V",,,,"Feldman, Vitaly",,,On The Power of Membership Queries in Agnostic Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the properties of the agnostic learning framework of Haussler (1992) and Kearns, Schapire, and Sellie (1994). In particular, we address the question: is there any situation in which membership queries are useful in agnostic learning? Our results show that the answer is negative for distribution-independent agnostic learning and positive for agnostic learning with respect to a specific marginal distribution. Namely, we give a simple proof that any concept class learnable agnostically by a distribution-independent algorithm with access to membership queries is also learnable agnostically without membership queries. This resolves an open problem posed by Kearns et al. (1994). For agnostic learning with respect to the uniform distribution over {0; 1}(n) we show a concept class that is learnable with membership queries but computationally hard to learn from random examples alone (assuming that one-way functions exist).",,,,,,"Feldman, Vitaly/0000-0002-3904-759X",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,163,182,,,,,,,,,,,,,,,,WOS:000270824200003,0
J,"George, A; Powell, WB; Kulkarni, SR",,,,"George, Abraham; Powell, Warren B.; Kulkarni, Sanjeev R.",,,Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well.",,,,,"Powell, Warren/N-8263-2019",,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2079,2111,,,,,,,,,,,,,,,,WOS:000262637300002,0
J,"Seeger, MW",,,,"Seeger, Matthias W.",,,Bayesian inference and optimal design for the sparse linear model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The linear model with sparsity-favouring prior on the coefficients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identification from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in significant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identification application appears in Steinke et al. (2007).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,759,813,,,,,,,,,,,,,,,,WOS:000256642100008,0
J,"Wang, JH; Shen, XT",,,,"Wang, Junhui; Shen, Xiaotong",,,Large margin semi-supervised learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In classification, semi-supervised learning occurs when a large amount of unlabeled data is available with only a small number of labeled data. In such a situation, how to enhance predictability of classification through unlabeled data is the focus. In this article, we introduce a novel large margin semi-supervised learning methodology, using grouping information from unlabeled data, together with the concept of margins, in a form of regularization controlling the interplay between labeled and unlabeled data. Based on this methodology, we develop two specific machines involving support vector machines and psi-learning, denoted as SSVM and SPSI, through difference convex programming. In addition, we estimate the generalization error using both labeled and unlabeled data, for tuning regularizers. Finally, our theoretical and numerical analyses indicate that the proposed methodology achieves the desired objective of delivering high performance in generalization, particularly against some strong performers.",,,,,,"WANG, Junhui/0000-0002-9165-5664",,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1867,1891,,,,,,,,,,,,,,,,WOS:000252744400007,0
J,"Feldman, V",,,,"Feldman, Vitaly",,,Attribute-efficient and non-adaptive learning of parities and DNF expressions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th Annual Conference on Learning Theory (COLT 2005),"JUN 27-30, 2005","Bertinoro, ITALY","Machine Learning Journal,Google,Bertinoro Int Ctr Informat,PASCAL,Univ Studi Milano",,,,"We consider the problems of attribute-efficient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0,1}(n). We show that attribute-efficient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the first evidence that attribute-efficient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries ( MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modification of Levin's algorithm for locating a weakly-correlated parity due to Bshouty et al. ( 1999), we give the first non-adaptive and attribute-efficient algorithm for learning DNF with respect to the uniform distribution. Our algorithm runs in time (O) over tilde (ns(4) /epsilon) and uses (O) over tilde (s(4) . log(2) n /epsilon) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modified to tolerate random persistent classification noise in MQs.",,,,,,"Feldman, Vitaly/0000-0002-3904-759X",,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1431,1460,,,,,,,,,,,,,,,,WOS:000249353700003,0
J,"Mease, D; Wyner, AJ; Buja, A",,,,"Mease, David; Wyner, Abraham J.; Buja, Andreas",,,Boosted classification trees and class probability/quantile estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The standard by which binary classifiers are usually judged, misclassification error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P [ y = 1 vertical bar x]. Boosted classification trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classification with unequal costs or, equivalently, classification at quantiles other than 1/2, and 2) estimation of the conditional class probability function P [ y = 1 vertical bar x]. We first examine whether the latter problem, estimation of P [ y = 1 vertical bar x], can be solved with Logit-Boost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overfit P [ y = 1 vertical bar x] even though they perform well as classifiers. A major negative point of the present article is the disconnect between class probability estimation and classification. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (JOUS-Boost). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overfitting, but for arbitrary misclassification costs and, equivalently, arbitrary quantile boundaries. We then use collections of classifiers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,409,439,,,,,,,,,,,,,,,,WOS:000247002700002,0
J,"Moser, B",,,,"Moser, Bernhard",,,On representing and generating kernels by fuzzy equivalence relations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernels are two-placed functions that can be interpreted as inner products in some Hilbert space. It is this property which makes kernels predestinated to carry linear models of learning, optimization or classification strategies over to non-linear variants. Following this idea, various kernel-based methods like support vector machines or kernel principal component analysis have been conceived which prove to be successful for machine learning, data mining and computer vision applications. When applying a kernel-based method a central question is the choice and the design of the kernel function. This paper provides a novel view on kernels based on fuzzy-logical concepts which allows to incorporate prior knowledge in the design process. It is demonstrated that kernels mapping to the unit interval with constant one in its diagonal can be represented by a commonly used fuzzy-logical formula for representing fuzzy rule bases. This means that a great class of kernels can be represented by fuzzy-logical concepts. Apart from this result, which only guarantees the existence of such a representation, constructive examples are presented and the relation to unlabeled learning is pointed out.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2603,2620,,,,,,,,,,,,,,,,WOS:000245390800003,0
J,"Bach, FR; Jordan, MI",,,,"Bach, Francis R.; Jordan, Michael I.",,,"Learning spectral clustering, with application to speech separation",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters, with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive new cost functions for spectral clustering based on measures of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing these cost functions with respect to the partition leads to new spectral clustering algorithms. Minimizing with respect to the similarity matrix leads to algorithms for learning the similarity matrix from fully labelled data sets. We apply our learning algorithm to the blind one-microphone speech separation problem, casting the problem as one of segmentation of the spectrogram.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,1963,2001,,,,,,,,,,,,,,,,WOS:000245390500002,0
J,"Climer, S; Zhang, WX",,,,"Climer, Sharlee; Zhang, Weixiong",,,"Rearrangement clustering: Pitfalls, remedies, and applications",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a matrix of values in which the rows correspond to objects and the columns correspond to features of the objects, rearrangement clustering is the problem of rearranging the rows of the matrix such that the sum of the similarities between adjacent rows is maximized. Referred to by various names and reinvented several times, this clustering technique has been extensively used in many fields over the last three decades. In this paper, we point out two critical pitfalls that have been previously overlooked. The first pitfall is deleterious when rearrangement clustering is applied to objects that form natural clusters. The second concerns a similarity metric that is commonly used. We present an algorithm that overcomes these pitfalls. This algorithm is based on a variation of the Traveling Salesman Problem. It offers an extra benefit as it automatically determines cluster boundaries. Using this algorithm, we optimally solve four benchmark problems and a 2,467-gene expression data clustering problem. As expected, our new algorithm identifies better clusters than those found by previous approaches in all five cases. Overall, our results demonstrate the benefits of rectifying the pitfalls and exemplify the usefulness of this clustering technique. Our code is available at our websites.",,,,,"Climer, Sharlee/A-6960-2019","Climer, Sharlee/0000-0003-2731-7377; Zhang, Weixiong/0000-0002-4998-9791",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,919,943,,,,,,,,,,,,,,,,WOS:000245388400001,0
J,"Klivans, AR; Servedio, RA",,,,"Klivans, AR; Servedio, RA",,,Toward attribute efficient learning of decision lists and parities,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider two well-studied problems regarding attribute efficient learning: learning decision lists and learning parity functions. First, we give an algorithm for learning decision lists of length k over n variables using 2((O) over tilde (k1/3)) log n examples and time n((O) over tilde (k2/3)). This is the first algorithm for learning decision lists that has both subexponential sample complexity and subexponential running time in the relevant parameters. Our approach establishes a relationship between attribute efficient learning and polynomial threshold functions and is based on a new construction of low degree, low weight polynomial threshold functions for decision lists. For a wide range of parameters our construction matches a lower bound due to Beigel for decision lists and gives an essentially optimal tradeoff between polynomial threshold function degree and weight. Second, we give an algorithm for learning an unknown parity function on k out of n variables using O(n(1-1/k)) examples in poly(n) time. For k = o(log n) this yields a polynomial time algorithm with sample complexity o(n); this is the first polynomial time algorithm for learning parity on a superconstant number of variables with sublinear sample complexity. We also give a simple algorithm for learning an unknown length-k parity using O(k log n) examples in n(k/2) time, which improves on the naive n(k) time bound of exhaustive search.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2006,7,,,,,,587,602,,,,,,,,,,,,,,,,WOS:000237359100001,0
J,"Luo, T; Kramer, K; Goldgof, DB; Hall, LO; Samson, S; Remsen, A; Hopkins, T",,,,"Luo, T; Kramer, K; Goldgof, DB; Hall, LO; Samson, S; Remsen, A; Hopkins, T",,,Active learning to recognize multiple types of plankton,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an active learning method which reduces the labeling effort of domain experts in multi-class classification problems. Active learning is applied in conjunction with support vector machines to recognize underwater zooplankton from higher-resolution, new generation SIPPER II images. Most previous work on active learning with support vector machines only deals with two class problems. In this paper, we propose an active learning approach breaking ties for multi-class support vector machines using the one-vs-one approach with a probability approximation. Experimental results indicate that our approach often requires significantly less labeled images to reach a given accuracy than the approach of labeling the least certain test example and random sampling. It can also be applied in batch mode resulting in an accuracy comparable to labeling one image at a time and retraining.",,,,,"Goldgof, Dmitry/ABF-1366-2020",,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,589,613,,,,,,,,,,,,,,,,WOS:000236329600008,0
J,"Marchand, M; Sokolova, M",,,,"Marchand, M; Sokolova, M",,,Learning with decision lists of data-dependent features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a learning algorithm for decision lists which allows features that are constructed from the data and allows a trade-off between accuracy and complexity. We provide bounds on the generalization error of this learning algorithm in terms of the number of errors and the size of the classifier it finds on the training data. We also compare its performance on some natural data sets with the set covering machine and the support vector machine. Furthermore, we show that the proposed bounds on the generalization error provide effective guides for model selection.",,,,,,"Marchand, Mario/0000-0002-7078-7393",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,427,451,,,,,,,,,,,,,,,,WOS:000236329600003,0
J,"Winn, J; Bishop, CM",,,,"Winn, J; Bishop, CM",,,Variational message passing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence ( unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES ('Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,661,694,,,,,,,,,,,,,,,,WOS:000236329600011,0
J,"Kaariainen, M; Malinen, T; Elomaa, T",,,,"Kaariainen, M; Malinen, T; Elomaa, T",,,Selective Rademacher penalization and reduced error pruning of decision trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Rademacher penalization is a modern technique for obtaining data-dependent bounds on the generalization error of classifiers. It appears to be limited to relatively simple hypothesis classes because of computational complexity issues. In this paper we, nevertheless, apply Rademacher penalization to the in practice important hypothesis class of unrestricted decision trees by considering the prunings of a given decision tree rather than the tree growing phase. This study constitutes the first application of Rademacher penalization to hypothesis classes that have practical significance. We present two variations of the approach, one in which the hypothesis class consists of all prunings of the initial tree and another in which only the prunings that are accurate on growing data are taken into account. Moreover, we generalize the error-bounding approach from binary classification to multi-class situations. Our empirical experiments indicate that the proposed new bounds outperform distribution-independent bounds for decision tree prunings and provide non-trivial error estimates on real-world data sets.",,,,,"Elomaa, Tapio P/G-4233-2014; Elomaa, Tapio/AAE-6936-2020","Elomaa, Tapio/0000-0002-5230-0027",,,,,,,,,,,,,1532-4435,,,,,SEP,2004,5,,,,,,1107,1126,,,,,,,,,,,,,,,,WOS:000236328100002,0
J,"Herbrich, R; Graepel, T",,,,"Herbrich, R; Graepel, T",,,Introduction to the special issue on learning theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,755,757,,10.1162/1532443041424283,0,,,,,,,,,,,,,WOS:000223238800001,0
J,"Tong, S; Koller, D",,,,"Tong, S; Koller, D",,,Support vector machine active learning with applications to text classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2002,2,1,,,,,45,66,,10.1162/153244302760185243,0,,,,,,,,,,,,,WOS:000173838200003,0
J,"Dejean, H",,,,"Dejean, H",,,Learning rules and their exceptions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present in this article a top-down inductive system, ALLiS, for learning linguistic structures. Two difficulties came up during the. development of the system: the presence of a significant amount of noise in the data and the presence of exceptions linguistically motivated. It is then a challenge for an inductive system to learn rules from this kind of data. This leads us to add a specific mechanism, refinement, which enables learning rules and their exceptions. In the first part of this article we evaluate the usefulness of this device and show that it improves results when learning linguistic structures. In the second part, we explore how to improve the efficiency of the system by using prior knowledge. Since Natural Language is a strongly structured object, it may be important to investigate whether linguistic knowledge can help to make natural language learning more efficiently and accurately. This article presents some experiments demonstrating that linguistic knowledge improves learning. The system has been applied to the shared task of the CoNLL'00 workshop.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,669,693,,10.1162/153244302320884588,0,,,,,,,,,,,,,WOS:000179542800006,0
J,"Ji, KY; Yang, JJ; Liang, YB",,,,"Ji, Kaiyi; Yang, Junjie; Liang, Yingbin",,,Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algo-rithm has been widely used due to its simplicity and effectiveness. However, the conver-gence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algo-rithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an epsilon-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number N of inner-stage steps in order for N-step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766885900001,0
J,"Santhanam, N; Anantharam, V; Szpankowski, W",,,,"Santhanam, Narayana; Anantharam, Venkat; Szpankowski, Wojciech",,,Data-Derived Weak Universal Consistency,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many current applications in data science need rich model classes to adequately represent the statistics that may be driving the observations. Such rich model classes may be too complex to admit uniformly consistent estimators. In such cases, it is conventional to settle for estimators with guarantees on convergence rate where the performance can be bounded in a model-dependent way, i.e. pointwise consistent estimators. But this viewpoint has the practical drawback that estimator performance is a function of the unknown model within the model class that is being estimated. Even if an estimator is consistent, how well it is doing at any given time may not be clear, no matter what the sample size of the observations. In these cases, a line of analysis favors sample dependent guarantees. We explore this framework by studying rich model classes that may only admit pointwise consistency guarantees, yet enough information about the unknown model driving the observations needed to gauge estimator accuracy can be inferred from the sample at hand. In this paper we obtain a novel characterization of lossless compression problems over a countable alphabet in the data-derived framework in terms of what we term deceptive distributions. We also show that the ability to estimate the redundancy of compressing memoryless sources is equivalent to learning the underlying single-letter marginal in a data-derived fashion. We expect that the methodology underlying such characterizations in a data derived estimation framework will be broadly applicable to a wide range of estimation problems, enabling a more systematic approach to data-derived guarantees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766884500001,0
J,"Chatterji, NS; Long, PM; Bartlett, PL",,,,"Chatterji, Niladri S.; Long, Philip M.; Bartlett, Peter L.",,,When Does Gradient Descent with Logistic Loss Find Interpolating Two-layer Networks?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the training of finite-width two-layer smoothed ReLU networks for binary classification using the logistic loss. We show that gradient descent drives the training loss to zero if the initial loss is small enough. When the data satisfies certain cluster and separation conditions and the network is wide enough, we show that one step of gradient descent reduces the loss sufficiently that the first result applies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687114400001,0
J,"Gyorfi, L; Weiss, R",,,,"Gyorfi, Laszlo; Weiss, Roi",,,Universal consistency and rates of convergence of multiclass prototype algorithms in metric spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study universal consistency and convergence rates of simple nearest-neighbor prototype rules for the problem of multiclass classification in metric spaces. We first show that a novel data-dependent partitioning rule, named Proto-NN, is universally consistent in any metric space that admits a universally consistent rule. Proto-NN is a significant simplification of OptiNet, a recently proposed compression-based algorithm that, to date, was the only algorithm known to be universally consistent in such a general setting. Practically, Proto-NN is simpler to implement and enjoys reduced computational complexity. We then proceed to study convergence rates of the excess error probability. We first obtain rates for the standard k-NN rule under a margin condition and a new generalized Lipschitz condition. The latter is an extension of a recently proposed modified-Lipschitz condition from Rd to metric spaces. Similarly to the modified-Lipschitz condition, the new condition avoids any boundness assumptions on the data distribution. While obtaining rates for Proto-NN is left open, we show that a second prototype rule that hybridizes between k-NN and Proto-NN achieves the same rates as k-NN while enjoying similar computational advantages as Proto-NN. However, as k-NN, this hybrid rule is not consistent in general.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687153100001,0
J,"Kroemer, O; Niekum, S; Konidaris, G",,,,"Kroemer, Oliver; Niekum, Scott; Konidaris, George",,,"A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500030,0
J,"Liu, L; Mukherjee, R; Robins, JM; Tchetgen, ET",,,,"Liu, Lin; Mukherjee, Rajarshi; Robins, James M.; Tchetgen, Eric Tchetgen",,,Adaptive Estimation of Nonparametric Functionals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We provide general adaptive upper bounds for estimating nonparametric functionals based on second-order U-statistics arising from finite-dimensional approximation of the infinite-dimensional models. We then provide examples of functionals for which the theory produces rate optimally matching adaptive upper and lower bounds. Our results are automatically adaptive in both parametric and nonparametric regimes of estimation and are automatically adaptive and semiparametric efficient in the regime of parametric convergence rate.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,99,,,,,,,,,,,,,,,WOS:000663151200001,0
J,"Liu, M; Luo, YZ; Wang, LM; Xie, YC; Yuan, H; Gui, SR; Yu, HY; Xu, Z; Zhang, JT; Liu, Y; Yan, KQ; Liu, HR; Fu, C; Oztekin, B; Zhang, X; Ji, SW",,,,"Liu, Meng; Luo, Youzhi; Wang, Limei; Xie, Yaochen; Yuan, Hao; Gui, Shurui; Yu, Haiyang; Xu, Zhao; Zhang, Jingtun; Liu, Yi; Yan, Keqiang; Liu, Haoran; Fu, Cong; Oztekin, Bora; Zhang, Xuan; Ji, Shuiwang",,,DIG: A Turnkey Library for Diving into Graph Deep Learning Research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a turnkey library that provides a unified testbed for higher level, research-oriented graph deep learning tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.",,,,,"Xie, Yaochen/GOV-6217-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706445000001,0
J,"Liu, SF; Owen, AB",,,,"Liu, Sifan; Owen, Art B.",,,Quasi-Monte Carlo Quasi-Newton in Variational Bayes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many machine learning problems optimize an objective that must be measured with noise. The primary method is a first order stochastic gradient descent using one or more Monte Carlo (MC) samples at each step. There are settings where ill-conditioning makes second order methods such as limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) more effective. We study the use of randomized quasi-Monte Carlo (RQMC) sampling for such problems. When MC sampling has a root mean squared error (RMSE) of O(n(-1/2)) then RQMC has an RMSE of o(n(-1/2)) that can be close to O(n(-3/2)) in favorable settings. We prove that improved sampling accuracy translates directly to improved optimization. In our empirical investigations for variational Bayes, using RQMC with stochastic quasi-Newton method greatly speeds up the optimization, and sometimes finds a better parameter value than MC does.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706449200001,0
J,"Lu, F; Maggioni, M; Tang, S",,,,"Lu, Fei; Maggioni, Mauro; Tang, Sui",,,Learning interaction kernels in heterogeneous systems of agents from multiple trajectories,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Systems of interacting particles, or agents, have wide applications in many disciplines, including Physics, Chemistry, Biology and Economics. These systems are governed by interaction laws, which are often unknown: estimating them from observation data is a fundamental task that can provide meaningful insights and accurate predictions of the behaviour of the agents. In this paper, we consider the inverse problem of learning interaction laws given data from multiple trajectories, in a nonparametric fashion, when the interaction kernels depend on pairwise distances. We establish a condition for learnability of interaction kernels, and construct an estimator based on the minimization of a suitably regularized least squares functional, that is guaranteed to converge, in a suitable L-2 space, at the optimal min-max rate for 1-dimensional nonparametric regression. We propose an efficient learning algorithm to construct such estimator, which can be implemented in parallel for multiple trajectories and is therefore well-suited for the high dimensional, big data regime. Numerical simulations on a variety examples, including opinion dynamics, predator-prey and swarm dynamics and heterogeneous particle dynamics, suggest that the learnability condition is satisfied in models used in practice, and the rate of convergence of our estimator is consistent with the theory. These simulations also suggest that our estimators are robust to noise in the observations, and can produce accurate predictions of trajectories in large time intervals, even when they are learned from observations in short time intervals.",,,,,,"Lu, Fei/0000-0001-6842-7922; Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500032,0
J,"Petzka, H; Sminchisescu, C",,,,"Petzka, Henning; Sminchisescu, Cristian",,,Non-attracting Regions of Local Minima in Deep and Wide Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications. Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima. Despite recent progress, the reason for this outcome is not fully understood. Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good? We provide a construction to show that suboptimal local minima (i.e., non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions. The local minima obtained by our construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve. For extremely wide neural networks of decreasing width after the wide layer, we prove that every suboptimal local minimum belongs to such a connected set. This provides a partial explanation for the successful application of deep neural networks. In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700318500001,0
J,"Yu, SY; Chhetri, SR; Canedo, A; Goyal, P; Al Faruque, MA",,,,"Yu, Shih-Yuan; Chhetri, Sujit Rokka; Canedo, Arquimedes; Goyal, Palash; Al Faruque, Mohammad Abdullah",,,Pykg2vec: A Python Library for Knowledge Graph Embedding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pykg2vec is a Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 25 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of PyTorch and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, evaluation of KGE tasks, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-0/pykg2vec(dagger).",,,,,,"Goyal, Palash/0000-0003-2455-2160",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500016,0
J,"Azais, R; Ingels, F",,,,"Azais, Romain; Ingels, Florian",,,The Weight Function in the Subtree Kernel is Decisive,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tree data are ubiquitous because they model a large variety of situations, e.g., the architecture of plants, the secondary structure of RNA, or the hierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data is difficult per se. In this paper, we focus on the subtree kernel that is a convolution kernel for tree data introduced by Vishwanathan and Smola in the early 2000's. More precisely, we investigate the influence of the weight function from a theoretical perspective and in real data applications. We establish on a 2-classes stochastic model that the performance of the subtree kernel is improved when the weight of leaves vanishes, which motivates the definition of a new weight function, learned from the data and not fixed by the user as usually done. To this end, we define a unified framework for computing the subtree kernel from ordered or unordered trees, that is particularly suitable for tuning parameters. We show through eight real data classification problems the great efficiency of our approach, in particular for small data sets, which also states the high importance of the weight function. Finally, a visualization tool of the significant features is derived.",,,,,,"Ingels, Florian/0000-0002-8556-0087",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000019,0
J,"Bilgrau, AE; Peeters, CFW; Eriksen, PS; Bogsted, M; van Wieringen, WN",,,,"Bilgrau, Anders Ellern; Peeters, Carel F. W.; Eriksen, Poul Svante; Bogsted, Martin; van Wieringen, Wessel N.",,,Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of jointly estimating multiple inverse covariance matrices from high-dimensional data consisting of distinct classes. An l(2)-penalized maximum likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other l(2) -penalized estimators as special cases. In addition, the approach allows specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and subtypes of diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 12 large-scale gene expression data sets of diffuse large B-cell lymphoma subtypes. The estimator and its related procedures are incorporated into the R-package rags2ridges.",,,,,"van Wieringen, Wessel N/F-9850-2016","van Wieringen, Wessel N/0000-0002-5100-9123; Peeters, Carel/0000-0001-5766-9969; Bogsted, Martin/0000-0001-9192-1814",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,26,,,,,,,,,,,,,,,WOS:000520962000001,0
J,"Grunwald, PD; Mehta, NA",,,,"Grunwald, Peter D.; Mehta, Nishant A.",,,Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to eta-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate eta is set correctly. For general loss functions, our bounds rely on two separate conditions: the v -GRIP(generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter v in the v-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the v-GRIP conditions generalize; favorable v in combination with small model complexity leads to (O) over tilde (1/n) rates. The witness condition allows us to connect the excess risk to an annealed version thereof, by which we generalize several previous results connecting Hellinger and Renyi divergence to KL divergence.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000008,0
J,"Kandasamy, K; Vysyaraju, KR; Neiswanger, W; Paria, B; Collins, CR; Schneider, J; Poczos, B; Xing, EP",,,,"Kandasamy, Kirthevasan; Vysyaraju, Karun Raju; Neiswanger, Willie; Paria, Biswajit; Collins, Christopher R.; Schneider, Jeff; Poczos, Barnabas; Xing, Eric P.",,,Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly. github . io.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,78,,,,,,,,,,,,,,,WOS:000542194600004,0
J,"Liu, Y; Fang, ZY; He, YB; Geng, Z; Liu, CC",,,,"Liu, Yue; Fang, Zhuangyan; He, Yangbo; Geng, Zhi; Liu, Chunchen",,,Local Causal Network Learning for Finding Pairs of Total and Direct Effects,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In observational studies, it is important to evaluate not only the total effect but also the direct and indirect effects of a treatment variable on a response variable. In terms of local structural learning of causal networks, we try to find all possible pairs of total and direct causal effects, which can further be used to calculate indirect causal effects. An intuitive global learning approach is first to find an essential graph over all variables representing all Markov equivalent causal networks, and then enumerate all equivalent networks and estimate a pair of the total and direct effects for each of them. However, it could be inefficient to learn an essential graph and enumerate equivalent networks when the true causal graph is large. In this paper, we propose a local learning approach instead. In the local learning approach, we first learn locally a chain component containing the treatment. Then, if necessary, we learn locally a chain component containing the response. Next, we locally enumerate all possible pairs of the treatment's parents and the response's parents. Finally based on these pairs, we find all possible pairs of total and direct effects of the treatment on the response.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,148,,,,,,,,,,,,,,,WOS:000570103700001,0
J,"Rainforth, T; Golinski, A; Wood, F; Zaidi, S",,,,"Rainforth, Tom; Golinski, Adam; Wood, Frank; Zaidi, Sheheryar",,,Target-Aware Bayesian Inference: How to Beat Optimal Conventional Estimators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions a computational pipeline that is inefficient when the target function(s) are known upfront. We address this inefficiency by introducing a framework for target-aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard O(1/N) Monte Carlo rate, potentially producing rates as fast as O(1/N-2). We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target aware inference scheme and demonstrate the substantial benefits this can yield.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,88,,,,,,,,,,,,,,,WOS:000545026600001,0
J,"Stich, SU; Karimireddy, SP",,,,"Stich, Sebastian U.; Karimireddy, Sai Praneeth",,,The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Updates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze (stochastic) gradient descent (SGD) with delayed updates on smooth quasi-convex and non-convex functions and derive concise, non-asymptotic, convergence rates. We show that the rate of convergence in all cases consists of two terms: (i) a stochastic term which is not affected by the delay, and (ii) a higher order deterministic term which is only linearly slowed down by the delay. Thus, in the presence of noise, the effects of the delay become negligible after a few iterations and the algorithm converges at the same optimal rate as standard SGD. This result extends a line of research that showed similar results in the asymptotic regime or for strongly-convex quadratic functions only. We further show similar results for SGD with more intricate form of delayed gradients-compressed gradients under error compensation and for local SGD where multiple workers perform local steps before communicating with each other. In all of these settings, we improve upon the best known rates. These results show that SGD is robust to compressed and/or delayed stochastic gradient updates. This is in particular important for distributed parallel implementations, where asynchronous and communication efficient methods are the key to achieve linear speedups for optimization with multiple devices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,237,,,,,,,,,,,,,,,WOS:000608908400001,0
J,"Ward, R; Wu, XX; Bottou, L",,,,"Ward, Rachel; Wu, Xiaoxia; Bottou, Leon",,,AdaGrad stepsizes: Sharp convergence over nonconvex landscapes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization. We bridge this gap by providing theoretical guarantees for the convergence of AdaGrad for smooth, nonconvex functions. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the O(log(N)/root N) rate in the stochastic setting, and at the optimal O(1/N) rate in the batch (non-stochastic) setting - in this sense, our convergence guarantees are sharp. In particular, the convergence of AdaGrad-Norm is robust to the choice of all hyperparameters of the algorithm, in contrast to stochastic gradient descent whose convergence depends crucially on tuning the step-size to the (generally unknown) Lipschitz smoothness constant and level of stochastic noise on the gradient. Extensive numerical experiments are provided to corroborate our theoretical findings; moreover, the experiments suggest that the robustness of AdaGrad-Norm extends to the models in deep learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,219,,,,,,,,,,,,,,,WOS:000605739400001,0
J,"Yu, H; Neely, MJ",,,,"Yu, Hao; Neely, Michael J.",,,A Low Complexity Algorithm with O(root T) Regret and O(1) Constraint Violations for Online Convex Optimization with Long Term Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve O(root T) regret and O(T-3/4) constraint violations for general problems and another algorithm to achieve an O(T-2/3) bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve O(T-max{theta,T-1-theta}) regret and O (T1-theta/2) constraint violations where theta is an element of(0, 1). The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an O (root T) regret bound with O(1) constraint violations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300001,0
J,"Zhou, DR; Xu, P; Gu, QQ",,,,"Zhou, Dongruo; Xu, Pan; Gu, Quanquan",,,Stochastic Nested Variance Reduction for Nonconvex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study nonconvex optimization problems, where the objective function is either an average of n nonconvex functions or the expectation of some stochastic function. We propose a new stochastic gradient descent algorithm based on nested variance reduction, namely, Stochastic Nested Variance-Reduced Gradient descent (SNVRG). Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K 1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, SNVRG converges to an 6-approximate first-order stationary point within O (n A 6-2 + 63 A n1/262)1 number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG 0(n+n2/36-2) and that of SCSG 0(n A 6-2 +6-1 /3 A n2/36-2). For gradient dominated functions, SNVRG also achieves better gradient complexity than the state-of-the-art algorithms. Based on SNVRG, we further propose two algorithms that can find local minima faster than state-of-the-art algorithms in both finite-sum and general stochastic (online) nonconvex optimization. In particular, for finite-sum optimization problems, the proposed SNVRG Neon2finite algorithm achieves o(n1/26-2 n3/46H7/2) gradient complexity to converge to an (6, 6H) -second-order stationary point, which outperforms SVRG+Neon2fin'te (Allen-Zhu and Li, 2018), the best existing algorithm, in a wide regime. For general stochastic optimization problems, the proposed SNVRG Neon2'nlmne achieves 6(6-3 + 6H-5 +6-26H-3) gradient complexity, which is better than both SVRG Neon2'1' (Allen-Zhu and Li, 2018) and Natasha2 (Allen-Zhu, 2018a) in certain regimes. Thorough experimental results on different nonconvex optimization problems back up our theory.",,,,,"Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022; Zhou, Dongruo/GYJ-3503-2022","Xu, Pan/0000-0002-2559-8622; ",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,103,,,,,,,,,,,,,,,WOS:000546624000001,0
J,"Elser, V; Schmidt, D; Yedidia, J",,,,"Elser, Veit; Schmidt, Dan; Yedidia, Jonathan",,,Monotone Learning with Rectified Wire Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new neural network model, together with a tractable and monotone online learning algorithm. Our model describes feed-forward networks for classification, with one output node for each class. The only nonlinear operation is rectification using a ReLU function with a bias. However, there is a rectifier on every edge rather than at the nodes of the network. There are also weights, but these are positive, static, and associated with the nodes. Our rectified wire networks are able to represent arbitrary Boolean functions. Only the bias parameters, on the edges of the network, are learned. Another departure in our approach, from standard neural networks, is that the loss function is replaced by a constraint. This constraint is simply that the value of the output node associated with the correct class should be zero. Our model has the property that the exact norm-minimizing parameter update, required to correctly classify a training item, is the solution to a quadratic program that can be computed with a few passes through the network. We demonstrate a training algorithm using this update, called sequential deactivation (SDA), on MNIST and some synthetic datasets. Upon adopting a natural choice for the nodal weights, SDA has no hyperparameters other than those describing the network structure. Our experiments explore behavior with respect to network size and depth in a family of sparse expander networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,27,,,,,,,,,,,,,,,WOS:000458669800001,0
J,"Pfister, N; Weichwald, S; Buhlmann, P; Scholkopf, B",,,,"Pfister, Niklas; Weichwald, Sebastian; Buhlmann, Peter; Schoelkopf, Bernhard",,,Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015; Pfister, Niklas/0000-0001-6203-9777; Weichwald, Sebastian/0000-0003-0169-7244",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,147,,,,,,,,,,,,,,,WOS:000491132200011,0
J,"Saberian, M; Vasconcelos, N",,,,"Saberian, Mohammad; Vasconcelos, Nuno",,,"Multiclass Boosting: Margins, Codewords, Losses, and Algorithms",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of multiclass boosting is considered. A new formulation is presented, combining multi-dimensional predictors, multi-dimensional real-valued codewords, and proper multiclass margin loss functions. This leads to a number of contributions, such as maximum capacity code-word sets, a family of proper and margin enforcing losses, denoted as gamma - phi losses, and two new multiclass boosting algorithms. These are descent procedures on the functional space spanned by a set of weak learners. The first, CD-MCBoost, is a coordinate descent procedure that updates one predictor component at a time. The second, GD-MCBoost, a gradient descent procedure that updates all components jointly. Both MCBoost algorithms are defined with respect to a gamma - phi loss and can reduce to classical boosting procedures (such as AdaBoost and LogitBoost) for binary problems. Beyond the algorithms themselves, the proposed formulation enables a unified treatment of many previous multiclass boosting algorithms. This is used to show that the latter implement different combinations of optimization strategy, codewords, weak learners, and loss function, highlighting some of their deficiencies. It is shown that no previous method matches the support of MCBoost for real codewords of maximum capacity, a proper margin-enforcing loss function, and any family of multidimensional predictors and weak learners. Experimental results confirm the superiority of MCBoost, showing that the two proposed MCBoost algorithms outperform comparable prior methods on a number of datasets.",,,,,,"Vasconcelos, Nuno/0000-0002-9024-4302",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,137,,,,,,,,,,,,,,,WOS:000491132200001,0
J,"Zheng, ZM; Bahadori, MT; Liu, Y; Lv, JC",,,,"Zheng, Zemin; Bahadori, M. Taha; Liu, Yan; Lv, Jinchi",,,Scalable Interpretable Multi-Response Regression via SEED,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse reduced-rank regression is an important tool for uncovering meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-r sparse singular value decomposition from a generalized eigenvalue problem to find the optimal low-rank and sparse matrix estimate. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Moreover, SEED employs only basic matrix operations that can be efficiently parallelized in high performance computing devices. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.",,,,,"liu, yan/HGV-1365-2022","LIU, Yan/0000-0003-4242-4840",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,107,,,,,,,,,,,,,,,WOS:000476623400001,0
J,"Arunachalam, S; de Wolf, R",,,,"Arunachalam, Srinivasan; de Wolf, Ronald",,,Optimal Quantum Sample Complexity of Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In learning theory, the VC dimension of a concept class C is the most common way to measure its richness. A fundamental result says that the number of examples needed to learn an unknown target concept c is an element of C under an unknown distribution D, is tightly determined by the VC dimension d of the concept class C. Specifically, in the PAC model circle minus(d/epsilon + log(1/(delta)/epsilon) examples are necessary and sufficient for a learner to output, with probability 1 - delta, a hypothesis h that is epsilon-close to the target concept c (measured under D). In the related agnostic model, where the samples need not come from a c is an element of C, we know that circle minus(d/epsilon(2) + log(1/(delta)/epsilon(2)) examples are necessary and sufficient to output an hypothesis h epsilon C whose error is at most E worse than the error of the best concept in C. Here we analyze quantum sample complexity, where each example is a coherent quantum state. This model was introduced by Bshouty and Jackson (1999), who showed that quantum examples are more powerful than classical examples in some fixed-distribution settings. However, Atici and Servedio (2005), improved by Zhang (2010), showed that in the PAC setting (where the learner has to succeed for every distribution), quantum examples cannot be much more powerful: the required number of quantum examples is Omega(d(1-eta)/epsilon + d + log (1/(delta)/epsilon) for arbitrarily small constant eta > 0. Our main result is that quantum and classical sample complexity are in fact equal up to constant factors in both the PAC and agnostic models. We give two proof approaches. The first is a fairly simple information-theoretic argument that yields the above two classical bounds and yields the same bounds for quantum sample complexity up to a log(d/epsilon) factor. We then give a second approach that avoids the log-factor loss, based on analyzing the behavior of the Pretty Good Measurement on the quantum state-identification problems that correspond to learning. This shows classical and quantum sample complexity are equal up to constant factors for every concept class C.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,71,,,,,,,,,,,,,,,WOS:000452056200001,0
J,"Chen, LK; Wu, WB",,,,"Chen, Likai; Wu, Wei Biao",,,Concentration inequalities for empirical processes of linear time series,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper considers suprema of empirical processes for linear time series indexed by functional classes. We derive an upper bound for the tail probability of the suprema under conditions on the size of the function class, the sample size, temporal dependence and the moment conditions of the underlying time series. Due to the dependence and heavy-tailness, our tail probability bound is substantially different from those classical exponential bounds obtained under the independence assumption in that it involves an extra polynomial decaying term. We allow both short-and long-range dependent processes. For empirical processes indexed by half intervals, our tail probability inequality is sharp up to a multiplicative constant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,231,,,,,,,,,,,,,,,WOS:000440886100001,0
J,"Grunewalder, S",,,,"Grunewalder, Steffen",,,Compact Convex Projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the usefulness of conditional gradient like methods for determining projections onto convex sets, in particular, projections onto naturally arising convex sets in reproducing kernel Hilbert spaces. Our work is motivated by the recently introduced kernel herding algorithm which is closely related to the Conditional Gradient Method (CGM). It is known that the herding algorithm converges with a rate of 1/t, where t counts the number of iterations, when a point in the interior of a convex set is approximated. We generalize this result and we provide a necessary and sufficient condition for the algorithm to approximate projections with a rate of 1/t. The CGM, which is in general vastly superior to the herding algorithm, achieves only an inferior rate of 1/root t in this setting. We study the usefulness of such projection algorithms further by exploring ways to use these for solving concrete machine learning problems. In particular, we derive non-parametric regression algorithms which use at their core a slightly modified kernel herding algorithm to determine projections. We derive bounds to control approximation errors of these methods and we demonstrate via experiments that the developed regressors are en-par with state-of-the-art regression algorithms for large scale problems.",,,,,,"Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,219,,,,,,,,,,,,,,,WOS:000438188100001,0
J,"Guo, WL; Wei, HK; Ong, YS; Hervas, JR; Zhao, JS; Wang, H; Zhang, KJ",,,,"Guo, Weili; Wei, Haikun; Ong, Yew-Soon; Hervas, Jaime Rubio; Zhao, Junsheng; Wang, Hai; Zhang, Kanjian",,,Numerical Analysis near Singularities in RBF Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The existence of singularities often affects the learning dynamics in feedforward neural networks. In this paper, based on theoretical analysis results, we numerically analyze the learning dynamics of radial basis function (RBF) networks near singularities to understand to what extent singularities influence the learning dynamics. First, we show the explicit expression of the Fisher information matrix for RBF networks. Second, we demonstrate through numerical simulations that the singularities have a significant impact on the learning dynamics of RBF networks. Our results show that overlap singularities mainly have influence on the low dimensional RBF networks and elimination singularities have a more significant impact to the learning processes than overlap singularities in both low and high dimensional RBF networks, whereas the plateau phenomena are mainly caused by the elimination singularities. The results can also be the foundation to investigate the singular learning dynamics in deep feedforward neural networks.",,,,,"Ong, Yew-Soon/A-3733-2011","Rubio Hervas, Jaime/0000-0002-3249-9788; Ong, Yew-Soon/0000-0002-4480-169X",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,1,,,,,,,,,,,,,,,WOS:000443221400001,0
J,"Hensman, J; Durrande, N; Solin, A",,,,"Hensman, James; Durrande, Nicolas; Solin, Arno",,,Variational Fourier Features for Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.",,,,,"Solin, Arno/G-6859-2012","Solin, Arno/0000-0002-0958-7886",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,52,151,,,,,,,,,,,,,,,WOS:000431708300001,0
J,"Hubara, I; Courbariaux, M; Soudry, D; El-Yaniv, R; Bengio, Y",,,,"Hubara, Itay; Courbariaux, Matthieu; Soudry, Daniel; El-Yaniv, Ran; Bengio, Yoshua",,,Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a method to train Quantized Neural Networks (QNNs) neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,187,,,,,,,,,,,,,,,WOS:000435449800001,0
J,"Perkovic, E; Textor, J; Kalisch, M; Maathuis, MH",,,,"Perkovic, Emilija; Textor, Johannes; Kalisch, Markus; Maathuis, Marloes H.",,,Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs.",,,,,"Textor, Johannes/E-1792-2016","Textor, Johannes/0000-0002-0459-9458; Perkovic, Emilija/0000-0003-3198-9213",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,2018,,,,,,,,,,,,,,,WOS:000438188900001,0
J,"van de Geer, S",,,,"van de Geer, Sara",,,On Tight Bounds for the Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present upper and lower bounds for the prediction error of the Lasso. For the case of random Gaussian design, we show that under mild conditions the prediction error of the Lasso is up to smaller order terms dominated by the prediction error of its noiseless counterpart. We then provide exact expressions for the prediction error of the latter, in terms of compatibility constants. Here, we assume the active components of the underlying regression function satisfy some betamin condition. For the case of fixed design, we provide upper and lower bounds, again in terms of compatibility constants. As an example, we give an up to a logarithmic term tight bound for the least squares estimator with total variation penalty.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,46,,,,,,,,,,,,,,,WOS:000448369100001,0
J,"Wang, YN; Singh, A",,,,"Wang, Yining; Singh, Aarti",,,Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns.",,,,,,"Wang, Yining/0000-0001-9410-0392",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,42,,,,,,,,,,,,,,,,WOS:000433251300001,0
J,"Yang, TB; Lin, QH",,,,"Yang, Tianbao; Lin, Qihang",,,RSG: Beating Subgradient Method without Smoothness and Strong Convexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the efficiency of a Restarted SubGradient (RSG) method that periodically restarts the standard subgradient method (SG). We show that, when applied to a broad class of convex optimization problems, RSG method can find an is an element of -optimal solution with a lower complexity than the SG method. In particular, we first show that RSG can reduce the dependence of SG's iteration complexity on the distance between the initial solution and the optimal set to that between the is an element of -level set and the optimal set multiplied by a logarithmic factor. Moreover, we show the advantages of RSG over SG in solving a broad family of problems that satisfy a local error bound condition, and also demonstrate its advantages for three specific families of convex optimization problems with different power constants in the local error bound condition. (a) For the problems whose epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with local quadratic growth property in the 6-sublevel set, RSG has an O(1/is an element of log(1/is an element of)) iteration complexity. (c) For the problems that admit a local Kurdyka-Lojasiewicz property with a power constant of beta is an element of E[0, 1), RSG has an O(1/is an element of(2 beta) log(1/is an element of)) iteration complexity. The novelty of our analysis lies at exploiting the lower bound of the first-order optimality residual at the 6-level set. It is this novelty that allows us to explore the local properties of functions (e.g., local quadratic growth property, local Kurdyka-Lojasiewicz property, more generally local error bound conditions) to develop the improved convergence of RSG. We also develop a practical variant of RSG enjoying faster convergence than the SG method, which can be run without knowing the involved parameters in the local error bound condition. We demonstrate the effectiveness of the proposed algorithms on several machine learning tasks including regression, classification and matrix completion.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,33,6,,,,,,,,,,,,,,,WOS:000443222500001,0
J,"Yousefi, N; Lei, YW; Kloft, M; Mollaghasemi, M; Anagnostopoulos, GC",,,,"Yousefi, Niloofar; Lei, Yunwen; Kloft, Marius; Mollaghasemi, Mansooreh; Anagnostopoulos, Georgios C.",,,Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), with which we establish sharp excess risk bounds for MTL in terms of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for any norm regularized hypothesis classes, which applies not only to MTL, but also to the standard Single-Task Learning (STL) setting. By combining both results, one can easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including-as we demonstrate-Schatten norm, group norm, and graph regularized MTL. The derived bounds reflect a relationship akin to a conservation law of asymptotic convergence rates. When compared to the rates obtained via a traditional, global Rademacher analysis, this very relationship allows for trading off slower rates with respect to the number of tasks for faster rates with respect to the number of available samples per task.",,,,,"Lei, Yunwen/V-2782-2018","Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444405700001,0
J,"Ashraphijuo, M; Wang, XD; Aggarwal, V",,,,"Ashraphijuo, Morteza; Wang, Xiaodong; Aggarwal, Vaneet",,,Rank Determination for Low-Rank Data Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, fundamental conditions on the sampling patterns have been obtained for finite completability of low-rank matrices or tensors given the corresponding ranks. In this paper, we consider the scenario where the rank is not given and we aim to approximate the unknown rank based on the location of sampled entries and some given completion. We consider a number of data models, including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor and Tucker tensor. For each of these data models, we provide an upper bound on the rank when an arbitrary low-rank completion is given. We characterize these bounds both deterministically, i.e., with probability one given that the sampling pattern satisfies certain combinatorial properties, and probabilistically, i.e., with high probability given that the sampling probability is above some threshold. Moreover, for both single-view matrix and CP tensor, we are able to show that the obtained upper bound is exactly equal to the unknown rank if the lowest-rank completion is given. Furthermore, we provide numerical experiments for the case of single-view matrix, where we use nuclear norm minimization to find a low-rank completion of the sampled data and we observe that in most of the cases the proposed upper bound on the rank is equal to the true rank.",,,,,"Aggarwal, Vaneet/A-4843-2017","Aggarwal, Vaneet/0000-0001-9131-4723",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,98,,,,,,,,,,,,,,,WOS:000412486600001,0
J,"Bardenet, R; Doucet, A; Holmes, C",,,,"Bardenet, Remi; Doucet, Arnaud; Holmes, Chris",,,On Markov chain Monte Carlo methods for tall data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number n of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than O(n) data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor.",,,,,,"Doucet, Arnaud/0000-0002-7662-419X; Bardenet, Remi/0000-0002-1094-9493",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,43,,,,,,,,,,,,,,,,WOS:000405964500001,0
J,"Bouchard-Cote, A; Doucet, A; Roth, A",,,,"Bouchard-Cote, Alexandre; Doucet, Arnaud; Roth, Andrew",,,Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at https://github.com/aroth85/pgsm",,,,,,"Doucet, Arnaud/0000-0002-7662-419X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,28,,,,,,,,,,,,,,,WOS:000399842500001,0
J,"Ding, AA; Dy, JG; Li, Y; Chang, Y",,,,"Ding, A. Adam; Dy, Jennifer G.; Li, Yi; Chang, Yale",,,A Robust-Equitable Measure for Feature Ranking and Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, not all the features used to represent data samples are important. Often only a few features are relevant for the prediction task. The choice of dependence measures often affect the final result of many feature selection methods. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable, a concept proposed by Reshef et al. (2011); that is, the dependence measure treats linear and nonlinear relationships equally. Recently, Kinney and Atwal (2014) gave a mathematical definition of self-equitability. In this paper, we introduce a new concept of robust-equitability and identify a robust-equitable copula dependence measure, the robust copula dependence (RCD) measure. RCD is based on the L-1-distance of the copula density from uniform and we show that it is equitable under both equitability definitions. We also prove theoretically that RCD is much easier to estimate than mutual information. Because of these theoretical properties, the RCD measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. Experiments on both synthetic and real-world data sets confirm the theoretical analysis, and illustrate the advantage of using the dependence measure RCD for feature selection.",,,,,"Ding, A. Adam/K-6687-2018","Ding, A. Adam/0000-0003-1397-2442",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,46,71,,,,,,,,,,,,,,,WOS:000412059000001,0
J,"Gerber, S; Maggioni, M",,,,"Gerber, Samuel; Maggioni, Mauro",,,Multiscale Strategies for Computing Optimal Transport,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high-dimensions, but are close to being intrinsically low-dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy.",,,,,"Gerber, Samuel/AAK-1278-2020; Maggioni, Martina Anna A/M-2931-2016","Gerber, Samuel/0000-0001-9521-3145; Maggioni, Martina Anna A/0000-0002-6319-8566; Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,72,,,,,,,,,,,,,,,WOS:000412059500001,0
J,"Vinogradska, J; Bischoff, B; Nguyen-Tuong, D; Peters, J",,,,"Vinogradska, Julia; Bischoff, Bastian; Duy Nguyen-Tuong; Peters, Jan",,,Stability of Controllers for Gaussian Process Dynamics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step towards widespread deployment of learning control, we provide stability analysis tools for controllers acting on dynamics represented by Gaussian processes (GPs). We consider differentiable Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For both cases, we analyze finite and infinite time horizons. Furthermore, we study the effect of disturbances on the stability results. Empirical evaluations on simulated benchmark problems support our theoretical results.",,,,,"Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,100,,,,,,,,,,,,,,,WOS:000413448300001,0
J,"Wong, RKW; Lee, TCM",,,,"Wong, Raymond K. W.; Lee, Thomas C. M.",,,Matrix Completion with Noisy Entries and Outliers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is de fined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting.",,,,,,"Lee, Thomas/0000-0001-7067-405X; Wong, Raymond K. W./0000-0001-9342-3755",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,147,,,,,,,,,,,,,,,WOS:000424550000001,0
J,"Ackerman, M; Ben-David, S",,,,"Ackerman, Margareta; Ben-David, Shai",,,A Characterization of Linkage-Based Hierarchical Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The class of linkage-based algorithms is perhaps the most popular class of hierarchical algorithms. We identify two properties of hierarchical algorithms, and prove that linkage based algorithms are the only ones that satisfy both of these properties. Our characterization clearly delineates the difference between linkage-based algorithms and other hierarchical methods. We formulate an intuitive notion of locality of a hierarchical algorithm that distinguishes between linkage-based and global hierarchical algorithms like bisecting k-means, and prove that popular divisive hierarchical algorithms produce clusterings that cannot be produced by any linkage-based algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,231,,,,,,,,,,,,,,,WOS:000391917600001,0
J,"Asbeh, N; Lerner, B",,,,"Asbeh, Nuaman; Lerner, Boaz",,,Learning Latent Variable Models by Pairwise Cluster Comparison Part II - Algorithm and Evaluation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is important for causal discovery to identify any latent variables that govern a problem and the relationships among them, given measurements in the observed world. In Part I of this paper, we were interested in learning a discrete latent variable model (LVM) and introduced the concept of pairwise cluster comparison (PCC) to identify causal relationships from clusters of data points and an overview of a two-stage algorithm for learning PCC (LPCC). First, LPCC learns exogenous latent variables and latent colliders, as well as their observed descendants, by using pairwise comparisons between data clusters in the measurement space that may explain latent causes. Second, LPCC identifies endogenous latent non-colliders with their observed children. In Part I, we showed that if the true graph has no serial connections, then LPCC returns the true graph, and if the true graph has a serial connection, then LPCC returns a pattern of the true graph. In this paper (Part II), we formally introduce the LPCC algorithm that implements the PCC concept. In addition, we thoroughly evaluate LPCC using simulated and real-world data sets in comparison to state-of-the-art algorithms. Besides using three real-world data sets, which have already been tested in learning an LVM, we also evaluate the algorithms using data sets that represent two original problems. The first problem is identifying young drivers' involvement in road accidents, and the second is identifying cellular subpopulations of the immune system from mass cytometry. The results of our evaluation show that LPCC improves in accuracy with the sample size, can learn large LVMs, and is accurate in learning compared to state-of-the-art algorithms. The code for the LPCC algorithm and data sets used in the experiments reported here are available online.",,,,,"LERNER, BOAZ/AAJ-4064-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,233,,,,,,,,,,,,,,,WOS:000391918200001,0
J,"Kpotufe, S; Boularias, A; Schultz, T; Kim, K",,,,"Kpotufe, Samory; Boularias, Abdeslam; Schultz, Thomas; Kim, Kyoungok",,,Gradients Weights improve Regression and Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In regression problems over R-d, the unknown function f often varies more in some coordinates than in others. We show that weighting each coordinate i according to an estimate of the variation of f along coordinate i -e.g. the L-1 norm of the ith-directional derivative of f -is an efficient way to significantly improve the performance of distance-based regressors such as kernel and kappa-NN regressors. The approach, termed Gradient Weighting (GW), consists of a first pass regression estimate f(n) which serves to evaluate the directional derivatives of f, and a second-pass regression estimate on the re-weighted data. The GW approach can be instantiated for both regression and classification, and is grounded in strong theoretical principles having to do with the way regression bias and variance are affected by a generic feature-weighting scheme. These theoretical principles provide further technical foundation for some existing feature-weighting heuristics that have proved successful in practice. We propose a simple estimator of these derivative norms and prove its consistency. The proposed estimator computes efficiently and easily extends to run online. We then derive a classification version of the GW approach which evaluates on real-worlds datasets with as much success as its regression counterpart.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,22,,,,,,,,,,,,,,,WOS:000391474400001,0
J,"Silva, R; Evans, R",,,,"Silva, Ricardo; Evans, Robin",,,Causal Inference through a Witness Protection Program,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the most fundamental problems in causal inference is the estimation of a causal effect when treatment and outcome are confounded. This is difficult in an observational study, because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest weak paths in an unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of path cancellations that imply conditional independencies but do not rule out the existence of confounding causal paths. The output is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice as a complement to other tools in observational studies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,56,,,,,,,,,,,,,,,WOS:000391489000001,0
J,"Sutter, T; Ganguly, A; Koeppl, H",,,,"Sutter, Tobias; Ganguly, Arnab; Koeppl, Heinz",,,A Variational Approach to Path Estimation and Parameter Inference of Hidden Diffusion Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a hidden Markov model, where the signal process, given by a diffusion, is only indirectly observed through some noisy measurements. The article develops a variational method for approximating the hidden states of the signal process given the full set of observations. This, in particular, leads to systematic approximations of the smoothing densities of the signal process. The paper then demonstrates how an efficient inference scheme, based on this variational approach to the approximation of the hidden states, can be designed to estimate the unknown parameters of stochastic differential equations. Two examples at the end illustrate the efficacy and the accuracy of the presented method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,190,,,,,,,,,,,,,,,WOS:000391826400001,0
J,"Wang, SS; Zhang, ZH; Zhang, T",,,,"Wang, Shusen; Zhang, Zhihua; Zhang, Tong",,,Towards More Efficient SPSD Matrix Approximation and CUR Matrix Decomposition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Symmetric positive semi-definite (SPSD) matrix approximation methods have been extensively used to speed up large-scale eigenvalue computation and kernel learning methods. The standard sketch based method, which we call the prototype model, produces relatively accurate approximations, but is inefficient on large square matrices. The Nystrom method is highly efficient, but can only achieve low accuracy. In this paper we propose a novel model that we call the fast SPSD matrix approximation model. The fast model is nearly as efficient as the Nystrom method and as accurate as the prototype model. We show that the fast model can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size n to achieve 1 + epsilon relative-error, whereas both the prototype model and the Nystrom method cost at least quadratic time to attain comparable error bound. Empirical comparisons among the prototype model, the Nystrom method, and our fast model demonstrate the superiority of the fast model. We also contribute new understandings of the Nystrom method. The Nystrom method is a special instance of our fast model and is approximation to the prototype model. Our technique can be straightforwardly applied to make the CUR matrix decomposition more efficiently computed without much affecting the accuracy.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,210,,,,,,,,,,,,,,,WOS:000391834000001,0
J,"Wei, E; Luke, S",,,,"Wei, Ermo; Luke, Sean",,,Lenient Learning in Independent-Learner Stochastic Cooperative Games,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce the Lenient Multiagent Reinforcement Learning 2 (LMRL2) algorithm for independent-learner stochastic cooperative games. LMRL2 is designed to overcome a pathology called relative overgeneralization, and to do so while still performing well in games with stochastic transitions, stochastic rewards, and miscoordination. We discuss the existing literature, then compare LMRL2 against other algorithms drawn from the literature which can be used for games of this kind: traditional (Distributed) Q-learning, Hysteretic Q-learning, WoLF-PHC, SOoN, and (for repeated games only) FMQ. The results show that LMRL2 is very effective in both of our measures (complete and correct policies), and is found in the top rank more often than any other technique. LMRL2 is also easy to tune: though it has many available parameters, almost all of them stay at default settings. Generally the algorithm is optimally tuned with a single parameter, if any. We then examine and discuss a number of side-issues and options for LMRL2.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,84,,,,,,,,,,,,,,,WOS:000391527100001,0
J,"Wiens, J; Guttag, J; Horvitz, E",,,,"Wiens, Jenna; Guttag, John; Horvitz, Eric",,,Patient Risk Stratification with Time-Varying Parameters: A Multitask Learning Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The proliferation of electronic health records (EHRs) frames opportunities for using machine learning to build models that help healthcare providers improve patient outcomes. However, building useful risk stratification models presents many technical challenges including the large number of factors (both intrinsic and extrinsic) influencing a patient's risk of an adverse outcome and the inherent evolution of that risk over time. We address these challenges in the context of learning a risk stratification model for predicting which patients are at risk of acquiring a Clostridium difficile infection (CDI). We take a novel data-centric approach, leveraging the contents of EHRs from nearly 50,000 hospital admissions. We show how, by adapting techniques from multitask learning, we can learn models for patient risk stratification with unprecedented classification performance. Our model, based on thousands of variables, both time-varying and time-invariant, changes over the course of a patient admission. Applied to a held out set of approximately 25,000 patient admissions, we achieve an area under the receiver operating characteristic curve of 0.81 (95% CI 0.78-0.84). The model has been integrated into the health record system at a large hospital in the US, and can be used to produce daily risk estimates for each inpatient. While more complex than traditional risk stratification methods, the widespread development and use of such data-driven models could ultimately enable cost-effective, targeted prevention strategies that lead to better patient outcomes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,79,,,,,,,,,,,,,,,WOS:000391525200001,0
J,"Kirichenko, A; van Zanten, H",,,,"Kirichenko, Alisa; van Zanten, Harry",,,Optimality of Poisson Processes Intensity Learning with Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we provide theoretical support for the so-called Sigmoidal Gaussian Cox Process approach to learning the intensity of an inhomogeneous Poisson process on a d-dimensional domain. This method was proposed by Adams, Murray and MacKay (ICML, 2009), who developed a tractable computational approach and showed in simulation and real data experiments that it can work quite satisfactorily. The results presented in the present paper provide theoretical underpinning of the method. In particular, we show how to tune the priors on the hyper parameters of the model in order for the procedure to automatically adapt to the degree of smoothness of the unknown intensity, and to achieve optimal convergence rates.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2909,2919,,,,,,,,,,,,,,,,WOS:000369888000020,0
J,"Herbster, M; Pasteris, S; Pontil, M",,,,"Herbster, Mark; Pasteris, Stephen; Pontil, Massimiliano",,,Predicting a Switching Sequence of Graph Labelings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of predicting online the labeling of a graph. We consider a novel setting for this problem in which, in addition to observing vertices and labels on the graph, we also observe a sequence of just vertices on a second graph. A latent labeling of the second graph selects one of K labelings to be active on the first graph. We propose a polynomial time algorithm for online prediction in this setting and derive a mistake bound for the algorithm. The bound is controlled by the geometric cut of the observed and latent labelings, as well as the resistance diameters of the graphs. When specialized to multitask prediction and online switching problems the bound gives new and sharper results under certain conditions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,2003,2022,,,,,,,,,,,,,,,,WOS:000369887300011,0
J,"Mackey, L; Talwalkar, A; Jordan, MI",,,,"Mackey, Lester; Talwalkar, Ameet; Jordan, Michael I.",,,Distributed Matrix Completion and Robust Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"If learning methods are to scale to the massive sizes of modern data sets, it is essential for the field of machine learning to embrace parallel and distributed computing. Inspired by the recent development of matrix factorization methods with rich theory but poor computational complexity and by the relative ease of mapping matrices onto distributed architectures, we introduce a scalable divide-and-conquer framework for noisy matrix factorization. We present a thorough theoretical analysis of this framework in which we characterize the statistical errors introduced by the divide step and control their magnitude in the conquer step, so that the overall algorithm enjoys high-probability estimation guarantees comparable to those of its base algorithm. We also present experiments in collaborative filtering and video background modeling that demonstrate the near-linear to superlinear speed-ups attainable with this approach.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X; Mackey, Lester/0000-0002-1102-0387",,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,913,960,,,,,,,,,,,,,,,,WOS:000369886300008,0
J,"Jawanpuria, P; Nath, JS; Ramakrishnan, G",,,,"Jawanpuria, Pratik; Nath, Jagarlapudi Saketha; Ramakrishnan, Ganesh",,,Generalized Hierarchical Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper generalizes the framework of Hierarchical Kernel Learning (HKL) and illustrates its utility in the domain of rule learning. HKL involves Multiple Kernel Learning over a set of given base kernels assumed to be embedded on a directed acyclic graph. This paper proposes a two-fold generalization of HKL: the first is employing a generic l(1)/l(p) block-norm regularizer (rho is an element of (1, 2]) that alleviates a key limitation of the HKL formulation. The second is a generalization to the case of multi-class, multi-label and more generally, multi-task applications. The main technical contribution of this work is the derivation of a highly specialized partial dual of the proposed generalized HKL formulation and an efficient mirror descent based active set algorithm for solving it. Importantly, the generic regularizer enables the proposed formulation to be employed in the Rule Ensemble Learning (REL) where the goal is to construct an ensemble of conjunctive propositional rules. Experiments on benchmark REL data sets illustrate the efficacy of the proposed generalizations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,617,652,,,,,,,,,,,,,,,,WOS:000369886000010,0
J,"Pitsikalis, V; Katsamanis, A; Theodorakis, S; Maragos, P",,,,"Pitsikalis, Vassilis; Katsamanis, Athanasios; Theodorakis, Stavros; Maragos, Petros",,,Multimodal Gesture Recognition via Multiple Hypotheses Rescoring,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature descriptors of the hands' movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further re fined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%.",,,,,"Katsamanis, Nassos/AAK-8217-2020",,,,,,,,,,,,,,1532-4435,,,,,FEB,2015,16,,,,,,255,284,,,,,,,,,,,,,,,,WOS:000369885800004,0
J,"Thon, M; Jaeger, H",,,,"Thon, Michael; Jaeger, Herbert",,,"Links Between Multiplicity Automata, Observable Operator Models and Predictive State Representations - a Unified Learning Framework",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic multiplicity automata (SMA) are weighted finite automata that generalize probabilistic automata. They have been used in the context of probabilistic grammatical inference. Observable operator models (OOMs) are a generalization of hidden Markov models, which in turn are models for discrete-valued stochastic processes and are used ubiquitously in the context of speech recognition and bio-sequence modeling. Predictive state representations (PSRs) extend OOMs to stochastic input-output systems and are employed in the context of agent modeling and planning. We present SMA, OOMs, and PSRs under the common framework of sequential systems, which are an algebraic characterization of multiplicity automata, and examine the precise relationships between them. Furthermore, we establish a unified approach to learning such models from data. Many of the learning algorithms that have been proposed can be understood as variations of this basic learning scheme, and several turn out to be closely related to each other, or even equivalent.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2015,16,,,,,,103,147,,,,,,,,,,,,,,,,WOS:000369885500004,0
J,"Fournier-Viger, P; Gomariz, A; Gueniche, T; Soltani, A; Wu, CW; Tseng, VS",,,,"Fournier-Viger, Philippe; Gomariz, Antonio; Gueniche, Ted; Soltani, Azadeh; Wu, Cheng-Wei; Tseng, Vincent S.",,,SPMF: A Java Open-Source Pattern Mining Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list.",,,,,"Fournier-viger, Philippe/W-1275-2019; Soltani, Azadeh/AAA-6000-2022","Soltani, Azadeh/0000-0003-3090-7992",,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3389,3393,,,,,,,,,,,,,,,,WOS:000353126200002,0
J,"Reece, S; Ghosh, S; Rogers, A; Roberts, S; Jennings, NR",,,,"Reece, Steven; Ghosh, Siddhartha; Rogers, Alex; Roberts, Stephen; Jennings, Nicholas R.",,,Efficient State-Space Inference of Periodic Latent Force Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent force models (LFM) are principled approaches to incorporating solutions to differential equations within non-parametric inference methods. Unfortunately, the development and application of LFMs can be inhibited by their computational cost, especially when closed-form solutions for the LFM are unavailable, as is the case in many real world problems where these latent forces exhibit periodic behaviour. Given this, we develop a new sparse representation of LFMs which considerably improves their computational efficiency, as well as broadening their applicability, in a principled way, to domains with periodic or near periodic latent forces. Our approach uses a linear basis model to approximate one generative model for each periodic force. We assume that the latent forces are generated from Gaussian process priors and develop a linear basis model which fully expresses these priors. We apply our approach to model the thermal dynamics of domestic buildings and show that it is effective at predicting day-ahead temperatures within the homes. We also apply our approach within queueing theory in which quasi-periodic arrival rates are modelled as latent forces. In both cases, we demonstrate that our approach can be implemented efficiently using state-space methods which encode the linear dynamic systems via LFMs. Further, we show that state estimates obtained using periodic latent force models can reduce the root mean squared error to 17% of that from non-periodic models and 27% of the nearest rival approach which is the resonator model (Sarkka et al., 2012; Hartikainen et al., 2012).",,,,,"Jennings, Nick/HGU-8308-2022","Jennings, Nick/0000-0003-0166-248X",,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2337,2397,,,,,,,,,,,,,,,,WOS:000344638400001,0
J,"Kolar, M; Liu, H; Xing, EP",,,,"Kolar, Mladen; Liu, Han; Xing, Eric P.",,,Graph Estimation From Multi-Attribute Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Undirected graphical models are important in a number of modern applications that involve exploring or exploiting dependency structures underlying the data. For example, they are often used to explore complex systems where connections between entities are not well understood, such as in functional brain networks or genetic networks. Existing methods for estimating structure of undirected graphical models focus on scenarios where each node represents a scalar random variable, such as a binary neural activation state or a continuous mRNA abundance measurement, even though in many real world problems, nodes can represent multivariate variables with much richer meanings, such as whole images, text documents, or multi-view feature vectors. In this paper, we propose a new principled framework for estimating the structure of undirected graphical models from such multivariate (or multi-attribute) nodal data. The structure of a graph is inferred through estimation of non-zero partial canonical correlation between nodes. Under a Gaussian model, this strategy is equivalent to estimating conditional independencies between random vectors represented by the nodes and it generalizes the classical problem of covariance selection (Dempster, 1972). We relate the problem of estimating non-zero partial canonical correlations to maximizing a penalized Gaussian likelihood objective and develop a method that efficiently maximizes this objective. Extensive simulation studies demonstrate the effectiveness of the method under various conditions. We provide illustrative applications to uncovering gene regulatory networks from gene and protein pro files, and uncovering brain connectivity graph from positron emission tomography data. Finally, we provide sufficient conditions under which the true graphical structure can be recovered correctly.",,,,,"Liu, Han/P-7105-2018",,,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1713,1750,,,,,,,,,,,25620892,,,,,WOS:000344638100004,0
J,"Mohan, K; Fazel, M",,,,"Mohan, Karthik; Fazel, Maryam",,,Iterative Reweighted Algorithms for Matrix Rank Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of minimizing the rank of a matrix subject to affine constraints has applications in several areas including machine learning, and is known to be NP-hard. A tractable relaxation for this problem is nuclear norm (or trace norm) minimization, which is guaranteed to find the minimum rank matrix under suitable assumptions. In this paper, we propose a family of Iterative Reweighted Least Squares algorithms IRLS-p (with 0 <= p <= 1), as a computationally efficient way to improve over the performance of nuclear norm minimization. The algorithms can be viewed as (locally) minimizing certain smooth approximations to the rank function. When p = 1, we give theoretical guarantees similar to those for nuclear norm minimization, that is, recovery of low-rank matrices under certain assumptions on the operator defining the constraints. For p < 1, IRLS-p shows better empirical performance in terms of recovering low-rank matrices than nuclear norm minimization. We provide an efficient implementation for IRLS-p, and also present a related family of algorithms, sIRLS-p. These algorithms exhibit competitive run times and improved recovery when compared to existing algorithms for random instances of the matrix completion problem, as well as on the MovieLens movie recommendation data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3441,3473,,,,,,,,,,,,,,,,WOS:000313200200010,0
J,"Kiraly, FJ; von Bunau, P; Meinecke, FC; Blythe, DAJ; Muller, KR",,,,"Kiraly, Franz J.; von Buenau, Paul; Meinecke, Frank C.; Blythe, Duncan A. J.; Mueller, Klaus-Robert",,,Algebraic Geometric Comparison of Probability Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel algebraic algorithmic framework for dealing with probability distributions represented by their cumulants such as the mean and covariance matrix. As an example, we consider the unsupervised learning problem of finding the subspace on which several probability distributions agree. Instead of minimizing an objective function involving the estimated cumulants, we show that by treating the cumulants as elements of the polynomial ring we can directly solve the problem, at a lower computational cost and with higher accuracy. Moreover, the algebraic viewpoint on probability distributions allows us to invoke the theory of algebraic geometry, which we demonstrate in a compact proof for an identifiability criterion.",,,,,"Muller, Klaus R/C-3196-2013; Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,855,903,,,,,,,,,,,,,,,,WOS:000303772100015,0
J,"Abrahamsen, TJ; Hansen, LK",,,,"Abrahamsen, Trine Julie; Hansen, Lars Kai",,,A Cure for Variance Inflation in High Dimensional Kernel Principal Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Small sample high-dimensional principal component analysis (PCA) suffers from variance inflation and lack of generalizability. It has earlier been pointed out that a simple leave-one-out variance renormalization scheme can cure the problem. In this paper we generalize the cure in two directions: First, we propose a computationally less intensive approximate leave-one-out estimator, secondly, we show that variance inflation is also present in kernel principal component analysis (kPCA) and we provide a non-parametric renormalization scheme which can quite efficiently restore generalizability in kPCA. As for PCA our analysis also suggests a simplified approximate expression.",,,,,,"Hansen, Lars Kai/0000-0003-0442-5877",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,2027,2044,,,,,,,,,,,,,,,,WOS:000293757200009,0
J,"Sinz, F; Bethge, M",,,,"Sinz, Fabian; Bethge, Matthias",,,L-p-Nested Symmetric Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce a new family of probability densities called L-p-nested symmetric distributions. The common property, shared by all members of the new class, is the same functional form rho(x)= (rho) over tilde (f(x)), where f is a nested cascade of L-p-norms parallel to x parallel to(p) = (Sigma vertical bar x(i)vertical bar(p))(1/p). L-p-nested symmetric distributions thereby are a special case of nu-spherical distributions for which f is only required to be positively homogeneous of degree one. While both, nu-spherical and L-p-nested symmetric distributions, contain many widely used families of probability models such as the Gaussian, spherically and elliptically symmetric distributions, L-p-spherically symmetric distributions, and certain types of independent component analysis (ICA) and independent subspace analysis (ISA) models, nu-spherical distributions are usually computationally intractable. Here we demonstrate that L-p-nested symmetric distributions are still computationally feasible by deriving an analytic expression for its normalization constant, gradients for maximum likelihood estimation, analytic expressions for certain types of marginals, as well as an exact and efficient sampling algorithm. We discuss the tight links of L-p-nested symmetric distributions to well known machine learning methods such as ICA, ISA and mixed norm regularizers, and introduce the nested radial factorization algorithm (NRF), which is a form of non-linear ICA that transforms any linearly mixed, non-factorial L-p-nested symmetric source into statistically independent signals. As a corollary, we also introduce the uniform distribution on the L-p-nested unit sphere.",,,,,"Sinz, Fabian/E-6708-2010; Bethge, Matthias/B-1554-2008","Sinz, Fabian/0000-0002-1348-9736; ",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3409,3451,,,,,,,,,,,,,,,,WOS:000286637200005,0
J,"Zhang, ZH; Dai, GA; Xu, CF; Jordan, MI",,,,"Zhang, Zhihua; Dai, Guang; Xu, Congfu; Jordan, Michael I.",,,"Regularized Discriminant Analysis, Ridge Regression and Beyond",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fisher linear discriminant analysis (FDA) and its kernel extension-kernel discriminant analysis (KDA)-are well known methods that consider dimensionality reduction and classification jointly. While widely deployed in practical problems, there are still unresolved issues surrounding their efficient implementation and their relationship with least mean squares procedures. In this paper we address these issues within the framework of regularized estimation. Our approach leads to a flexible and efficient implementation of FDA as well as KDA. We also uncover a general relationship between regularized discriminant analysis and ridge regression. This relationship yields variations on conventional FDA based on the pseudoinverse and a direct equivalence to an ordinary least squares estimator.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2199,2228,,,,,,,,,,,,,,,,WOS:000282523300006,0
J,"Sonnenburg, S; Ratsch, G; Henschel, S; Widmer, C; Behr, J; Zien, A; de Bona, F; Binder, A; Gehl, C; Franc, V",,,,"Sonnenburg, Soeren; Raetsch, Gunnar; Henschel, Sebastian; Widmer, Christian; Behr, Jonas; Zien, Alexander; de Bona, Fabio; Binder, Alexander; Gehl, Christian; Franc, Vojtech",,,The SHOGUN Machine Learning Toolbox,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hiddenMarkov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. SHOGUN is implemented in C++ and interfaces to MATLAB (TM), R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.",,,,,,"Ratsch, Gunnar/0000-0001-5486-8532; Binder, Alexander/0000-0001-9605-6209",,,,,,,,,,,,,1532-4435,,,,,JUN,2010,11,,,,,,1799,1802,,,,,,,,,,,,,,,,WOS:000282522400001,0
J,"Sriperumbudur, BK; Gretton, A; Fukumizu, K; Scholkopf, B; Lanckriet, GRG",,,,"Sriperumbudur, Bharath K.; Gretton, Arthur; Fukumizu, Kenji; Schoelkopf, Bernhard; Lanckriet, Gert R. G.",,,Hilbert Space Embeddings and Metrics on Probability Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be defined as the distance between distribution embeddings: we denote this as gamma(k), indexed by the kernel function k that defines the inner product in the RKHS. We present three theoretical properties of gamma(k). First, we consider the question of determining the conditions on the kernel k for which gamma(k) is a metric: such k are denoted characteristic kernels. Unlike pseudometrics, a metric is zero only when two distributions coincide, thus ensuring the RKHS embedding maps all distributions uniquely (i.e., the embedding is injective). While previously published conditions may apply only in restricted circumstances (e. g., on compact domains), and are difficult to check, our conditions are straightforward and intuitive: integrally strictly positive definite kernels are characteristic. Alternatively, if a bounded continuous kernel is translation-invariant on R-d, then it is characteristic if and only if the support of its Fourier transform is the entire R-d. Second, we show that the distance between distributions under gamma(k) results from an interplay between the properties of the kernel and the distributions, by demonstrating that distributions are close in the embedding space when their differences occur at higher frequencies. Third, to understand the nature of the topology induced by g gamma(k), we relate g gamma(k) to other popular metrics on probability measures, and present conditions on the kernel k under which gamma(k) metrizes the weak topology.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Gretton, Arthur/0000-0003-3169-7624; Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1517,1561,,,,,,,,,,,,,,,,WOS:000282521500009,0
J,"Shi, JN; Yin, WT; Osher, S; Sajda, P",,,,"Shi, Jianing; Yin, Wotao; Osher, Stanley; Sajda, Paul",,,A Fast Hybrid Algorithm for Large-Scale l(1)-Regularized Logistic Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"l(1)-regularized logistic regression, also known as sparse logistic regression, is widely used in machine learning, computer vision, data mining, bioinformatics and neural signal processing. The use of l(1) regularization attributes attractive properties to the classifier, such as feature selection, robustness to noise, and as a result, classifier generality in the context of supervised learning. When a sparse logistic regression problem has large-scale data in high dimensions, it is computationally expensive to minimize the non-differentiable l(1)-norm in the objective function. Motivated by recent work (Koh et al., 2007; Hale et al., 2008), we propose a novel hybrid algorithm based on combining two types of optimization iterations: one being very fast and memory friendly while the other being slower but more accurate. Called hybrid iterative shrinkage (HIS), the resulting algorithm is comprised of a fixed point continuation phase and an interior point phase. The first phase is based completely on memory efficient operations such as matrix-vector multiplications, while the second phase is based on a truncated Newton's method. Furthermore, we show that various optimization techniques, including line search and continuation, can significantly accelerate convergence. The algorithm has global convergence at a geometric rate (a Q-linear rate in optimization terminology). We present a numerical comparison with several existing algorithms, including an analysis using benchmark data from the UCI machine learning repository, and show our algorithm is the most computationally efficient without loss of accuracy.",,,,,"Yin, Wotao/A-5472-2011","Yin, Wotao/0000-0001-6697-9731",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,713,741,,,,,,,,,,,,,,,,WOS:000277186500011,0
J,"Teo, CH; Vishwanathan, SVN; Smola, A; Le, QV",,,,"Teo, Choon Hui; Vishwanathan, S. V. N.; Smola, Alex; Le, Quoc V.",,,Bundle Methods for Regularized Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L-1 and L-2 penalties. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/epsilon) steps to e precision for general convex problems and in O(log(1/epsilon)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,311,365,,,,,,,,,,,,,,,,WOS:000277186400010,0
J,"Huang, J; Guestrin, C; Guibas, L",,,,"Huang, Jonathan; Guestrin, Carlos; Guibas, Leonidas",,,Fourier Theoretic Probabilistic Inference over Permutations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Permutations are ubiquitous in many real-world problems, such as voting, ranking, and data association. Representing uncertainty over permutations is challenging, since there are n! possibilities, and typical compact and factorized probability distribution representations, such as graphical models, cannot capture the mutual exclusivity constraints associated with permutations. In this paper, we use the low-frequency terms of a Fourier decomposition to represent distributions over permutations compactly. We present Kronecker conditioning, a novel approach for maintaining and updating these distributions directly in the Fourier domain, allowing for polynomial time bandlimited approximations. Low order Fourier-based approximations, however, may lead to functions that do not correspond to valid distributions. To address this problem, we present a quadratic program defined directly in the Fourier domain for projecting the approximation onto a relaxation of the polytope of legal marginal distributions. We demonstrate the effectiveness of our approach on a real camera-based multi-person tracking scenario.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2009,10,,,,,,997,1070,,,,,,,,,,,,,,,,WOS:000270824800001,0
J,"Bubeck, S; von Luxburg, U",,,,"Bubeck, Sebastien; von Luxburg, Ulrike",,,Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering is often formulated as a discrete optimization problem. The objective is to find, among all partitions of the data set, the best one according to some quality measure. However, in the statistical setting where we assume that the finite data set has been sampled from some underlying space, the goal is not to find the best partition of the given sample, but to approximate the true partition of the underlying space. We argue that the discrete optimization approach usually does not achieve this goal, and instead can lead to inconsistency. We construct examples which provably have this behavior. As in the case of supervised learning, the cure is to restrict the size of the function classes under consideration. For appropriate small function classes we can prove very general consistency theorems for clustering optimization schemes. As one particular algorithm for clustering with a restricted function space we introduce nearest neighbor clustering. Similar to the k-nearest neighbor classifier in supervised learning, this algorithm can be seen as a general baseline algorithm to minimize arbitrary clustering objective functions. We prove that it is statistically consistent for all commonly used clustering objective functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,657,698,,,,,,,,,,,,,,,,WOS:000270824500005,0
J,"Bromberg, F; Margaritis, D",,,,"Bromberg, Facundo; Margaritis, Dimitris",,,Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of improving the reliability of independence-based causal discovery algorithms that results from the execution of statistical independence tests on small data sets, which typically have low reliability. We model the problem as a knowledge base containing a set of independence facts that are related through Pearl's well-known axioms. Statistical tests on finite data sets may result in errors in these tests and inconsistencies in the knowledge base. We resolve these inconsistencies through the use of an instance of the class of defeasible logics called argumentation, augmented with a preference function, that is used to reason about and possibly correct errors in these tests. This results in a more robust conditional independence test, called an argumentative independence test. Our experimental evaluation shows clear positive improvements in the accuracy of argumentative over purely statistical tests. We also demonstrate significant improvements on the accuracy of causal structure discovery from the outcomes of independence tests both on sampled data from randomly generated causal models and on real-world data sets.",,,,,,"Bromberg, Facundo/0000-0003-2638-1350",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,301,340,,,,,,,,,,,,,,,,WOS:000270824200008,0
J,"Rakotomamonjy, A; Bach, FR; Canu, S; Grandvalet, Y",,,,"Rakotomamonjy, Alain; Bach, Francis R.; Canu, Stephane; Grandvalet, Yves",,,SimpleMKL,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multiple kernel learning (MKL) aims at simultaneously learning a kernel and the associated predictor in supervised learning settings. For the support vector machine, an efficient and general multiple kernel learning algorithm, based on semi-infinite linear programming, has been recently proposed. This approach has opened new perspectives since it makes MKL tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs numerous iterations for converging towards a reasonable solution. In this paper, we address the MKL problem through a weighted 2-norm regularization formulation with an additional constraint on the weights that encourages sparse kernel combinations. Apart from learning the combination, we solve a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels. We propose an algorithm, named SimpleMKL, for solving this MKL problem and provide a new insight on MKL algorithms based on mixed-norm regularization by showing that the two approaches are equivalent. We show how SimpleMKL can be applied beyond binary classification, for problems like regression, clustering (one-class classification) or multiclass classification. Experimental results show that the proposed algorithm converges rapidly and that its efficiency compares favorably to other MKL algorithms. Finally, we illustrate the usefulness of MKL for some regressors based on wavelet kernels and on some model selection problems related to multiclass classification problems.",,,,,,"Canu, Stephane/0000-0002-7602-4557",,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2491,2521,,,,,,,,,,,,,,,,WOS:000262637600004,0
J,"Biau, G; Devroye, L; Lugosi, G",,,,"Biau, Gerard; Devroye, Luc; Lugosi, Gabor",,,Consistency of Random Forests and Other Averaging Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the last years of his life, Leo Breiman promoted random forests for use in classification. He suggested using averaging as a means of obtaining good discrimination rules. The base classifiers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classifiers, including one suggested by Breiman, are not universally consistent.",,,,,,"Lugosi, Gabor/0000-0003-1614-5901",,,,,,,,,,,,,1532-4435,,,,,SEP,2008,9,,,,,,2015,2033,,,,,,,,,,,,,,,,WOS:000262637100003,0
J,"Becerra-Bonache, L; de la Higuera, C; Janodet, JC; Tantini, F",,,,"Becerra-Bonache, Leonor; de la Higuera, Colin; Janodet, Jean-Christophe; Tantini, Frederic",,,Learning Balls of Strings from Edit Corrections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th European Conference on Machine Learning (ECML 2007)/11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2007),"SEP 17-21, 2007","Warsaw Univ, Warsaw, POLAND","Warsaw Univ, Fac Math, Informat & Mech,Polish Acad Sci, Inst Comp Sci,European Off Aerosp Res & Dev,Air Force Off Sci Res,USAF Res Lab",Warsaw Univ,,,"When facing the question of learning languages in realistic settings, one has to tackle several problems that do not admit simple solutions. On the one hand, languages are usually defined by complex grammatical mechanisms for which the learning results are predominantly negative, as the few algorithms are not really able to cope with noise. On the other hand, the learning settings themselves rely either on too simple information (text) or on unattainable one (query systems that do not exist in practice, nor can be simulated). We consider simple but sound classes of languages defined via the widely used edit distance: the balls of strings. We propose to learn them with the help of a new sort of queries, called the correction queries: when a string is submitted to the Oracle, either she accepts it if it belongs to the target language, or she proposes a correction, that is, a string of the language close to the query with respect to the edit distance. We show that even if the good balls are not learnable in Angluin's MAT model, they can be learned from a polynomial number of correction queries. Moreover, experimental evidence simulating a human Expert shows that this algorithm is resistant to approximate answers.",,,,,,", Colin/0000-0002-1703-9572",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1841,1870,,,,,,,,,,,,,,,,WOS:000262636800008,0
J,"Friedman, J; Hastie, T; Tibshirani, R",,,,"Friedman, Jerome; Hastie, Trevor; Tibshirani, Robert",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,175,180,,,,,,,,,,,,,,,,WOS:000256641800005,0
J,"Guermeur, Y",,,,"Guermeur, Yann",,,VC theory of large margin multi-category classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the context of discriminant analysis, Vapnik's statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in finite sets, typically the set of categories itself. The case of classes of vector-valued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a naive extension of the results devoted to the computation of dichotomies. Third, most of the classification problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classifiers is introduced. Central in this theory are generalized VC dimensions called the gamma-Psi-dimensions. First, a uniform convergence bound on the risk of the classifiers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the gamma-Psi-dimensions thanks to generalizations of Sauer's lemma, as is illustrated in the specific case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2007,8,,,,,,2551,2594,,,,,,,,,,,,,,,,WOS:000252744900002,0
J,"Pillai, NS; Wu, Q; Liang, F; Mukherjee, S; Wolpert, RL",,,,"Pillai, Natesh S.; Wu, Qiang; Liang, Feng; Mukherjee, Sayan; Wolpert, Robert L.",,,Characterizing the function space for Bayesian Kernel models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator defined as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior specifications in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and Levy processes, with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classification problem.",,,,,"Wu, Qiang/B-1620-2008; Liang, Feng/GZK-4305-2022; Liang, Fenghua/HHM-3798-2022","Wu, Qiang/0000-0002-4698-6966; Mukherjee, Sayan/0000-0002-6715-3920",,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1769,1797,,,,,,,,,,,,,,,,WOS:000252744400004,0
J,"Nilsson, R; Pena, JM; Bjorkegren, J",,,,"Nilsson, Roland; Pena, Jose M.; Bjorkegren, Johan",,,Consistent feature selection for pattern recognition in polynomial time,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze two different feature selection problems: finding a minimal feature set optimal for classification ( MINIMAL-OPTIMAL) vs. finding all features relevant to the target variable ( ALL-RELEVANT). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL-RELEVANT is much harder than MINIMAL-OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks.",,,,,"tegner, jesper N/R-5095-2017; Nilsson, Roland/AAJ-3519-2021","tegner, jesper N/0000-0002-9568-5588; Nilsson, Roland/0000-0002-6020-7498",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,589,612,,,,,,,,,,,,,,,,WOS:000247002700007,0
J,"Khardon, R; Wachman, G",,,,"Khardon, Roni; Wachman, Gabriel",,,Noise tolerant variants of the Perceptron algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A large number of variants of the Perceptron algorithm have been proposed and partially evaluated in recent work. One type of algorithm aims for noise tolerance by replacing the last hypothesis of the perceptron with another hypothesis or a vote among hypotheses. Another type simply adds a margin term to the perceptron in order to increase robustness and accuracy, as done in support vector machines. A third type borrows further from support vector machines and constrains the update function of the perceptron in ways that mimic soft-margin techniques. The performance of these algorithms, and the potential for combining different techniques, has not been studied in depth. This paper provides such an experimental study and reveals some interesting facts about the algorithms. In particular the perceptron with margin is an effective method for tolerating noise and stabilizing the algorithm. This is surprising since the margin in itself is not designed or used for noise tolerance, and there are no known guarantees for such performance. In most cases, similar performance is obtained by the voted-perceptron which has the advantage that it does not require parameter selection. Techniques using soft margin ideas are run-time intensive and do not give additional performance benefits. The results also highlight the difficulty with automatic parameter selection which is required with some of these variants.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,227,248,,,,,,,,,,,,,,,,WOS:000247002600002,0
J,"Teboulle, M",,,,"Teboulle, Marc",,,A unified continuous optimization framework for center-based clustering methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,65,102,,,,,,,,,,,,,,,,WOS:000247002500003,0
J,"Opper, M; Winther, O",,,,"Opper, M; Winther, O",,,Expectation consistent approximate inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel framework for approximations to intractable probabilistic models which is based on a free energy formulation. The approximation can be understood as replacing an average over the original intractable distribution with a tractable one. It requires two tractable probability distributions which are made consistent on a set of moments and encode different features of the original intractable distribution. In this way we are able to use Gaussian approximations for models with discrete or bounded variables which allow us to include non-trivial correlations. These are neglected in many other methods. We test the framework on toy benchmark problems for binary variables on fully connected graphs and 2D grids and compare with other methods, such as loopy belief propagation. Good performance is already achieved by using single nodes as tractable substructures. Significant improvements are obtained when a spanning tree is used instead.",,,,,,"Winther, Ole/0000-0002-1966-3205",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2177,2204,,,,,,,,,,,,,,,,WOS:000236331100011,0
J,"Wolf, L; Shashua, A",,,,"Wolf, L; Shashua, A",,,Feature selection for unsupervised and supervised inference: The emergence of sparsity in a weight-based approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,9th IEEE International Conference on Computer Vision,"OCT 13-16, 2003","NICE, FRANCE","IEEE Comp Soc, TC Pattern Anal & Machine Intelligence",,,,"The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classification tasks, for example, it is not uncommon to have 10(4) to 10(7) features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email. In this work we present a definition of relevancy based on spectral properties of the Laplacian of the features' measurement matrix. The feature selection process is then based on a continuous ranking of the features defined by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a biased non-negativity of a key matrix in the process. As a result, a simple least-squares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2005,6,,,,,,1855,1887,,,,,,,,,,,,,,,,WOS:000236330700004,0
J,"Murphy, SA",,,,"Murphy, SA",,,A generalization error for Q-learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Planning problems that involve learning a policy from a single training set of finite horizon trajectories arise in both social science and medical fields. We consider Q-learning with function approximation for this setting and derive an upper bound on the generalization error. This upper bound is in terms of quantities minimized by a Q-learning algorithm, the complexity of the approximation space and an approximation term due to the mismatch between Q-learning and the goal of learning a policy that maximizes the value function.",,,,,,"Murphy, Susan A/0000-0002-2032-4286",,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1073,1097,,,,,,,,,,,16763665,,,,,WOS:000236329900003,0
J,"Meir, R; Zhang, T",,,,"Meir, R; Zhang, T",,,Generalization error bounds for Bayesian mixture algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,"Bayesian approaches to learning and estimation have played a significant role in the Statistics literature over many years. While they are often provably optimal in a frequentist setting, and lead to excellent performance in practical applications, there have not been many precise characterizations of their performance for finite sample sizes under general conditions. In this paper we consider the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space. Similarly to what is observed in practice, our results demonstrate that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects. Our results, while being data-dependent in nature, are insensitive to the underlying model assumptions, and apply whether or not these hold. At a technical level, the approach applies to unbounded functions, constrained only by certain moment conditions. Finally, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Boosting and Bagging.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,839,860,,10.1162/1532443041424300,0,,,,,,,,,,,,,WOS:000223238800005,0
J,"Gavinsky, D",,,,"Gavinsky, D",,,Optimally-smooth adaptive boosting and application to agnostic learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a new boosting algorithm that is the first such algorithm to be both smooth and adaptive. These two features make possible performance improvements for many learning tasks whose solutions use a boosting technique. The boosting approach was originally suggested for the standard PAC model; we analyze possible applications of boosting in the context of agnostic learning, which is more realistic than the PAC model. We derive a lower bound for the final error achievable by boosting in the agnostic model and show that our algorithm actually achieves that accuracy (within a constant factor). We note that the idea of applying boosting in the agnostic model was first suggested by Ben-David, Long and Mansour (2001) and the solution they give is improved in the present paper. The accuracy we achieve is exponentially better with respect to the standard agnostic accuracy parameter beta. We also describe the construction of a boosting tandem whose asymptotic number of iterations is the lowest possible (in both gamma and epsilon) and whose smoothness is optimal in terms of O((.)). This allows adaptively solving problems whose solution is based on smooth boosting (like noise tolerant boosting and DNF membership learning), while preserving the original (non-adaptive) solution's complexity.",,,,,"Gavinsky, Dmitry/D-5438-2014",,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,101,117,,10.1162/153244304322765667,0,,,,,,,,,,,,,WOS:000221043500006,0
J,"Singer, B; Veloso, M",,,,"Singer, B; Veloso, M",,,Learning to construct fast signal processing implementations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"A single signal processing algorithm can be represented by many mathematically equivalent formulas. However, when these formulas are implemented in code and run on real machines, they have very different runtimes. Unfortunately, it is extremely difficult to model this broad performance range. Further, the space of formulas for real signal transforms is so large that it is impossible to search it exhaustively for fast implementations. We approach this search question as a control learning problem. We present a new method for learning to generate fast formulas, allowing us to intelligently search through only the most promising formulas. Our approach incorporates signal processing knowledge, hardware features, and formula performance data to learn to construct fast formulas. Our method learns from performance data for a few formulas of one size and then can construct formulas that will have the fastest runtimes possible across many sizes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,887,919,,10.1162/jmlr.2003.3.4-5.887,0,,,,,,,,,,,,,WOS:000184926200012,0
J,"Strens, MJA; Moore, AW",,,,"Strens, MJA; Moore, AW",,,Policy search using paired comparisons,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"Direct policy search is a practical way to solve reinforcement learning (RL) problems involving continuous state and action spaces. The goal becomes finding policy parameters that maximize a noisy objective function. The Pegasus method converts this stochastic optimization problem into a deterministic one, by using fixed start states and fixed random number sequences for comparing policies (Ng and Jordan, 2000). We evaluate Pegasus, and new paired comparison methods, using the mountain car problem, and a difficult pursuer-evader problem. We conclude that: (i) paired tests can improve performance of optimization procedures; (ii) several methods are available to reduce the 'overfitting' effect found with Pegasus; (iii) adapting the number of trials used for each comparison yields faster learning; (iv) pairing also helps stochastic search methods such as differential evolution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,921,950,,10.1162/jmlr.2003.3.4-5.921,0,,,,,,,,,,,,,WOS:000184926200013,0
J,"Seeger, M",,,,"Seeger, M",,,PAC-Bayesian generalisation error bounds for Gaussian process classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Approximate Bayesian Gaussian process (GP) classification techniques are powerful nonparametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distribution-free generalisation error bounds for a wide range of approximate Bayesian CP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,233,269,,10.1162/153244303765208386,0,,,,,,,,,,,,,WOS:000182488500003,0
J,"Meila, M; Jordan, MI",,,,"Meila, M; Jordan, MI",,,Learning with mixtures of trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes the mixtures-of-trees model, a probabilistic model for discrete multidimensional domains. Mixtures-of-trees generalize the probabilistic trees of Chow and Liu (1968) in a different and complementary direction to that of Bayesian networks. We present efficient algorithms for learning mixtures-of-trees models in maximum likelihood and Bayesian frameworks. We also discuss additional efficiencies that can be obtained when data are sparse, and we present data structures and algorithms that exploit such sparseness. Experimental results demonstrate the performance of the model for both density estimation and classification. We also discuss the sense in which tree-based classifiers perform an implicit form of feature selection, and demonstrate a resulting insensitivity to irrelevant attributes.",,,,,"Jordan, Michael I/C-5253-2013",,,,,,,,,,,,,,1532-4435,,,,,OCT,2001,1,1,,,,,1,48,,10.1162/153244301753344605,0,,,,,,,,,,,,,WOS:000173336700001,0
J,"Tipping, ME",,,,"Tipping, ME",,,Sparse Bayesian learning and the relevance vector machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2001,1,3,,,,,211,244,,10.1162/15324430152748236,0,,,,,,,,,,,,,WOS:000173336900003,0
J,"Wei, KX; Aviles-Rivero, A; Liang, JW; Fu, Y; Huang, H; Schonlieb, CB",,,,"Wei, Kaixuan; Aviles-Rivero, Angelica; Liang, Jingwei; Fu, Ying; Huang, Hua; Schonlieb, Carola-Bibiane",,,TFPnP: Tuning-free Plug-and-Play Proximal Algorithms with Applications to Inverse Imaging Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Plug-and-Play (PnP) is a non-convex optimization framework that combines proximal algorithms, for example, the alternating direction method of multipliers (ADMM), with advanced denoising priors. Over the past few years, great empirical success has been obtained by PnP algorithms, especially for the ones that integrate deep learning-based denoisers. However, a key problem of PnP approaches is the need for manual parameter tweaking which is essential to obtain high-quality results across the high discrepancy in imaging conditions and varying scene content. In this work, we present a class of tuning free PnP proximal algorithms that can determine parameters such as denoising strength, termination time, and other optimization-specific parameters automatically. A core part of our approach is a policy network for automated parameter search which can be effectively learned via a mixture of model-free and model-based deep reinforcement learning strategies. We demonstrate, through rigorous numerical and visual experiments, that the learned policy can customize parameters to different settings, and is often more efficient and effective than existing handcrafted criteria. Moreover, we discuss several practical considerations of PnP denoisers, which together with our learned policy yield state-of-the-art results. This advanced performance is prevalent on both linear and nonlinear exemplar inverse imaging problems, and in particular shows promising results on compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval.",,,,,,"Wei, Kaixuan/0000-0002-9887-0455",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,1,,,,,,,,,,,,,,,WOS:000766877600001,0
J,"Bakhtin, A; Deng, YT; Gross, S; Ott, M; Ranzato, M; Szlam, A",,,,"Bakhtin, Anton; Deng, Yuntian; Gross, Sam; Ott, Myle; Ranzato, Marc'Aurelio; Szlam, Arthur",,,Residual Energy-Based Models for Text,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Current large-scale auto-regressive language models (Radford et al., 2019; Liu et al., 2018; Graves, 2013) display impressive fluency and can generate convincing text. In this work we start by asking the question: Can the generations of these models be reliably distinguished from real text by statistical discriminators? We find experimentally that the answer is affirmative when we have access to the training data for the model, and guardedly affirmative even if we do not. This suggests that the auto-regressive models can be improved by incorporating the (globally normalized) discriminators into the generative process. We give a formalism for this using the Energy-Based Model framework, and show that it indeed improves the results of the generative models, measured both in terms of perplexity and in terms of human evaluation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500040,0
J,"Blom, T; van Diepen, MM; Mooij, JM",,,,"Blom, Tineke; van Diepen, Mirthe M.; Mooij, Joris M.",,,Conditional independences and causal relations implied by sets of equations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Real-world complex systems are often modelled by sets of equations with endogenous and exogenous variables. What can we say about the causal and probabilistic aspects of variables that appear in these equations without explicitly solving the equations? We make use of Simon's causal ordering algorithm (Simon, 1953) to construct a causal ordering graph and prove that it expresses the effects of soft and perfect interventions on the equations under certain unique solvability assumptions. We further construct a Markov ordering graph and prove that it encodes conditional independences in the distribution implied by the equations with independent random exogenous variables, under a similar unique solvability assumption. We discuss how this approach reveals and addresses some of the limitations of existing causal modelling frameworks, such as causal Bayesian networks and structural causal models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687037700001,0
J,"Deng, SF; Ling, SY; Strohmer, T",,,,"Deng, Shaofeng; Ling, Shuyang; Strohmer, Thomas",,,"Strong Consistency, Graph Laplacians, and the Stochastic Block Model",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Spectral clustering has become one of the most popular algorithms in data clustering and community detection. We study the performance of classical two-step spectral clustering via the graph Laplacian to learn the stochastic block model. Our aim is to answer the following question: when is spectral clustering via the graph Laplacian able to achieve strong consistency, i.e., the exact recovery of the underlying hidden communities? Our work provides an entrywise analysis (an l(infinity)-norm perturbation bound) of the Fiedler eigenvector of both the unnormalized and the normalized Laplacian associated with the adjacency matrix sampled from the stochastic block model. We prove that spectral clustering is able to achieve exact recovery of the planted community structure under conditions that match the information-theoretic limits.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,117,,,,,,,,,,,,,,,WOS:000663170400001,0
J,"Hoefler, T; Alistarh, D; Ben-Nun, T; Dryden, N; Peste, A",,,,"Hoefler, Torsten; Alistarh, Dan; Ben-Nun, Tal; Dryden, Nikoli; Peste, Alexandra",,,Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.",,,,,"Hoefler, Torsten/AAB-7478-2022","Alistarh, Dan/0000-0003-3650-940X",,,,,,,,,,,,,1532-4435,,,,,,2021,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000706836800001,0
J,"Kwon, J",,,,"Kwon, Joon",,,Refined approachability algorithms and application to regret minimization with global costs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Blackwell's approachability is a framework where two players, the Decision Maker and the Environment, play a repeated game with vector-valued payoffs. The goal of the Decision Maker is to make the average payoff converge to a given set called the target. When this is indeed possible, simple algorithms which guarantee the convergence are known. This abstract tool was successfully used for the construction of optimal strategies in various repeated games, but also found several applications in online learning. By extending an approach proposed by Abernethy et al. (2011), we construct and analyze a class of Follow the Regularized Leader algorithms (FTRL) for Blackwell's approachability which are able to minimize not only the Euclidean distance to the target set (as it is often the case in the context of Blackwell's approachability) but a wide range of distance-like quantities. This flexibility enables us to apply these algorithms to closely minimize the quantity of interest in various online learning problems. In particular, for regret minimization with l(p) global costs, we obtain the first bounds with explicit dependence in p and the dimension d.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706884000001,0
J,"Lybrand, E; Saab, R",,,,"Lybrand, Eric; Saab, Rayan",,,A Greedy Algorithm for Quantizing Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new computationally efficient method for quantizing the weights of pre trained neural networks that is general enough to handle both multi-layer perceptrons and convolutional neural networks. Our method deterministically quantizes layers in an iterative fashion with no complicated re-training required. Specifically, we quantize each neuron, or hidden unit, using a greedy path-following algorithm. This simple algorithm is equivalent to running a dynamical system, which we prove is stable for quantizing a single-layer neural network (or, alternatively, for quantizing the first layer of a multi-layer network) when the training data are Gaussian. We show that under these assumptions, the quantization error decays with the width of the layer, i.e., its level of over-parametrization. We provide numerical experiments, onY multi-layer networks, to illustrate the performance of our methods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network using ImageNet data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700317700001,0
J,"Papamakarios, G; Nalisnick, E; Rezende, DJ; Mohamed, S; Lakshminarayanan, B",,,,"Papamakarios, George; Nalisnick, Eric; Rezende, Danilo Jimenez; Mohamed, Shakir; Lakshminarayanan, Balaji",,,Normalizing Flows for Probabilistic Modeling and Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,57,,,,,,,,,,,,,,,WOS:000656357300001,0
J,"Wang, JY; Joshi, G",,,,"Wang, Jianyu; Joshi, Gauri",,,Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When training machine learning models using stochastic gradient descent (SGD) with a large number of nodes or massive edge devices, the communication cost of synchronizing gradients at every iteration is a key bottleneck that limits the scalability of the system and hinders the benefit of parallel computation. Local-update SGD algorithms, where worker nodes perform local iterations of SGD and periodically synchronize their local models, can effectively reduce the communication frequency and save the communication delay. In this paper, we propose a powerful framework, named Cooperative SGD, that subsumes a variety of local-update SGD algorithms (such as local SGD, elastic averaging SGD, and decentralized parallel SGD) and provides a unified convergence analysis. Notably, special cases of the unified convergence analysis provided by the cooperative SGD framework yield 1) the first convergence analysis of elastic averaging SGD for general non-convex objectives, and 2) improvements upon previous analyses of local SGD and decentralized parallel SGD. Moreover, we design new algorithms such as elastic averaging SGD with overlapped computation and communication, and decentralized periodic averaging which are shown to be 4x or more faster than the baseline in reaching the same training loss.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706883500001,0
J,"Xing, ZR; Carbonetto, P; Stephens, M",,,,"Xing, Zhengrong; Carbonetto, Peter; Stephens, Matthew",,,Flexible Signal Denoising via Flexible Empirical Bayes Shrinkage,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Signal denoising-also known as non-parametric regression-is often performed through shrinkage estimation in a transformed (e.g., wavelet) domain; shrinkage in the transformed domain corresponds to smoothing in the original domain. A key question in such applications is how much to shrink, or, equivalently, how much to smooth. Empirical Bayes shrinkage methods provide an attractive solution to this problem; they use the data to estimate a distribution of underlying effects, hence automatically select an appropriate amount of shrinkage. However, most existing implementations of empirical Bayes shrinkage are less flexible than they could be-both in their assumptions on the underlying distribution of effects, and in their ability to handle heteroskedasticity-which limits their signal denoising applications. Here we address this by adopting a particularly flexible, stable and computationally convenient empirical Bayes shrinkage method and applying it to several signal denoising problems. These applications include smoothing of Poisson data and heteroskedastic Gaussian data. We show through empirical comparisons that the results are competitive with other methods, including both simple thresholding rules and purpose-built empirical Bayes procedures. Our methods are implemented in the R package sma shr, SMoothing by Adaptive SHrinkage in R, available at https://www.github.com/stephenslab/smashr.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,93,,,,,,,,,,,,,,,WOS:000663147200001,0
J,"Zhan, GD; Bao, XC; Lessard, L; Grosse, R",,,,"Zhan, Guodong; Bao, Xuchan; Lessard, Laurent; Grosse, Roger",,,A Unified Analysis of First-Order Methods for Smooth Games via Integral Quadratic Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The theory of integral quadratic constraints (IQCs) allows the certification of exponential convergence of interconnected systems containing nonlinear or uncertain elements. In this work, we adapt the IQC theory to study first-order methods for smooth and strongly-monotone games and show how to design tailored quadratic constraints to get tight upper bounds of convergence rates. Using this framework, we recover the existing bound for the gradient method (GD), derive sharper bounds for the proximal point method (PPM) and optimistic gradient method (OG), and provide for the first time a global convergence rate for the negative momentum method (NM) with an iteration complexity O(k(1.5)), which matches its known lower bound. In addition, for time-varying systems, we prove that the gradient method with optimal step size achieves the fastest provable worst-case convergence rate with quadratic Lyapunov functions. Finally, we further extend our analysis to stochastic games and study the impact of multiplicative noise on different algorithms. We show that it is impossible for an algorithm with one step of memory to achieve acceleration if it only queries the gradient once per batch (in contrast with the stochastic strongly-convex optimization setting, where such acceleration has been demonstrated). However, we exhibit an algorithm which achieves acceleration with two gradient queries per batch. Our code is made public at https://github.com/gd-zhang/IQC-Game.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,103,,,,,,,,,,,,,,,WOS:000663157200001,0
J,"Asadi, AR; Abbe, E",,,,"Asadi, Amir R.; Abbe, Emmanuel",,,Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive generalization and excess risk bounds for neural networks using a family of complexity measures based on a multilevel relative entropy. The bounds are obtained by introducing the notion of generated hierarchical coverings of neural networks and by using the technique of chaining mutual information introduced by Asadi et al. '18. The resulting bounds are algorithm-dependent and multiscale: they exploit the multilevel structure of neural networks. This, in turn, leads to an empirical risk minimization problem with a multilevel entropic regularization. The minimization problem is resolved by introducing a multiscale extension of the celebrated Gibbs posterior distribution, proving that the derived distribution achieves the unique minimum. This leads to a new training procedure for neural networks with performance guarantees, which exploits the chain rule of relative entropy rather than the chain rule of derivatives (as in backpropagation), and which takes into account the interactions between different scales of the hypothesis sets of neural networks corresponding to different depths of the hidden layers. To obtain an efficient implementation of the latter, we further develop a multilevel Metropolis algorithm simulating the multiscale Gibbs distribution, with an experiment for a two-layer neural network on the MNIST data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,139,,,,,,,,,,,,,,,WOS:000558791500001,0
J,"Binois, M; Picheny, V; Taillandier, P; Habbal, A",,,,"Binois, M.; Picheny, V; Taillandier, P.; Habbal, A.",,,The Kalai-Smorodinsky solution for many-objective Bayesian optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives. While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space. A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on four problems with respectively four, six, eight, and nine objectives. The method is available in the R package GPGame available on CRAN at https://cran.r-project.org/package=GPGame.",,,,,"Binois, Mickael/AID-7610-2022","Binois, Mickael/0000-0002-7225-1680; Taillandier, Patrick/0000-0003-2939-4827",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,150,,,,,,,,,,,,,,,WOS:000570113800001,0
J,"Borisyak, M; Ryzhikov, A; Ustyuzhanin, A; Derkach, D; Ratnikov, F; Mineeva, O",,,,"Borisyak, Maxim; Ryzhikov, Artem; Ustyuzhanin, Andrey; Derkach, Denis; Ratnikov, Fedor; Mineeva, Olga",,,(1+epsilon)-class Classification: an Anomaly Detection Method for Highly Imbalanced or Incomplete Data Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Anomaly detection is not an easy problem since distribution of anomalous samples is unknown a priori. We explore a novel method that gives a trade-off possibility between one-class and two-class approaches, and leads to a better performance on anomaly detection problems with small or non-representative anomalous samples. The method is evaluated using several data sets and compared to a set of conventional one-class and two-class approaches.",,,,,"Derkach, Denis/AAY-5330-2020; Ustyuzhanin, Andrey E/K-7189-2013; Borisyak, Maxim A/K-8856-2015","Derkach, Denis/0000-0001-5871-0628; Ustyuzhanin, Andrey E/0000-0001-7865-2357; Borisyak, Maxim A/0000-0002-1493-0319",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000024,0
J,"Frongillo, R; Nobel, A",,,,"Frongillo, Rafael; Nobel, Andrew",,,Memoryless Sequences for General Losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One way to define the randomness of a fixed individual sequence is to ask how hard it is to predict relative to a given loss function. A sequence is memoryless if, with respect to average loss, no continuous function can predict the next entry of the sequence from a finite window of previous entries better than a constant prediction. For squared loss, memoryless sequences are known to have stochastic attributes analogous to those of truly random sequences. In this paper, we address the question of how changing the loss function changes the set of memoryless sequences, and in particular, the stochastic attributes they possess. For convex differentiable losses we establish that the statistic or property elicited by the loss determines the identity and stochastic attributes of the corresponding memoryless sequences. We generalize these results to convex non-differentiable losses, under additional assumptions, and to non-convex Bregman divergences. In particular, our results show that any Bregman divergence has the same set of memoryless sequences as squared loss. We apply our results to price calibration in prediction markets.",,,,,,"Frongillo, Rafael/0000-0002-0170-7572",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000542194600003,0
J,"Gu, JY; Zhou, Q",,,,"Gu, Jiaying; Zhou, Qing",,,"Learning Big Gaussian Bayesian Networks: Partition, Estimation and Fusion",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structure learning of Bayesian networks has always been a challenging problem. Nowadays, massive-size networks with thousands or more of nodes but fewer samples frequently appear in many areas. We develop a divide-and-conquer framework, called partition-estimation-fusion (PEF), for structure learning of such big networks. The proposed method first partitions nodes into clusters, then learns a subgraph on each cluster of nodes, and finally fuses all learned subgraphs into one Bayesian network. The PEF method is designed in a flexible way so that any structure learning method may be used in the second step to learn a subgraph structure as either a DAG or a CPDAG. In the clustering step, we adapt hierarchical clustering to automatically choose a proper number of clusters. In the fusion step, we propose a novel hybrid method that sequentially adds edges between subgraphs. Extensive numerical experiments demonstrate the competitive performance of our PEF method, in terms of both speed and accuracy compared to existing methods. Our method can improve the accuracy of structure learning by 20% or more, while reducing running time up to two orders-of-magnitude.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,158,,,,,,,,,,,,,,,WOS:000570147100001,0
J,"Huang, BW; Zhang, K; Zhang, JJ; Ramsey, J; Sanchez-Romero, R; Glymour, C; Scholkopf, B",,,,"Huang, Biwei; Zhang, Kun; Zhang, Jiji; Ramsey, Joseph; Sanchez-Romero, Ruben; Glymour, Clark; Schoelkopf, Bernhard",,,Causal Discovery from Heterogeneous/Nonstationary Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is commonplace to encounter heterogeneous or nonstationary data, of which the underlying generating process changes across domains or over time. Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper, we develop a framework for causal discovery from such data, called Constraint-based causal Discovery from heterogeneous/NOnstationary Data (CD-NOD), to find causal skeleton and directions and estimate the properties of mechanism changes. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a method to determine causal orientations by making use of independent changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. After learning the causal structure, next, we investigate how to efficiently estimate the driving force of the nonstationarity of a causal mechanism. That is, we aim to extract from data a low-dimensional representation of changes. The proposed methods are nonparametric, with no hard restrictions on data distributions and causal mechanisms, and do not rely on window segmentation. Furthermore, we find that data heterogeneity benefits causal structure identification even with particular types of confounders. Finally, we show the connection between heterogeneity/nonstationarity and soft intervention in causal discovery. Experimental results on various synthetic and real-world data sets (task-fMRI and stock market data) are presented to demonstrate the efficacy of the proposed methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,89,,,,,,,,,,,,,,,WOS:000545026700001,0
J,"Lindauer, M; Hutter, F",,,,"Lindauer, Marius; Hutter, Frank",,,Best Practices for Scientific Research on Neural Architecture Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finding a well-performing architecture is often tedious for both deep learning practitioners and researchers, leading to tremendous interest in the automation of this task by means of neural architecture search (NAS). Although the community has made major strides in developing better NAS methods, the quality of scientific empirical evaluations in the young field of NAS is still lacking behind that of other areas of machine learning. To address this issue, we describe a set of possible issues and ways to avoid them, leading to the NAS best practices checklist available at http://automl.org/nas_checklist.pdf.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,243,,,,,,,,,,,,,,,WOS:000608915100001,0
J,"Paul, S; Chatzilygeroudis, K; Ciosek, K; Mouret, JB; Osborne, MA; Whiteson, S",,,,"Paul, Supratik; Chatzilygeroudis, Konstantinos; Ciosek, Kamil; Mouret, Jean-Baptiste; Osborne, Michael A.; Whiteson, Shimon",,,Robust Reinforcement Learning with Bayesian Optimisation and Quadrature,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This article considers the problem of finding a robust policy while taking into account the impact of environment variables. We present alternating optimisation and quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. We also present transferable ALOQ (TALOQ), for settings where simulator inaccuracies lead to difficulty in transferring the learnt policy to the physical system. We show that our algorithms are robust to the presence of significant rare events, which may not be observable under random sampling but play a substantial role in determining the optimal policy. Experimental results across different domains show that our algorithms learn robust policies efficiently.",,,,,"Chatzilygeroudis, Konstantinos/GVU-1982-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,151,,,,,,,,,,,,,,,WOS:000570114200001,0
J,"Tong, X; Xia, LC; Wang, JC; Feng, Y",,,,"Tong, Xin; Xia, Lucy; Wang, Jiacheng; Feng, Yang",,,Neyman-Pearson classification: parametrics and sample size requirement,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level alpha. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong et al. (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class 0 observation as class 1 under the 0-1 coding) upper bound alpha with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class 0, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class 0 observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class 0 observations, we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300012,0
J,"Alaya, MZ; Bussy, S; Gaiffas, S; Guilloux, A",,,,"Alaya, Mokhtar Z.; Bussy, Simon; Gaiffas, Stephane; Guilloux, Agathe",,,Binarsity: a penalization for one-hot encoded features in linear supervised learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint. This induces two interesting properties on the model weights of the one-hot encoded features: they are piecewise constant, and are eventually block sparse. Non-asymptotic oracle inequalities for generalized linear models are proposed. Moreover, under a sparse additive model assumption, we prove that our procedure matches the state-of-the-art in this setting. Numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard l(1) penalization.",,,,,"Alaya, Mokhtar Z./AAM-9242-2021; Guilloux, Agathe/T-2577-2017","Alaya, Mokhtar Z./0000-0002-1103-6944; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,118,,,,,,,,,,,,,,,WOS:000487068900002,0
J,"Arlot, S; Celisse, A; Harchaoui, Z",,,,"Arlot, Sylvain; Celisse, Alain; Harchaoui, Zaid",,,A Kernel Multiple Change-point Algorithm via Model Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a general formulation of the multiple change-point problem, in which the data is assumed to belong to a set equipped with a positive semidefinite kernel. We propose a model-selection penalty allowing to select the number of change points in Harchaoui and Cappe's kernel-based change-point detection method. The model-selection penalty generalizes non-asymptotic model-selection penalties for the change-in-mean problem with univariate data. We prove a non-asymptotic oracle inequality for the resulting kernel-based change-point detection method, whatever the unknown number of change points, thanks to a concentration result for Hilbert-space valued random variables which may be of independent interest. Experiments on synthetic and real data illustrate the proposed method, demonstrating its ability to detect subtle changes in the distribution of data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,162,,,,,,,,,,,,,,,WOS:000506403100002,0
J,"Cesa-Bianchi, N; Gentile, C; Mansour, Y",,,,"Cesa-Bianchi, Nicolo; Gentile, Claudio; Mansour, Yishay",,,Delay and Cooperation in Nonstochastic Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce Exp3-Coop, a cooperative version of the Exp3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order root(d + 1 + K/N alpha(<= d)) (T ln K), where alpha(<= d) is the independence number of the d-th power of the communication graph G. We then show that for any connected graph, for d = root K the regret bound is K-1/4 root T, strictly better than the minimax regret root KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret root T ln K when G is dense. When G has sparse components, we show that a variant of Exp3-Coop, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,17,,,,,,,,,,,,,,,WOS:000458666900001,0
J,"Chen, H; Raskutti, G; Yuan, M",,,,"Chen, Han; Raskutti, Garvesh; Yuan, Ming",,,Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the problem of learning high-dimensional tensor regression problems with low-rank structure. One of the core challenges associated with learning high-dimensional models is computation since the underlying optimization problems are often non-convex. While convex relaxations could lead to polynomial-time algorithms they are often slow in practice. On the other hand, limited theoretical guarantees exist for non-convex methods. In this paper we provide a general framework that provides theoretical guarantees for learning high-dimensional tensor regression models under different low-rank structural assumptions using the projected gradient descent algorithm applied to a potentially non-convex constraint set Theta in terms of its localized Gaussian width (due to Gaussian design). We juxtapose our theoretical results for non-convex projected gradient descent algorithms with previous results on regularized convex approaches. The two main differences between the convex and non-convex approach are: (i) from a computational perspective whether the non-convex projection operator is computable and whether the projection has desirable contraction properties and (ii) from a statistical error bound perspective, the non-convex approach has a superior rate for a number of examples. We provide three concrete examples of low-dimensional structure which address these issues and explain the pros and cons for the non-convex and convex approaches. We supplement our theoretical results with simulations which show that, under several common settings of generalized low rank tensor regression, the projected gradient descent approach is superior both in terms of statistical error and run-time provided the step-sizes of the projected descent algorithm are suitably chosen.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,5,,,,,,,,,,,,,,,WOS:000458662500001,0
J,"Chen, HB; Valeriote, M",,,,"Chen, Hubie; Valeriote, Matthew",,,Learnability of Solutions to Conjunctive Queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of learning the solution space of an unknown formula has been studied in multiple embodiments in computational learning theory. In this article, we study a family of such learning problems; this family contains, for each relational structure, the problem of learning the solution space of an unknown conjunctive query evaluated on the structure. A progression of results aimed to classify the learnability of each of the problems in this family, and thus far a culmination thereof was a positive learnability result generalizing all previous ones. This article completes the classi fi cation program towards which this progression of results strived, by presenting a negative learnability result that complements the mentioned positive learnability result. In addition, a further negative learnability result is exhibited, which indicates a dichotomy within the problems to which the fi rst negative result applies. In order to obtain our negative results, we make use of universal- algebraic concepts.",,,,,,"Valeriote, Matt/0000-0001-6568-7526",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,67,,,,,,,,,,,,,,,WOS:000467894500001,0
J,"Franks, AM; Hoff, P",,,,"Franks, Alexander M.; Hoff, Peter",,,Shared Subspace Models for Multi-Group Covariance Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a model-based method for evaluating heterogeneity among several p x p covariance matrices in the large p, small n setting. This is done by assuming a spiked covariance model for each group and sharing information about the space spanned by the group-level eigenvectors. We use an empirical Bayes method to identify a low-dimensional subspace which explains variation across all groups and use an MCMC algorithm to estimate the posterior uncertainty of eigenvectors and eigenvalues on this subspace. The implementation and utility of our model is illustrated with analyses of high-dimensional multivariate gene expression.",,,,,,"Franks, Alexander/0000-0002-9329-206X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,171,,,,,,,,,,,,,,,WOS:000506403100011,0
J,"Herlands, W; Neill, DB; Nickisch, H; Wilson, AG",,,,"Herlands, William; Neill, Daniel B.; Nickisch, Hannes; Wilson, Andrew Gordon",,,Change Surfaces for Expressive Multidimensional Changepoints and Counterfactual Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Identifying changes in model parameters is fundamental in machine learning and statistics. However, standard changepoint models are limited in expressiveness, often addressing unidimensional problems and assuming instantaneous changes. We introduce change surfaces as a multidimensional and highly expressive generalization of changepoints. We provide a model-agnostic formalization of change surfaces, illustrating how they can provide variable, heterogeneous, and non-monotonic rates of change across multiple dimensions. Additionally, we show how change surfaces can be used for counterfactual prediction. As a concrete instantiation of the change surface framework, we develop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual prediction with Bayesian posterior mean and credible sets, as well as massive scalability by introducing novel methods for additive nonseparable kernels. Using two large spatio-temporal datasets we employ GPCS to discover and characterize complex changes that can provide scientific and policy relevant insights. Specifically, we analyze twentieth century measles incidence across the United States and discover previously unknown heterogeneous changes after the introduction of the measles vaccine. Additionally, we apply the model to requests for lead testing kits in New York City, discovering distinct spatial and demographic patterns.",,,,,"Nickisch, Hannes/I-7049-2017","Nickisch, Hannes/0000-0003-1604-6647",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,99,,,,,,,,,,,,,,,WOS:000476621500001,0
J,"Karakus, C; Sun, YF; Diggavi, S; Yin, WT",,,,"Karakus, Can; Sun, Yifan; Diggavi, Suhas; Yin, Wotao",,,Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Performance of distributed optimization and learning systems is bottlenecked by straggler nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is encoded to have an over-complete representation with built-in redundancy, and the straggling nodes in the system are dynamically treated as missing, or as erasures at every iteration, whose loss is compensated by the embedded redundancy. For quadratic loss functions, we show that under a simple encoding scheme, many optimization algorithms (gradient descent, L-BFGS, and proximal gradient) operating under data parallelism converge to an approximate solution even when stragglers are ignored. Furthermore, we show a similar result for a wider class of convex loss functions when operating under model parallelism. The applicable classes of objectives covers several popular learning problems such as linear regression, LASSO, support vector machine, collaborative filtering, and generalized linear models including logistic regression. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies.",,,,,,"Sun, Yifan/0000-0003-2475-3843",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,72,,,,,,,,,,,,,,,WOS:000467895300001,0
J,"Meshi, O; London, B; Weller, A; Sontag, D",,,,"Meshi, Ofer; London, Ben; Weller, Adrian; Sontag, David",,,Train and Test Tightness of LP Relaxations in Structured Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structured prediction is used in areas including computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation for the striking observation that approximations based on linear programming (LP) relaxations are often tight (exact) on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that this training tightness generalizes to test data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,13,,,,,,,,,,,,,,,WOS:000458666200001,0
J,"Na, S; Yang, ZR; Wang, ZR; Kolar, M",,,,"Na, Sen; Yang, Zhuoran; Wang, Zhaoran; Kolar, Mladen",,,High-dimensional Varying Index Coefficient Models via Stein's Identity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the parameter estimation problem for a varying index coefficient model in high dimensions. Unlike the most existing works that iteratively estimate the parameters and link functions, based on the generalized Stein's identity, we propose computationally efficient estimators for the high-dimensional parameters without estimating the link functions. We consider two different setups where we either estimate each sparse parameter vector individually or estimate the parameters simultaneously as a sparse or low-rank matrix. For all these cases, our estimators are shown to achieve optimal statistical rates of convergence (up to logarithmic terms in the low-rank setting). Moreover, throughout our analysis, we only require the covariate to satisfy certain moment conditions, which is significantly weaker than the Gaussian or elliptically symmetric assumptions that are commonly made in the existing literature. Finally, we conduct extensive numerical experiments to corroborate the theoretical results.",,,,,"Wang, Zhaoran/P-7113-2018",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,152,,,,,,,,,,,,,,,WOS:000491132200016,0
J,"Xia, D; Zhou, F",,,,"Xia, Dong; Zhou, Fan",,,The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The higher order singular value decomposition (HOSVD) of tensors is a generalization of matrix SVD. The perturbation analysis of HOSVD under random noise is more delicate than its matrix counterpart. Recently, polynomial time algorithms have been proposed where statistically optimal estimates of the singular subspaces and the low rank tensors are attainable in the Euclidean norm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and introduce estimators of the singular subspaces with sharp deviation bounds in the sup-norm. We also investigate a low rank tensor denoising estimator and demonstrate its fast convergence rate with respect to the entry-wise errors. The sup-norm perturbation bounds reveal unconventional phase transitions for statistical learning applications such as the exact clustering in high dimensional Gaussian mixture model and the exact support recovery in sub-tensor localizations. In addition, the bounds established for HOSVD also elaborate the one-sided sup-norm perturbation bounds for the singular subspaces of unbalanced (or fat) matrices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,61,,,,,,,,,,,,,,,WOS:000467879200001,0
J,"Acharya, J; Falahatgar, M; Jafarpour, A; Orlitsky, A; Suresh, AT",,,,"Acharya, Jayadev; Falahatgar, Moein; Jafarpour, Ashkan; Orlitsky, Alon; Suresh, Ananda Theertha",,,Maximum Selection and Sorting with Adversarial Comparators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study maximum selection and sorting of n numbers using imperfect pairwise comparators. The imperfect comparator returns the larger of the two inputs if the inputs are more than a given threshold apart and an adversarially-chosen input otherwise. We consider two adversarial models: a non-adaptive adversary that decides on the outcomes in advance and an adaptive adversary that decides on the outcome of each comparison depending on the previous comparisons and outcomes. Against the non-adaptive adversary, we derive a maximum-selection algorithm that uses at most 2n comparisons in expectation and a sorting algorithm that uses at most 2n ln n comparisons in expectation. In the presence of the adaptive adversary, the proposed maximum-selection algorithm uses Theta(n log(1/is an element of)) comparisons to output a correct answer with probability at least 1 - epsilon, resolving an open problem in Ajtai et al. (2015). Our study is motivated by a density-estimation problem. Given samples from an unknown distribution, we would like to find a distribution among a known class of n candidate distributions that is close to the underlying distribution in l(1) distance. Scheffe's algorithm, for example, in Devroye and Lugosi (2001) outputs a distribution at an l(1) distance at most 9 times the minimum and runs in time Theta(n(2) log n). Using our algorithm, the runtime reduces to Theta(n log n).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,59,,,,,,,,,,,,,,,WOS:000452043900001,0
J,"Doring, M; Gyorfi, LA; Walk, H",,,,"Doering, Maik; Gyorfi, Laszlo; Walk, Harro",,,Rate of Convergence of k-Nearest-Neighbor Classification Rule,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A binary classification problem is considered. The excess error probability of the k-nearest-neighbor classification rule according to the error probability of the Bayes decision is revisited by a decomposition of the excess error probability into approximation and estimation errors. Under a weak margin condition and under a modified Lipschitz condition or a local Lipschitz condition, tight upper bounds are presented such that one avoids the condition that the feature vector is bounded. The concept of modified Lipschitz condition is applied for discrete distributions, too. As a consequence of both concepts, we present the rate of convergence of L-2 error for the corresponding nearest neighbor regression estimate.",,,,,"D√∂ring, Maik/AAA-4557-2020",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000438192200001,0
J,"Leblond, R; Pedregosa, F; Lacoste-Julien, S",,,,"Leblond, Remi; Pedregosa, Fabian; Lacoste-Julien, Simon",,,Improved Asynchronous Parallel Optimization Analysis for Stochastic Incremental Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As data sets continue to increase in size and multi-core computer architectures are developed, asynchronous parallel optimization algorithms become more and more essential to the field of Machine Learning. Unfortunately, conducting the theoretical analysis asynchronous methods is difficult, notably due to the introduction of delay and inconsistency in inherently sequential algorithms. Handling these issues often requires resorting to simplifying but unrealistic assumptions. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced perturbed iterate framework that resolves it. We demonstrate the usefulness of our new framework by analyzing three distinct asynchronous parallel incremental optimization algorithms: Hogwild asynchronous Sgd), Kromagnon asynchronous Svrg) and Asaga, a novel asynchronous parallel version of the incremental gradient algorithm Saga that enjoys fast linear convergence rates. We are able to both remove problematic assumptions and obtain better theoretical results. Notably, we prove that Asaga and Kromagnon can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedups as well as the hardware overhead. Finally, we investigate the overlap constant, an ill-understood but central quantity for the theoretical analysis of asynchronous parallel algorithms. We find that it encompasses much more complexity than suggested in previous work, and often is order-of-magnitude bigger than traditionally thought.",,,,,"Pedregosa, Fabian/U-3477-2019",,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454480400001,0
J,"Natarajan, N; Dhillon, IS; Ravikumar, P; Tewari, A",,,,"Natarajan, Nagarajan; Dhillon, Inderjit S.; Ravikumar, Pradeep; Tewari, Ambuj",,,Cost-Sensitive Learning with Noisy Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study binary classification in the presence of class-conditional random noise, where the learner gets to see labels that are flipped independently with some probability, and where the flip probability depends on the class. Our goal is to devise learning algorithms that are efficient and statistically consistent with respect to commonly used utility measures. In particular, we look at a family of measures motivated by their application in domains where cost-sensitive learning is necessary (for example, when there is class imbalance). In contrast to most of the existing literature on consistent classification that are limited to the classical 0-1 loss, our analysis includes more general utility measures such as the AM measure (arithmetic mean of True Positive Rate and True Negative Rate). For this problem of cost-sensitive learning under class-conditional random noise, we develop two approaches that are based on suitably modifying surrogate losses. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical utility maximization in the presence of i.i.d. data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that using unbiased estimator leads to an efficient algorithm for empirical maximization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong utility bounds. This approach implies that methods already used in practice, such as biased SVM and weighted logistic regression, are provably noise-tolerant. For two practically important measures in our family, we show that the proposed methods are competitive with respect to recently proposed methods for dealing with label noise in several benchmark data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000433251100001,0
J,"Raviv, D; Hazan, T; Osadchy, M",,,,"Raviv, Dolev; Hazan, Tamir; Osadchy, Margarita",,,Hinge-Minimax Learner for the Ensemble of Hyperplanes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this work we consider non-linear classifiers that comprise intersections of hyperplanes. We learn these classifiers by minimizing the minimax bound over the negative training examples and the hinge type loss of the positive training examples. These classifiers fit typical real-life datasets that consist of a small number of positive data points and a large number of negative data points. Such an approach is computationally appealing since the majority of training examples (belonging to the negative class) are represented by the statistics of their distribution, which is used in a single constraint on the empirical risk, as opposed to SVM, in which the number of variables is equal to the size of the training set. We first focus on intersection of K hyperplanes, for which we provide empirical risk bounds. We show that these bounds are dimensionally independent and decay as K/root m for m samples. We then extend the K-hyperplane mixed risk to the latent mixed risk for training a union of C K-hyperplane models, which can form an arbitrary complex, piecewise linear boundaries. We propose efficient algorithms for training the proposed models. Finally, we show how to combine hinge-minimax training with deep architectures and extend it to multi-class settings using transfer learning. The empirical evaluation of the proposed models shows their advantage over the existing methods in a small training labeled data regime.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,62,,,,,,,,,,,,,,,WOS:000452045000001,0
J,"Soudry, D; Hoffer, E; Nacson, MS; Gunasekar, S; Srebro, N",,,,"Soudry, Daniel; Hoffer, Elad; Nacson, Mor Shpigel; Gunasekar, Suriya; Srebro, Nathan",,,The Implicit Bias of Gradient Descent on Separable Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,70,,,,,,,,,,,,,,,WOS:000452055700001,0
J,"Andersen, MR; Vehtari, A; Winther, O; Hansen, LK",,,,"Andersen, Michael Riis; Vehtari, Aki; Winther, Ole; Hansen, Lars Kai",,,Bayesian Inference for Spatio-temporal Spike-and-Slab Priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this work, we address the problem of solving a series of underdetermined linear inverse problemblems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets.",,,,,,"Winther, Ole/0000-0002-1966-3205; Andersen, Michael/0000-0002-7411-5842; Vehtari, Aki/0000-0003-2164-9469; Hansen, Lars Kai/0000-0003-0442-5877",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,139,,,,,,,,,,,,,,,WOS:000424548200001,0
J,"Bach, F",,,,"Bach, Francis",,,On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive de finite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2 -and L-infinity -norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000399841600001,0
J,"Bach, SH; Broecheler, M; Huang, B; Getoor, L",,,,"Bach, Stephen H.; Broecheler, Matthias; Huang, Bert; Getoor, Lise",,,Hinge-Loss Markov Random Fields and Probabilistic Soft Logic,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,109,,,,,,,,,,,,,,,WOS:000424538000001,0
J,"Lemaitre, G; Nogueira, F; Aridas, CK",,,,"Lemaitre, Guillaume; Nogueira, Fernando; Aridas, Christos K.",,,Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from https://github.com/scikit-learn-contrib/imbalanced-learn.",,,,,,"Aridas, Christos/0000-0002-5021-1442; Lemaitre, Guillaume/0000-0002-0897-6791",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,17,,,,,,,,,,,,,,,WOS:000399838700001,0
J,"Si, S; Hsieh, CJ; Dhillon, IS",,,,"Si, Si; Hsieh, Cho-Jui; Dhillon, Inderjit S.",,,Memory Efficient Kernel Approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework - Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard Nystrom approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,20,,,,,,,,,,,,,,,WOS:000399840700001,0
J,"Chen, W; Wang, YJ; Yuan, Y; Wang, QS",,,,"Chen, Wei; Wang, Yajun; Yuan, Yang; Wang, Qinshi",,,Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline (alpha,beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability beta generates an alpha fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize (alpha,beta)-approximation regret, which is the difference between the alpha,beta fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,50,,,,,,,,,,,,,,,WOS:000391486100001,0
J,"Hazan, E; Karnin, Z",,,,"Hazan, Elad; Karnin, Zohar",,,Volumetric Spanners: An Efficient Exploration Basis for Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Numerous learning problems that contain exploration, such as experiment design, multiarm bandits, online routing, search result aggregation and many more, have been studied extensively in isolation. In this paper we consider a generic and efficiently computable method for action space exploration based on convex geometry. We de fine a novel geometric notion of an exploration mechanism with low variance called volumetric spanners, and give efficient algorithms to construct such spanners. We describe applications of this mechanism to the problem of optimal experiment design and the general framework for decision making under uncertainty of bandit linear optimization. For the latter we give efficient and near-optimal regret algorithm over general convex sets. Previously such results were known only for specific convex sets, or under special conditions such as the existence of an efficient self-concordant barrier for the underlying set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,119,,,,,,,,,,,,,,,WOS:000391552700001,0
J,"Klenske, ED; Hennig, P",,,,"Klenske, Edgar D.; Hennig, Philipp",,,Dual Control for Approximate Bayesian Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Control of non-episodic, finite-horizon dynamical systems with uncertain dynamics poses a tough and elementary case of the exploration-exploitation trade-off. Bayesian reinforcement learning, reasoning about the effect of actions and future observations, offers a principled solution, but is intractable. We review, then extend an old approximate approach from control theory-where the problem is known as dual control-in the context of modern regression methods, specifically generalized linear regression. Experiments on simulated systems show that this framework offers a useful approximation to the intractable aspects of Bayesian RL, producing structured exploration strategies that differ from standard RL approaches. We provide simple examples for the use of this framework in (approximate) Gaussian process regression and feedforward neural networks for the control of exploration.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,127,,,,,,,,,,,,,,,WOS:000391655200001,0
J,"Li, B; Sahoo, D; Hoi, SCH",,,,"Li, Bin; Sahoo, Doyen; Hoi, Steven C. H.",,,OLPS: A Toolbox for On-Line Portfolio Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for On-Line Portfolio Selection (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies. OLPS is an open-source project released under Apache License (version 2.0), which is available at https://github.com/OLPS/ or http://OLPS.stevenhoi.org/.",,,,,,"Hoi, Steven/0000-0002-4584-3453",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,35,,,,,,,,,,,,,,,WOS:000391481100001,0
J,"Ramaswamy, HG; Agarwal, S",,,,"Ramaswamy, Harish G.; Agarwal, Shivani",,,Convex Calibration Dimension for Multiclass Loss Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study consistency properties of surrogate loss functions for general multiclass learning problems, de fined by a general multiclass loss matrix. We extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be calibrated with respect to a loss matrix in this setting. We then introduce the notion of convex calibration dimension of a multiclass loss matrix, which measures the smallest 'size' of a prediction space in which it is possible to design a convex surrogate that is calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, we apply our framework to study various subset ranking losses, and use the convex calibration dimension as a tool to show both the existence and non-existence of various types of convex calibrated surrogates for these losses. Our results strengthen recent results of Duchi et al. (2010) and Calauzenes et al. (2012) on the non-existence of certain types of convex calibrated surrogates in subset ranking. We anticipate the convex calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.",,,,,,"Ramaswamy, Harish/0000-0002-1679-8745",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,14,,,,,,,,,,,,,,,WOS:000391471200001,0
J,"Sehulam, P; Saria, S",,,,"Sehulam, Peter; Saria, Suchi",,,Integrative Analysis using Coupled Latent Variable Models for Individualizing Prognoses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Complex chronic diseases (e.g., autism, lupus, and Parkinson's) are remarkably heterogeneous across individuals. This heterogeneity makes treatment difficult for caregivers because they cannot accurately predict the way in which the disease will progress in order to guide treatment decisions. Therefore, tools that help to predict the trajectory of these complex chronic diseases can help to improve the quality of health care. To build such tools, we can leverage clinical markers that are collected at baseline when a patient first presents and longitudinally over time during follow-up visits. Because complex chronic diseases are typically systemic, the longitudinal markers often track disease progression in multiple organ systems. In this paper, our goal is to predict a function of time that models the future trajectory of a single target clinical marker tracking a disease process of interest. We want to make these predictions using the histories of many related clinical markers as input. Our proposed solution tackles several key challenges. First, we can easily handle irregularly and sparsely sampled markers, which are standard in clinical data. Second, the number of parameters and the computational complexity of learning our model grows linearly in the number of marker types included in the model. This makes our approach applicable to diseases where many different markers are recorded over time. Finally, our model accounts for latent factors influencing disease expression, whereas standard regression models rely on observed features alone to explain variability. Moreover, our approach can be applied dynamically in continous-time and updates its predictions as soon as any new data is available. We apply our approach to the problem of predicting lung disease trajectories in scleroderma, a complex autoimmune disease. We show that our model improves over state-of-the-art baselines in predictive accuracy and we provide a qualitative analysis of our model's output. Finally, the variability of disease presentation in scleroderma makes clinical trial recruitment challenging. We show that a prognostic tool that integrates multiple types of routinely collected longitudinal data can be used to identify individuals at greatest risk of rapid progression and to target trial recruitment.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,35,232,,,,,,,,,,,,,,,WOS:000391918600001,0
J,"Shin, HC; Lu, L; Kim, L; Seff, A; Yao, JH; Summers, RM",,,,"Shin, Hoo-Chang; Lu, Le; Kim, Lauren; Seff, Ari; Yao, Jainhua; Summers, Ronald M.",,,Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of similar to 216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data.",,,,,"Summers, Ronald/AAX-6290-2021; Yao, Jianhua/GQZ-6627-2022; Lu, Le/AAD-7619-2020","Yao, Jianhua/0000-0001-9157-9596; Lu, Le/0000-0002-6799-9416",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,107,,,,,,,,,,,,,,,WOS:000391546500001,0
J,"Sutton, RS; Mahmood, AR; White, M",,,,"Sutton, Richard S.; Mahmood, A. Rupam; White, Martha",,,An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD(lambda)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD(lambda), and GQ(lambda). Compared to these methods, our emphatic TD(lambda) is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states.",,,,,"White, Martha/AAF-7066-2020","White, Martha/0000-0002-5356-2950",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,73,,,,,,,,,,,,,,,WOS:000391524100001,0
J,"van Laarhoven, T; Marchiori, E",,,,"van Laarhoven, Twan; Marchiori, Elena",,,Local Network Community Detection with Continuous Optimization of Conductance and Weighted Kernel K-Means,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Local network community detection is the task of finding a single community of nodes concentrated around few given seed nodes in a localized way. Conductance is a popular objective function used in many algorithms for local community detection. This paper studies a continuous relaxation of conductance. We show that continuous optimization of this objective still leads to discrete communities. We investigate the relation of conductance with weighted kernel k-means for a single community, which leads to the introduction of a new objective function, u-conductance. Conductance is obtained by setting sigma, to 0. Two algorithms, EMC and PGDc, are proposed to locally optimize sigma-conductance and automatically tune the parameter or. They are based on expectation maximization and projected gradient descent, respectively. We prove locality and give performance guarantees for EMC and PGDc for a class of dense and well separated communities centered around the seeds. Experiments are conducted on networks with ground-truth communities, comparing to state-of-the-art graph diffusion algorithms for conductance optimization. On large graphs, results indicate that EMC and PGDc stay localized and produce communities most similar to the ground, while graph diffusion algorithms generate large communities of lower quality.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,147,,,,,,,,,,,,,,,WOS:000391662200001,0
J,"Zhao, S; Gao, C; Mukherjee, S; Engelhardt, BE",,,,"Zhao, Shiwen; Gao, Chuan; Mukherjee, Sayan; Engelhardt, Barbara E.",,,Bayesian group factor analysis with structured sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent factor models are the canonical statistical tool for exploratory analyses of low-dimensional linear structure for a matrix of p features across n samples. We develop a structured Bayesian group factor analysis model that extends the factor model to multiple coupled observation matrices; in the case of two observations, this reduces to a Bayesian model of canonical correlation analysis. Here, we carefully define a structured Bayesian prior that encourages both element-wise and column-wise shrinkage and leads to desirable behavior on high-dimensional data. In particular, our model puts a structured prior on the joint factor loading matrix, regularizing at three levels, which enables element-wise sparsity and unsupervised recovery of latent factors corresponding to structured variance across arbitrary subsets of the observations. In addition, our structured prior allows for both dense and sparse latent factors so that covariation among either all features or only a subset of features can be recovered. We use fast parameter-expanded expectation-maximization for parameter estimation in this model. We validate our method on simulated data with substantial structure. We show results of our method applied to three high-dimensional data sets, comparing results against a number of state-of-the-art approaches. These results illustrate useful properties of our model, including i) recovering sparse signal in the presence of dense effects; ii) the ability to scale naturally to large numbers of observations; iii) flexible observation- and factor-specific regularization to recover factors with a wide variety of sparsity levels and percentage of variance explained; and iv) tractable inference that scales to modern genomic and text data sizes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,196,,,,,,,,,,,,,,,WOS:000391827500001,0
J,"Bontempi, G; Flauder, M",,,,"Bontempi, Gianluca; Flauder, Maxime",,,From Dependency to Causality: A Machine Learning Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The relationship between statistical dependency and causality lies at the heart of all statistical approaches to causal inference. Recent results in the ChaLearn cause-effect pair challenge have shown that causal directionality can be inferred with good accuracy also in Markov indistinguishable configurations thanks to data driven approaches. This paper proposes a supervised machine learning approach to infer the existence of a directed causal link between two variables in multivariate settings with n > 2 variables. The approach relies on the asymmetry of some conditional (in) dependence relations between the members of the Markov blankets of two variables causally connected. Our results show that supervised learning methods may be successfully used to extract causal information on the basis of asymmetric statistical descriptors also for n > 2 variate distributions.,,,,,"Bontempi, Gianluca/ABE-4365-2020; Bontempi, Gianluca/J-7121-2012","Bontempi, Gianluca/0000-0001-8621-316X; Bontempi, Gianluca/0000-0001-8621-316X",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2437,2457,,,,,,,,,,,,,,,,WOS:000369888000003,0
J,"Curtin, RR; Lee, D; March, WB; Ram, P",,,,"Curtin, Ryan R.; Lee, Dongryeol; March, William B.; Ram, Parikshit",,,Plug-and-Play Dual-Tree Algorithm Runtime Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Numerous machine learning algorithms contain pairwise statistical problems at their core that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worst case runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3269,3297,,,,,,,,,,,,,,,,WOS:000369888000030,0
J,"Dhillon, PS; Foster, DP; Ungar, LH",,,,"Dhillon, Paramveer S.; Foster, Dean P.; Ungar, Lyle H.",,,Eigenwords: Spectral Word Embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings low dimensional real vectors (called Eigenwords) that capture the meaning of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3035,3078,,,,,,,,,,,,,,,,WOS:000369888000024,0
J,"Masnadi-Shirazi, H; Vasconcelos, N",,,,"Masnadi-Shirazi, Hamed; Vasconcelos, Nuno",,,A View of Margin Losses as Regularizers of Probability Estimates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularization is commonly used in classifier design, to assure good generalization. Classical regularization enforces a cost on classifier complexity, by constraining parameters. This is usually combined with a margin loss, which favors large-margin decision rules. A novel and unified view of this architecture is proposed, by showing that margin losses act as regularizers of posterior class probabilities, in a way that amplifies classical parameter regularization. The problem of controlling the regularization strength of a margin loss is considered, using a decomposition of the loss in terms of a link and a binding function. The link function is shown to be responsible for the regularization strength of the loss, while the binding function determines its outlier robustness. A large class of losses is then categorized into equivalence classes of identical regularization strength or outlier robustness. It is shown that losses in the same regularization class can be parameterized so as to have tunable regularization strength. This parameterization is finally used to derive boosting algorithms with loss regularization (BoostLR). Three classes of tunable regularization losses are considered in detail. Canonical losses can implement all regularization behaviors but have no flexibility in terms of outlier modeling. Shrinkage losses support equally parameterized link and binding functions, leading to boosting algorithms that implement the popular shrinkage procedure. This offers a new explanation for shrinkage as a special case of loss-based regularization. Finally, ca-tunable losses enable the independent parameterization of link and binding functions, leading to boosting algorithms of great flexibility. This is illustrated by the derivation of an algorithm that generalizes both AdaBoost and LogitBoost, behaving as either one when that best suits the data to classify. Various experiments provide evidence of the benefits of probability regularization for both classification and estimation of posterior class probabilities.",,,,,,"Vasconcelos, Nuno/0000-0002-9024-4302",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2751,2795,,,,,,,,,,,,,,,,WOS:000369888000014,0
J,"Mokhtari, A; Ribeiro, A",,,,"Mokhtari, Aryan; Ribeiro, Alejandro",,,Global Convergence of Online Limited Memory BFGS,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Global convergence of an online (stochastic) limited memory version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method for solving optimization problems with stochastic objectives that arise in large scale machine learning is established. Lower and upper bounds on the Hessian eigenvalues of the sample functions are shown to suffice to guarantee that the curvature approximation matrices have bounded determinants and traces, which, in turn, permits establishing convergence to optimal arguments with probability 1. Experimental evaluation on a search engine advertising problem showcase reductions in convergence time relative to stochastic gradient descent algorithms.",,,,,,"Ribeiro, Alejandro/0000-0003-4230-9906",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3151,3181,,,,,,,,,,,,,,,,WOS:000369888000027,0
J,"Pourhabib, A; Mallick, BK; Ding, Y",,,,"Pourhabib, Arash; Mallick, Bani K.; Ding, Yu",,,Absent Data Generating Classifier for Imbalanced Class Sizes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose an algorithm for two-class classification problems when the training data are imbalanced. This means the number of training instances in one of the classes is so low that the conventional classification algorithms become ineffective in detecting the minority class. We present a modification of the kernel Fisher discriminant analysis such that the imbalanced nature of the problem is explicitly addressed in the new algorithm formulation. The new algorithm exploits the properties of the existing minority points to learn the effects of other minority data points, had they actually existed. The algorithm proceeds iteratively by employing the learned properties and conditional sampling in such a way that it generates sufficient artificial data points for the minority set, thus enhancing the detection probability of the minority class. Implementing the proposed method on a number of simulated and real data sets, we show that our proposed method performs competitively compared to a set of alternative state-of-the-art imbalanced classification algorithms.",,,,,,"Pourhabib, Arash/0000-0002-9394-8196",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2695,2724,,,,,,,,,,,,,,,,WOS:000369888000012,0
J,"Zhang, YC; Duchi, J; Wainwright, M",,,,"Zhang, Yuchen; Duchi, John; Wainwright, Martin",,,Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with Minimax Optimal Rates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a decomposition-based scalable approach to kernel ridge regression, and show that it achieves minimax optimal convergence rates under relatively mild conditions. The method is simple to describe: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset using a careful choice of the regularization parameter, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our two main theorems establish that despite the computational speed-up, statistical optimality is retained: as long as m is not too large, the partition-based estimator achieves the statistical minimax rate over all estimators using the set of N samples. As concrete examples, our theory guarantees that the number of subsets m may grow nearly linearly for finite-rank or Gaussian kernels and polynomially in N for Sobolev spaces, which in turn allows for substantial reductions in computational cost. We conclude with experiments on both simulated data and a music-prediction task that complement our theoretical results, exhibiting the computational and statistical benefits of our approach.",,,,,"Zhang, Yuchen/GYI-8858-2022","Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3299,3340,,,,,,,,,,,,,,,,WOS:000369888000031,0
J,"Sunehag, P; Hutter, M",,,,"Sunehag, Peter; Hutter, Marcus",,,"Rationality, Optimism and Guarantees in General Reinforcement Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article,1 we present a top-down theoretical study of general reinforcement learning agents. We begin with rational agents with unlimited resources and then move to a setting where an agent can only maintain a limited number of hypotheses and optimizes plans over a horizon much shorter than what the agent designer actually wants. We axiomatize what is rational in such a setting in a manner that enables optimism, which is important to achieve systematic explorative behavior. Then, within the class of agents deemed rational, we achieve convergence and finite-error bounds. Such results are desirable since they imply that the agent learns well from its experiences, but the bounds do not directly guarantee good performance and can be achieved by agents doing things one should obviously not. Good performance cannot in fact be guaranteed for any agent in fully general settings. Our approach is to design agents that learn well from experience and act rationally. We introduce a framework for general reinforcement learning agents based on rationality axioms for a decision function and an hypothesis-generating function designed so as to achieve guarantees on the number errors. We will consistently use an optimistic decision function but the hypothesis-generating function needs to change depending on what is known/assumed. We investigate a number of natural situations having either a frequentist or Bayesian flavor, deterministic or stochastic environments and either finite or countable hypothesis class. Further, to achieve sufficiently good bounds as to hold promise for practical success we introduce a notion of a class of environments being generated by a set of laws. None of the above has previously been done for fully general reinforcement learning environments.",,,,,,"Hutter, Marcus/0000-0002-3263-4097",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1345,1390,,,,,,,,,,,,,,,,WOS:000369887100002,0
J,"Krueger, T; Panknin, D; Braun, M",,,,"Krueger, Tammo; Panknin, Danny; Braun, Mikio",,,Fast Cross-Validation via Sequential Testing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the increasing size of today's data sets, finding the right parameter configuration in model selection via cross-validation can be an extremely time-consuming task. In this paper we propose an improved cross-validation procedure which uses nonparametric testing coupled with sequential analysis to determine the best parameter set on linearly increasing subsets of the data. By eliminating underperforming candidates quickly and keeping promising candidates as long as possible, the method speeds up the computation while preserving the power of the full cross-validation. Theoretical considerations underline the statistical power of our procedure. The experimental evaluation shows that our method reduces the computation time by a factor of up to 120 compared to a full cross-validation with a negligible impact on the accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2015,16,,,,,,1103,1155,,,,,,,,,,,,,,,,WOS:000369886600001,0
J,"Lecci, F; Rinaldo, A; Wasserman, L",,,,"Lecci, Fabrizio; Rinaldo, Alessandro; Wasserman, Larry",,,Statistical Analysis of Metric Graph Reconstruction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A metric graph is a 1-dimensional stratified metric space consisting of vertices and edges or loops glued together. Metric graphs can be naturally used to represent and model data that take the form of noisy filamentary structures, such as street maps, neurons, networks of rivers and galaxies. We consider the statistical problem of reconstructing the topology of a metric graph embedded in 1D from a random sample. We derive lower and upper bounds on the minimax risk for the noiseless case and tubular noise case. The upper bound is based on the reconstruction algorithm given in Aanjaneya et al. (2012).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3425,3446,,,,,,,,,,,,,,,,WOS:000344638800019,0
J,"Jin, JS; Zhang, CH; Zhang, Q",,,,"Jin, Jiashun; Zhang, Cun-Hui; Zhang, Qi",,,Optimality of Graph let Screening in High Dimensional Variable Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider a linear model Y = X beta+sigma z, where X has n rows and p columns and z similar to N (0, I-n). We assume both p and n are large, including the case of p >> n. The unknown signal vector beta is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection). We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix G = X' X is sparse in the sense that each row has relatively few large entries (diagonals of G are normalized to 1). The sparsity of G naturally induces the sparsity of the so-called Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other. We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential chi(2)-tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes. For any variable selection procedure beta, we measure its performance by the Hamming distance between the sign vectors of beta and beta, and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors. We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, well-known procedures such as the L-0/L-1-penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set. The the presented algorithm is implemented as R-CRAN package Screen Clean and in matlab (available at http : //www.stat.cmu.edu/similar to jiashun/Research/software/GS-matlab/).",,,,,,"Zhang, Qi/0000-0001-6197-0973",,,,,,,,,,,,,1532-4435,,,,,AUG,2014,15,,,,,,2723,2772,,,,,,,,,,,,,,,,WOS:000344638600004,0
J,"Fox-Roberts, P; Rosten, E",,,,"Fox-Roberts, Patrick; Rosten, Edward",,,Unbiased Generative Semi-Supervised Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reliable semi-supervised learning, where a small amount of labelled data is complemented by a large body of unlabelled data, has been a long-standing goal of the machine learning community. However, while it seems intuitively obvious that unlabelled data can aid the learning process, in practise its performance has often been disappointing. We investigate this by examining generative maximum likelihood semi-supervised learning and derive novel upper and lower bounds on the degree of bias introduced by the unlabelled data. These bounds improve upon those provided in previous work, and are specifically applicable to the challenging case where the model is unable to exactly fit to the underlying distribution a situation which is common in practise, but for which fewer guarantees of semi-supervised performance have been found. Inspired by this new framework for analysing bounds, we propose a new, simple reweighing scheme which provides a provably unbiased estimator for arbitrary model/distribution pairs-an unusual property for a semi-supervised algorithm. This reweighing introduces no additional computational complexity and can be applied to very many models. Additionally, we provide specific conditions demonstrating the circumstance under which the unlabelled data will lower the estimator variance, thereby improving convergence.",,,,,,"Rosten, Edward/0000-0001-8675-4230",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,367,443,,,,,,,,,,,,,,,,WOS:000335457700001,0
J,"Chen, TQ; Zhang, WN; Lu, QX; Chen, KL; Zheng, Z; Yu, Y",,,,"Chen, Tianqi; Zhang, Weinan; Lu, Qiuxia; Chen, Kailong; Zheng, Zhao; Yu, Yong",,,SVDFeature: A Toolkit for Feature-based Collaborative Filtering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3619,3622,,,,,,,,,,,,,,,,WOS:000314529000006,0
J,"Parrado-Hernandez, E; Ambroladze, A; Shawe-Taylor, J; Sun, SL",,,,"Parrado-Hernandez, Emilio; Ambroladze, Amiran; Shawe-Taylor, John; Sun, Shiliang",,,PAC-Bayes Bounds with Data Dependent Priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the prior PAC-Bayes bound and explores its capabilities as a tool to provide tight predictions of SVMs' generalization. The computation of the bound involves estimating a prior of the distribution of classifiers from the available data, and then manipulating this prior in the usual PAC-Bayes generalization bound. We explore two alternatives: to learn the prior from a separate data set, or to consider an expectation prior that does not need this separate data set. The prior PAC-Bayes bound motivates two SVM-like classification algorithms, prior SVM and eta-prior SVM, whose regularization term pushes towards the minimization of the prior PAC-Bayes bound. The experimental work illustrates that the new bounds can be significantly tighter than the original PAC-Bayes bound when applied to SVMs, and among them the combination of the prior PAC-Bayes bound and the prior SVM algorithm gives the tightest bound.",,,,,"PARRADO-HERNANDEZ, EMILIO/ABH-2027-2020","PARRADO-HERNANDEZ, EMILIO/0000-0003-2146-2135; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3507,3531,,,,,,,,,,,,,,,,WOS:000314529000002,0
J,"Ben-Tal, A; Bhadra, S; Bhattacharyya, C; Nemirovski, A",,,,"Ben-Tal, Aharon; Bhadra, Sahely; Bhattacharyya, Chiranjib; Nemirovski, Arkadi",,,Efficient Methods for Robust Classification Under Uncertainty in Kernel Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we study the problem of designing SVM classifiers when the kernel matrix, K, is affected by uncertainty. Specifically K is modeled as a positive affine combination of given positive semi definite kernels, with the coefficients ranging in a norm-bounded uncertainty set. We treat the problem using the Robust Optimization methodology. This reduces the uncertain SVM problem into a deterministic conic quadratic problem which can be solved in principle by a polynomial time Interior Point (IP) algorithm. However, for large-scale classification problems, IP methods become intractable and one has to resort to first-order gradient type methods. The strategy we use here is to reformulate the robust counterpart of the uncertain SVM problem as a saddle point problem and employ a special gradient scheme which works directly on the convex-concave saddle function. The algorithm is a simplified version of a general scheme due to Juditski and Nemirovski (2011). It achieves an O(1/T-2) reduction of the initial error after T iterations. A comprehensive empirical study on both synthetic data and real-world protein structure data sets show that the proposed formulations achieve the desired robustness, and the saddle point based algorithm outperforms the IP method significantly.",,,,,"Nemirovski, Arkadi S/A-8375-2009; Bhadra, Sahely/AAS-9381-2021","Nemirovski, Arkadi S/0000-0002-5001-7420; ",,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2923,2954,,,,,,,,,,,,,,,,WOS:000313200000004,0
J,"Kumar, S; Mohri, M; Talwalkar, A",,,,"Kumar, Sanjiv; Mohri, Mehryar; Talwalkar, Ameet",,,Sampling Methods for the Nystrom Method,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Nystrom method is an efficient technique to generate low-rank matrix approximations and is used in several large-scale learning applications. A key aspect of this method is the procedure according to which columns are sampled from the original matrix. In this work, we explore the efficacy of a variety of fixed and adaptive sampling schemes. We also propose a family of ensemble-based sampling algorithms for the Nystrom method. We report results of extensive experiments that provide a detailed comparison of various fixed and adaptive sampling techniques, and demonstrate the performance improvement associated with the ensemble Nystrom method when used in conjunction with either fixed or adaptive sampling schemes. Corroborating these empirical findings, we present a theoretical analysis of the Nystrom method, providing novel error bounds guaranteeing a better convergence rate of the ensemble Nystrom method in comparison to the standard Nystrom method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,981,1006,,,,,,,,,,,,,,,,WOS:000303773100003,0
J,"El-Yaniv, R; Wiener, Y",,,,"El-Yaniv, Ran; Wiener, Yair",,,Active Learning via Perfect Selective Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discover a strong relation between two known learning models: stream-based active learning and perfect selective classification (an extreme case of 'classification with a reject option'). For these models, restricted to the realizable case, we show a reduction of active learning to selective classification that preserves fast rates. Applying this reduction to recent results for selective classification, we derive exponential target-independent label complexity speedup for actively learning general (non-homogeneous) linear classifiers when the data distribution is an arbitrary high dimensional mixture of Gaussians. Finally, we study the relation between the proposed technique and existing label complexity measures, including teaching dimension and disagreement coefficient.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,255,279,,,,,,,,,,,,,,,,WOS:000303046000002,0
J,"Muller, JS; von Bunau, P; Meinecke, FC; Kiraly, FJ; Muller, KR",,,,"Mueller, Jan Saputra; von Buenau, Paul; Meinecke, Frank C.; Kiraly, Franz J.; Mueller, Klaus-Robert",,,The Stationary Subspace Analysis Toolbox,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Stationary Subspace Analysis (SSA) algorithm linearly factorizes a high-dimensional time series into stationary and non-stationary components. The SSA Toolbox is a platform-independent efficient stand-alone implementation of the SSA algorithm with a graphical user interface written in Java, that can also be invoked from the command line and from Matlab. The graphical interface guides the user through the whole process; data can be imported and exported from comma separated values (CSV) and Matlab's. mat files.",,,,,"Muller, Klaus R/C-3196-2013; Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,3065,3069,,,,,,,,,,,,,,,,WOS:000298103200011,0
J,"Visweswaran, S; Cooper, GF",,,,"Visweswaran, Shyam; Cooper, Gregory F.",,,Learning Instance-Specific Predictive Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces a Bayesian algorithm for constructing predictive models from data that are optimized to predict a target variable well for a particular instance. This algorithm learns Markov blanket models, carries out Bayesian model averaging over a set of models to predict a target variable of the instance at hand, and employs an instance-specific heuristic to locate a set of suitable models to average over. We call this method the instance-specific Markov blanket (ISMB) algorithm. The ISMB algorithm was evaluated on 21 UCI data sets using five different performance measures and its performance was compared to that of several commonly used predictive algorithms, including nave Bayes, C4.5 decision tree, logistic regression, neural networks, k-Nearest Neighbor, Lazy Bayesian Rules, and AdaBoost. Over all the data sets, the ISMB algorithm performed better on average on all performance measures against all the comparison algorithms.",,,,,,"Visweswaran, Shyam/0000-0002-2079-8684",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3333,3369,,,,,,,,,,,25045325,,,,,WOS:000286637200003,0
J,"Segata, N; Blanzieri, E",,,,"Segata, Nicola; Blanzieri, Enrico",,,Fast and Scalable Local Kernel Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A computationally efficient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classification accuracies by dividing the separation function in local optimisation problems that can be handled very efficiently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability.",,,,,"Segata, Nicola/AAB-2980-2019; Segata, Nicola/K-7240-2016","Blanzieri, Enrico/0000-0001-6524-0601; Segata, Nicola/0000-0002-1583-5794",,,,,,,,,,,,,1532-4435,,,,,JUN,2010,11,,,,,,1883,1926,,,,,,,,,,,,,,,,WOS:000282522400005,0
J,"McDowell, LK; Gupta, KM; Aha, DW",,,,"McDowell, Luke K.; Gupta, Kalyan Moy; Aha, David W.",,,Cautious Collective Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many collective classification (CC) algorithms have been shown to increase accuracy when instances are interrelated. However, CC algorithms must be carefully applied because their use of estimated labels can in some cases decrease accuracy. In this article, we show that managing this label uncertainty through cautious algorithmic behavior is essential to achieving maximal, robust performance. First, we describe cautious inference and explain how four well-known families of CC algorithms can be parameterized to use varying degrees of such caution. Second, we introduce cautious learning and show how it can be used to improve the performance of almost any CC algorithm, with or without cautious inference. We then evaluate cautious inference and learning for the four collective inference families, with three local classifiers and a range of both synthetic and real-world data. We find that cautious learning and cautious inference typically outperform less cautious approaches. In addition, we identify the data characteristics that predict more substantial performance differences. Our results reveal that the degree of caution used usually has a larger impact on performance than the choice of the underlying inference algorithm. Together, these results identify the most appropriate CC algorithms to use for particular task characteristics and explain multiple conflicting findings from prior CC research.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2777,2836,,,,,,,,,,,,,,,,WOS:000273877300003,0
J,"Shi, QF; Petterson, J; Dror, G; Langford, J; Smola, A; Vishwanathan, SVN",,,,"Shi, Qinfeng; Petterson, James; Dror, Gideon; Langford, John; Smola, Alex; Vishwanathan, S. V. N.",,,Hash Kernels for Structured Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose hashing to facilitate efficient kernels. This generalizes previous work using sampling and we show a principled way to compute the kernel matrix for data streams and sparse feature spaces. Moreover, we give deviation bounds from the exact kernel matrix. This has applications to estimation on strings and graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2615,2637,,,,,,,,,,,,,,,,WOS:000272346600007,0
J,"Fan, JQ; Samworth, R; Wu, YC",,,,"Fan, Jianqing; Samworth, Richard; Wu, Yichao",,,Ultrahigh Dimensional Feature Selection: Beyond The Linear Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variable selection in high-dimensional space characterizes many contemporary problems in scientific discovery and decision making. Many frequently-used techniques are based on independence screening; examples include correlation ranking (Fan & Lv, 2008) or feature selection using a two-sample t-test in high-dimensional classification (Tibshirani et al., 2003). Within the context of the linear model, Fan & Lv ( 2008) showed that this simple correlation ranking possesses a sure independence screening property under certain conditions and that its revision, called iteratively sure independent screening ( ISIS), is needed when the features are marginally unrelated but jointly related to the response variable. In this paper, we extend ISIS, without explicit definition of residuals, to a general pseudo-likelihood framework, which includes generalized linear models as a special case. Even in the least-squares setting, the new method improves ISIS by allowing feature deletion in the iterative process. Our technique allows us to select important features in high-dimensional classification where the popularly used two-sample t-method fails. A new technique is introduced to reduce the false selection rate in the feature screening stage. Several simulated and two real data examples are presented to illustrate the methodology.",,,,,"Fan, Jianqing/B-2115-2008",,,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2013,2038,,,,,,,,,,,21603590,,,,,WOS:000272346100002,0
J,"Ferrer, L; Sonmez, K; Shriberg, E",,,,"Ferrer, Luciana; Sonmez, Kemal; Shriberg, Elizabeth",,,An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a method for training support vector machine (SVM)-based classification systems for combination with other classification systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system's individual performance. That is, the new system takes one for the team, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other.",,,,,"Sonmez, Kemal/ABF-3992-2020","Sonmez, Kemal/0000-0002-2816-6438",,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2079,2114,,,,,,,,,,,,,,,,WOS:000272346100004,0
J,"Fleuret, F; Geman, D",,,,"Fleuret, Francois; Geman, Donald",,,Stationary Features and Cat Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most discriminative techniques for detecting instances from object categories in still images consist of looping over a partition of a pose space with dedicated binary classifiers. The efficiency of this strategy for a complex pose, that is, for fine-grained descriptions, can be assessed by measuring the effect of sample size and pose resolution on accuracy and computation. Two conclusions emerge: (1) fragmenting the training data, which is inevitable in dealing with high in-class variation, severely reduces accuracy; (2) the computational cost at high resolution is prohibitive due to visiting a massive pose partition. To overcome data-fragmentation we propose a novel framework centered on pose-indexed features which assign a response to a pair consisting of an image and a pose, and are designed to be stationary: the probability distribution of the response is always the same if an object is actually present. Such features allow for efficient, one-shot learning of pose-specific classifiers. To avoid expensive scene processing, we arrange these classifiers in a hierarchy based on nested partitions of the pose; as in previous work on coarse-to-fine search, this allows for efficient processing. The hierarchy is then folded for training: all the classifiers at each level are derived from one base predictor learned from all the data. The hierarchy is unfolded for testing: parsing a scene amounts to examining increasingly finer object descriptions only when there is sufficient evidence for coarser ones. In this way, the detection results are equivalent to an exhaustive search at high resolution. We illustrate these ideas by detecting and localizing cats in highly cluttered greyscale scenes.",,,,,"Geman, Donald/A-3325-2010",,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2549,2578,,,,,,,,,,,,,,,,WOS:000262637600006,0
J,"He, YB; Geng, Z",,,,"He, Yang-Bo; Geng, Zhi",,,Active Learning of Causal Networks with Intervention Experiments and Optimal Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The causal discovery from data is important for various scientific investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we first find a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph.",,,,,"He, Yangbo/K-8159-2013",,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2523,2547,,,,,,,,,,,,,,,,WOS:000262637600005,0
J,"Airoldi, EM; Blei, DM; Fienberg, SE; Xing, EP",,,,"Airoldi, Edoardo M.; Blei, David M.; Fienberg, Stephen E.; Xing, Eric P.",,,Mixed Membership Stochastic Blockmodels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider data consisting of pairwise measurements, such as presence or absence of links between pairs of objects. These data arise, for instance, in the analysis of protein interactions and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing pairwise measurements with probabilistic models requires special assumptions, since the usual independence or exchangeability assumptions no longer hold. Here we introduce a class of variance allocation models for pairwise measurements: mixed membership stochastic blockmodels. These models combine global parameters that instantiate dense patches of connectivity (blockmodel) with local parameters that instantiate node-specific variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodels with applications to social networks and protein interaction networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2008,9,,,,,,1981,2014,,,,,,,,,,,21701698,,,,,WOS:000262637100002,0
J,"Bach, FR",,,,"Bach, Francis R.",,,Consistency of trace norm minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufficient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulfilled.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1019,1048,,,,,,,,,,,,,,,,WOS:000258646300003,0
J,"Garriga, GC; Kralj, P; Lavrac, N",,,,"Garriga, Gemma C.; Kralj, Petra; Lavrac, Nada",,,Closed sets for labeled data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Closed sets have been proven successful in the context of compacted data representation for association rule learning. However, their use is mainly descriptive, dealing only with unlabeled data. This paper shows that when considering labeled data, closed sets can be adapted for classification and discrimination purposes by conveniently contrasting covering properties on positive and negative examples. We formally prove that these sets characterize the space of relevant combinations of features for discriminating the target class. In practice, identifying relevant/irrelevant combinations of features through closed sets is useful in many applications: to compact emerging patterns of typical descriptive mining applications, to reduce the number of essential rules in classification, and to efficiently learn subgroup descriptions, as demonstrated in real-life subgroup discovery experiments on a high dimensional microarray data set.",,,,,"Novak, Petra Kralj/AAE-5852-2020","Kralj Novak, Petra/0000-0003-3385-6430",,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,559,580,,,,,,,,,,,,,,,,WOS:000256642100001,0
J,"Chapelle, O; Sindhwani, V; Keerthi, SS",,,,"Chapelle, Olivier; Sindhwani, Vikas; Keerthi, Sathiya S.",,,Optimization techniques for semi-supervised support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines ((SVMs)-V-3) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving (SVMs)-V-3. This paper reviews key ideas in this literature. The performance and behavior of various (SVM)-V-3 algorithms is studied together, under a common experimental setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,203,233,,,,,,,,,,,,,,,,WOS:000256641800009,0
J,"Chhabra, M; Jacobs, RA; Stefankovic, D",,,,"Chhabra, Manu; Jacobs, Robert A.; Stefankovic, Daniel",,,Behavioral shaping for geometric concepts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a search problem, an agent uses the membership oracle of a target concept to find a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to find a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efficient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R subset of A subset of R-d one can obtain a rectangle A' superset of R with vol(A')/vol(R) <= 2, using only O(d.vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in R-d we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the final section, we explore connections between our bootstrapping method and active learning. Specifically, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d1n(1/epsilon)) labeled samples.",,,,,"Jacobs, Robert/B-7670-2013","Jacobs, Robert/0000-0001-6607-908X; Stefankovic, Daniel/0000-0002-4849-7955",,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1835,1865,,,,,,,,,,,,,,,,WOS:000252744400006,0
J,"Esmeir, S; Markovitch, S",,,,"Esmeir, Saher; Markovitch, Shaul",,,Anytime learning of decision trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The majority of existing algorithms for learning decision trees are greedy-a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difficult. Furthermore, they require a fixed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,891,933,,,,,,,,,,,,,,,,WOS:000248351700001,0
J,"Malioutov, DM; Johnson, JK; Willsky, AS",,,,"Malioutov, Dmitry M.; Johnson, Jason K.; Willsky, Alan S.",,,Walk-sums and belief propagation in Gaussian graphical models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new framework based on walks in a graph for analysis and inference in Gaussian graphical models. The key idea is to decompose the correlation between each pair of variables as a sum over all walks between those variables in the graph. The weight of each walk is given by a product of edgewise partial correlation coefficients. This representation holds for a large class of Gaussian graphical models which we call walk-summable. We give a precise characterization of this class of models, and relate it to other classes including diagonally dominant, attractive, non-frustrated, and pairwise-normalizable. We provide a walk-sum interpretation of Gaussian belief propagation in trees and of the approximate method of loopy belief propagation in graphs with cycles. The walk-sum perspective leads to a better understanding of Gaussian belief propagation and to stronger results for its convergence in loopy graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2031,2064,,,,,,,,,,,,,,,,WOS:000245390500004,0
J,"Vert, R; Vert, JP",,,,"Vert, Regis; Vert, Jean-Philippe",,,Consistency and convergence rates of one-class SVMs and related algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We determine the asymptotic behaviour of the function computed by support vector machines (SVM) and related algorithms that minimize a regularized empirical convex loss function in the reproducing kernel Hilbert space of the Gaussian RBF kernel, in the situation where the number of examples tends to infinity, the bandwidth of the Gaussian kernel tends to 0, and the regularization parameter is held fixed. Non-asymptotic convergence bounds to this limit in the L-2 sense are provided, together with upper bounds on the classification error that is shown to converge to the Bayes risk, therefore proving the Bayes-consistency of a variety of methods although the regularization term does not vanish. These results are particularly relevant to the one-class SVM, for which the regularization can not vanish by construction, and which is shown for the first time to be a consistent density level set estimator.",,,,,,"Vert, Jean-Philippe/0000-0001-9510-8441",,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,817,854,,,,,,,,,,,,,,,,WOS:000240173400005,0
J,"Khardon, R; Servedio, RA",,,,"Khardon, R; Servedio, RA",,,Maximum margin algorithms with Boolean kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent work has introduced Boolean kernels with which one can learn linear threshold functions over a feature space containing all conjunctions of length up to k (for any 1 <= k <= n) over the original n Boolean features in the input space. This motivates the question of whether maximum margin algorithms such as Support Vector Machines can learn Disjunctive Normal Form expressions in the Probably Approximately Correct (PAC) learning model by using this kernel. We study this question, as well as a variant in which structural risk minimization (SRM) is performed where the class hierarchy is taken over the length of conjunctions. We show that maximum margin algorithms using the Boolean kernels do not PAC learn t(n)term DNF for any t( n) = w(1), even when used with such a SRM scheme. We also consider PAC learning under the uniform distribution and show that if the kernel uses conjunctions of length (w) over tilde(root n) then the maximum margin hypothesis will fail on the uniform distribution as well. Our results concretely illustrate that margin based algorithms may overfit when learning simple target functions with natural kernels.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1405,1429,,,,,,,,,,,,,,,,WOS:000236330100006,0
J,"Nakamura, A; Schmitt, M; Schmitt, N; Simon, HU",,,,"Nakamura, A; Schmitt, M; Schmitt, N; Simon, HU",,,Inner product spaces for Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian networks have become one of the major models used for statistical inference. We study the question whether the decisions computed by a Bayesian network can be represented within a low-dimensional inner product space. We focus on two-label classification tasks over the Boolean domain. As main results we establish upper and lower bounds on the dimension of the inner product space for Bayesian networks with an explicitly given (full or reduced) parameter collection. In particular, these bounds are tight up to a factor of 2. For some nontrivial cases of Bayesian networks we even determine the exact values of this dimension. We further consider logistic autoregressive Bayesian networks and show that every sufficiently expressive inner product space must have dimension at least Omega(n(2)), where n is the number of network nodes. We also derive the bound 2(Omega(n)) for an artificial variant of this network, thereby demonstrating the limits of our approach and raising an interesting open question. As a major technical contribution, this work reveals combinatorial and algebraic structures within Bayesian networks such that known methods for the derivation of lower bounds on the dimension of inner product spaces can be brought into play.",,,,,,"Nakamura, Atsuyoshi/0000-0001-7078-8655; Schmitt, Niels/0000-0002-8329-6833; Simon, Hans/0000-0002-1587-0944",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1383,1403,,,,,,,,,,,,,,,,WOS:000236330100005,0
J,"Murray, JF; Hughes, GF; Kreutz-Delgado, K",,,,"Murray, JF; Hughes, GF; Kreutz-Delgado, K",,,Machine learning methods for predicting failures in hard drives: A multiple-instance application,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,783,816,,,,,,,,,,,,,,,,WOS:000236329700003,0
J,"Mannor, S; Tsitsiklis, JN",,,,"Mannor, S; Tsitsiklis, JN",,,The sample complexity of exploration in the multi-armed bandit problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the multi-armed bandit problem under the PAC (probably approximately correct) model. It was shown by Even-Dar et al. ( 2002) that given n arms, a total of O (n/epsilon(2)) log (1/delta)) trials suffices in order to find an epsilon-optimal arm with probability at least 1 d. We establish a matching lower bound on the expected number of trials under any sampling policy. We furthermore generalize the lower bound, and show an explicit dependence on the ( unknown) statistics of the arms. We also provide a similar bound within a Bayesian setting. The case where the statistics of the arms are known but the identities of the arms are not, is also discussed. For this case, we provide a lower bound of Theta((1/epsilon(2)) (n + log (1/delta))) on the expected number of trials, as well as a sampling policy with a matching upper bound. If instead of the expected number of trials, we consider the maximum ( over all sample paths) number of trials, we establish a matching upper and lower bound of the form Theta((n/epsilon(2)) log (1/delta)). Finally, we derive lower bounds on the expected regret, in the spirit of Lai and Robbins.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,623,648,,,,,,,,,,,,,,,,WOS:000236327700003,0
J,"von Luxburg, U; Bousquet, O",,,,"von Luxburg, U; Bousquet, O",,,Distance-based classification with Lipschitz functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of this article is to develop a framework for large margin classification in metric spaces. We want to find a generalization of linear decision functions for metric spaces and define a corresponding notion of margin such that the decision function separates the training points with a large margin. It will turn out that using Lipschitz functions as decision functions, the inverse of the Lipschitz constant can be interpreted as the size of a margin. In order to construct a clean mathematical setup we isometrically embed the given metric space into a Banach space and the space of Lipschitz functions into its dual space. To analyze the resulting algorithm, we prove several representer theorems. They state that there always exist solutions of the Lipschitz classifier which can be expressed in terms of distance functions to training points. We provide generalization bounds for Lipschitz classifiers in terms of the Rademacher complexities of some Lipschitz function classes. The generality of our approach can be seen from the fact that several well-known algorithms are special cases of the Lipschitz classifier, among them the support vector machine, the linear programming machine, and the 1-nearest neighbor classifier.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,669,695,,,,,,,,,,,,,,,,WOS:000236327700005,0
J,"von Luxburg, U; Bousquet, O; Scholkopf, B",,,,"von Luxburg, U; Bousquet, O; Scholkopf, B",,,A compression approach to support vector model selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we investigate connections between statistical learning theory and data compression on the basis of support vector machine (SVM) model selection. Inspired by several generalization bounds we construct compression coefficients for SVMs which measure the amount by which the training labels can be compressed by a code built from the separating hyperplane. The main idea is to relate the coding precision to geometrical concepts such as the width of the margin or the shape of the data in the feature space. The so derived compression coefficients combine well known quantities such as the radius-margin term R-2/p(2), the eigenvalues of the kernel matrix, and the number of support vectors. To test whether they are useful in practice we ran model selection experiments on benchmark data sets. As a result we found that compression coefficients can fairly accurately predict the parameters for which the test error is minimized.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,1532-4435,,,,,APR,2004,5,,,,,,293,323,,,,,,,,,,,,,,,,WOS:000236327400001,0
J,"Huang, FH; Gao, SQ; Pei, J; Huang, H",,,,"Huang, Feihu; Gao, Shangqian; Pei, Jian; Huang, Heng",,,Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the paper, we propose a class of accelerated zeroth-order and first-order momentum methods for both nonconvex mini-optimization and minimax-optimization. Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM) method for black-box mini-optimization where only function values can be obtained. Moreover, we prove that our Acc-ZOM method achieves a lower query complexity of (O) over tilde (d(3/4)epsilon(-3)) for finding an epsilon-stationary point, which improves the best known result by a factor of O(d(1/4)) where d denotes the variable dimension. In particular, our Acc-ZOM does not need large batches required in the existing zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated zeroth-order momentum descent ascent (Acc-ZOMDA) method for black-box minimax optimization, where only function values can be obtained. Our Acc-ZOMDA obtains a low query complexity of (O) over tilde((d(1) + d(2))(3/4)kappa(4:5)(y)epsilon(-3)) without requiring large batches for finding an epsilon-stationary point, where d(1) and d(2) denote variable dimensions and kappa(y) is condition number. Moreover, we propose an accelerated first-order momentum descent ascent (Acc-MDA) method for minimax optimization, whose explicit gradients are accessible. Our Acc-MDA achieves a low gradient complexity of (O) over tilde (kappa(4.5)(y)epsilon(-3)) without requiring large batches for finding an epsilon-stationary point. In particular, our Acc-MDA can obtain a lower gradient complexity of (O) over tilde (kappa(2.5)(y)epsilon(-3)) with a batch size O(kappa(4)(y)), which improves the best known result by a factor of O(kappa(1/2)(y)). Extensive experimental results on black-box adversarial attack to deep neural networks and poisoning attack to logistic regression demonstrate efficiency of our algorithms.",,,,,,"Pei, Jian/0000-0002-2200-8711",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,70,,,,,,,,,,,,,,,,WOS:000766877500001,0
J,"Lou, ZP; Zhu, WR; Wu, WB",,,,"Lou, Zhipeng; Zhu, Wanrong; Wu, Wei Biao",,,Beyond Sub-Gaussian Noises: Sharp Concentration Analysis for Stochastic Gradient Descent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the concentration property of stochastic gradient descent (SGD) solutions. In existing concentration analyses, researchers impose restrictive requirements on the gradient noise, such as boundedness or sub-Gaussianity. We consider a much richer class of noise where only finitely-many moments are required, thus allowing heavy-tailed noises. In particular, we obtain Nagaev type high-probability upper bounds for the estimation errors of averaged stochastic gradient descent (ASGD) in a linear model. Specifically, we prove that, after T steps of SGD, the ASGD estimate achieves an O(root log(1/delta)/T + (delta Tq-1)(-1/q)) error rate with probability at least 1 - delta, where q > 2 controls the tail of the gradient noise. In comparison, one has the O(root log(1/delta)/T) error rate for sub-Gaussian noises. We also show that the Nagaev type upper bound is almost tight through an example, where the exact asymptotic form of the tail probability can be derived. Our concentration analysis indicates that, in the case of heavy-tailed noises, the polynomial dependence on the failure probability delta is generally unavoidable for the error rate of SGD.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752328200001,0
J,"Vono, M; Paulin, D; Doucet, A",,,,"Vono, Maxime; Paulin, Daniel; Doucet, Arnaud",,,Efficient MCMC Sampling with Dimension-Free Convergence Rate using ADMM-type Splitting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Performing exact Bayesian inference for complex models is computationally intractable. Markov chain Monte Carlo (MCMC) algorithms can provide reliable approximations of the posterior distribution but are expensive for large data sets and high-dimensional models. A standard approach to mitigate this complexity consists in using subsampling techniques or distributing the data across a cluster. However, these approaches are typically unreliable in high-dimensional scenarios. We focus here on a recent alternative class of MCMC schemes exploiting a splitting strategy akin to the one used by the celebrated alternating direction method of multipliers (ADMM) optimization algorithm. These methods appear to provide empirically state-of-the-art performance but their theoretical behavior in high dimension is currently unknown. In this paper, we propose a detailed theoretical study of one of these algorithms known as the split Gibbs sampler. Under regularity conditions, we establish explicit convergence rates for this scheme using Ricci curvature and coupling ideas. We support our theory with numerical illustrations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,69,,,,,,,,,,,,,,,,WOS:000752354200001,0
J,"Agrawal, R; Horel, T",,,,"Agrawal, Rohit; Horel, Thibaut",,,Optimal Bounds between f-Divergences and Integral Probability Metrics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The families of f-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are widely used to quantify the similarity between probability distributions. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the f-divergence, we derive a generalization of the moment-generating function, which we show exactly characterizes the best lower bound of the f-divergence as a function of a given IPM. Using this characterization, we obtain new bounds while also recovering in a unified manner well-known results, such as Hoeffding's lemma, Pinsker's inequality and its extension to subgaussian functions, and the Hammersley-Chapman-Robbins bound. This characterization also allows us to prove new results on topological properties of the divergence which may be of independent interest.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,128,,,,,,,,,,,,,,,WOS:000663181900001,0
J,"Christof, C",,,,"Christof, Constantin",,,On the Stability Properties and the Optimization Landscape of Training Problems with Squared Loss for Neural Networks and General Nonlinear Conic Approximation Schemes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the optimization landscape and the stability properties of training problems with squared loss for neural networks and general nonlinear conic approximation schemes in a deterministic setting. It is demonstrated that, if a nonlinear conic approximation scheme is considered that is (in an appropriately defined sense) more expressive than a classical linear approximation approach and if there exist unrealizable label vectors, then a training problem with squared loss is necessarily unstable in the sense that its solution set depends discontinuously on the label vector in the training data. We further prove that the same effects that are responsible for these instability properties are also the reason for the emergence of saddle points and spurious local minima, which may be arbitrarily far away from global solutions, and that neither the instability of the training problem nor the existence of spurious local minima can, in general, be overcome by adding a regularization term to the objective function that penalizes the size of the parameters in the approximation scheme. The latter results are shown to be true regardless of whether the assumption of realizability is satisfied or not. It is further established that there exists a direct and quantifiable relationship between the analyzed instability properties and the expressiveness of the considered approximation instrument and that the set of training label vectors and, in the regularized case, Tikhonov regularization parameters that give rise to spurious local minima has a nonempty interior. We demonstrate that our analysis in particular applies to training problems for free-knot interpolation schemes and deep and shallow neural networks with variable widths that involve an arbitrary mixture of various activation functions (e.g., binary, sigmoid, tanh, arctan, soft-sign, ISRU, soft-clip, SQNL, ReLU, leaky ReLU, soft-plus, bent identity, SILU, ISRLU, and ELU). In summary, the findings of this paper illustrate that the improved approximation properties of neural networks and general nonlinear conic approximation instruments come at a price and are linked in a direct and quantifiable way to undesirable properties of the optimization problems that have to be solved in order to train them.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,77,,,,,,,,,,,,,,,,WOS:000726664300001,0
J,"Fan, JQ; Jiang, B; Sun, Q",,,,"Fan, Jianqing; Jiang, Bai; Sun, Qiang",,,Hoeffding's Inequality for General Markov Chains and Its Applications to Statistical Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper establishes Hoeffding's lemma and inequality for bounded functions of general-state space and not necessarily reversible Markov chains. The sharpness of these results is characterized by the optimality of the ratio between variance proxies in the Markov-dependent and independent settings. The boundedness of functions is shown necessary for such results to hold in general. To showcase the usefulness of the new results, we apply them for non-asymptotic analyses of MCMC estimation, respondent-driven sampling and high-dimensional covariance matrix estimation on time series data with a Markovian nature. In addition to statistical problems, we also apply them to study the time-discounted rewards in econometric models and the multi-armed bandit problem with Markovian rewards arising from the field of machine learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687041100001,0
J,"Lim, SH",,,,"Lim, Soon Hoe",,,Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recurrent neural networks (RNNs) are brain-inspired models widely used in machine learning for analyzing sequential data. The present work is a contribution towards a deeper understanding of how RNNs process input signals using the response theory from nonequilibrium statistical mechanics. For a class of continuous-time stochastic RNNs (SRNNs) driven by an input signal, we derive a Volterra type series representation for their output. This representation is interpretable and disentangles the input signal from the SRNN architecture. The kernels of the series are certain recursively defined correlation functions with respect to the unperturbed dynamics that completely determine the output. Exploiting connections of this representation and its implications to rough paths theory, we identify a universal feature {the response feature, which turns out to be the signature of tensor product of the input signal and a natural support basis. In particular, we show that SRNNs, with only the weights in the readout layer optimized and the weights in the hidden layer kept fixed and not optimized, can be viewed as kernel machines operating on a reproducing kernel Hilbert space associated with the response feature.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500047,0
J,"Petersen, L; Hansen, NR",,,,"Petersen, Lasse; Hansen, Niels Richard",,,Testing Conditional Independence via Quantile Regression Based Partial Copulas,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The partial copula provides a method for describing the dependence between two random variables X and Y conditional on a third random vector Z in terms of nonparametric residuals U-1 and U-2. This paper develops a nonparametric test for conditional independence by combining the partial copula with a quantile regression based method for estimating the nonparametric residuals. We consider a test statistic based on generalized correlation between U-1 and U-2 and derive its large sample properties under consistency assumptions on the quantile regression procedure. We demonstrate through a simulation study that the resulting test is sound under complicated data generating distributions. Moreover, in the examples considered the test is competitive to other state-of-the-art conditional independence tests in terms of level and power, and it has superior power in cases with conditional variance heterogeneity of X and Y given Z.",,,,,"; Hansen, Niels Richard/N-8616-2014","Petersen, Lasse/0000-0002-8195-7650; Hansen, Niels Richard/0000-0003-3883-365X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656372200001,0
J,"Yang, ZB; Zhang, AJ",,,,"Yang, Zebin; Zhang, Aijun",,,Hyperparameter Optimization via Sequential Uniform Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hyperparameter optimization (HPO) plays a central role in the automated machine learning (AutoML). It is a challenging task as the response surfaces of hyperparameters are generally unknown, hence essentially a global optimization problem. This paper reformulates HPO as a computer experiment and proposes a novel sequential uniform design (SeqUD) strategy with three-fold advantages: a) the hyperparameter space is adaptively explored with evenly spread design points, without the need of expensive meta-modeling and acquisition optimization; b) the batch-by-batch design points are sequentially generated with parallel processing support; c) a new augmented uniform design algorithm is developed for the efficient real-time generation of follow-up design points. Extensive experiments are conducted on both global optimization tasks and HPO applications. The numerical results show that the proposed SeqUD strategy outperforms benchmark HPO methods, and it can be therefore a promising and competitive alternative to existing AutoML tools.",,,,,,"Yang, Zebin/0000-0001-5683-7502",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700316400001,0
J,"Zhuo, BM; Gao, C",,,,"Zhuo, Bumeng; Gao, Chao",,,Mixing Time of Metropolis-Hastings for Bayesian Community Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the computational complexity of a Metropolis-Hastings algorithm for Bayesian community detection. We first establish a posterior strong consistency result for a natural prior distribution on stochastic block models under the optimal signal-to-noise ratio condition in the literature. We then give a set of conditions that guarantee rapidly mixing of a simple Metropolis-Hastings algorithm. The mixing time analysis is based on a careful study of posterior ratios and a canonical path argument to control the spectral gap of the Markov chain.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500010,0
J,"Bing, X; Bunea, F; Wegkamp, M",,,,"Bing, Xin; Bunea, Florentina; Wegkamp, Marten",,,Optimal Estimation of Sparse Topic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Topic models have become popular tools for dimension reduction and exploratory analysis of text data which consists in observed frequencies of a vocabulary of p words in n documents, stored in a p x n matrix. The main premise is that the mean of this data matrix can be factorized into a product of two non-negative matrices: a p x K word-topic matrix A and a K x n topic-document matrix W. This paper studies the estimation of A that is possibly element-wise sparse, and the number of topics K is unknown. In this under-explored context, we derive a new minimax lower bound for the estimation of such A and propose a new computationally efficient algorithm for its recovery. We derive a finite sample upper bound for our estimator, and show that it matches the minimax lower bound in many scenarios. Our estimate adapts to the unknown sparsity of A and our analysis is valid for any finite n, p, K and document lengths. Empirical results on both synthetic data and semi-synthetic data show that our proposed estimator is a strong competitor of the existing state-of-the-art algorithms for both non-sparse A and sparse A, and has superior performance is many scenarios of interest.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,177,,,,,,,,,,,,,,,WOS:000570208200001,0
J,"Ciliberto, C; Rosasco, L; Rudi, A",,,,"Ciliberto, Carlo; Rosasco, Lorenzo; Rudi, Alessandro",,,A General Framework for Consistent Structured Prediction with Implicit Loss Embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose and analyze a novel theoretical and algorithmic framework for structured prediction. While so far the term has referred to discrete output spaces, here we consider more general settings, such as manifolds or spaces of probability measures. We define structured prediction as a problem where the output space lacks a vectorial structure. We identify and study a large class of loss functions that implicitly defines a suitable geometry on the problem. The latter is the key to develop an algorithmic framework amenable to a sharp statistical analysis and yielding efficient computations. When dealing with output spaces with infinite cardinality, a suitable implicit formulation of the estimator is shown to be crucial.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,98,,,,,,,,,,,,,,,WOS:000546143800001,0
J,"Hoffmann, F; Hosseini, B; Ren, Z; Stuart, AM",,,,"Hoffmann, Franca; Hosseini, Bamdad; Ren, Zhi; Stuart, Andrew M.",,,Consistency of Semi-Supervised Learning Algorithms on Graphs: Probit and One-Hot Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph-based semi-supervised learning is the problem of propagating labels from a small number of labelled data points to a larger set of unlabelled data. This paper is concerned with the consistency of optimization-based techniques for such problems, in the limit where the labels have small noise and the underlying unlabelled data is well clustered. We study graph-based probit for binary classification, and a natural generalization of this method to multi-class classification using one-hot encoding. The resulting objective function to be optimized comprises the sum of a quadratic form defined through a rational function of the graph Laplacian, involving only the unlabelled data, and a fidelity term involving only the labelled data. The consistency analysis sheds light on the choice of the rational function defining the optimization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,186,,,,,,,,,,,,,,,WOS:000570236800001,0
J,"Li, XX; Li, RZ; Xia, ZM; Xu, C",,,,"Li, Xingxiang; Li, Runze; Xia, Zhiming; Xu, Chen",,,Distributed Feature Screening via Componentwise Debiasing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Feature screening is a powerful tool in processing high-dimensional data. When the sample size N and the number of features p are both large, the implementation of classic screening methods can be numerically challenging. In this paper, we propose a distributed screening framework for big data setup. In the spirit of divide-and-conquer, the proposed framework expresses a correlation measure as a function of several component parameters, each of which can be distributively estimated using a natural U-statistic from data segments. With the component estimates aggregated, we obtain a final correlation estimate that can be readily used for screening features. This framework enables distributed storage and parallel computing and thus is computationally attractive. Due to the unbiased distributive estimation of the component parameters, the final aggregated estimate achieves a high accuracy that is insensitive to the number of data segments m. Under mild conditions, we show that the aggregated correlation estimator is as efficient as the centralized estimator in terms of the probability convergence bound and the mean squared error rate; the corresponding screening procedure enjoys sure screening property for a wide range of correlation measures. The promising performances of the new method are supported by extensive numerical examples.",,,,,"Li, Runze/ABF-1320-2020; Li, Runze/HCH-8063-2022; Li, Runze/C-5444-2013","Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300024,0
J,"Liu, YT; Pages, G",,,,"Liu, Yating; Pages, Gilles",,,Convergence Rate of Optimal Quantization and Application to the Clustering Performance of the Empirical Measure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the convergence rate of the optimal quantization for a probability measure sequence (mu(n))(n is an element of N)* on R-d converging in the Wasserstein distance in two aspects: the first one is the convergence rate of optimal quantizer x((n)) is an element of (R-d)(K) of mu(n) at level K; the other one is the convergence rate of the distortion function valued at x((n)), called the performance of x((n)). Moreover, we also study the mean performance of the optimal quantization for the empirical measure of a distribution mu with finite second moment but possibly unbounded support. As an application, we show an upper bound with a convergence rate O(log n/root n) of the mean performance for the empirical measure of the multidimensional normal distribution N (m, Sigma) and of distributions with hyper-exponential tails. This extends the results from Biau et al. (2008) obtained for compactly supported distribution. We also derive an upper bound which is sharper in the quantization level K but suboptimal in n by applying results in Fournier and Guillin (2015).",,,,,"Pages, Gilles/AAK-7653-2021","Pages, Gilles/0000-0001-6487-3079; Liu, Yating/0000-0002-5041-3140",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,86,,,,,,,,,,,,,,,WOS:000545026400001,0
J,"Slawski, M; Ben-David, E; Li, P",,,,"Slawski, Martin; Ben-David, Emanuel; Li, Ping",,,Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as Unlabeled Sensing and Regression with Unknown Permutation. In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses m per observation increase. Particularly, for m = Omega(log n), the required signal-to-noise ratio no longer depends on the sample size n. Numerical results on synthetic and real data are presented to support the main findings of our analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,204,,,,,,,,,,,,,,,WOS:000590007800001,0
J,"Tonolini, F; Radford, J; Turpin, A; Faccio, D; Murray-Smith, R",,,,"Tonolini, Francesco; Radford, Jack; Turpin, Alex; Faccio, Daniele; Murray-Smith, Roderick",,,Variational Inference for Computational Imaging Inverse Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.",,,,,"Turpin, Alejandro/K-4732-2013","Turpin, Alejandro/0000-0001-7877-782X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,179,,,,,,,,,,,,,,,WOS:000570234100001,0
J,"Yuan, XT; Liu, B; Wang, LZ; Liu, QS; Metaxas, DN",,,,"Yuan, Xiao-Tong; Liu, Bo; Wang, Lezi; Liu, Qingshan; Metaxas, Dimitris N.",,,Dual Iterative Hard Thresholding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Iterative Hard Thresholding (IHT) is a popular class of first-order greedy selection methods for loss minimization under cardinality constraint. The existing IHT-style algorithms, however, are proposed for minimizing the primal formulation. It is still an open issue to explore duality theory and algorithms for such a non-convex and NP-hard combinatorial optimization problem. To address this issue, we develop in this article a novel duality theory for l(2)-regularized empirical risk minimization under cardinality constraint, along with an IHT-style algorithm for dual optimization. Our sparse duality theory establishes a set of sufficient and/or necessary conditions under which the original non-convex problem can be equivalently or approximately solved in a concave dual formulation. In view of this theory, we propose the Dual IHT (DIHT) algorithm as a super-gradient ascent method to solve the non-smooth dual problem with provable guarantees on primal-dual gap convergence and sparsity recovery. Numerical results confirm our theoretical predictions and demonstrate the superiority of DIHT to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.(1)",,,,,"Liu, Qing/GWC-9222-2022; liu, qingqing/HHD-0360-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,152,,,,,,,,,,,,,,,WOS:000570114500001,0
J,"Zhou, ZX; Amini, AA",,,,"Zhou, Zhixin; Amini, Arash A.",,,Optimal Bipartite Network Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study bipartite community detection in networks, or more generally the network biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems. The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of average degree growth, from sparse networks with average degrees growing arbitrarily slowly to fairly dense networks with average degrees of order root n. As a special case, we recover the known exact recovery threshold in the log n regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class by numerical simulations.",,,,,"Zhou, Zhixin/AAW-6707-2021; Zhou, Zhixin/GSD-7016-2022","ZHOU, Zhixin/0000-0003-3737-9248",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000015,0
J,"Duchi, J; Namkoong, H",,,,"Duchi, John; Namkoong, Hongseok",,,Variance-based Regularization with Convex Objectives,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally e ffi cient trading between approximation and estimation error. Our approach builds o ff of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of fi nite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certi fi cates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classi fi cation problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,68,,,,,,,,,,,,,,,WOS:000467894600001,0
J,"Kabashima, Y",,,,"Kabashima, Yoshiyuki",,,Semi-Analytic Resampling in Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An approximate method for conducting resampling in Lasso, the l(1) penalized linear regression, in a semi-analytic manner is developed, whereby the average over the resampled datasets is directly computed without repeated numerical sampling, thus enabling an inference free of the statistical fluctuations due to sampling fi niteness, as well as a signi fi cant reduction of computational time. The proposed method is based on a message passing type algorithm, and its fast convergence is guaranteed by the state evolution analysis, when covariates are provided as zero-mean independently and identically distributed Gaussian random variables. It is employed to implement bootstrapped Lasso (Bolasso) and stability selection, both of which are variable selection methods using resampling in conjunction with Lasso, and resolves their disadvantage regarding computational cost. To examine approximation accuracy and e ffi ciency, numerical experiments were carried out using simulated datasets. Moreover, an application to a real-world dataset, the wine quality dataset, is presented. To process such real-world datasets, an objective criterion for determining the relevance of selected variables is also introduced by the addition of noise variables and resampling. MATLAB codes implementing the proposed method are distributed in (Obuchi, 2018).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,70,,,,,,,,,,,,,,,WOS:000467894800001,0
J,"Venturi, L; Bandeira, AS; Bruna, J",,,,"Venturi, Luca; Bandeira, Afonso S.; Bruna, Joan",,,Spurious Valleys in One-hidden-layer Neural Network Optimization Landscapes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Neural networks provide a rich class of high-dimensional, non-convex optimization problems. Despite their non-convexity, gradient-descent methods often successfully optimize these models. This has motivated a recent spur in research attempting to characterize properties of their loss surface that may explain such success. In this paper, we address this phenomenon by studying a key topological property of the loss: the presence or absence of spurious valleys, defined as connected components of sub-level sets that do not include a global minimum. Focusing on a class of one-hidden-layer neural networks defined by smooth (but generally non-linear) activation functions, we identify a notion of intrinsic dimension and show that it provides necessary and sufficient conditions for the absence of spurious valleys. More concretely, finite intrinsic dimension guarantees that for sufficiently overparametrised models no spurious valleys exist, independently of the data distribution. Conversely, infinite intrinsic dimension implies that spurious valleys do exist for certain data distributions, independently of model overparametrisation. Besides these positive and negative results, we show that, although spurious valleys may exist in general, they are confined to low risk levels and avoided with high probability on overparametrised models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,133,,,,,,,,,,,,,,,WOS:000487068900017,0
J,"Wang, JL; Zhang, T",,,,"Wang, Jialei; Zhang, Tong",,,Utilizing Second Order Information in Minibatch Stochastic Variance Reduced Proximal Iterations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel minibatch stochastic optimization method for empirical risk minimization of linear predictors. The method efficiently leverages both sub-sampled first-order and higher-order information, by incorporating variance-reduction and acceleration techniques. We prove improved iteration complexity over state-of-the-art methods under suitable conditions. In particular, the approach enjoys global fast convergence for quadratic convex objectives and local fast convergence for general convex objectives. Experiments are provided to demonstrate the empirical advantage of the proposed method over existing approaches in the literature.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,42,,,,,,,,,,,,,,,WOS:000463320000001,0
J,"Wang, XZ; Yang, ZY; Chen, X; Liu, WD",,,,"Wang, Xiaozhou; Yang, Zhuoyi; Chen, Xi; Liu, Weidong",,,Distributed Inference for Linear Support Vector Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The growing size of modern data brings many new challenges to existing statistical inference methodologies and theories, and calls for the development of distributed inferential approaches. This paper studies distributed inference for linear support vector machine (SVM) for the binary classification task. Despite a vast literature on SVM, much less is known about the inferential properties of SVM, especially in a distributed setting. In this paper, we propose a multi-round distributed linear-type (MDL) estimator for conducting inference for linear SVM. The proposed estimator is computationally efficient. In particular, it only requires an initial SVM estimator and then successively refines the estimator by solving simple weighted least squares problem. Theoretically, we establish the Bahadur representation of the estimator. Based on the representation, the asymptotic normality is further derived, which shows that the MDL estimator achieves the optimal statistical efficiency, i.e., the same efficiency as the classical linear SVM applying to the entire data set in a single machine setup. Moreover, our asymptotic result avoids the condition on the number of machines or data batches, which is commonly assumed in distributed estimation literature, and allows the case of diverging dimension. We provide simulation studies to demonstrate the performance of the proposed MDL estimator.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,113,,,,,,,,,,,,,,,WOS:000476624500001,0
J,"Ah-Pine, J",,,,"Ah-Pine, Julien",,,An Efficient and Effective Generic Agglomerative Hierarchical Clustering Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce an agglomerative hierarchical clustering (AHC) framework which is generic, efficient and effective. Our approach embeds a sub-family of Lance-Williams (LW) clusterings and relies on inner-products instead of squared Euclidean distances. We carry out a constrained bottom-up merging procedure on a sparsified normalized inner-product matrix. Our method is named SNK-AHC for Sparsified Normalized Kernel matrix based AHC. SNK-AHC is more scalable than the classic dissimilarity matrix based AHC. It can also produce better results when clusters have arbitrary shapes. Artificial and real-world benchmarks are used to exemplify these points. From a theoretical standpoint, SNK-AHC provides another interpretation of the classic techniques which relies on the concept of weighted penalized similarities. The differences between group average, Mcquitty, centroid, median and Ward, can be explained by their distinct averaging strategies for aggregating clusters inter-similarities and intra-similarities. Other features of SNK-AHC are examined. We provide sufficient conditions in order to have monotonic dendrograms, we elaborate a stored data matrix approach for centroid and median, we underline the diagonal translation invariance property of group average, Mcquitty and Ward and we show to what extent SNK-AHC can determine the number of clusters.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444407700001,0
J,"Borgs, C; Chayes, JT; Cohn, H; Holden, N",,,,"Borgs, Christian; Chayes, Jennifer T.; Cohn, Henry; Holden, Nina",,,Sparse Exchangeable Graphs and Their Limits via Graphon Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in R+. Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over sigma-finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a sigma-finite measure space (S, S, mu) and the connection probabilities by an integrable function W : S x S -> [ 0,1], we construct a random family (G(t))(t) (>= 0) of growing graphs such that the vertices of G(t) are given by a Poisson point process on S with intensity t mu, with two points x, y of the point process connected with probability W(x,y). We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly in finite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on R+ equipped with Lebesgue measure.",,,,,,"Borgs, Christian/0000-0001-5653-0498",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,71,,,,,,,,,,,,,,,,WOS:000435628300001,0
J,"Li, ZF; Tewari, A",,,,"Li, Zifan; Tewari, Ambuj",,,Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,183,,,,,,,,,,,,,,,WOS:000435449100001,0
J,"Lucic, M; Faulkner, M; Krause, A; Feldman, D",,,,"Lucic, Mario; Faulkner, Matthew; Krause, Andreas; Feldman, Dan",,,Training Gaussian Mixture Models at Scale via Coresets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"How can we train a statistical mixture model on a massive data set? In this work we show how to construct coresets for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real-world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,160,,,,,,,,,,,,,,,WOS:000433253700001,0
J,"Szabo, Z; Sriperumbudur, BK",,,,"Szabo, Zoltan; Sriperumbudur, Bharath K.",,,Characteristic and Universal Tensor Product Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,233,,,,,,,,,,,,,,,WOS:000440886600001,0
J,"Tosh, C; Dasgupta, S",,,,"Tosh, Christopher; Dasgupta, Sanjoy",,,Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper presents NP-hardness and hardness of approximation results for maximum likelihood estimation of mixtures of spherical Gaussians.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,175,,,,,,,,,,,,,,,WOS:000435442900001,0
J,"Bach, F",,,,"Bach, Francis",,,Breaking the Curse of Dimensionality with Convex Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,19,,,,,,,,,,,,,,,WOS:000399840100001,0
J,"Hallac, D; Wong, C; Diamond, S; Sharang, A; Sosic, R; Boyd, S; Leskovec, J",,,,"Hallac, David; Wong, Christopher; Diamond, Steven; Sharang, Abhijit; Sosic, Rok; Boyd, Stephen; Leskovec, Jure",,,SnapVX: A Network-Based Convex Optimization Solver,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"SnapVX is a high-performance Python solver for convex optimization problems defined on networks. For these problems, it provides a fast and scalable solution with guaranteed global convergence. SnapVX combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use interface with out-of-the-box functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, and solve large optimization problems from a variety of different applications.",,,,,,"Leskovec, Jure/0000-0002-5411-923X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,4,,,,,,,,,,,,,,,WOS:000397018200001,0
J,"Kucukelbir, A; Tran, D; Ranganath, R; Gelman, A; Blei, DM",,,,"Kucukelbir, Alp; Tran, Dustin; Ranganath, Rajesh; Gelman, Andrew; Blei, David M.",,,Automatic Differentiation Variational Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy ADVI as part of Stan, a probabilistic programming system.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,45,,,,,,,,,,,,,,,,WOS:000399838100001,0
J,"Lin, L; Li, J",,,,"Lin, Lin; Li, Jia",,,Clustering with Hidden Markov Model on Variable Blocks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods.",,,,,,"Lin, Lin/0000-0002-7464-1172",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,110,,,,,,,,,,,,,,,WOS:000424538900001,0
J,"Papyan, V; Romano, Y; Elad, M",,,,"Papyan, Vardan; Romano, Yaniv; Elad, Michael",,,Convolutional Neural Networks Analyzed via Convolutional Sparse Coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Convolutional neural networks (CNN) have led to many state-of-the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees.",,,,,", Miki/AAH-4640-2019",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,52,83,,,,,,,,,,,,,,,WOS:000412062800001,0
J,"van Hoof, H; Neumann, G; Peters, J",,,,"van Hoof, Herke; Neumann, Gerhard; Peters, Jan",,,Non-parametric Policy Search with Limited Information Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non-parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data.",,,,,"van Hoof, Herke/N-7775-2017; Peters, Jan R/D-5068-2009; Peters, Jan/P-6027-2019","van Hoof, Herke/0000-0002-1583-3692; Peters, Jan R/0000-0002-5266-8091; Peters, Jan/0000-0002-5266-8091",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000412059700001,0
J,"Andresen, A; Spokoiny, V",,,,"Andresen, Andreas; Spokoiny, Vladimir",,,Convergence of an Alternating Maximization Procedure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive two convergence results for a sequential alternating maximization procedure to approximate the maximizer of random functionals such as the realized log likelihood in MLE estimation. We manage to show that the sequence attains the same deviation properties as shown for the profile M-estimator by Andresen and Spokoiny (2013), that means a finite sample Wilks and Fisher theorem. Further under slightly stronger smoothness constraints on the random functional we can show nearly linear convergence to the global maximizer if the starting point for the procedure is well chosen.",,,,,"Spokoiny, Vladimir/AAF-4942-2021; Spokoiny, Vladimir G./L-5441-2015","Spokoiny, Vladimir/0000-0002-2040-3427; Spokoiny, Vladimir G./0000-0002-2040-3427",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,63,,,,,,,,,,,,,,,WOS:000391522300001,0
J,"Baktashmotlagh, M; Harandi, M; Salzmann, M",,,,"Baktashmotlagh, Mahsa; Harandi, Mehrtash; Salzmann, Mathieu",,,Distribution-Matching Embedding for Visual Domain Adaptation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Distribution-Matching Embedding approach: An unsupervised domain adaptation method that overcomes this issue by mapping the data to a latent space where the distance between the empirical distributions of the source and target examples is minimized. In other words, we seek to extract the information that is invariant across the source and target data. In particular, we study two different distances to compare the source and target distributions: the Maximum Mean Discrepancy and the Hellinger distance. Furthermore, we show that our approach allows us to learn either a linear embedding, or a nonlinear one. We demonstrate the benefits of our approach on the tasks of visual object recognition, text categorization, and WiFi localization.",,,,,"Harandi, Mehrtash/D-6586-2018","Harandi, Mehrtash/0000-0002-6937-6300; Salzmann, Mathieu/0000-0002-8347-8637",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,108,,,,,,,,,,,,,,,WOS:000391547000001,0
J,"Barber, RF; Sidky, EY",,,,"Barber, Rina Foygel; Sidky, Emil Y.",,,MOCCA: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many optimization problems arising in high-dimensional statistics decompose naturally into a sum of several terms, where the individual terms are relatively simple but the composite objective function can only be optimized with iterative algorithms. In this paper, we are interested in optimization problems of the form F(Kx) + G(x), where K is a fixed linear transformation, while F and G are functions that may be nonconvex and/or nondifferentiable. In particular, if either of the terms are nonconvex, existing alternating minimization techniques may fail to converge; other types of existing approaches may instead be unable to handle nondifferentiability. We propose the MOCCA (mirrored convex/concave) algorithm, a primal/dual optimization approach that takes a local convex approximation to each term at every iteration. Inspired by optimization problems arising in computed tomography (CT) imaging, this algorithm can handle a range of nonconvex composite optimization problems, and offers theoretical guarantees for convergence when the overall problem is approximately convex (that is, any concavity in one term is balanced out by convexity in the other term). Empirical results show fast convergence for several structured signal recovery problems.",,,,,"Sidky, Emil/P-7957-2019",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,144,,,,,,,,,,29391859,,,,,WOS:000391661700001,0
J,"Bischl, B; Lang, M; Kotthoff, L; Schiffner, J; Richter, J; Studerus, E; Casalicchio, G; Jones, ZM",,,,"Bischl, Bernd; Lang, Michel; Kotthoff, Lars; Schiffner, Julia; Richter, Jajob; Studerus, Erich; Casalicchio, Giuseppe; Jones, Zachary M.",,,mlr: Machine Learning in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The MLR package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperpa-rameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.",,,,,"Kotthoff, Lars/AFV-6526-2022; Studerus, Erich/AAB-1632-2020","Studerus, Erich/0000-0003-4233-0182; Richter, Jakob/0000-0003-4481-5554; Kotthoff, Lars/0000-0003-4635-6873; Casalicchio, Giuseppe/0000-0001-5324-5966; Lang, Michel/0000-0001-9754-0393",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,170,,,,,,,,,,,,,,,WOS:000391675400001,0
J,"Clemencon, S; Colin, I; Bellet, A",,,,"Clemencon, Stephan; Colin, Igor; Bellet, Aurelien",,,Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a wide range of statistical learning problems such as ranking, clustering or metric learning among others, the risk is accurately estimated by U-statistics of degree d >= 1, i.e. functionals of the training data with low variance that take the form of averages over k-tuples. From a computational perspective, the calculation of such statistics is highly expensive even for a moderate sample size n, as it requires averaging O (n(d)) terms. This makes learning procedures relying on the optimization of such data functionals hardly feasible in practice. It is the major goal of this paper to show that, strikingly, such empirical risks can be replaced by drastically computationally simpler Monte-Carlo estimates based on O (n) terms only, usually referred to as incomplete U-statistics, without damaging the O-P (1/root n) learning rate of Empirical Risk Minimization (ERM) procedures. For this purpose, we establish uniform deviation results describing the error made when approximating a U-process by its incomplete version under appropriate complexity assumptions. Extensions to model selection, fast rate situations and various sampling techniques are also considered, as well as an application to stochastic gradient descent for ERM. Finally, numerical examples are displayed in order to provide strong empirical evidence that the approach we promote largely surpasses more naive subsampling techniques.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,76,,,,,,,,,,,,,,,WOS:000391524700001,0
J,"Lyzinski, V; Levin, K; Fishkind, DE; Priebe, CE",,,,"Lyzinski, Vince; Levin, Keith; Fishkind, Donniell E.; Priebe, Carey E.",,,On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph Matching,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a graph in which a few vertices are deemed interesting a priori, the vertex nomination task is to order the remaining vertices into a nomination list such that there is a concentration of interesting vertices at the top of the list. Previous work has yielded several approaches to this problem, with theoretical results in the setting where the graph is drawn from a stochastic block model (SBM), including a vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove that maximum likelihood (ML)-based vertex nomination is consistent, in the sense that the performance of the ML-based scheme asymptotically matches that of the Bayes optimal scheme. We prove theorems of this form both when model parameters are known and unknown. Additionally, we introduce and prove consistency of a related, more scalable restricted-focus ML vertex nomination scheme. Finally, we incorporate vertex and edge features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,179,,,,,,,,,,,,,,,WOS:000391681600001,0
J,"Nishyama, Y; Fukumizu, K",,,,"Nishyama, Yu; Fukumizu, Kenji",,,Characteristic Kernels and Infinitely Divisible Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We connect shift-invariant characteristic kernels to infinitely divisible distributions on R-d. Characteristic kernels play an important role in machine learning applications with their kernel means to distinguish any two probability measures. The contribution of this paper is twofold. First, we show, using the Levy-Khintchine formula, that any shift-invariant kernel given by a bounded, continuous, and symmetric probability density function (pdf) of an infinitely divisible distribution on R-d is characteristic. We mention some closure properties of such characteristic kernels under addition, pointwise product, and convolution. Second, in developing various kernel mean algorithms, it is fundamental to compute the following values: (i) kernel mean values m(p)(x), x is an element of X, and (ii) kernel mean RKHS inner products < m(P), m(Q)> (H), for probability measures P; Q. If P; Q, and kernel k are Gaussians, then the computation of (i) and (ii) results in Gaussian pdfs that are tractable. We generalize this Gaussian combination to more general cases in the class of in finitely divisible distributions. We then introduce a conjugate kernel and a convolution trick, so that the above (i) and (ii) have the same pdf form, expecting tractable computation at least in some cases. As specific instances, we explore alpha-stable distributions and a rich class of generalized hyperbolic distributions, where the Laplace, Cauchy, and Student's t distributions are included.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,180,,,,,,,,,,,,,,,WOS:000391682300001,0
J,"van Seijen, H; Mahmood, AR; Pilarski, PM; Machado, MC; Sutton, RS",,,,"van Seijen, Harm; Mahmood, A. Rupam; Pilarski, Patrick M.; Machado, Marlos C.; Sutton, Richard S.",,,True Online Temporal-Difference Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The temporal-difference methods TD(lambda) and Sarsa(lambda) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD(lambda) and true online Sarsa(lambda), respectively (van Seijen & Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD(lambda)/Sarsa(lambda) with regular TD(lambda)/Sarsa(lambda) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. Besides the empirical results, we provide an in-dept analysis of the theory behind true online temporal-difference learning. In addition, we show that new true online temporal-difference methods can be derived by making changes to the online forward view and then rewriting the update equations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,145,,,,,,,,,,,,,,,WOS:000391661800001,0
J,"Wang, YJ; Chen, TL; Zeng, DL",,,,"Wang, Yuanjia; Chen, Tianle; Zeng, Donglin",,,Support Vector Hazards Machine: A Counting Process Framework for Learning Risk Scores for Censored Outcomes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning risk scores to predict dichotomous or continuous outcomes using machine learning approaches has been studied extensively. However, how to learn risk scores for time-to-event outcomes subject to right censoring has received little attention until recently. Existing approaches rely on inverse probability weighting or rank-based regression, which may be inefficient. In this paper, we develop a new support vector hazards machine (SVHM) approach to predict censored outcomes. Our method is based on predicting the counting process associated with the time-to-event outcomes among subjects at risk via a series of support vector machines. Introducing counting processes to represent time-to-event data leads to a connection between support vector machines in supervised learning and hazards regression in standard survival analysis. To account for different at risk populations at observed event times, a time-varying off set is used in estimating risk scores. The resulting optimization is a convex quadratic programming problem that can easily incorporate nonlinearity using kernel trick. We demonstrate an interesting link from the pro filed empirical risk function of SVHM to the Cox partial likelihood. We then formally show that SVHM is optimal in discriminating covariate-specific hazard function from population average hazard function, and establish the consistency and learning rate of the predicted risk using the estimated risk scores. Simulation studies show improved prediction accuracy of the event times using SVHM compared to existing machine learning methods and standard conventional approaches. Finally, we analyze two real world biomedical study data where we use clinical markers and neuroimaging biomarkers to predict age-at-onset of a disease, and demonstrate superiority of SVHM in distinguishing high risk versus low risk subjects.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,37,167,,,,,,,,,,28066157,,,,,WOS:000391671800001,0
J,"Chen, YD; Bhojanapalli, S; Sanghavi, S; Ward, R",,,,"Chen, Yudong; Bhojanapalli, Srinadh; Sanghavi, Sujay; Ward, Rachel",,,"Completing Any Low-rank Matrix, Provably",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Matrix completion, i.e., the exact and provable recovery of a low-rank matrix from a small subset of its elements, is currently only known to be possible if the matrix satisfies a restrictive structural constraint known as incoherence-on its row and column spaces. In these cases, the subset of elements is assumed to be sampled uniformly at random. In this paper, we show that any rank-r n-by-n matrix can be exactly recovered from as few as O(nr log(2) n) randomly chosen elements, provided this random choice is made according to a specific biased distribution suitably dependent on the coherence structure of the matrix: the probability of any element being sampled should be at least a constant times the sum of the leverage scores of the corresponding row and column. Moreover, we prove that this specific form of sampling is nearly necessary, in a natural precise sense; this implies that many other perhaps more intuitive sampling schemes fail. We further establish three ways to use the above result for the setting when leverage scores are not known a priori. (a) We describe a provably-correct sampling strategy for the case when only the column space is incoherent and no assumption or knowledge of the row space is required. (b) We propose a two-phase sampling procedure for general matrices that first samples to estimate leverage scores followed by sampling for exact recovery. These two approaches assume control over the sampling procedure. (c) By using our main theorem in a reverse direction, we provide an analysis showing the advantages of the (empirically successful) weighted nuclear/trace-norm minimization approach over the vanilla un-weighted formulation given non-uniformly distributed observed elements. This approach does not require controlled sampling or knowledge of the leverage scores.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2999,3034,,,,,,,,,,,,,,,,WOS:000369888000023,0
J,"Helmbold, DP; Long, PM",,,,"Helmbold, David P.; Long, Philip M.",,,On the Inductive Bias of Dropout,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager et al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimized. We show: center dot when the dropout-regularized criterion has a unique minimizer, center dot when the dropout-regularization penalty goes to infinity with the weights, and when it remains bounded, center dot that the dropout regularization can be non-monotonic as individual weights increase from 0, and center dot that the dropout regularization penalty may not be convex. This last point is particularly surprising because the combination of dropout regularization with any convex loss proxy is always a convex function. In order to contrast dropout regularization with L-2 regularization, we formalize the notion of when different random sources of data are more compatible with different regularizers. We then exhibit distributions that are provably more compatible with dropout regularization than L-2 regularization, and vice versa. These sources provide additional insight into how the inductive biases of dropout and L-2 regularization differ. We provide some similar results for L-1 regularization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3403,3454,,,,,,,,,,,,,,,,WOS:000369888000034,0
J,"Huang, FR; Niranjan, UN; Hakeem, MU; Anandkumar, A",,,,"Huang, Furong; Niranjan, U. N.; Hakeem, Mohammad Umar; Anandkumar, Animashree",,,Online Tensor Methods for Learning Latent Variable Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce an online tensor decomposition based approach for two latent variable modeling problems namely, (1) community detection, in which we learn the latent communities that the social actors in social networks belong to, and (2) topic modeling, in which we infer hidden topics of text articles. We consider decomposition of moment tensors using stochastic gradient descent. We conduct optimization of multilinear operations in SGD and avoid directly forming the tensors, to save computational and storage costs. We present optimized algorithm in two platforms. Our GPU-based implementation exploits the parallelism of SIMD architectures to allow for maximum speed-up by a careful optimization of storage and data transfer, whereas our CPU-based implementation uses efficient sparse matrix computations and is suitable for large sparse data sets. For the community detection problem, we demonstrate accuracy and computational efficiency on Facebook, Yelp and DBLP data sets, and for the topic modeling problem, we also demonstrate good performance on the New York Times data set. We compare our results to the state-of-the-art algorithms such as the variational method, and report a gain of accuracy and a gain of several orders of magnitude in the execution time.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2797,2835,,,,,,,,,,,,,,,,WOS:000369888000015,0
J,"Neumann, M; Huang, S; Marthaler, DE; Kersting, K",,,,"Neumann, Marion; Huang, Shan; Marthaler, Daniel E.; Kersting, Kristian",,,pyGPs - A Python Library for Gaussian Process Regression and Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce pyGPs, an object-oriented implementation of Gaussian processes (gps) for machine learning. The library provides a wide range of functionalities reaching from simple gp specification via mean and covariance and gp inference to more complex implementations of hyperparameter optimization, sparse approximations, and graph based learning. Using Python we focus on usability for both users and researchers. Our main goal is to offer a user-friendly and flexible implementation of gps for machine learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2611,2616,,,,,,,,,,,,,,,,WOS:000369888000009,0
J,"Swaminathan, A; Joachims, T",,,,"Swaminathan, Adith; Joachims, Thorsten",,,Batch Learning from Logged Bandit Feedback through Counterfactual Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a learning principle and an efficient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We first address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method-called Policy Optimizer for Exponential Models (POEM)-for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient stochastic gradient optimization. The effectiveness and efficiency of POEM is evaluated on several simulated multi-label classification problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art.",,,,,,"Joachims, Thorsten/0000-0003-3654-3683",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1731,1755,,,,,,,,,,,,,,,,WOS:000369887300003,0
J,"Judah, K; Fern, AP; Dietterich, TG; Tadepalli, P",,,,"Judah, Kshitij; Fern, Alan P.; Dietterich, Thomas G.; Tadepalli, Prasad",,,Active Imitation Learning: Formal and Practical Reductions to IID Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In standard passive imitation learning, the goal is to learn a policy that performs as well as a target policy by passively observing full execution trajectories of it. Unfortunately, generating such trajectories can require substantial expert effort and be impractical in some cases. In this paper, we consider active imitation learning with the goal of reducing this effort by querying the expert about the desired action at individual states, which are selected based on answers to past queries and the learner's interactions with an environment simulator. We introduce a new approach based on reducing active imitation learning to active i.i.d. learning, which can leverage progress in the i.i.d. setting. Our first contribution is to analyze reductions for both non-stationary and stationary policies, showing for the first time that the label complexity (number of queries) of active imitation learning can be less than that of passive learning. Our second contribution is to introduce a practical algorithm inspired by the reductions, which is shown to be highly effective in five test domains compared to a number of alternatives.",,,,,,"Tadepalli, Prasad/0000-0003-2736-3912",,,,,,,,,,,,,1532-4435,,,,,DEC,2014,15,,,,,,3925,3963,,,,,,,,,,,,,,,,WOS:000354999700004,0
J,"Hazan, E; Kale, S",,,,"Hazan, Elad; Kale, Satyen",,,Beyond the Regret Minimization Barrier: Optimal Algorithms for Stochastic Strongly-Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We give novel algorithms for stochastic strongly-convex optimization in the gradient oracle model which return a O(1/T)-approximate solution after T iterations. The first algorithm is deterministic, and achieves this rate via gradient updates and historical averaging. The second algorithm is randomized, and is based on pure gradient steps with a random step size. This rate of convergence is optimal in the gradient oracle model. This improves upon the previously known best rate of O(log(T)/T), which was obtained by applying an online strongly-convex optimization algorithm with regret O(log(T)) to the batch setting. We complement this result by proving that any algorithm has expected regret of Omega(log(T)) in the online stochastic strongly-convex optimization setting. This shows that any online-to-batch conversion is inherently suboptimal for stochastic strongly-convex optimization. This is the first formal evidence that online convex optimization is strictly more difficult than batch stochastic convex optimization.(1)",,,,,,"Hazan, Elad/0000-0002-1566-3216",,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2489,2512,,,,,,,,,,,,,,,,WOS:000344638400004,0
J,"Ruiz, FJR; Valera, I; Blanco, C; Perez-Cruz, F",,,,"Ruiz, Francisco J. R.; Valera, Isabel; Blanco, Carlos; Perez-Cruz, Fernando",,,Bayesian Nonparametric Comorbidity Analysis of Psychiatric Disorders,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database.",,,,,"; Blanco, Carlos/I-4906-2013","RODRIGUEZ RUIZ, FRANCISCO JESUS/0000-0002-2200-901X; perez-cruz, fernando/0000-0001-8996-5076; Blanco, Carlos/0000-0001-6187-3057",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1215,1247,,,,,,,,,,,,,,,,WOS:000338420000001,0
J,"Wang, SS; Zhang, ZH",,,,"Wang, Shusen; Zhang, Zhihua",,,Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The CUR matrix decomposition and the Nystrom approximation are two important low-rank matrix approximation techniques. The Nystrom method approximates a symmetric positive semidefinite matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystrom approximation. In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystrom algorithms with expected relative-error bounds. The proposed CUR and Nystrom algorithms also have low time complexity and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystrom method and the ensemble Nystrom method. The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices.",,,,,,"Wang, Shusen/0000-0003-3928-6782",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2729,2769,,,,,,,,,,,,,,,,WOS:000327007400008,0
J,"Chalupka, K; Williams, CKI; Murray, I",,,,"Chalupka, Krzysztof; Williams, Christopher K. I.; Murray, Iain",,,A Framework for Evaluating Approximation Methods for Gaussian Process Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n(2)) space and O(n(3)) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e. g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,333,350,,,,,,,,,,,,,,,,WOS:000315981900002,0
J,"Riihimaki, J; Jylanki, P; Vehtari, A",,,,"Riihimaki, Jaakko; Jylanki, Pasi; Vehtari, Aki",,,Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers probabilistic multinomial probit classification using Gaussian process (GP) priors. Challenges with multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classification accuracy the differences between all the methods were small from a practical point of view.",,,,,,"Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,75,109,,,,,,,,,,,,,,,,WOS:000314530200003,0
J,"Lizotte, DJ; Bowling, M; Murphy, SA",,,,"Lizotte, Daniel J.; Bowling, Michael; Murphy, Susan A.",,,Linear Fitted-Q Iteration with Multiple Reward Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a general and detailed development of an algorithm for finite-horizon fitted-Q iteration with an arbitrary number of reward signals and linear value function approximation using an arbitrary number of state features. This includes a detailed treatment of the 3-reward function case using triangulation primitives from computational geometry and a method for identifying globally dominated actions. We also present an example of how our methods can be used to construct a real-world decision aid by considering symptom reduction, weight gain, and quality of life in sequential treatments for schizophrenia. Finally, we discuss future directions in which to take this work that will further enable our methods to make a positive impact on the field of evidence-based clinical decision support.",,,,,"Lizotte, Daniel/AAT-3170-2020","Murphy, Susan A/0000-0002-2032-4286",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3253,3295,,,,,,,,,,,23741197,,,,,WOS:000313200200005,0
J,"Ramsahai, RR",,,,"Ramsahai, Roland R.",,,Causal Bounds and Observable Constraints for Non-deterministic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional independence relations involving latent variables do not necessarily imply observable independences. They may imply inequality constraints on observable parameters and causal bounds, which can be used for falsification and identification. The literature on computing such constraints often involve a deterministic underlying data generating process in a counterfactual framework. If an analyst is ignorant of the nature of the underlying mechanisms then they may wish to use a model which allows the underlying mechanisms to be probabilistic. A method of computation for a weaker model without any determinism is given here and demonstrated for the instrumental variable model, though applicable to other models. The approach is based on the analysis of mappings with convex polytopes in a decision theoretic framework and can be implemented in readily available polyhedral computation software. Well known constraints and bounds are replicated in a probabilistic model and novel ones are computed for instrumental variable models without non-deterministic versions of the randomization, exclusion restriction and monotonicity assumptions respectively.",,,,,,"Ramsahai, Roland/0000-0002-7349-1977",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,829,848,,,,,,,,,,,,,,,,WOS:000303772100013,0
J,"Carvalho, AM; Roos, T; Oliveira, AL; Myllymaki, P",,,,"Carvalho, Alexandra M.; Roos, Teemu; Oliveira, Arlindo L.; Myllymaki, Petri",,,Discriminative Learning of Bayesian Networks via Factorized Conditional Log-Likelihood,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose an efficient and parameter-free scoring criterion, the factorized conditional log-likelihood (fCLL), for learning Bayesian network classifiers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efficient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classifiers. Results on a large suite of benchmark data sets from the UCI repository show that fCLL-trained classifiers achieve at least as good accuracy as the best compared classifiers, using significantly less computational resources.",,,,,"Oliveira, Arlindo L/C-1700-2008; Carvalho, Alexandra/C-9060-2009; Myllymaki, Petri/O-4113-2014","Oliveira, Arlindo L/0000-0001-8638-5594; Roos, Teemu/0000-0001-9470-3759; Carvalho, Alexandra/0000-0001-6607-7711; Myllymaki, Petri/0000-0001-9095-282X",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2181,2210,,,,,,,,,,,,,,,,WOS:000293757900003,0
J,"Lauer, F; Guermeur, Y",,,,"Lauer, Fabien; Guermeur, Yann",,,MSVMpack: A Multi-Class Support Vector Machine Package,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes MSVMpack, an open source software package dedicated to our generic model of multi-class support vector machine. All four multi-class support vector machines (M-SVMs) proposed so far in the literature appear as instances of this model. MSVMpack provides for them the first unified implementation and offers a convenient basis to develop other instances. This is also the first parallel implementation for M-SVMs. The package consists in a set of command-line tools with a callable library. The documentation includes a tutorial, a user's guide and a developer's guide.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2293,2296,,,,,,,,,,,,,,,,WOS:000293757900006,0
J,"Chaudhuri, K; Monteleoni, C; Sarwate, AD",,,,"Chaudhuri, Kamalika; Monteleoni, Claire; Sarwate, Anand D.",,,Differentially Private Empirical Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the epsilon-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",,,,,,"Monteleoni, Claire/0000-0002-9488-0517; Sarwate, Anand/0000-0001-6123-5282",,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,1069,1109,,,,,,,,,,,21892342,,,,,WOS:000289635000010,0
J,"Syed, Z; Guttag, J",,,,"Syed, Zeeshan; Guttag, John",,,Unsupervised Similarity-Based Risk Stratification for Cardiovascular Events Using Long-Term Time-Series Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In medicine, one often bases decisions upon a comparative analysis of patient data. In this paper, we build upon this observation and describe similarity-based algorithms to risk stratify patients for major adverse cardiac events. We evolve the traditional approach of comparing patient data in two ways. First, we propose similarity-based algorithms that compare patients in terms of their long-term physiological monitoring data. Symbolic mismatch identifies functional units in long-term signals and measures changes in the morphology and frequency of these units across patients. Second, we describe similarity-based algorithms that are unsupervised and do not require comparisons to patients with known outcomes for risk stratification. This is achieved by using an anomaly detection framework to identify patients who are unlike other patients in a population and may potentially be at an elevated risk. We demonstrate the potential utility of our approach by showing how symbolic mismatch-based algorithms can be used to classify patients as being at high or low risk of major adverse cardiac events by comparing their long-term electrocardiograms to that of a large population. We describe how symbolic mismatch can be used in three different existing methods: one-class support vector machines, nearest neighbor analysis, and hierarchical clustering. When evaluated on a population of 686 patients with available long-term electrocardiographic data, symbolic mismatch-based comparative approaches were able to identify patients at roughly a two-fold increased risk of major adverse cardiac events in the 90 days following acute coronary syndrome. These results were consistent even after adjusting for other clinical risk variables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,999,1024,,,,,,,,,,,,,,,,WOS:000289635000008,0
J,"Yuan, GX; Chang, KW; Hsieh, CJ; Lin, CJ",,,,"Yuan, Guo-Xun; Chang, Kai-Wei; Hsieh, Cho-Jui; Lin, Chih-Jen",,,A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale linear classification is widely used in many areas. The L1-regularized form can be applied for feature selection; however, its non-differentiability causes more difficulties in training. Although various optimization methods have been proposed in recent years, these have not yet been compared suitably. In this paper, we first broadly review existing methods. Then, we discuss state-of-the-art software packages in detail and propose two efficient implementations. Extensive comparisons indicate that carefully implemented coordinate descent methods are very suitable for training large document data.",,,,,"; Chang, Kai-Wei/M-6055-2016","Lin, Chih-Jen/0000-0003-4684-8747; Chang, Kai-Wei/0000-0001-5365-0072",,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3183,3234,,,,,,,,,,,,,,,,WOS:000285643600007,0
J,"Jaimovich, A; Meshi, O; McGraw, I; Elidan, G",,,,"Jaimovich, Ariel; Meshi, Ofer; McGraw, Ian; Elidan, Gal",,,FastInf: An Efficient Approximate Inference Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The FastInf C++ library is designed to perform memory and time efficient approximate inference in large-scale discrete undirected graphical models. The focus of the library is propagation based approximate inference methods, ranging from the basic loopy belief propagation algorithm to propagation based on convex free energies. Various message scheduling schemes that improve on the standard synchronous or asynchronous approaches are included. Also implemented are a clique tree based exact inference, Gibbs sampling, and the mean field algorithm. In addition to inference, FastInf provides parameter estimation capabilities as well as representation and learning of shared parameters. It offers a rich interface that facilitates extension of the basic classes to other inference and learning methods.",,,,,"Jaimovich, Ariel/K-6628-2015","Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1733,1736,,,,,,,,,,,,,,,,WOS:000282522000006,0
J,"Gomez, V; Kappen, HJ; Chertkov, M",,,,"Gomez, Vicenc; Kappen, Hilbert J.; Chertkov, Michael",,,Approximate Inference on Planar Graphs using Loop Calculus and Belief Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce novel results for approximate inference on planar graphical models using the loop calculus framework. The loop calculus (Chertkov and Chernyak, 2006a) allows to express the exact partition function of a graphical model as a finite sum of terms that can be evaluated once the belief propagation (BP) solution is known. In general, full summation over all correction terms is intractable. We develop an algorithm for the approach presented in Chertkov et al. (2008) which represents an efficient truncation scheme on planar graphs and a new representation of the series in terms of Pfaffians of matrices. We analyze the performance of the algorithm for models with binary variables and pairwise interactions on grids and other planar graphs. We study in detail both the loop series and the equivalent Pfaffian series and show that the first term of the Pfaffian series for the general, intractable planar model, can provide very accurate approximations. The algorithm outperforms previous truncation schemes of the loop series and is competitive with other state of the art methods for approximate inference.",,,,,"G√≥mez, Vicen√ß/D-1984-2009; Kappen, H.J./L-4425-2015; Chertkov, Michael/O-8828-2015","G√≥mez, Vicen√ß/0000-0001-5146-7645; Chertkov, Michael/0000-0002-6758-515X",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1273,1296,,,,,,,,,,,,,,,,WOS:000282521500003,0
J,"Erhan, D; Bengio, Y; Courville, A; Manzagol, PA; Vincent, P; Bengio, S",,,,"Erhan, Dumitru; Bengio, Yoshua; Courville, Aaron; Manzagol, Pierre-Antoine; Vincent, Pascal; Bengio, Samy",,,Why Does Unsupervised Pre-training Help Deep Learning?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.",,,,,,"Erhan, Dumitru/0000-0001-7650-1475",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,625,660,,,,,,,,,,,,,,,,WOS:000277186500007,0
J,"Rieck, K; Krueger, T; Brefeld, U; Muller, KR",,,,"Rieck, Konrad; Krueger, Tammo; Brefeld, Ulf; Mueller, Klaus-Robert",,,Approximate Tree Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Convolution kernels for trees provide simple means for learning with tree-structured data. The computation time of tree kernels is quadratic in the size of the trees, since all pairs of nodes need to be compared. Thus, large parse trees, obtained from HTML documents or structured network data, render convolution kernels inapplicable. In this article, we propose an effective approximation technique for parse tree kernels. The approximate tree kernels (ATKs) limit kernel computation to a sparse subset of relevant subtrees and discard redundant structures, such that training and testing of kernel-based learning methods are significantly accelerated. We devise linear programming approaches for identifying such subsets for supervised and unsupervised learning tasks, respectively. Empirically, the approximate tree kernels attain run-time improvements up to three orders of magnitude while preserving the predictive accuracy of regular tree kernels. For unsupervised tasks, the approximate tree kernels even lead to more accurate predictions by identifying relevant dimensions in feature space.",,,,,"Muller, Klaus R/C-3196-2013; Mueller, Klaus-Robert/Y-3547-2019; Rieck, Konrad/F-2233-2010","Mueller, Klaus-Robert/0000-0002-3861-7685; Rieck, Konrad/0000-0002-5054-8758",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,555,580,,,,,,,,,,,,,,,,WOS:000277186500004,0
J,"Gyorgy, A; Lugosi, G; Ottucsak, G",,,,"Gyorgy, Andras; Lugosi, Gabor; Ottucsak, Gyoergy",,,On-Line Sequential Bin Packing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a sequential version of the classical bin packing problem in which items are received one by one. Before the size of the next item is revealed, the decision maker needs to decide whether the next item is packed in the currently open bin or the bin is closed and a new bin is opened. If the new item does not fit, it is lost. If a bin is closed, the remaining free space in the bin accounts for a loss. The goal of the decision maker is to minimize the loss accumulated over n periods. We present an algorithm that has a cumulative loss not much larger than any strategy in a finite class of reference strategies for any sequence of items. Special attention is payed to reference strategies that use a fixed threshold at each step to decide whether a new bin is opened. Some positive and negative results are presented for this case.",,,,,,"Lugosi, Gabor/0000-0003-1614-5901; Gyorgy, Andras/0000-0003-0586-4337",,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,89,109,,,,,,,,,,,,,,,,WOS:000277186400004,0
J,"Elidan, G; Gould, S",,,,"Elidan, Gal; Gould, Stephen",,,Learning Bounded Treewidth Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufficiently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one. We demonstrate the effectiveness of our treewidth-friendly method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth.",,,,,"Elidan, GAl/A-7380-2009","Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2699,2731,,,,,,,,,,,,,,,,WOS:000263240700004,0
J,"Dhurandhar, A; Dobra, A",,,,"Dhurandhar, Amit; Dobra, Alin",,,Probabilistic Characterization of Random Decision Trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classifiers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classification model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classification algorithm. The specific contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classification algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman's formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2321,2348,,,,,,,,,,,,,,,,WOS:000262637300010,0
J,"Braun, ML; Buhmann, JM; Muller, KR",,,,"Braun, Mikio L.; Buhmann, Joachim M.; Mueller, Klaus-Robert",,,On Relevant Dimensions in Kernel Feature Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that the relevant information of a supervised learning problem is contained up to negligible error in a finite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufficiently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efficient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classification. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classification results.",,,,,"Muller, Klaus R/C-3196-2013; Buhmann, Joachim/AAU-4760-2020; Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1875,1908,,,,,,,,,,,,,,,,WOS:000262636800010,0
J,"Chu, TJ; Glymour, C",,,,"Chu, Tianjiao; Glymour, Clark",,,Search for additive nonlinear time series causal models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efficiently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model specification procedure is able to extract relatively detailed causal information. We investigate the finite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,967,991,,,,,,,,,,,,,,,,WOS:000258645300006,0
J,"Ye, JP; Ji, SW; Chen, JH",,,,"Ye, Jieping; Ji, Shuiwang; Chen, Jianhui",,,Multi-class discriminant kernel learning via convex programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularized kernel discriminant analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. Its performance depends on the selection of kernels. In this paper, we consider the problem of multiple kernel learning (MKL) for RKDA, in which the optimal kernel matrix is obtained as a linear combination of pre-specified kernel matrices. We show that the kernel learning problem in RKDA can be formulated as convex programs. First, we show that this problem can be formulated as a semidefinite program (SDP). Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a convex quadratically constrained quadratic programming (QCQP) formulation for kernel learning in RKDA. A semi-infinite linear programming (SILP) formulation is derived to further improve the efficiency. We extend these formulations to the multi-class case based on a key result established in this paper. That is, the multi-class RKDA kernel learning problem can be decomposed into a set of binary-class kernel learning problems which are constrained to share a common kernel. Based on this decomposition property, SDP formulations are proposed for the multi-class case. Furthermore, it leads naturally to QCQP and SILP formulations. As the performance of RKDA depends on the regularization parameter, we show that this parameter can also be optimized in a joint framework with the kernel. Extensive experiments have been conducted and analyzed, and connections to other algorithms are discussed.",,,,,,"Ji, Shuiwang/0000-0002-4205-4563",,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,719,758,,,,,,,,,,,,,,,,WOS:000256642100007,0
J,"Krause, A; Singh, A; Guestrin, C",,,,"Krause, Andreas; Singh, Ajit; Guestrin, Carlos",,,"Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E- optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.",,,,,"Krause, Andreas/A-5888-2008","Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,235,284,,,,,,,,,,,,,,,,WOS:000256641800010,0
J,"Dinuzzo, F; Neve, M; De Nicolao, G; Gianazza, UP",,,,"Dinuzzo, Francesco; Neve, Marta; De Nicolao, Giuseppe; Gianazza, Ugo Pietro",,,On the representer theorem and equivalent degrees of freedom of SVR,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of epsilon-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to define a C-p-like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set).",,,,,"Gianazza, Ugo/B-5792-2014","Gianazza, Ugo/0000-0003-2558-560X",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2467,2495,,,,,,,,,,,,,,,,WOS:000252744800009,0
J,"Laviolette, F; Marchand, M",,,,"Laviolette, Francois; Marchand, Mario",,,PAC-Bayes risk bounds for stochastic averages and majority votes of sample-compressed classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a PAC-Bayes theorem for the sample-compression setting where each classifier is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classifiers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classifier vanishes. For posteriors having all their weights on a single sample-compressed classifier, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classifier of a data-dependent ensemble may abstain of predicting a class label.",,,,,,"Marchand, Mario/0000-0002-7078-7393",,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1461,1487,,,,,,,,,,,,,,,,WOS:000249353700004,0
J,"Dudik, M; Phillips, SJ; Schapire, RE",,,,"Dudik, Miroslav; Phillips, Steven J.; Schapire, Robert E.",,,Maximum entropy density estimation with generalized regularization and an application to species distribution modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a unified and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including l(1), l(2), l(2)(2), and l(1) + l(2)(2) style regularization. We propose an algorithm solving a large and general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and unifies techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2007,8,,,,,,1217,1260,,,,,,,,,,,,,,,,WOS:000248351800001,0
J,"Pan, W",,,,"Pan, Wei",,,Penalized model-based clustering with application to variable selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variable selection in clustering analysis is both challenging and important. In the context of model-based clustering analysis with a common diagonal covariance matrix, which is especially suitable for high dimension, low sample size settings, we propose a penalized likelihood approach with an L-1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to fit our proposed model, and propose a modified BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression profiles demonstrate the utility of our method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1145,1164,,,,,,,,,,,,,,,,WOS:000248351700009,0
J,"Sugiyama, M; Krauledat, M; Muller, KR",,,,"Sugiyama, Masashi; Krauledat, Matthias; Mueller, Klaus-Robert",,,Covariate shift adaptation by importance weighted cross validation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.",,,,,"Sugiyama, Masashi/AEO-1176-2022; Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013","Sugiyama, Masashi/0000-0001-6658-6743; Mueller, Klaus-Robert/0000-0002-3861-7685; ",,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,985,1005,,,,,,,,,,,,,,,,WOS:000248351700003,0
J,"Loosli, G; Canu, S",,,,"Loosli, Gaelle; Canu, Stephane",,,Comments on the Core Vector Machines: Fast SVM training on very large data sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion.",,,,,,"Canu, Stephane/0000-0002-7602-4557",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,291,301,,,,,,,,,,,,,,,,WOS:000247002600005,0
J,"Xue, Y; Liao, XJ; Carin, L; Krishnapuram, B",,,,"Xue, Ya; Liao, Xuejun; Carin, Lawrence; Krishnapuram, Balaji",,,Multi-task learning for classification with Dirichlet process priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider the problem of learning logistic-regression models for multiple classification tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning ( MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process ( DP) based statistical model to learn the extent of similarity between classification tasks, we develop computationally efficient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning ( SMTL) situation in which classifiers for multiple tasks are learned jointly using a variational Bayesian ( VB) algorithm. Second, we consider an asymmetric multi-task learning ( AMTL) formulation in which the posterior density function from the SMTL model parameters ( from previous tasks) is used as a prior for a new task: this approach has the significant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo ( MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: ( a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and ( b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simplified approximations to DP.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,35,63,,,,,,,,,,,,,,,,WOS:000247002500002,0
J,"de Campos, LM",,,,"de Campos, Luis M.",,,A scoring function for learning Bayesian networks based on mutual information and conditional independence tests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new scoring function for learning Bayesian networks from data using score+search algorithms. This is based on the concept of mutual information and exploits some well-known properties of this measure in a novel way. Essentially, a statistical independence test based on the chi-square distribution, associated with the mutual information measure, together with a property of additive decomposition of this measure, are combined in order to measure the degree of interaction between each variable and its parent variables in the network. The result is a non-Bayesian scoring function called MIT (mutual information tests) which belongs to the family of scores based on information theory. The MIT score also represents a penalization of the Kullback-Leibler divergence between the joint probability distributions associated with a candidate network and with the available data set. Detailed results of a complete experimental evaluation of the proposed scoring function and its comparison with the well-known K2, BDeu and BIC/MDL scores are also presented.",,,,,"de Campos, Luis M/B-7436-2012; de Campos, Luis M./O-3731-2019","de Campos, Luis M/0000-0001-9125-1195; de Campos, Luis M./0000-0001-9125-1195",,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2149,2187,,,,,,,,,,,,,,,,WOS:000245390500008,0
J,"Goldsmith, J; Sloan, RH",,,,"Goldsmith, J; Sloan, RH",,,New horn revision algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A revision algorithm is a learning algorithm that identifies the target concept, starting from an initial concept. Such an algorithm is considered efficient if its complexity ( =in terms of the measured resource) is polynomial in the syntactic distance between the initial and the target concept, but only polylogarithmic in the number of variables in the universe. We give efficient revision algorithms in the model of learning with equivalence and membership queries. The algorithms work in a general revision model where both deletion and addition revision operators are allowed. In this model one of the main open problems is the efficient revision of Horn formulas. Two revision algorithms are presented for special cases of this problem: for depth-1 acyclic Horn formulas, and for definite Horn formulas with unique heads.",,,,,"goldsmith, judy/AAE-6217-2020","Goldsmith, Judy/0000-0002-8383-5390",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,1919,1938,,,,,,,,,,,,,,,,WOS:000236331100002,0
J,"Maurer, A",,,,"Maurer, A",,,Algorithmic stability and meta-learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A mechnism of transfer learning is analysed, where samples drawn from different learning tasks of an environment are used to improve the learners performance on a new task. We give a general method to prove generalisation error bounds for such meta-algorithms. The method can be applied to the bias learning model of J. Baxter and to derive novel generalisation bounds for meta-algorithms searching spaces of uniformly stable algorithms. We also present an application to regularized least squares regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2005,6,,,,,,967,994,,,,,,,,,,,,,,,,WOS:000236329800002,0
J,"Agarwal, S; Graepel, T; Herbrich, R; Har-Peled, S; Roth, D",,,,"Agarwal, S; Graepel, T; Herbrich, R; Har-Peled, S; Roth, D",,,Generalization bounds for the area under the ROC curve,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,10th International Workshop on Artificial Intelligence and Statistics,"JAN, 2005",BARBADOS,,,,,"We study generalization properties of the area under the ROC curve (AUC), a quantity that has been advocated as an evaluation criterion for the bipartite ranking problem. The AUC is a different term than the error rate used for evaluation in classification problems; consequently, existing generalization bounds for the classification error rate cannot be used to draw conclusions about the AUC. In this paper, we define the expected accuracy of a ranking function (analogous to the expected error rate of a classification function), and derive distribution-free probabilistic bounds on the deviation of the empirical AUC of a ranking function (observed on a finite data sequence) from its expected accuracy. We derive both a large deviation bound, which serves to bound the expected accuracy of a ranking function in terms of its empirical AUC on a test sequence, and a uniform convergence bound, which serves to bound the expected accuracy of a learned ranking function in terms of its empirical AUC on a training sequence. Our uniform convergence bound is expressed in terms of a new set of combinatorial parameters that we term the bipartite rank-shatter coefficients; these play the same role in our result as do the standard VC-dimension related shatter coefficients (also known as the growth function) in uniform convergence results for the classification error rate. A comparison of our result with a recent uniform convergence result derived by Freund et al. (2003) for a quantity closely related to the AUC shows that the bound provided by our result can be considerably tighter.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,393,425,,,,,,,,,,,,,,,,WOS:000236329600002,0
J,"Elidan, G; Friedman, N",,,,"Elidan, G; Friedman, N",,,Learning hidden variable networks: The information bottleneck approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A central challenge in learning probabilistic graphical models is dealing with domains that involve hidden variables. The common approach for learning model parameters in such domains is the expectation maximization ( EM) algorithm. This algorithm, however, can easily get trapped in suboptimal local maxima. Learning the model structure is even more challenging. The structural EM algorithm can adapt the structure in the presence of hidden variables, but usually performs poorly without prior knowledge about the cardinality and location of the hidden variables. In this work, we present a general approach for learning Bayesian networks with hidden variables that overcomes these problems. The approach builds on the information bottleneck framework of Tishby et al. (1999). We start by proving formal correspondence between the information bottleneck objective and the standard parametric EM functional. We then use this correspondence to construct a learning algorithm that combines an information-theoretic smoothing term with a continuation procedure. Intuitively, the algorithm bypasses local maxima and achieves superior solutions by following a continuous path from a solution of, an easy and smooth, target function, to a solution of the desired likelihood function. As we show, our algorithmic framework allows learning of the parameters as well as the structure of a network. In addition, it also allows us to introduce new hidden variables during model selection and learn their cardinality. We demonstrate the performance of our procedure on several challenging real-life data sets.",,,,,"Elidan, GAl/A-7380-2009; Friedman, Nir/H-9692-2012","Friedman, Nir/0000-0002-9678-3550; Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,81,127,,,,,,,,,,,,,,,,WOS:000236328800004,0
J,"Chen, DR; Wu, Q; Ying, YM; Zhou, DX",,,,"Chen, DR; Wu, Q; Ying, YM; Zhou, DX",,,Support vector machine soft margin classifiers: Error analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The purpose of this paper is to provide a PAC error analysis for the q-norm soft margin classifier, a support vector machine classification algorithm. It consists of two parts: regularization error and sample error. While many techniques are available for treating the sample error, much less is known for the regularization error and the corresponding approximation error for reproducing kernel Hilbert spaces. We are mainly concerned about the regularization error. It is estimated for general distributions by a K-functional in weighted L-q spaces. For weakly separable distributions (i.e., the margin may be zero) satisfactory convergence rates are provided by means of separating functions. A projection operator is introduced, which leads to better sample error estimates especially for small complexity kernels. The misclassification error is bounded by the V-risk associated with a general class of loss functions V. The difficulty of bounding the offset is overcome. Polynomial kernels and Gaussian kernels are used to demonstrate the main results. The choice of the regularization parameter plays an important role in our analysis.",,,,,"Zhou, Ding-Xuan/B-3160-2013; Wu, Qiang/B-1620-2008; Ying, Yiming/A-4196-2013; Ying, Yiming/AGD-7246-2022","Zhou, Ding-Xuan/0000-0003-0224-9216; Wu, Qiang/0000-0002-4698-6966; Ying, Yiming/0000-0001-7345-6672; Ying, Yiming/0000-0001-7345-6672",,,,,,,,,,,,,1532-4435,,,,,SEP,2004,5,,,,,,1143,1175,,,,,,,,,,,,,,,,WOS:000236328100004,0
J,"Blum, A; Jackson, J; Sandholm, T; Zinkevich, M",,,,"Blum, A; Jackson, J; Sandholm, T; Zinkevich, M",,,Preference elicitation and query learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we explore the relationship between preference elicitation, a learning-style problem that arises in combinatorial auctions, and the problem of learning via queries studied in computational learning theory. Preference elicitation is the process of asking questions about the preferences of bidders so as to best divide some set of goods. As a learning problem, it can be thought of as a setting in which there are multiple target concepts that can each be queried separately, but where the goal is not so much to learn each concept as it is to produce an optimal example. In this work, we prove a number of similarities and differences between two-bidder preference elicitation and query learning, giving both separation results and proving some connections between these problems.",,,,,,"Jackson, Jeffrey/0000-0003-3794-3793",,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,649,667,,,,,,,,,,,,,,,,WOS:000236327700004,0
J,"Vovk, V",,,,"Vovk, V",,,A universal well-calibrated algorithm for on-line classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of on-line classification in which the prediction algorithm, for each significance level delta, is required to output as its prediction a range of labels ( intuitively, those labels deemed compatible with the available data at the level delta) rather than just one label; as usual, the examples are assumed to be generated independently from the same probability distribution P. The prediction algorithm is said to be well-calibrated for P and delta if the long-run relative frequency of errors does not exceed d almost surely w.r. to P. For well-calibrated algorithms we take the number of uncertain predictions ( i.e., those containing more than one label) as the principal measure of predictive performance. The main result of this paper is the construction of a prediction algorithm which, for any ( unknown) P and any delta: ( a) makes errors independently and with probability d at every trial ( in particular, is well-calibrated for P and delta); (b) makes in the long run no more uncertain predictions than any other prediction algorithm that is well-calibrated for P and delta; ( c) processes example n in time O(logn).",,,,,,"Vovk, Vladimir/0000-0003-2602-6877",,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,575,604,,,,,,,,,,,,,,,,WOS:000236327700001,0
J,"Saul, LK; Roweis, ST",,,,"Saul, LK; Roweis, ST",,,"Think globally, fit locally: Unsupervised learning of low dimensional manifolds",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE-though capable of generating highly nonlinear embeddings-are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance-both successes and failures-and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2004,4,2,,,,,119,155,,10.1162/153244304322972667,0,,,,,,,,,,,,,WOS:000221043600001,0
J,"Megyesi, B",,,,"Megyesi, B",,,Shallow parsing with PoS taggers and linguistic features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Three data-driven publicly available part-of-speech taggers are applied to shallow parsing of Swedish texts. The phrase structure is represented by nine types of phrases in a hierarchical structure containing labels for every constituent type the token belongs to in the parse tree. The encoding is based on the concatenation of the phrase tags on the path from lowest to higher nodes. Various linguistic features are used in learning; the taggers are trained on the basis of lexical information only, part-of-speech only, and a combination of both, to predict the phrase structure of the tokens with or without part-of-speech. Special attention is directed to the taggers' sensitivity to different types of linguistic information included in learning, as well as the taggers' sensitivity to the size and the various types of training data sets. The method can be easily transferred to other languages.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,639,668,,10.1162/153244302320884579,0,,,,,,,,,,,,,WOS:000179542800005,0
J,"Fine, S; Scheinberg, K",,,,"Fine, S; Scheinberg, K",,,Efficient SVM training using low-rank kernel representations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally. we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method.",,,,,,", Katya/0000-0003-3547-1841",,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,243,264,,10.1162/15324430260185619,0,,,,,,,,,,,,,WOS:000176055300008,0
J,"Ho, J; Saharia, C; Chan, W; Fleet, DJ; Norouzi, M; Salimans, T",,,,"Ho, Jonathan; Saharia, Chitwan; Chan, William; Fleet, David J.; Norouzi, Mohammad; Salimans, Tim",,,Cascaded Diffusion Models for High Fidelity Image Generation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x 128 and 4.88 at 256x 256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000752301300001,0
J,"Tang, WJ; Ma, JQ; Mei, QZ; Zhu, J",,,,"Tang, Weijing; Ma, Jiaqi; Mei, Qiaozhu; Zhu, Ji",,,SODEN: A Scalable Continuous-Time Survival Model through Ordinary Differential Equation Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a flexible model for survival analysis using neural networks along with scalable optimization algorithms. One key technical challenge for directly applying maximum likelihood estimation (MLE) to censored data is that evaluating the objective function and its gradients with respect to model parameters requires the calculation of integrals. To address this challenge, we recognize from a novel perspective that the MLE for censored data can be viewed as a differential-equation constrained optimization problem. Following this connection, we model the distribution of event time through an ordinary differential equation and utilize efficient ODE solvers and adjoint sensitivity analysis to numerically evaluate the likelihood and the gradients. Using this approach, we are able to 1) provide a broad family of continuous-time survival distributions without strong structural assumptions, 2) obtain powerful feature representations using neural networks, and 3) allow efficient estimation of the model in large-scale applications using stochastic gradient descent. Through both simulation studies and real-world data examples, we demonstrate the effectiveness of the proposed method in comparison to existing state-of-theart deep learning survival analysis models. The implementation of the proposed SODEN approach has been made publicly available at https://github.com/jiaqima/SODEN.",,,,,"zhu, ji/HDM-1538-2022",,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752347900001,0
J,"Duan, LL; Dunson, DB",,,,"Duan, Leo L.; Dunson, David B.",,,Bayesian Distance Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Model-based clustering is widely used in a variety of application areas. However, fundamental concerns remain about robustness. In particular, results can be sensitive to the choice of kernel representing the within-cluster data density. Leveraging on properties of pairwise differences between data points, we propose a class of Bayesian distance clustering methods, which rely on modeling the likelihood of the pairwise distances in place of the original data. Although some information in the data is discarded, we gain substantial robustness to modeling assumptions. The proposed approach represents an appealing middle ground between distance- and model-based clustering, drawing advantages from each of these canonical approaches. We illustrate dramatic gains in the ability to infer clusters that are not well represented by the usual choices of kernel. A simulation study is included to assess performance relative to competitors, and we apply the approach to clustering of brain genome expression data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,35782785,,,,,WOS:000706871100001,0
J,"Eftekhari, H; Banerjee, M; Ritov, Y",,,,"Eftekhari, Hamid; Banerjee, Moulinath; Ritov, Yaacov",,,Inference In High-dimensional Single-Index Models Under Symmetric Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of statistical inference for regression coefficients in a high-dimensional single-index model is considered. Under elliptical symmetry, the single index model can be reformulated as a proxy linear model whose regression parameter is identifiable. We construct estimates of the regression coefficients of interest that are similar to the debiased lasso estimates in the standard linear model and exhibit similar properties: root n-consistency and asymptotic normality. The procedure completely bypasses the estimation of the unknown link function, which can be extremely challenging depending on the underlying structure of the problem. Furthermore, under Gaussianity, we propose more efficient estimates of the coefficients by expanding the link function in the Hermite polynomial basis. Finally, we illustrate our approach via carefully designed simulation experiments.",,,,,"Eftekhari, Hamid/AAQ-8344-2021","Eftekhari, Hamid/0000-0003-2433-4243",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500027,0
J,"Hamer, V; Dupont, P",,,,"Hamer, Victor; Dupont, Pierre",,,An Importance Weighted Feature Selection Stability Measure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Current feature selection methods, especially applied to high dimensional data, tend to suffer from instability since marginal modifications in the data may result in largely distinct selected feature sets. Such instability strongly limits a sound interpretation of the selected variables by domain experts. Defining an adequate stability measure is also a research question. In this work, we propose to incorporate into the stability measure the importances of the selected features in predictive models. Such feature importances are directly proportional to feature weights in a linear model. We also consider the generalization to a non-linear setting. We illustrate, theoretically and experimentally, that current stability measures are subject to undesirable behaviors, for example, when they are jointly optimized with predictive accuracy. Results on micro-array and mass-spectrometric data show that our novel stability measure corrects for overly optimistic stability estimates in such a bi-objective context, which leads to improved decision-making. It is also shown to be less prone to the under-or over-estimation of the stability value in feature spaces with groups of highly correlated variables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,57,,,,,,,,,,,,,,,,WOS:000663170000001,0
J,"Hao, BT; Wang, BX; Wang, PY; Zhang, JF; Yang, J; Sun, WW",,,,"Hao, Botao; Wang, Boxiang; Wang, Pengyuan; Zhang, Jingfei; Yang, Jian; Sun, Will Wei",,,Sparse Tensor Additive Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tensors are becoming prevalent in modern applications such as medical imaging and digital marketing. In this paper, we propose a sparse tensor additive regression (STAR) that models a scalar response as a flexible nonparametric function of tensor covariates. The proposed model effectively exploits the sparse and low-rank structures in the tensor additive regression. We formulate the parameter estimation as a non-convex optimization problem, and propose an efficient penalized alternating minimization algorithm. We establish a non-asymptotic error bound for the estimator obtained from each iteration of the proposed algorithm, which reveals an interplay between the optimization error and the statistical rate of convergence. We demonstrate the efficacy of STAR through extensive comparative simulation studies, and an application to the click-through-rate prediction in online advertising.",,,,,"Zhang, Jing/HII-4294-2022; Wang, Pengyuan/ABG-1291-2021","Wang, Pengyuan/0000-0001-8689-3011",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656364200001,0
J,"Lemhadri, I; Ruan, F; Abraham, L; Tibshirani, R",,,,"Lemhadri, Ismael; Ruan, Feng; Abraham, Louis; Tibshirani, Robert",,,LassoNet: A Neural Network with Feature Sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Much work has been done recently to make neural networks more interpretable, and one approach is to arrange for the network to use only a subset of the available features. In linear models, Lasso (or l(1)-regularized) regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science. However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global feature selection. Our approach achieves feature sparsity by adding a skip (residual) layer and allowing a feature to participate in any hidden layer only if its skip-layer representative is active. Unlike other approaches to feature selection for neural nets, our method uses a modified objective function with constraints, and so integrates feature selection with the parameter learning directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. We apply LassoNet to a number of real-data problems and find that it significantly outperforms state-of-the-art methods for feature selection and regression. LassoNet uses projected proximal gradient descent, and generalizes directly to deep networks. It can be implemented by adding just a few lines of code to a standard neural network.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,127,,,,,,,,,,,,,,,WOS:000663180800001,0
J,"Liu, ML; Xia, Y; Cho, K; Cai, TX",,,,"Liu, Molei; Xia, Yin; Cho, Kelly; Cai, Tianxi",,,Integrative High Dimensional Multiple Testing with Heterogeneity under Data Sharing Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Identifying informative predictors in a high dimensional regression model is a critical step for association analysis and predictive modeling. Signal detection in the high dimensional setting often fails due to the limited sample size. One approach to improving power is through meta-analyzing multiple studies which address the same scientific question. However, integrative analysis of high dimensional data from multiple studies is challenging in the presence of between-study heterogeneity. The challenge is even more pronounced with additional data sharing constraints under which only summary data can be shared across different sites. In this paper, we propose a novel data shielding integrative large-scale testing (DSILT) approach to signal detection allowing between-study heterogeneity and not requiring the sharing of individual level data. Assuming the underlying high dimensional regression models of the data differ across studies yet share similar support, the proposed method incorporates proper integrative estimation and debiasing procedures to construct test statistics for the overall effects of specific covariates. We also develop a multiple testing procedure to identify significant effects while controlling the false discovery rate (FDR) and false discovery proportion (FDP). Theoretical comparisons of the new testing procedure with the ideal individual-level meta-analysis (ILMA) approach and other distributed inference methods are investigated. Simulation studies demonstrate that the proposed testing procedure performs well in both controlling false discovery and attaining power. The new method is applied to a real example detecting interaction effects of the genetic variants for statins and obesity on the risk for type II diabetes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,126,,,,,,,,,,,,,,,WOS:000663179500001,0
J,"Nguyen, LM; Tran-Dinh, Q; Phan, DT; Nguyen, PH; van Dijk, M",,,,"Nguyen, Lam M.; Quoc Tran-Dinh; Phan, Dzung T.; Phuong Ha Nguyen; van Dijk, Marten",,,A Unified Convergence Analysis for Shuffling-Type Gradient Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a unified convergence analysis for a class of generic shuffling-type gradient methods for solving finite-sum optimization problems. Our analysis works with any sampling without replacement strategy and covers many known variants such as randomized reshuffling, deterministic or randomized single permutation, and cyclic and incremental gradient schemes. We focus on two different settings: strongly convex and nonconvex problems, but also discuss the non-strongly convex case. Our main contribu-tion consists of new non-asymptotic and asymptotic convergence rates for a wide class of shuffling-type gradient methods in both nonconvex and convex settings. We also study uni-formly randomized shuffling variants with different learning rates and model assumptions. While our rate in the nonconvex case is new and significantly improved over existing works under standard assumptions, the rate on the strongly convex one matches the existing best-known rates prior to this paper up to a constant factor without imposing a bounded gradient condition. Finally, we empirically illustrate our theoretical results via two nu-merical examples: nonconvex logistic regression and neural network training examples. As byproducts, our results suggest some appropriate choices for diminishing learning rates in certain shuffling variants.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706449100001,0
J,"Patil, V; Ghalme, G; Nair, V; Narahari, Y",,,,"Patil, Vishakha; Ghalme, Ganesh; Nair, Vineet; Narahari, Y.",,,Achieving Fairness in the Stochastic Multi-Armed Bandit Problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study an interesting variant of the stochastic multi-armed bandit problem, which we call the FAIR-MAB problem, where, in addition to the objective of maximizing the sum of expected rewards, the algorithm also needs to ensure that at any time, each arm is pulled at least a pre-specified fraction of times. We investigate the interplay between learning and fairness in terms of a pre-specified vector denoting the fractions of guaranteed pulls. We define a fairness-aware regret, which we call r-Regret, that takes into account the above fairness constraints and extends the conventional notion of regret in a natural way. Our primary contribution is to obtain a complete characterization of a class of FAIR-MAB algorithms via two parameters: the unfairness tolerance and the learning algorithm used as a black-box. For this class of algorithms, we provide a fairness guarantee that holds uniformly over time, irrespective of the chosen learning algorithm. Further, when the learning algorithm is UCB1, we show that our algorithm achieves constant r-Regret for a large enough time horizon. Finally, we analyze the cost of fairness in terms of the conventional notion of regret. We conclude by experimentally validating our theoretical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700320100001,0
J,"Traca, S; Rudin, C; Yan, WY",,,,"Traca, Stefano; Rudin, Cynthia; Yan, Weiyu",,,Regulating Greed Over Time in Multi-Armed Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In retail, there are predictable yet dramatic time-dependent patterns in customer behavior, such as periodic changes in the number of visitors, or increases in customers just before major holidays. The current paradigm of multi-armed bandit analysis does not take these known patterns into account. This means that for applications in retail, where prices are fixed for periods of time, current bandit algorithms will not suffice. This work provides a remedy that takes the time-dependent patterns into account, and we show how this remedy is implemented for the UCB, epsilon-greedy, and UCB-L algorithms, and also through a new policy called the variable arm pool algorithm. In the corrected methods, exploitation (greed) is regulated over time, so that more exploitation occurs during higher reward periods, and more exploration occurs in periods of low reward. In order to understand why regret is reduced with the corrected methods, we present a set of bounds that provide insight into why we would want to exploit during periods of high reward, and discuss the impact on regret. Our proposed methods perform well in experiments, and were inspired by a high-scoring entry in the Exploration and Exploitation 3 contest using data from Yahoo! Front Page. That entry heavily used time-series methods to regulate greed over time, which was substantially more effective than other contextual bandit methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500003,0
J,"Wang, FC; Janson, L",,,,"Wang, Feicheng; Janson, Lucas",,,Exact Asymptotics for Linear Quadratic Adaptive Control,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC) . By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically- exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,112,,,,,,,,,,,,,,,,WOS:000726705700001,0
J,"Wang, TY; Morucci, M; Awan, MU; Liu, YM; Roy, S; Rudin, C; Volfovsky, A",,,,"Wang, Tianyu; Morucci, Marco; Awan, M. Usaid; Liu, Yameng; Roy, Sudeepa; Rudin, Cynthia; Volfovsky, Alexander",,,FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A classical problem in causal inference is that of matching, where treatment units need to be matched to control units based on covariate information. In this work, we propose a method that computes high quality almost-exact matches for high-dimensional categorical datasets. This method, called FLAME (Fast Large-scale Almost Matching Exactly), learns a distance metric for matching using a hold-out training data set. In order to perform matching efficiently for large datasets, FLAME leverages techniques that are natural for query processing in the area of database management, and two implementations of FLAME are provided: the first uses SQL queries and the second uses bit-vector techniques. The algorithm starts by constructing matches of the highest quality (exact matches on all covariates), and successively eliminates variables in order to match exactly on as many variables as possible, while still maintaining interpretable high-quality matches and balance between treatment and control groups. We leverage these high quality matches to estimate conditional average treatment effects (CATEs). Our experiments show that FLAME scales to huge datasets with millions of observations where existing state-of-the-art methods fail, and that it achieves significantly better performance than other matching methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500031,0
J,"Zhang, WR; Krehbiel, S; Tuo, R; Mei, YJ; Cummings, R",,,,"Zhang, Wanrong; Krehbiel, Sara; Tuo, Rui; Mei, Yajun; Cummings, Rachel",,,Single and Multiple Change-Point Detection with Differential Privacy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The change-point detection problem seeks to identify distributional changes at an unknown change-point k* in a stream of data. This problem appears in many important practical settings involving personal data, including biosurveillance, fault detection, finance, signal detection, and security systems. The field of differential privacy offers data analysis tools that provide powerful worst-case privacy guarantees. We study the statistical problem of change-point detection through the lens of differential privacy. We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and provide empirical validation of our results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500029,0
J,"Zhang, YQ; Bi, X; Tang, NAS; Qu, AN",,,,"Zhang, Yanqing; Bi, Xuan; Tang, Niansheng; Qu, Annie",,,Dynamic Tensor Recommender Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recommender systems have been extensively used by the entertainment industry, business marketing and the biomedical industry. In addition to its capacity of providing preference-based recommendations as an unsupervised learning methodology, it has been also proven useful in sales forecasting, product introduction and other production related businesses. Since some consumers and companies need a recommendation or prediction for future budget, labor and supply chain coordination, dynamic recommender systems for precise forecasting have become extremely necessary. In this article, we propose a new recommendation method, namely the dynamic tensor recommender system (DTRS), which aims particularly at forecasting future recommendation. The proposed method utilizes a tensor-valued function of time to integrate time and contextual information, and creates a time-varying coefficient model for temporal tensor factorization through a polynomial spline approximation. Major advantages of the proposed method include competitive future recommendation predictions and effective prediction interval estimations. In theory, we establish the convergence rate of the proposed tensor factorization and asymptotic normality of the spline coefficient estimator. The proposed method is applied to simulations, IRI marketing data and Last.fm data. Numerical studies demonstrate that the proposed method outperforms existing methods in terms of future time forecasting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,35,,,,,,,,,,,,,,,,WOS:000656365000001,0
J,"Bargiacchi, E; Roijers, DM; Nowe, A",,,,"Bargiacchi, Eugenio; Roijers, Diederik M.; Nowe, Ann",,,AI-Toolbox: A C plus plus library for Reinforcement Learning and Planning (with Python Bindings),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality.",,,,,,"Bargiacchi, Eugenio/0000-0003-2824-7200; Roijers, Diederik/0000-0002-2825-2491",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,102,,,,,,,,,,,,,,,WOS:000546623100001,0
J,"Blanco, V; Puerto, J; Rodriguez-Chia, AM",,,,"Blanco, Vctor; Puerto, Justo; Rodriguez-Chia, Antonio M.",,,On l(p)-Support Vector Machines and Multidimensional Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we extend the methodology developed for Support Vector Machines (SVM) using the l(2)-norm (l(2)-SVM) to the more general case of l(p)-norms with p > 1 (l(p)-SVM). We derive second order cone formulations for the resulting dual and primal problems. The concept of kernel function, widely applied in l(2)-SVM, is extended to the more general case of l(p)-norms with p > 1 by defining a new operator called multidimensional kernel. This object gives rise to reformulations of dual problems, in a transformed space of the original data, where the dependence on the original data always appear as homogeneous polynomials. We adapt known solution algorithms to efficiently solve the primal and dual resulting problems and some computational experiments on real-world datasets are presented showing rather good behavior in terms of the accuracy of l(p)-SVM with p > 1.",,,,,"Blanco, Victor/E-7839-2010; Rodr√≠guez-Ch√≠a, Antonio Manuel/K-4435-2014","Blanco, Victor/0000-0002-7762-6461; Rodr√≠guez-Ch√≠a, Antonio Manuel/0000-0002-1708-2108",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300014,0
J,"Bunel, R; Turkaslan, I; Torr, PHS; Kumar, MP; Lu, JY; Kohli, P",,,,"Bunel, Rudy; Turkaslan, Ilker; Torr, Philip H. S.; Kumar, M. Pawan; Lu, Jingyue; Kohli, Pushmeet",,,Branch and Bound for Piecewise Linear Neural Network Verification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. In this context, verification involves proving or disproving that an NN model satisfies certain input-output properties. Despite the reputation of learned NN models as black boxes, and the theoretical hardness of proving useful properties about them, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. However, these methods are still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases. With the help of the BaB framework, we make three key contributions. Firstly, we identify new methods that combine the strengths of multiple existing approaches, accomplishing significant performance improvements over previous state of the art. Secondly, we introduce an effective branching strategy on ReLU non-linearities. This branching strategy allows us to efficiently and successfully deal with high input dimensional problems with convolutional network architecture, on which previous methods fail frequently. Finally, we propose comprehensive test data sets and benchmarks which includes a collection of previously released testcases. We use the data sets to conduct a thorough experimental comparison of existing and new algorithms and to provide an inclusive analysis of the factors impacting the hardness of verification problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000017,0
J,"Meng, HM; Zhao, YQ; Fu, HD; Qiao, XY",,,,"Meng, Haomiao; Zhao, Ying-Qi; Fu, Haoda; Qiao, Xingye",,,Near-optimal Individualized Treatment Recommendations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The individualized treatment recommendation (ITR) is an important analytic framework for precision medicine. The goal of ITR is to assign the best treatments to patients based on their individual characteristics. From the machine learning perspective, the solution to the ITR problem can be formulated as a weighted classification problem to maximize the mean benefit from the recommended treatments given patients' characteristics. Several ITR methods have been proposed in both the binary setting and the multicategory setting. In practice, one may prefer a more flexible recommendation that includes multiple treatment options. This motivates us to develop methods to obtain a set of near-optimal individualized treatment recommendations alternative to each other, called alternative individualized treatment recommendations (A-ITR). We propose two methods to estimate the optimal A-ITR within the outcome weighted learning (OWL) framework. Simulation studies and a real data analysis for Type 2 diabetic patients with injectable antidiabetic treatments are conducted to show the usefulness of the proposed A-ITR framework. We also show the consistency of these methods and obtain an upper bound for the risk between the theoretically optimal recommendation and the estimated one. An R package aitr has been developed, found at https://github.com/menghaomiao/aitr",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,183,,,,,,,,,,34335111,,,,,WOS:000570235800001,0
J,"Naitzat, G; Zhitnikov, A; Lim, LH",,,,"Naitzat, Gregory; Zhitnikov, Andrey; Lim, Lek-Heng",,,Topology of Deep Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study how the topology of a data set M = MaUMb subset of R-d, representing two classes a and b in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., one with perfect accuracy on training set and near-zero generalization error (approximate to 0:01%). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrarily well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of M we begin with, when passed through a well-trained neural network f : R-d -> R-p, there is a vast reduction in the Betti numbers of both components Ma and Mb; in fact they nearly always reduce to their lowest possible values: beta(k)(f(M-i)) = 0 for k >= 1 and beta(0)(f(M-i)) = 1, i = a, b. (2) The reduction in Betti numbers is significantly faster for ReLU activation than for hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. (3) Shallow and deep networks transform data sets differently - a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,184,,,,,,,,,,,,,,,WOS:000570236200001,0
J,"Probst, M; Rothlauf, F",,,,"Probst, Malte; Rothlauf, Franz",,,Harmless Overfitting: Using Denoising Autoencoders in Estimation of Distribution Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Estimation of Distribution Algorithms (EDAs) are metaheuristics where learning a model and sampling new solutions replaces the variation operators recombination and mutation used in standard Genetic Algorithms. The choice of these models as well as the corresponding training processes are subject to the bias/variance tradeoff, also known as under- and overfitting: simple models cannot capture complex interactions between problem variables, whereas complex models are susceptible to modeling random noise. This paper suggests using Denoising Autoencoders (DAEs) as generative models within EDAs (DAE-EDA). The resulting DAE-EDA is able to model complex probability distributions. Furthermore, overfitting is less harmful, since DAEs overfit by learning the identity function. This overfitting behavior introduces unbiased random noise into the samples, which is no major problem for the EDA but just leads to higher population diversity. As a result, DAE-EDA runs for more generations before convergence and searches promising parts of the solution space more thoroughly. We study the performance of DAE-EDA on several combinatorial single-objective optimization problems. In comparison to the Bayesian Optimization Algorithm, DAE-EDA requires a similar number of evaluations of the objective function but is much faster and can be parallelized efficiently, making it the preferred choice especially for large and difficult optimization problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000542194600001,0
J,"Javanmard, A; Nazerzadeh, H",,,,"Javanmard, Adel; Nazerzadeh, Hamid",,,Dynamic Pricing in High-dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the pricing problem faced by a firm that sells a large number of products, described via a wide range of features, to customers that arrive over time. Customers independently make purchasing decisions according to a general choice model that includes products features and customers' characteristics, encoded as d-dimensional numerical vectors, as well as the price offered. The parameters of the choice model are a priori unknown to the firm, but can be learned as the (binary-valued) sales data accrues over time. The firm's objective is to maximize its revenue. We benchmark the performance using the classic regret minimization framework where the regret is de fined as the expected revenue loss against a clairvoyant policy that knows the parameters of the choice model in advance, and always offers the revenue-maximizing price. This setting is motivated in part by the prevalence of online marketplaces that allow for real-time pricing. We assume a structured choice model, parameters of which depend on s(0) out of the d product features. Assuming that the market noise distribution is known, we propose a dynamic policy, called Regularized Maximum Likelihood Pricing (RMLP) that leverages the (sparsity) structure of the high-dimensional model and obtains a logarithmic regret in T. More specifically, the regret of our algorithm is of O(s(0) log d . log T). Furthermore, we show that no policy can obtain regret better than O(s(0) (log d + log T)). In addition, we propose a generalization of our policy to a setting that the market noise distribution is unknown but belongs to a parametrized family of distributions. This policy obtains regret of O(root(log d)T). We further show that no policy can obtain regret better than Omega(root T) in such environments.",,,,,"Javanmard, Adel/ABB-5000-2020",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,9,,,,,,,,,,,,,,,WOS:000458664000001,0
J,"Probst, P; Boulesteix, AL; Bischl, B",,,,"Probst, Philipp; Boulesteix, Anne-Laure; Bischl, Bernd",,,Tunability: Importance of Hyperparameters of Machine Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to choose adequate hyperparameter spaces for tuning.",,,,,"Boulesteix, Anne-Laure/A-3948-2010; Probst, Philipp/GYV-1134-2022","Boulesteix, Anne-Laure/0000-0002-2729-0947; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,53,,,,,,,,,,,,,,,WOS:000463323600001,0
J,"Shallue, CJ; Lee, J; Antognini, J; Sohl-Dickstein, J; Frostig, R; Dahl, GE",,,,"Shallue, Christopher J.; Lee, Jaehoon; Antognini, Joseph; Sohl-Dickstein, Jascha; Frostig, Roy; Dahl, George E.",,,Measuring the Effects of Data Parallelism on Neural Network Training,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.",,,,,,/0000-0002-8910-0950,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,112,,,,,,,,,,,,,,,WOS:000476624200001,0
J,"Wang, HY",,,,"Wang, HaiYing",,,More Efficient Estimation for Logistic Regression with Optimal Subsamples,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose improved estimation method for logistic regression based on subsamples taken according the optimal subsampling probabilities developed in Wang et al. (2018). Both asymptotic results and numerical results show that the new estimator has a higher estimation efficiency. We also develop a new algorithm based on Poisson subsampling, which does not require to approximate the optimal subsampling probabilities all at once. This is computationally advantageous when available random-access memory is not enough to hold the full data. Interestingly, asymptotic distributions also show that Poisson subsampling produces a more efficient estimator if the sampling ratio, the ratio of the subsample size to the full data sample size, does not converge to zero. We also obtain the unconditional asymptotic distribution for the estimator based on Poisson subsampling. Pilot estimators are required to calculate subsampling probabilities and to correct biases in un-weighted estimators; interestingly, even if pilot estimators are inconsistent, the proposed method still produce consistent and asymptotically normal estimators.",,,,,,"Wang, HaiYing/0000-0001-7729-0243",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,132,,,,,,,,,,,,,,,WOS:000487068900016,0
J,"Wang, PW; Lee, CP; Lin, CJ",,,,"Wang, Po-Wei; Lee, Ching-pei; Lin, Chih-Jen",,,The Common-directions Method for Regularized Empirical Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"State-of-the-art first- and second-order optimization methods are able to achieve either fast global linear convergence rates or quadratic convergence, but not both of them. In this work, we propose an interpolation between first- and second-order methods for regularized empirical risk minimization that exploits the problem structure to efficiently combine multiple update directions. Our method attains both optimal global linear convergence rate for first-order methods, and local quadratic convergence. Experimental results show that our method outperforms state-of-the-art first- and second-order optimization methods in terms of the number of data accesses, while is competitive in training time.",,,,,"Lee, Ching-pei/AAQ-4125-2021",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,58,,,,,,,,,,,,,,,WOS:000467878000001,0
J,"Huang, J; Jiao, YL; Liu, YY; Lu, XL",,,,"Huang, Jian; Jiao, Yuling; Liu, Yanyan; Lu, Xiliang",,,A Constructive Approach to L-0 Penalized Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the l(0)-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the l(2) estimation error of the solution sequence decays exponentially to the minimax error bound in O(log(R root J)) iterations, where J is the number of important predictors and R is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the l(infinity) estimation error decays to the optimal error bound in O(log(R)) iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is O(np) per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency.",,,,,"Lu, Xiliang/D-8770-2012","Lu, Xiliang/0000-0002-7592-5994",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,10,,,,,,,,,,,,,,,WOS:000443225000001,0
J,"Anandkumar, A; Ge, R; Janzamin, M",,,,"Anandkumar, Animashree; Ge, Rong; Janzamin, Majid",,,Analyzing Tensor Power Method Dynamics in Overcomplete Regime,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components k is much larger than the data dimension d, up to k = o(d(1.5)). We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,40,22,,,,,,,,,,,,,,,WOS:000399841700001,0
J,"Dieuleveut, A; Flammarion, N; Bach, F",,,,"Dieuleveut, Aymeric; Flammarion, Nicolas; Bach, Francis",,,"Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in O (1 / n(2)), and in terms of dependence on the noise and dimension d of the problem, as O (d / n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small in some distances while the optimal terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,101,,,,,,,,,,,,,,,WOS:000413448400001,0
J,"Dimitrakakis, C; Nelson, B; Zhang, ZH; Mitrokotsa, A; Rubinstein, BIP",,,,"Dimitrakakis, Christos; Nelson, Blaine; Zhang, Zuhe; Mitrokotsa, Aikaterini; Rubinstein, Benjamin I. P.",,,Differential Privacy for Bayesian Inference through Posterior Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we de fine differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions.",,,,,"Dimitrakakis, Christos/F-6404-2011","Dimitrakakis, Christos/0000-0002-5367-5189",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,11,,,,,,,,,,,,,,,WOS:000399835400001,0
J,"Hasenclever, L; Webb, S; Lienart, T; Vollmer, S; Lakshminarayanan, B; Blundell, C; Teh, YW",,,,"Hasenclever, Leonard; Webb, Stefan; Lienart, Thibaut; Vollmer, Sebastian; Lakshminarayanan, Balaji; Blundell, Charles; Teh, Yee Whye",,,Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,106,,,,,,,,,,,,,,,WOS:000413450800001,0
J,"Oliehoek, FA; Spaan, MTJ; Terwijn, B; Robbel, P; Messias, JV",,,,"Oliehoek, Frans A.; Spaan, Matthijs T. J.; Terwijn, Bas; Robbel, Philipp; Messias, Joao V.",,,The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single- and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single- and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm. Keywords: software, decision-theoretic planning, reinforcement learning, multiagent systems",,,,,,"Oliehoek, Frans/0000-0003-4372-5055",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000412070300001,0
J,"Park, YW; Klabjan, D",,,,"Park, Young Woong; Klabjan, Diego",,,Bayesian Network Learning via Topological Order,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics.",,,,,"Park, Young Woong/I-5667-2017; Klabjan, Diego/B-7469-2009","Park, Young Woong/0000-0003-0722-7729; ",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000413448000001,0
J,"Pinto, J; Fern, A",,,,"Pinto, Jervis; Fern, Alan",,,Learning Partial Policies to Speedup MDP Tree Search via Reduction to I. I. D. Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limits the search depth given a time bound. An obvious way to reduce action branching is to consider only a subset of potentially good actions at each state as specified by a provided partial policy. In this work, we consider offline learning of such partial policies with the goal of speeding up search without significantly reducing decision-making quality. Our first contribution consists of reducing the learning problem to set learning. We give a reduction-style analysis of three such algorithms, each making different assumptions, which relates the set learning objectives to the sub-optimality of search using the learned partial policies. Our second contribution is to describe concrete implementations of the algorithms within the popular framework of Monte-Carlo tree search. Finally, the third contribution is to evaluate the learning algorithms on two challenging MDPs with large action branching factors. The results show that the learned partial policies can significantly improve the anytime performance of Monte-Carlo tree search.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,35,65,,,,,,,,,,,,,,,WOS:000412057100001,0
J,"Shang, ZF; Cheng, G",,,,"Shang, Zuofeng; Cheng, Guang",,,Computational Limits of A Distributed Algorithm for Smoothing Spline,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we explore statistical versus computational trade-off to address a basic question in the application of a distributed algorithm: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of non parametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm considered in this paper, and turn out to be fully determined by the smoothness of the regression function. We name the asymptotic analysis on such split-and aggregation estimation/inference as splitotic theory. As a side remark, we argue that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,108,,,,,,,,,,,,,,,WOS:000413451400001,0
J,"Gillberg, J; Marttinen, P; Pirinen, M; Kangas, AJ; Soininen, P; Ali, M; Havulinna, AS; Jarvelin, MR; Ala-Korpela, M; Kaski, S",,,,"Gillberg, Jussi; Marttinen, Pekka; Pirinen, Matti; Kangas, Antti J.; Soininen, Pasi; Ali, Mehreen; Havulinna, Aki S.; Jarvelin, Marjo-Riitta; Ala-Korpela, Mika; Kaski, Samuel",,,Multiple Output Regression with Latent Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In high-dimensional data, structured noise caused by observed and unobserved factors affecting multiple target variables simultaneously, imposes a serious challenge for modeling, by masking the often weak signal. Therefore, (1) explaining away the structured noise in multiple-output regression is of paramount importance. Additionally, (2) assumptions about the correlation structure of the regression weights are needed. We note that both can be formulated in a natural way in a latent variable model, in which both the interesting signal and the noise are mediated through the same latent factors. Under this assumption, the signal model then borrows strength from the noise model by encouraging similar effects on correlated targets. We introduce a hyperparameter for the latent signal-to-noise ratio which turns out to be important for modelling weak signals, and an ordered infinite dimensional shrinkage prior that resolves the rotational unidentifiability in reduced-rank regression models. Simulations and prediction experiments with metabolite, gene expression, FMRI measurement, and macroeconomic time series data show that our model equals or exceeds the state-of-the-art performance and, in particular, outperforms the standard approach of assuming independent noise and signal models.",,,,,"Havulinna, Aki/AAM-4769-2021; Marttinen, Pekka E/N-6234-2015; Kaski, Samuel/B-6684-2008","Marttinen, Pekka E/0000-0001-7078-7927; Kaski, Samuel/0000-0003-1925-9154; Pirinen, Matti/0000-0002-1664-1350; Ala-Korpela, Mika/0000-0001-5905-1206; Jarvelin, Marjo-Riitta/0000-0002-2149-0630",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,122,,,,,,,,,,,,,,,WOS:000391654600001,0
J,"Kadri, H; Duflos, E; Preux, P; Canu, S; Rakotomamonjy, A; Audiffren, J",,,,"Kadri, Hachem; Duflos, Emmanuel; Preux, Philippe; Canu, Stephane; Rakotomamonjy, Alain; Audiffren, Julien",,,Operator-valued Kernels for Learning from Functional Response Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper(1) we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments",,,,,"Preux, Pierre-Marie/N-3538-2019; PREUX, Pierre-Marie/B-8393-2014","Preux, Pierre-Marie/0000-0002-2171-2977; PREUX, Pierre-Marie/0000-0002-2171-2977",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,20,,,,,,,,,,,,,,,WOS:000391473900001,0
J,"Shah, NB; Zhou, DY",,,,"Shah, Nihar B.; Zhou, Dengyong",,,Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-freelunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure.",,,,,", Gustavo/ABC-1706-2022; Camps-Valls, Gustavo/A-2532-2011","Camps-Valls, Gustavo/0000-0003-1683-2138",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,52,165,,,,,,,,,,,,,,,WOS:000391670300001,0
J,"Shimkin, N",,,,"Shimkin, Nahum",,,An Online Convex Optimization Approach to Black well's Approachability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions and corresponding approachability strategies that rely on computing a sequence of direction vectors in the payoff space. For convex target sets, these vectors are obtained as projections from the current average payoff vector to the set. A recent paper by Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on Online Linear Programming for obtaining alternative sequences of direction vectors. This is first implemented for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone. In this paper we present a more direct formulation that relies on general Online Convex Optimization (OCO) algorithms, along with basic properties of the support function of convex sets. This leads to a general class of approachability algorithms, depending on the choice of the OCO algorithm and the used norms. Blackwells original algorithm and its convergence are recovered when Follow The Leader (or a regularized version thereof) is used for the OCO algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,23,129,,,,,,,,,,,,,,,WOS:000391655600001,0
J,"Vollmer, SJ; Zygalakis, KC; Teh, YW",,,,"Vollmer, Sebastian J.; Zygalakis, Konstantinos C.; Teh, Yee Whye",,,Exploration of the (Non-)Asymptotic Bias and Variance of Stochastic Gradient Langevin Dynamics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis-Hastings accept-reject step, and it uses sequences of decreasing step sizes. In Teh et al. (2014), we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets.",,,,,"Zygalakis, Konstantinos/AAZ-8636-2020","Zygalakis, Konstantinos/0000-0002-3860-9167",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,45,159,,,,,,,,,,,,,,,WOS:000391667500001,0
J,"Wang, SS; Luo, L; Zhang, ZH",,,,"Wang, Shusen; Luo, Luo; Zhang, Zhihua",,,"SPSD Matrix Approximation vis Column Selection: Theories, Algorithms, and Extensions",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Symmetric positive semidefinite (SPSD) matrix approximation is an important problem with applications in kernel methods. However, existing SPSD matrix approximation methods such as the Nystrom method only have weak error bounds. In this paper we conduct in-depth studies of an SPSD matrix approximation model and establish strong relative-error bounds. We call it the prototype model for it has more efficient and effective extensions, and some of its extensions have high scalability. Though the prototype model itself is not suitable for large-scale data, it is still useful to study its properties, on which the analysis of its extensions relies. This paper offers novel theoretical analysis, efficient algorithms, and a highly accurate extension. First, we establish a lower error bound for the prototype model and improve the error bound of an existing column selection algorithm to match the lower bound. In this way, we obtain the first optimal column selection algorithm for the prototype model. We also prove that the prototype model is exact under certain conditions. Second, we develop a simple column selection algorithm with a provable error bound. Third, we propose a so-called spectral shifting model to make the approximation more accurate when the eigenvalues of the matrix decay slowly, and the improvement is theoretically quantified. The spectral shifting method can also be applied to improve other SPSD matrix approximation models.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,49,,,,,,,,,,,,,,,WOS:000391485800001,0
J,"Ryan, KJ; Culp, MV",,,,"Ryan, Kenneth Joseph; Culp, Mark Vere",,,On Semi-Supervised Linear Regression in Covariate Shift Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Semi-supervised learning approaches are trained using the full training (labeled) data and available testing (unlabeled) data. Demonstrations of the value of training with unlabeled data typically depend on a smoothness assumption relating the conditional expectation to high density regions of the marginal distribution and an inherent missing completely at random assumption for the labeling. So-called covariate shift poses a challenge for many existing semi-supervised or supervised learning techniques. Covariate shift models allow the marginal distributions of the labeled and unlabeled feature data to differ, but the conditional distribution of the response given the feature data is the same. An example of this occurs when a complete labeled data sample and then an unlabeled sample are obtained sequentially, as it would likely follow that the distributions of the feature data are quite different between samples. The value of using unlabeled data during training for the elastic net is justified geometrically in such practical covariate shift problems. The approach works by obtaining adjusted coefficients for unlabeled prediction which recalibrate the supervised elastic net to compromise: (i) maintaining elastic net predictions on the labeled data with (ii) shrinking unlabeled predictions to zero. Our approach is shown to dominate linear supervised alternatives on unlabeled response predictions when the unlabeled feature data are concentrated on a low dimensional manifold away from the labeled data and the true coefficient vector emphasizes directions away from this manifold. Large variance of the supervised predictions on the unlabeled set is reduced more than the increase in squared bias when the unlabeled responses are expected to be small, so an improved compromise within the bias-variance tradeoff is the rationale for this performance improvement. Performance is validated on simulated and real data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3183,3217,,,,,,,,,,,,,,,,WOS:000369888000028,0
J,"Taleghan, MA; Dietterich, TG; Crowley, M; Hall, K; Albers, HJ",,,,"Taleghan, Majid Alkaee; Dietterich, Thomas G.; Crowley, Mark; Hall, Kim; Albers, H. Jo",,,PAC Optimal MDP Planning with Application to Invasive Species Management,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilistic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8% and 47% in the number of simulator calls required to reach near-optimal policies.",,,,,"Crowley, Mark/AAF-3039-2021","Crowley, Mark/0000-0003-3921-4762",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3877,3903,,,,,,,,,,,,,,,,WOS:000369888000046,0
J,"Montufar, G; Morton, J",,,,"Montufar, Guido; Morton, Jason",,,Discrete Restricted Boltzmann Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe discrete restricted Boltzmann machines: probabilistic graphical models with bipartite interactions between visible and hidden discrete variables. Examples are binary restricted Boltzmann machines and discrete naive Bayes models. We detail the inference functions and distributed representations arising in these models in terms of configurations of projected products of simplices and normal fans of products of simplices. We bound the number of hidden variables, depending on the cardinalities of their state spaces, for which these models can approximate any probability distribution on their visible states to any given accuracy. In addition, we use algebraic methods and coding theory to compute their dimension.",,,,,,"Montufar Cuartas, Guido F./0000-0002-0131-2669",,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,653,672,,,,,,,,,,,,,,,,WOS:000369886300001,0
J,"Basu, S; Shojaie, A; Michailidis, G",,,,"Basu, Sumanta; Shojaie, Ali; Michailidis, George",,,Network Granger Causality with Inherent Grouping Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of estimating high-dimensional network models arises naturally in the analysis of many biological and socio-economic systems. In this work, we aim to learn a network structure from temporal panel data, employing the framework of Granger causal models under the assumptions of sparsity of its edges and inherent grouping structure among its nodes. To that end, we introduce a group lasso regression regularization framework, and also examine a thresholded variant to address the issue of group misspecification. Further, the norm consistency and variable selection consistency of the estimates are established, the latter under the novel concept of direction consistency. The performance of the proposed methodology is assessed through an extensive set of simulation studies and comparisons with existing techniques. The study is illustrated on two motivating examples coming from functional genomics and financial econometrics.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,417,453,,,,,,,,,,,34267606,,,,,WOS:000369886000003,0
J,"Li, XG; Zhao, T; Yuan, XM; Liu, H",,,,"Li, Xingguo; Zhao, Tuo; Yuan, Xiaoming; Liu, Han",,,The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, l(q) Lasso, and Dantzig selector) and their extensions to sparse precision matrix estimation (TIGER and CLIME). These methods exploit different nonsmooth loss functions to gain modeling flexibility, estimation robustness, and tuning insensitiveness. The developed solver is based on the alternating direction method of multipliers (ADMM). The package flare is coded in double precision C, and called from R by a user-friendly interface. The memory usage is optimized by using the sparse matrix output. The experiments show that flare is efficient and can scale up to large problems.",,,,,"Yuan, Xiaoming/B-7150-2009",,,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,553,557,,,,,,,,,,,28337074,,,,,WOS:000369886000008,0
J,"Jackson, JC; Wimmer, K",,,,"Jackson, Jeffrey C.; Wimmer, Karl",,,New Results for Random Walk Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a very strong positive result for passive learning algorithms, Bshouty et al. showed that DNF expressions are efficiently learnable in the uniform random walk model. It is natural to ask whether the more expressive class of thresholds of parities (TOP) can also be learned efficiently in this model, since both DNF and TOP are efficiently uniform-learnable from queries. However, the time bounds of the algorithms of Bshouty et al. are exponential for TOP. We present a new approach to weak parity learning that leads to quasi-efficient uniform random walk learnability of TOP. We also introduce a more general random walk model and give two positive results in this new model: DNF is efficiently learnable and juntas are efficiently agnostically learnable.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3635,3666,,,,,,,,,,,,,,,,WOS:000353126200010,0
J,"Chen, YD; Jalali, A; Sanghavi, S; Xu, H",,,,"Chen, Yudong; Jalali, Ali; Sanghavi, Sujay; Xu, Huan",,,Clustering Partially Observed Graphs via Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the problem of clustering a partially observed unweighted graph i.e., one where for some node pairs we know there is an edge between them, for some others we know there is no edge, and for the remaining we do not know whether or not there is an edge. We want to organize the nodes into disjoint clusters so that there is relatively dense (observed) connectivity within clusters, and sparse across clusters. We take a novel yet natural approach to this problem, by focusing on finding the clustering that minimizes the number of disagreements i.e., the sum of the number of (observed) missing edges within clusters, and (observed) present edges across clusters. Our algorithm uses convex optimization; its basis is a reduction of disagreement minimization to the problem of recovering an (unknown) low-rank matrix and an (unknown) sparse matrix from their partially observed sum. We evaluate the performance of our algorithm on the classical Planted Partition/Stochastic Block Model. Our main theorem provides sufficient conditions for the success of our algorithm as a function of the minimum cluster size, edge density and observation probability; in particular, the results characterize the tradeoff between the observation probability and the edge density gap. When there are a constant number of clusters of equal size, our results are optimal up to logarithmic factors.",,,,,"Xu, Huan/M-5155-2014; Jalali, Ali Akbar/T-5628-2018; Jalali, Amir/V-2562-2018; xu, huan/R-5436-2016","Jalali, Amir/0000-0002-0307-879X; xu, huan/0000-0002-5712-0308",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2213,2238,,,,,,,,,,,,,,,,WOS:000344638300011,0
J,"Oentaryo, R; Lim, EP; Finegold, M; Lo, D; Zhu, FD; Phua, C; Cheu, EY; Yap, GE; Sim, K; Nguyen, MN; Perera, K; Neupane, B; Faisal, M; Aung, Z; Woon, WL; Chen, W; Patel, D; Berrar, D",,,,"Oentaryo, Richard; Lim, Ee-Peng; Finegold, Michael; Lo, David; Zhu, Feida; Phua, Clifton; Cheu, Eng-Yeow; Yap, Ghim-Eng; Sim, Kelvin; Nguyen, Minh Nhut; Perera, Kasun; Neupane, Bijay; Faisal, Mustafa; Aung, Zeyar; Woon, Wei Lee; Chen, Wei; Patel, Dhaval; Berrar, Daniel",,,Detecting Click Fraud in Online Advertising: A Data Mining Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Click fraud-the deliberate clicking on advertisements with no real interest on the product or service offered-is one of the most daunting problems in online advertising. Building an effective fraud detection method is thus pivotal for online advertising businesses. We organized a Fraud Detection in Mobile Advertising (FDMA) 2012 Competition, opening the opportunity for participants to work on real-world fraud data from BuzzCity Pte. Ltd., a global mobile advertising company based in Singapore. In particular, the task is to identify fraudulent publishers who generate illegitimate clicks, and distinguish them from normal publishers. The competition was held from September 1 to September 30, 2012, attracting 127 teams from more than 15 countries. The mobile advertising data are unique and complex, involving heterogeneous information, noisy patterns with missing values, and highly imbalanced class distribution. The competition results provide a comprehensive study on the usability of data mining-based fraud detection approaches in practical setting. Our principal findings are that features derived from fine-grained time-series analysis are crucial for accurate fraud detection, and that ensemble methods offer promising solutions to highly-imbalanced nonlinear classification tasks with mixed variable types and noisy/missing patterns. The competition data remain available for further studies at http://palanteer.sis.smu.edu.sg/fdma2012/.",,,,,"Aung, Zeyar/P-5800-2019; Berrar, Daniel/M-8300-2018; Oentaryo, Richard/AAC-8098-2019; LIM, Ee Peng/E-8562-2012; Oentaryo, Richard/M-5948-2014; ZHU, Feida/E-8579-2012; LO, David/A-2493-2012","Aung, Zeyar/0000-0001-5990-9305; Berrar, Daniel/0000-0002-7038-2601; Oentaryo, Richard/0000-0002-4662-1561; LIM, Ee Peng/0000-0003-0065-8665; Oentaryo, Richard/0000-0002-4662-1561; ZHU, Feida/0000-0001-6077-4356; LO, David/0000-0002-4367-7201; Woon, Wei Lee/0000-0002-6155-1741",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,99,140,,,,,,,,,,,,,,,,WOS:000335457400003,0
J,"Watanabe, S",,,,"Watanabe, Sumio",,,A Widely Applicable Bayesian Information Criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature 1/logn, where n is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for or unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models.",,,,,"Watanabe, Sumio/C-3880-2015; Watanabe, Sumio/M-7370-2019","Watanabe, Sumio/0000-0001-8341-5639; ",,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,867,897,,,,,,,,,,,,,,,,WOS:000317461700006,0
J,"Schnitzer, D; Flexer, A; Schedl, M; Widmer, G",,,,"Schnitzer, Dominik; Flexer, Arthur; Schedl, Markus; Widmer, Gerhard",,,Local and Global Scaling Reduce Hubs in Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"'Hubness' has recently been identified as a general problem of high dimensional data spaces, manifesting itself in the emergence of objects, so-called hubs, which tend to be among the k nearest neighbors of a large number of data items. As a consequence many nearest neighbor relations in the distance space are asymmetric, that is, object y is amongst the nearest neighbors of x but not vice versa. The work presented here discusses two classes of methods that try to symmetrize nearest neighbor relations and investigates to what extent they can mitigate the negative effects of hubs. We evaluate local distance scaling and propose a global variant which has the advantage of being easy to approximate for large data sets and of having a probabilistic interpretation. Both local and global approaches are shown to be effective especially for high-dimensional data sets, which are affected by high hubness. Both methods lead to a strong decrease of hubness in these data sets, while at the same time improving properties like classification accuracy. We evaluate the methods on a large number of public machine learning data sets and synthetic data. Finally we present a real-world application where we are able to achieve significantly higher retrieval quality.",,,,,"Widmer, Gerhard/B-8218-2017","Widmer, Gerhard/0000-0003-3531-1282; Flexer, Arthur/0000-0002-1691-737X",,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2871,2902,,,,,,,,,,,,,,,,WOS:000313200000002,0
J,"Bruckner, M; Kanzow, C; Scheffer, T",,,,"Brueckner, Michael; Kanzow, Christian; Scheffer, Tobias",,,Static Prediction Games for Adversarial Learning Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The standard assumption of identically distributed training and test data is violated when the test data are generated in response to the presence of a predictive model. This becomes apparent, for example, in the context of email spam filtering. Here, email service providers employ spam filters, and spam senders engineer campaign templates to achieve a high rate of successful deliveries despite the filters. We model the interaction between the learner and the data generator as a static game in which the cost functions of the learner and the data generator are not necessarily antagonistic. We identify conditions under which this prediction game has a unique Nash equilibrium and derive algorithms that find the equilibrial prediction model. We derive two instances, the Nash logistic regression and the Nash support vector machine, and empirically explore their properties in a case study on email spam filtering.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2617,2654,,,,,,,,,,,,,,,,WOS:000309580600005,0
J,"Maurer, A; Pontil, M",,,,"Maurer, Andreas; Pontil, Massimiliano",,,Structured Sparsity and Generalization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,671,690,,,,,,,,,,,,,,,,WOS:000303772100007,0
J,"Shalit, U; Weinshall, D; Chechik, G",,,,"Shalit, Uri; Weinshall, Daphna; Chechik, Gal",,,Online Learning in the Embedded Manifold of Low-rank Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches to minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low-rank matrix). We build on recent advances in optimization over manifolds, and describe an iterative online learning procedure, consisting of a gradient step, followed by a second-order retraction back to the manifold. While the ideal retraction is costly to compute, and so is the projection operator that approximates it, we describe another retraction that can be computed efficiently. It has run time and memory complexity of O((n + m)k) for a rank-k matrix of dimension m x n, when using an online procedure with rank-one gradients. We use this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of documents represented as high dimensional vectors. LORETA improves the mean average precision over a passive-aggressive approach in a factorized model, and also improves over a full model trained on pre-selected features using the same memory requirements. We further adapt LORETA to learn positive semi-definite low-rank matrices, providing an online algorithm for low-rank metric learning. LORETA also shows consistent improvement over standard weakly supervised methods in a large (1600 classes and 1 million images, using ImageNet) multi-label image classification task.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,429,458,,,,,,,,,,,,,,,,WOS:000303046000007,0
J,"Dekel, O; Gilad-Bachrach, R; Shamir, O; Xiao, L",,,,"Dekel, Ofer; Gilad-Bachrach, Ran; Shamir, Ohad; Xiao, Lin",,,Optimal Distributed Online Prediction Using Mini-Batches,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Online prediction methods are typically presented as serial algorithms running on a single processor. However, in the age of web-scale prediction problems, it is increasingly common to encounter situations where a single processor cannot keep up with the high rate at which inputs arrive. In this work, we present the distributed mini-batch algorithm, a method of converting many serial gradient-based online prediction algorithms into distributed algorithms. We prove a regret bound for this method that is asymptotically optimal for smooth convex loss functions and stochastic inputs. Moreover, our analysis explicitly takes into account communication latencies between nodes in the distributed environment. We show how our method can be used to solve the closely-related distributed stochastic optimization problem, achieving an asymptotically linear speed-up over multiple processors. Finally, we demonstrate the merits of our approach on a web-scale online prediction problem.",,,,,"Gilad-Bachrach, Ran/ABG-6990-2020","Gilad-Bachrach, Ran/0000-0002-4001-8307",,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,165,202,,,,,,,,,,,,,,,,WOS:000303045100006,0
J,"Sumer, O; Acar, UA; Ihler, AT; Mettu, RR",,,,"Suemer, Oezguer; Acar, Umut A.; Ihler, Alexander T.; Mettu, Ramgopal R.",,,Adaptive Exact Inference in Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many algorithms and applications involve repeatedly solving variations of the same inference problem, for example to introduce new evidence to the model or to change conditional dependencies. As the model is updated, the goal of adaptive inference is to take advantage of previously computed quantities to perform inference more rapidly than from scratch. In this paper, we present algorithms for adaptive exact inference on general graphs that can be used to efficiently compute marginals and update MAP configurations under arbitrary changes to the input factor graph and its associated elimination tree. After a linear time preprocessing step, our approach enables updates to the model and the computation of any marginal in time that is logarithmic in the size of the input model. Moreover, in contrast to max-product our approach can also be used to update MAP configurations in time that is roughly proportional to the number of updated entries, rather than the size of the input model. To evaluate the practical effectiveness of our algorithms, we implement and test them using synthetic data as well as for two real-world computational biology applications. Our experiments show that adaptive inference can achieve substantial speedups over performing complete inference as the model undergoes small changes over time.",,,,,,"Ihler, Alexander/0000-0002-4331-1015",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3147,3186,,,,,,,,,,,,,,,,WOS:000298103700004,0
J,"Mes, MRK; Powell, WB; Frazier, PI",,,,"Mes, Martijn R. K.; Powell, Warren B.; Frazier, Peter I.",,,Hierarchical Knowledge Gradient for Sequential Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a sequential sampling policy for noisy discrete global optimization and ranking and selection, in which we aim to efficiently explore a finite set of alternatives before selecting an alternative as best when exploration stops. Each alternative may be characterized by a multidimensional vector of categorical and numerical attributes and has independent normal rewards. We use a Bayesian probability model for the unknown reward of each alternative and follow a fully sequential sampling policy called the knowledge-gradient policy. This policy myopically optimizes the expected increment in the value of sampling information in each time period. We propose a hierarchical aggregation technique that uses the common features shared by alternatives to learn about many alternatives from even a single measurement. This approach greatly reduces the measurement effort required, but it requires some prior knowledge on the smoothness of the function in the form of an aggregation function and computational issues limit the number of alternatives that can be easily considered to the thousands. We prove that our policy is consistent, finding a globally optimal alternative when given enough measurements, and show through simulations that it performs competitively with or significantly better than other policies.",,,,,"Powell, Warren/N-8263-2019; Mes, Martijn/P-9649-2015","Mes, Martijn/0000-0001-9676-5259",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2931,2974,,,,,,,,,,,,,,,,WOS:000298103200008,0
J,"Zavitsanos, E; Paliouras, G; Vouros, GA",,,,"Zavitsanos, Elias; Paliouras, Georgios; Vouros, George A.",,,Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents hHDP, a hierarchical algorithm for representing a document collection as a hierarchy of latent topics, based on Dirichlet process priors. The hierarchical nature of the algorithm refers to the Bayesian hierarchy that it comprises, as well as to the hierarchy of the latent topics. hHDP relies on nonparametric Bayesian priors and it is able to infer a hierarchy of topics, without making any assumption about the depth of the learned hierarchy and the branching factor at each level. We evaluate the proposed method on real-world data sets in document modeling, as well as in ontology learning, and provide qualitative and quantitative evaluation results, showing that the model is robust, it models accurately the training data set and is able to generalize on held-out data.",,,,,"Vouros, George/AAF-3229-2020; Vouros, George/AAO-8415-2021","Vouros, George/0000-0001-5451-622X",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2749,2775,,,,,,,,,,,,,,,,WOS:000298103200001,0
J,"Shervashidze, N; Schweitzer, P; van Leeuwen, EJ; Mehlhorn, K; Borgwardt, KM",,,,"Shervashidze, Nino; Schweitzer, Pascal; van Leeuwen, Erik Jan; Mehlhorn, Kurt; Borgwardt, Karsten M.",,,Weisfeiler-Lehman Graph Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.",,,,,,"Borgwardt, Karsten/0000-0001-7221-2393",,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2539,2561,,,,,,,,,,,,,,,,WOS:000298102900001,0
J,"Sriperumbudur, BK; Fukumizu, K; Lanckriet, GRG",,,,"Sriperumbudur, Bharath K.; Fukumizu, Kenji; Lanckriet, Gert R. G.",,,"Universality, Characteristic Kernels and RKHS Embedding of Measures",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Over the last few years, two different notions of positive definite (pd) kernels-universal and characteristic-have been developing in parallel in machine learning: universal kernels are proposed in the context of achieving the Bayes risk by kernel-based classification/regression algorithms while characteristic kernels are introduced in the context of distinguishing probability measures by embedding them into a reproducing kernel Hilbert space (RKHS). However, the relation between these two notions is not well understood. The main contribution of this paper is to clarify the relation between universal and characteristic kernels by presenting a unifying study relating them to RKHS embedding of measures, in addition to clarifying their relation to other common notions of strictly pd, conditionally strictly pd and integrally strictly pd kernels. For radial kernels on R-d, all these notions are shown to be equivalent.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2389,2410,,,,,,,,,,,,,,,,WOS:000293757900010,0
J,"Perchet, V",,,,"Perchet, Vianney",,,Internal Regret with Partial Monitoring: Calibration-Based Optimal Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide consistent random algorithms for sequential decision under partial monitoring, when the decision maker does not observe the outcomes but receives instead random feedback signals. Those algorithms have no internal regret in the sense that, on the set of stages where the decision maker chose his action according to a given law, the average payoff could not have been improved in average by using any other fixed law. They are based on a generalization of calibration, no longer defined in terms of a Voronoi diagram but instead of a Laguerre diagram (a more general concept). This allows us to bound, for the first time in this general framework, the expected average internal, as well as the usual external, regret at stage n by O(n(-1/3)), which is known to be optimal.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1893,1921,,,,,,,,,,,,,,,,WOS:000293757200004,0
J,"Wu, W; Xu, J; Li, H; Oyama, S",,,,"Wu, Wei; Xu, Jun; Li, Hang; Oyama, Satoshi",,,Learning a Robust Relevance Model for Search Using Kernel Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper points out that many search relevance models in information retrieval, such as the Vector Space Model, BM25 and Language Models for Information Retrieval, can be viewed as a similarity function between pairs of objects of different types, referred to as an S-function. An S-function is specifically defined as the dot product between the images of two objects in a Hilbert space mapped from two different input spaces. One advantage of taking this view is that one can take a unified and principled approach to address the issues with regard to search relevance. The paper then proposes employing a kernel method to learn a robust relevance model as an S-function, which can effectively deal with the term mismatch problem, one of the biggest challenges in search. The kernel method exploits a positive semi-definite kernel referred to as an S-kernel. The paper shows that when using an S-kernel the model learned by the kernel method is guaranteed to be an S-function. The paper then gives more general principles for constructing S-kernels. A specific implementation of the kernel method is proposed using the Ranking SVM techniques and click-through data. The proposed approach is employed to learn a relevance model as an extension of BM25, referred to as Robust BM25. Experimental results on web search and enterprise search data show that Robust BM25 significantly outperforms baseline methods and can successfully tackle the term mismatch problem.",,,,,"Oyama, Satoshi/AAZ-9838-2020; Xu, Jun/E-9681-2013","Xu, Jun/0000-0001-5861-5110",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1429,1458,,,,,,,,,,,,,,,,WOS:000292304000001,0
J,"Henao, R; Winther, O",,,,"Henao, Ricardo; Winther, Ole",,,Sparse Linear Identifiable Multivariate Modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.",,,,,"Henao, Ricardo/Q-1381-2016","Henao, Ricardo/0000-0003-4980-845X; Winther, Ole/0000-0002-1966-3205",,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,863,905,,,,,,,,,,,,,,,,WOS:000289635000005,0
J,"Liu, H; Xu, M; Gu, HJ; Gupta, A; Lafferty, J; Wasserman, L",,,,"Liu, Han; Xu, Min; Gu, Haijie; Gupta, Anupam; Lafferty, John; Wasserman, Larry",,,Forest Density Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study graph estimation and density estimation in high dimensions, using a family of density estimators based on forest structured undirected graphical models. For density estimation, we do not assume the true distribution corresponds to a forest; rather, we form kernel density estimates of the bivariate and univariate marginals, and apply Kruskal's algorithm to estimate the optimal forest on held out data. We prove an oracle inequality on the excess risk of the resulting estimator relative to the risk of the best forest. For graph estimation, we consider the problem of estimating forests with restricted tree sizes. We prove that finding a maximum weight spanning forest with restricted tree size is NP-hard, and develop an approximation algorithm for this problem. Viewing the tree size as a complexity parameter, we then select a forest using data splitting, and prove bounds on excess risk and structure selection consistency of the procedure. Experiments with simulated data and microarray data indicate that the methods are a practical alternative to Gaussian graphical models.",,,,,"Liu, Han/P-7105-2018",,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,907,951,,,,,,,,,,,,,,,,WOS:000289635000006,0
J,"Melacci, S; Belkin, M",,,,"Melacci, Stefano; Belkin, Mikhail",,,Laplacian Support Vector Machines Trained in the Primal,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from O(n(3)) to O(kn(2)), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach.",,,,,,"MELACCI, STEFANO/0000-0002-0415-0888",,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,1149,1184,,,,,,,,,,,,,,,,WOS:000289635000012,0
J,"Kumar, MP; Veksler, O; Torr, PHS",,,,"Kumar, M. Pawan; Veksler, Olga; Torr, Philip H. S.",,,Improved Moves for Truncated Convex Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of obtaining an approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model. For this problem, we propose two st-MINCUT based move making algorithms that we call Range Swap and Range Expansion. Our algorithms can be thought of as extensions of alpha beta-Swap and alpha-Expansion respectively that fully exploit the form of the pairwise potentials. Specifically, instead of dealing with one or two labels at each iteration, our methods explore a large search space by considering a range of labels (that is, an interval of consecutive labels). Furthermore, we show that Range Expansion provides the same multiplicative bounds as the standard linear programming (LP) relaxation in polynomial time. Compared to previous approaches based on the LP relaxation, for example interior-point algorithms or tree-reweighted message passing (TRW), our methods are faster as they use only the efficient st-MINCUT algorithm in their design. We demonstrate the usefulness of the proposed approaches on both synthetic and standard real data problems.",,,,,"Veksler, Olga/B-6549-2015","Veksler, Olga/0000-0002-9664-6601",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,31,67,,,,,,,,,,,,,,,,WOS:000287938500002,0
J,"Vincent, P; Larochelle, H; Lajoie, I; Bengio, Y; Manzagol, PA",,,,"Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine",,,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.",,,,,,"Larochelle, Hugo/0000-0002-2960-9288",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3371,3408,,,,,,,,,,,,,,,,WOS:000286637200004,0
J,"Clark, A; Eyraud, R; Habrard, A",,,,"Clark, Alexander; Eyraud, Remi; Habrard, Amaury",,,Using Contextual Representations to Efficiently Learn Context-Free Languages,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a polynomial update time algorithm for the inductive inference of a large class of context-free languages using the paradigm of positive data and a membership oracle. We achieve this result by moving to a novel representation, called Contextual Binary Feature Grammars (CBFGs), which are capable of representing richly structured context-free languages as well as some context sensitive languages. These representations explicitly model the lattice structure of the distribution of a set of substrings and can be inferred using a generalisation of distributional learning. This formalism is an attempt to bridge the gap between simple learnable classes and the sorts of highly expressive representations necessary for linguistic representation: it allows the learnability of a large class of context-free languages, that includes all regular languages and those context-free languages that satisfy two simple constraints. The formalism and the algorithm seem well suited to natural language and in particular to the modeling of first language acquisition. Preliminary experimental results confirm the effectiveness of this approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2707,2744,,,,,,,,,,,,,,,,WOS:000284040000005,0
J,"Cohn, I; El-Hay, T; Friedman, N; Kupferman, R",,,,"Cohn, Ido; El-Hay, Tal; Friedman, Nir; Kupferman, Raz",,,Mean Field Variational Approximation for Continuous-Time Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation provided by this language, inference in such models is intractable even in relatively simple structured networks. We introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate a joint distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. Here we describe the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale real-world inference problem.",,,,,"Friedman, Nir/H-9692-2012","Friedman, Nir/0000-0002-9678-3550",,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2745,2783,,,,,,,,,,,,,,,,WOS:000284040000006,0
J,"Yu, J; Vishwanathan, SVN; Gunter, S; Schraudolph, NN",,,,"Yu, Jin; Vishwanathan, S. V. N.; Guenter, Simon; Schraudolph, Nicol N.",,,A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We extend the well-known BFGS quasi-Newton method and its memory-limited variant LBFGS to the optimization of nonsmooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: the local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We prove that under some technical conditions, the resulting subBFGS algorithm is globally convergent in objective function value. We apply its memory-limited variant (subLBFGS) to L-2-regularized risk minimization with the binary hinge loss. To extend our algorithm to the multiclass and multilabel settings, we develop a new, efficient, exact line search algorithm. We prove its worst-case time complexity bounds, and show that our line search can also be used to extend a recently developed bundle method to the multiclass and multilabel settings. We also apply the direction-finding component of our algorithm to L1-regularized risk minimization with logistic loss. In all these contexts our methods perform comparable to or better than specialized state-of-the-art solvers on a number of publicly available data sets. An open source implementation of our algorithms is freely available.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1145,1200,,,,,,,,,,,,,,,,WOS:000277186600006,0
J,"Argyriou, A; Micchelli, CA; Pontil, M",,,,"Argyriou, Andreas; Micchelli, Charles A.; Pontil, Massimiliano",,,On Spectral Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the problem of learning a matrix W from a set of linear measurements. Our formulation consists in solving an optimization problem which involves regularization with a spectral penalty term. That is, the penalty term is a function of the spectrum of the covariance of W. Instances of this problem in machine learning include multi-task learning, collaborative filtering and multi-view learning, among others. Our goal is to elucidate the form of the optimal solution of spectral learning. The theory of spectral learning relies on the von Neumann characterization of orthogonally invariant norms and their association with symmetric gauge functions. Using this tool we formulate a representer theorem for spectral regularization and specify it to several useful example, such as Schatten p-norms, trace norm and spectral norm, which should proved useful in applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,935,953,,,,,,,,,,,,,,,,WOS:000277186500019,0
J,"Strehl, AL; Li, LH; Littman, ML",,,,"Strehl, Alexander L.; Li, Lihong; Littman, Michael L.",,,Reinforcement Learning in Finite MDPs: PAC Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These PAC-MDP algorithms include the well-known E-3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2413,2444,,,,,,,,,,,,,,,,WOS:000272346600001,0
J,"Tuv, E; Borisov, A; Runger, G; Torkkola, K",,,,"Tuv, Eugene; Borisov, Alexander; Runger, George; Torkkola, Kari",,,"Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for filters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1341,1366,,,,,,,,,,,,,,,,WOS:000270825000001,0
J,"Kulis, B; Sustik, MA; Dhillon, IS",,,,"Kulis, Brian; Sustik, Matyas A.; Dhillon, Inderjit S.",,,Low-Rank Kernel Learning with Bregman Matrix Divergences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study low-rank matrix nearness problems, with a focus on learning low-rank positive semidefinite (kernel) matrices for machine learning applications. We propose efficient algorithms that scale linearly in the number of data points and quadratically in the rank of the input matrix. Existing algorithms for learning kernel matrices often scale poorly, with running times that are cubic in the number of data points. We employ Bregman matrix divergences as the measures of nearness-these divergences are natural for learning low-rank kernels since they preserve rank as well as positive semidefiniteness. Special cases of our framework yield faster algorithms for various existing learning problems, and experimental results demonstrate that our algorithms can effectively learn both low-rank and full-rank kernel matrices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,341,376,,,,,,,,,,,,,,,,WOS:000270824200009,0
J,"Sun, X; Nobel, AB",,,,"Sun, Xing; Nobel, Andrew B.",,,On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with fixed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2431,2453,,,,,,,,,,,,,,,,WOS:000262637600001,0
J,"Huang, TK; Lin, CJ; Weng, RC",,,,"Huang, Tzu-Kuo; Lin, Chih-Jen; Weng, Ruby C.",,,Ranking Individuals by Group Comparisons,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes new approaches to rank individuals from their group comparison results. Many real-world problems are of this type. For example, ranking players from team comparisons is important in some sports. In machine learning, a closely related application is classification using coding matrices. Group comparison results are usually in two types: binary indicator outcomes (wins/losses) or measured outcomes (scores). For each type of results, we propose new models for estimating individuals' abilities, and hence a ranking of individuals. The estimation is carried out by solving convex minimization problems, for which we develop easy and efficient solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed models.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2187,2216,,,,,,,,,,,,,,,,WOS:000262637300006,0
J,"Jambeiro, J; Wainer, J",,,,"Jambeiro Filho, Jorge; Wainer, Jacques",,,HPB: A Model for Handling BN Nodes with High Cardinality Parents,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB). 1 The resulting Bayesian networks achieved significant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naive Bayes and tree augmented naive Bayes, over Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability.",,,,,"Wainer, Jacques/AAQ-6029-2021; Wainer, Jacques/B-4241-2012","Wainer, Jacques/0000-0001-5201-1244; Wainer, Jacques/0000-0001-5201-1244",,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2141,2170,,,,,,,,,,,,,,,,WOS:000262637300004,0
J,"Warmuth, MK; Kuzmin, D",,,,"Warmuth, Manfred K.; Kuzmin, Dima",,,Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We first develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n(2)) per trial, where n is the dimension of the instances.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2287,2320,,,,,,,,,,,,,,,,WOS:000262637300009,0
J,"Corani, G; Zaffalon, M",,,,"Corani, Giorgio; Zaffalon, Marco",,,Learning reliable classifiers from small or incomplete data sets: The naive credal classifier 2,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, the naive credal classifier, which is a set-valued counterpart of naive Bayes, is extended to a general and flexible treatment of incomplete data, yielding a new classifier called naive credal classifier 2 (NCC2). The new classifier delivers classifications that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classifications, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classifier, such as naive Bayes. This phenomenon adds even more value to the robust approach to classification implemented by NCC2.",,,,,"Zaffalon, Marco/M-7035-2017","Zaffalon, Marco/0000-0001-8908-1502; corani, giorgio/0000-0002-1541-8384",,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,581,621,,,,,,,,,,,,,,,,WOS:000256642100002,0
J,"Freund, Y; Schapire, RE",,,,"Freund, Yoav; Schapire, Robert E.",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,"freund, yoav/0000-0002-3850-6184",,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,171,174,,,,,,,,,,,,,,,,WOS:000256641800004,0
J,"Bartlett, PL; Tewari, A",,,,"Bartlett, Peter L.; Tewari, Ambuj",,,Sparseness vs estimating conditional probabilities: Some asymptotic results,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the nice properties of kernel classifiers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classifiers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions.",,,,,,"Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,775,790,,,,,,,,,,,,,,,,WOS:000247002800003,0
J,"Tatti, N",,,,"Tatti, Nikolaj",,,Distances between data sets based on summary statistics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The concepts of similarity and distance are crucial in data mining. We consider the problem of defining the distance between two data sets by comparing summary statistics computed from the data sets. The initial definition of our distance is based on geometrical notions of certain sets of distributions. We show that this distance can be computed in cubic time and that it has several intuitive properties. We also show that this distance is the unique Mahalanobis distance satisfying certain assumptions. We also demonstrate that if we are dealing with binary data sets, then the distance can be represented naturally by certain parity functions, and that it can be evaluated in linear time. Our empirical tests with real world data show that the distance works well.",,,,,,"Tatti, Nikolaj/0000-0002-2087-5360",,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,131,154,,,,,,,,,,,,,,,,WOS:000247002500005,0
J,"Bhatnagar, S; Borkar, VS; Akarapu, M",,,,"Bhatnagar, Shalabh; Borkar, Vivek S.; Akarapu, Madhukar",,,A simulation-based algorithm for ergodic control of Markov chains conditioned on rare events,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of long-run average cost control of Markov chains conditioned on a rare event. In a related recent work, a simulation based algorithm for estimating performance measures associated with a Markov chain conditioned on a rare event has been developed. We extend ideas from this work and develop an adaptive algorithm for obtaining, online, optimal control policies conditioned on a rare event. Our algorithm uses three timescales or step-size schedules. On the slowest timescale, a gradient search algorithm for policy updates that is based on one-simulation simultaneous perturbation stochastic approximation (SPSA) type estimates is used. Deterministic perturbation sequences obtained from appropriate normalized Hadamard matrices are used here. The fast timescale recursions compute the conditional transition probabilities of an associated chain by obtaining solutions to the multiplicative Poisson equation ( for a given policy estimate). Further, the risk parameter associated with the value function for a given policy estimate is updated on a timescale that lies in between the two scales above. We briefly sketch the convergence analysis of our algorithm and present a numerical application in the setting of routing multiple flows in communication networks.",,,,,,"Bhatnagar, Shalabh/0000-0001-7644-3914",,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,1937,1962,,,,,,,,,,,,,,,,WOS:000245390500001,0
J,"Chang, F; Lin, CC; Lu, CJ",,,,"Chang, Fu; Lin, Chin-Chin; Lu, Chi-Jen",,,Adaptive prototype learning algorithms: Theoretical and experimental studies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a number of adaptive prototype learning (APL) algorithms. They employ the same algorithmic scheme to determine the number and location of prototypes, but differ in the use of samples or the weighted averages of samples as prototypes, and also in the assumption of distance measures. To understand these algorithms from a theoretical viewpoint, we address their convergence properties, as well as their consistency under certain conditions. We also present a soft version of APL, in which a non-zero training error is allowed in order to enhance the generalization power of the resultant classifier. Applying the proposed algorithms to twelve UCI benchmark data sets, we demonstrate that they outperform many instance-based learning algorithms, the k-nearest neighbor rule, and support vector machines in terms of average test accuracy.",,,,,"Lu, Chi-Jen/AAQ-3728-2021",,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2125,2148,,,,,,,,,,,,,,,,WOS:000245390500007,0
J,"Ye, JP; Xiong, T",,,,"Ye, Jieping; Xiong, Tao",,,Computational and theoretical analysis of null space and orthogonal linear discriminant analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dimensionality reduction is an important pre-processing step in many applications. Linear discriminant analysis (LDA) is a classical statistical approach for supervised dimensionality reduction. It aims to maximize the ratio of the between-class distance to the within-class distance, thus maximizing the class discrimination. It has been used widely in many applications. However, the classical LDA formulation requires the nonsingularity of the scatter matrices involved. For undersampled problems, where the data dimensionality is much larger than the sample size, all scatter matrices are singular and classical LDA fails. Many extensions, including null space LDA (NLDA) and orthogonal LDA (OLDA), have been proposed in the past to overcome this problem. NLDA aims to maximize the between-class distance in the null space of the within-class scatter matrix, while OLDA computes a set of orthogonal discriminant vectors via the simultaneous diagonalization of the scatter matrices. They have been applied successfully in various applications. In this paper, we present a computational and theoretical analysis of NLDA and OLDA. Our main result shows that under a mild condition which holds in many applications involving high-dimensional data, NLDA is equivalent to OLDA. We have performed extensive experiments on various types of data and results are consistent with our theoretical analysis. We further apply the regularization to OLDA. The algorithm is called regularized OLDA (or ROLDA for short). An efficient algorithm is presented to estimate the regularization value in ROLDA. A comparative study on classification shows that ROLDA is very competitive with OLDA. This confirms the effectiveness of the regularization in ROLDA.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1183,1204,,,,,,,,,,,,,,,,WOS:000245388800002,0
J,"Cuturi, M; Fukumizu, K; Vert, JP",,,,"Cuturi, M; Fukumizu, K; Vert, JP",,,Semigroup kernels on measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a family of positive definite kernels on measures, characterized by the fact that the value of the kernel between two measures is a function of their sum. These kernels can be used to derive kernels on structured objects, such as images and texts, by representing these objects as sets of components, such as pixels or words, or more generally as measures on the space of components. Several kernels studied in this work make use of common quantities defined on measures such as entropy or generalized variance to detect similarities. Given an a priori kernel on the space of components itself, the approach is further extended by restating the previous results in a more efficient and flexible framework using the kernel trick. Finally, a constructive approach to such positive definite kernels through an integral representation theorem is proved, before presenting experimental results on a benchmark experiment of handwritten digits classification to illustrate the validity of the approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1169,1198,,,,,,,,,,,,,,,,WOS:000236329900006,0
J,"Chickering, DM; Heckerman, D; Meek, C",,,,"Chickering, DM; Heckerman, D; Meek, C",,,Large-sample learning of Bayesian networks is NP-hard,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we provide new complexity results for algorithms that learn discrete-variable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest structure for which the model is able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is NP-hard, even when any combination of one or more of the following hold: the generative distribution is perfect with respect to some DAG containing hidden variables; we are given an independence oracle; we are given an inference oracle; we are given an information oracle; we restrict potential solutions to structures in which each node has at most k parents, for all k >= 3. Our proof relies on a new technical result that we establish in the appendices. In particular, we provide a method for constructing the local distributions in a Bayesian network such that the resulting joint distribution is provably perfect with respect to the structure of the network.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1287,1330,,,,,,,,,,,,,,,,WOS:000236328300004,0
J,"Zhang, T",,,,"Zhang, T",,,Statistical analysis of some multi-category large margin classification methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The purpose of this paper is to investigate statistical properties of risk minimization based multicategory classification methods. These methods can be considered as natural extensions of binary large margin classification. We establish conditions that guarantee the consistency of classifiers obtained in the risk minimization framework with respect to the classification error. Examples are provided for four specific forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information can be useful for statistical inferencing tasks beyond classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1225,1251,,,,,,,,,,,,,,,,WOS:000236328300002,0
J,"Hutter, M",,,,"Hutter, M",,,Optimality of universal Bayesian sequence prediction for general loss and alphabet,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Various optimality properties of universal sequence predictors based on Bayes-mixtures in general, and Solomonoff's prediction scheme in particular, will be studied. The probability of observing x(t) at time t, given past observations x(1),...x(t-1) can be computed with the chain rule if the true generating distribution p of the sequences x(1)x(2)x(3)... is known. If mu is unknown, but known to belong to a countable or continuous class M one can base ones prediction on the Bayes-mixture defined as a omega(v)-weighted sum or integral of distributions v E M. The cumulative expected loss of the Bayes-optimal universal prediction scheme based on is shown to be close to the loss of the Bayes-optimal, but infeasible prediction scheme based on p. We show that the bounds are tight and that no other predictor can lead to significantly smaller bounds. Furthermore, for various performance measures, we show V Pareto-optimality of and give an Occam's razor argument that the choice w(v) similar to 2(=K(v)) for the weights is optimal, where K(v) is the length of the shortest program describing V. The results are applied to games of chance, defined as a sequence of bets, observations, and rewards. The prediction schemes (and bounds) are compared to the popular predictors based on expert advice. Extensions to infinite alphabets, partial, delayed and probabilistic prediction, classification, and more active systems are briefly discussed.",,,,,,"Hutter, Marcus/0000-0002-3263-4097",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,971,1000,,10.1162/1532443041827952,0,,,,,,,,,,,,,WOS:000231002600003,0
J,"Christensen, SW; Sinclair, I; Reed, PAS",,,,"Christensen, SW; Sinclair, I; Reed, PAS",,,Designing committees of models through deliberate weighting of data points,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the adaptive derivation of mathematical models from data, each data point should contribute with a weight reflecting the amount of confidence one has in it. When no additional information for data confidence is available, all the data points should be considered equal, and are also generally given the same weight. In the formation of committees of models, however, this is often not the case and the data points may exercise unequal, even random, influence over the committee formation. In this paper, a principled approach to committee design is presented. The construction of a committee design matrix is detailed through which each data point will contribute to the committee formation with a fixed weight, while contributing with different individual weights to the derivation of the different constituent models, thus encouraging model diversity whilst not biasing the committee inadvertently towards any particular data points. Not distinctly an algorithm, it is instead a framework within which several different committee approaches may be realised. Whereas the focus in he paper lies entirely on regression, the principles discussed extend readily to classification.",,,,,"Sinclair, Ian/ABB-5502-2021; Sinclair, Ian/G-4201-2010","Reed, Philippa/0000-0002-2258-0347",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,39,66,,10.1162/153244304322765630,0,,,,,,,,,,,,,WOS:000221043500003,0
J,"Zelenko, D; Aone, C; Richardella, A",,,,"Zelenko, D; Aone, C; Richardella, A",,,Kernel methods for relation extraction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Machine Learning Methods for Text and Images,2001,"VANCOUVER, CANADA",,,,,"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1083,1106,,10.1162/153244303322533205,0,,,,,,,,,,,,,WOS:000186002400004,0
J,"Perkins, TJ; Barto, AG",,,,"Perkins, TJ; Barto, AG",,,Lyapunov design for safe reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,803,832,,10.1162/jmlr.2003.3.4-5.803,0,,,,,,,,,,,,,WOS:000184926200009,0
J,"Arroyo, J; Athreya, A; Cape, J; Chen, GD; Priebe, CE; Vogelstein, JT",,,,"Arroyo, Jesus; Athreya, Avanti; Cape, Joshua; Chen, Guodong; Priebe, Carey E.; Vogelstein, Joshua T.",,,Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The development of models and methodology for the analysis of data from multiple heterogeneous networks is of importance both in statistical network theory and across a wide spectrum of application domains. Although single-graph analysis is well-studied, multiple graph inference is largely unexplored, in part because of the challenges inherent in appropriately modeling graph differences and yet retaining sufficient model simplicity to render estimation feasible. This paper addresses exactly this gap, by introducing a new model, the common subspace independent-edge multiple random graph model, which describes a heterogeneous collection of networks with a shared latent structure on the vertices but potentially different connectivity patterns for each graph. The model encompasses many popular network representations, including the stochastic blockmodel. The model is both flexible enough to meaningfully account for important graph differences, and tractable enough to allow for accurate inference in multiple networks. In particular, a joint spectral embedding of adjacency matrices-the multiple adjacency spectral embedding-leads to simultaneous consistent estimation of underlying parameters for each graph. Under mild additional assumptions, the estimates satisfy asymptotic normality and yield improvements for graph eigen-value estimation. In both simulated and real data, the model and the embedding can be deployed for a number of subsequent network inference tasks, including dimensionality reduction, classification, hypothesis testing, and community detection. Specifically, when the embedding is applied to a data set of connectomes constructed through diffusion magnetic resonance imaging, the result is an accurate classification of brain scans by human subject and a meaningful determination of heterogeneity across scans of different individuals.",,,,,,"Cape, Joshua/0000-0002-1471-1650",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,34650343,,,,,WOS:000700318200001,0
J,"Dantas, CF; Soubies, E; Fevotte, C",,,,"Dantas, Cassio F.; Soubies, Emmanuel; Fevotte, Cedric",,,Expanding Boundaries of Gap Safe Screening,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse optimization problems are ubiquitous in many fields such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the performance of these algorithms is known as safe screening: it allows the early identification of zero coordinates in the solution, which can then be eliminated to reduce the problem's size and accelerate convergence. In this work, we extend the existing Gap Safe screening framework by relaxing the global strong-concavity assumption on the dual cost function. Instead, we exploit local regularity properties, that is, strong concavity on well-chosen subsets of the domain. The non-negativity constraint is also integrated to the existing framework. Besides making safe screening possible to a broader class of functions that includes beta-divergences (e.g., the Kullback-Leibler divergence), the proposed approach also improves upon the existing Gap Safe screening rules on previously applicable cases (e.g., logistic regression). The proposed general framework is exemplified by some notable particular cases: logistic function, beta = 1.5 and Kullback-Leibler divergences. Finally, we showcase the effectiveness of the proposed screening rules with different solvers (coordinate descent, multiplicative-update and proximal gradient algorithms) and different data sets (binary classification, hyperspectral and count data).",,,,,"Soubies, Emmanuel/HGA-2611-2022","Soubies, Emmanuel/0000-0003-0571-6983",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706880000001,0
J,"Grunewalder, S; Khaleghi, A",,,,"Grunewalder, Steffen; Khaleghi, Azadeh",,,Oblivious Data for Fairness with Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the problem of algorithmic fairness in the case where sensitive and non-sensitive features are available and one aims to generate new, 'oblivious', features that closely approximate the non-sensitive features, and are only minimally dependent on the sensitive ones. We study this question in the context of kernel methods. We analyze a relaxed version of the Maximum Mean Discrepancy criterion which does not guarantee full independence but makes the optimization problem tractable. We derive a closed-form solution for this relaxed optimization problem and complement the result with a study of the dependencies between the newly generated features and the sensitive ones. Our key ingredient for generating such oblivious features is a Hilbert-space-valued conditional expectation, which needs to be estimated from data. We propose a plug-in approach and demonstrate how the estimation errors can be controlled. While our techniques help reduce the bias, we would like to point out that no post-processing of any dataset could possibly serve as an alternative to well-designed experiments.",,,,,,"Khaleghi, Azadeh/0000-0001-8643-5416; Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706874600001,0
J,"Gurbuzbalaban, M; Gao, XF; Hu, YH; Zhu, LJ",,,,"Gurbuzbalaban, Mert; Gao, Xuefeng; Hu, Yuanhan; Zhu, Lingjiong",,,Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic gradient Langevin dynamics (SGLD) and stochastic gradient Hamiltonian Monte Carlo (SGHMC) are two popular Markov Chain Monte Carlo (MCMC) algorithms for Bayesian inference that can scale to large datasets, allowing to sample from the posterior distribution of the parameters of a statistical model given the input data and the prior distribution over the model parameters. However, these algorithms do not apply to the decentralized learning setting, when a network of agents are working collaboratively to learn the parameters of a statistical model without sharing their individual data due to privacy reasons or communication constraints. We study two algorithms: Decentralized SGLD (DE-SGLD) and Decentralized SGHMC (DE-SGHMC) which are adaptations of SGLD and SGHMC methods that allow scaleable Bayesian inference in the decentralized setting for large datasets. We show that when the posterior distribution is strongly log-concave and smooth, the iterates of these algorithms converge linearly to a neighborhood of the target distribution in the 2-Wasserstein distance if their parameters are selected appropriately. We illustrate the efficiency of our algorithms on decentralized Bayesian linear regression and Bayesian logistic regression problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706879600001,0
J,"Lugosi, G; Truszkowski, J; Velona, V; Zwiernik, P",,,,"Lugosi, Gabor; Truszkowski, Jakub; Velona, Vasiliki; Zwiernik, Piotr",,,Learning partial correlation graphs and graphical models by covariance queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of recovering the structure underlying large Gaussian graphical models or, more generally, partial correlation graphs. In high-dimensional problems it is often too costly to store the entire sample covariance matrix. We propose a new input model in which one can query single entries of the covariance matrix. We prove that it is possible to recover the support of the inverse covariance matrix with low query and computational complexity. Our algorithms work in a regime when this support is represented by tree-like graphs and, more generally, for graphs of small treewidth. Our results demonstrate that for large classes of graphs, the structure of the corresponding partial correlation graphs can be determined much faster than even computing the empirical covariance matrix.",,,,,"Zwiernik, Piotr/H-4107-2015","Zwiernik, Piotr/0000-0003-3431-131X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706443300001,0
J,"Muandet, K; Kanagawa, M; Saengkyongam, S; Marukatat, S",,,,"Muandet, Krikamol; Kanagawa, Motonobu; Saengkyongam, Sorawit; Marukatat, Sanparith",,,Counterfactual Mean Embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modelling of outcome distributions associated with different interventions-known as counterfactual distributions-is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) which can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator.",,,,,,"Kanagawa, Motonobu/0000-0002-3948-8053",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687200700001,0
J,"Muehlebach, M; Jordan, MI",,,,"Muehlebach, Michael; Jordan, Michael, I",,,"Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the convergence rate of various momentum-based optimization algorithms from a dynamical systems point of view. Our analysis exploits fundamental topological properties, such as the continuous dependence of iterates on their initial conditions, to provide a simple characterization of convergence rates. In many cases, closed-form expressions are obtained that relate algorithm parameters to the convergence rate. The analysis encompasses discrete time and continuous time, as well as time-invariant and time-variant formulations, and is not limited to a convex or Euclidean setting. In addition, the article rigorously establishes why symplectic discretization schemes are important for momentum-based optimization algorithms, and provides a characterization of algorithms that exhibit accelerated convergence.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000656385500001,0
J,"Trillos, NG; Hoffmann, F; Hosseini, B",,,,"Trillos, Nicolas Garcia; Hoffmann, Franca; Hosseini, Bamdad",,,Geometric structure of graph Laplacian embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the spectral clustering procedure for identifying coarse structure in a data set x(1), ..., x(n), and in particular study the geometry of graph Laplacian embeddings which form the basis for spectral clustering algorithms. More precisely, we assume that the data are sampled from a mixture model supported on a manifold M embedded in R-d, and pick a connectivity length-scale epsilon > 0 to construct a kernelized graph Laplacian. We introduce a notion of a well-separated mixture model which only depends on the model itself, and prove that when the model is well separated, with high probability the embedded data set concentrates on cones that are centered around orthogonal vectors. Our results are meaningful in the regime where epsilon = epsilon(n) is allowed to decay to zero at a slow enough rate as the number of data points grows. This rate depends on the intrinsic dimension of the manifold on which the data is supported.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656363300001,0
J,"Jiyuan,; Liu, WD; Mao, XJ; Chen, X",,,,"Tu, Jiyuan; Liu, Weidong; Mao, Xiaojun; Chen, Xi",,,Variance Reduced Median-of-Means Estimator for Byzantine-Robust Distributed Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops an efficient distributed inference algorithm, which is robust against a moderate fraction of Byzantine nodes, namely arbitrary and possibly adversarial machines in a distributed learning system. In robust statistics, the median-of-means (MOM) has been a popular approach to hedge against Byzantine failures due to its ease of implementation and computational efficiency. However, the MOM estimator has the shortcoming in terms of statistical efficiency. The first main contribution of the paper is to propose a variance reduced median-of-means (VRMOM) estimator, which improves the statistical efficiency over the vanilla MOM estimator and is computationally as efficient as the MOM. Based on the proposed VRMOM estimator, we develop a general distributed inference algorithm that is robust against Byzantine failures. Theoretically, our distributed algorithm achieves a fast convergence rate with only a constant number of rounds of communications. We also provide the asymptotic normality result for the purpose of statistical inference. To the best of our knowledge, this is the first normality result in the setting of Byzantine-robust distributed learning. The simulation results are also presented to illustrate the effectiveness of our method.",,,,,"Mao, Xiaojun/AAS-9462-2020; Mao, Xiaojun/AEO-8855-2022","Mao, Xiaojun/0000-0002-9362-508X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656403500001,0
J,"Bassily, R; Nissim, K; Stemmer, U; Thakurta, A",,,,"Bassily, Raef; Nissim, Kobbi; Stemmer, Uri; Thakurta, Abhradeep",,,Practical Locally Private Heavy Hitters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present new practical local differentially private heavy hitters algorithms achieving optimal or near-optimal worst-case error and running time - TreeHist and Bitstogram. In both algorithms, server running time is (O) over tilde (n) and user running time is (O) over tilde (1), hence improving on the prior state-of-the-art result of Bassily and Smith [STOC 2015] requiring O(n(5/2)) server time and O(n(3/2)) user time. With a typically large number of participants in local algorithms (n in the millions), this reduction in time complexity, in particular at the user side, is crucial for making locally private heavy hitters algorithms usable in practice. We implemented Algorithm TreeHist to verify our theoretical analysis and compared its performance with the performance of Google's RAPPOR code.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300016,0
J,"Chamakh, L; Gobet, E; Szabo, Z",,,,"Chamakh, Linda; Gobet, Emmanuel; Szabo, Zoltan",,,Orlicz Random Fourier Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel techniques are among the most widely-applied and influential tools in machine learning with applications at virtually all areas of the field. To combine this expressive power with computational efficiency numerous randomized schemes have been proposed in the literature, among which probably random Fourier features (RFF) are the simplest and most popular. While RFFs were originally designed for the approximation of kernel values, recently they have been adapted to kernel derivatives, and hence to the solution of large-scale tasks involving function derivatives. Unfortunately, the understanding of the RFF scheme for the approximation of higher-order kernel derivatives is quite limited due to the challenging polynomial growing nature of the underlying function class in the empirical process. To tackle this difficulty, we establish a finite-sample deviation bound for a general class of polynomial-growth functions under alpha-exponential Orlicz condition on the distribution of the sample. Instantiating this result for RFFs, our finite-sample uniform guarantee implies a.s. convergence with tight rate for arbitrary kernel with alpha-exponential Orlicz spectrum and any order of derivative.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,145,,,,,,,,,,,,,,,WOS:000558805800001,0
J,"Colbert, BK; Peet, MM",,,,"Colbert, Brendon K.; Peet, Matthew M.",,,A Convex Parametrization of a New Class of Universal Kernel Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria - e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria - the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000020,0
J,"Engelhardt, D",,,,"Engelhardt, Dalit",,,Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,203,,,,,,,,,,,,,,,WOS:000590007100001,0
J,"Guo, J; He, H; He, T; Lausen, L; Li, M; Lin, HB; Shi, XJ; Wang, CG; Xie, JY; Zha, S; Zhang, A; Zhang, H; Zhang, Z; Zhang, ZY; Zheng, S; Zhu, Y",,,,"Guo, Jian; He, He; He, Tong; Lausen, Leonard; Li, Mu; Lin, Haibin; Shi, Xingjian; Wang, Chenguang; Xie, Junyuan; Zha, Sheng; Zhang, Aston; Zhang, Hang; Zhang, Zhi; Zhang, Zhongyue; Zheng, Shuai; Zhu, Yi",,,GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,23,,,,,,,,,,,,,,,WOS:000513691300023,0
J,"Lin, JH; Cevher, V",,,,"Lin, Junhong; Cevher, Volkan",,,Convergences of Regularized Algorithms and Stochastic Gradient Methods with Random Projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space as a special case. We first investigate regularized algorithms adapted to a projection operator on a closed subspace of the Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function. As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystrom regularized algorithms. Our results provide optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nystrom regularized algorithms, considering both the attainable and non-attainable cases, in the well-conditioned regimes. We then study stochastic gradient methods with projection over the subspace, allowing multi-pass over the data and minibatches, and we derive similar optimal statistical convergence results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300020,0
J,"Neykov, M; Wang, ZR; Liu, H",,,,"Neykov, Matey; Wang, Zhaoran; Liu, Han",,,Agnostic Estimation for Misspecified Phase Retrieval Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of noisy high-dimensional phase retrieval is to estimate an 8-sparse parameter beta* is an element of R-d from n realizations of the model Y = (X-inverted perpendicular beta*)(2) + epsilon. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which Y = f (X-inverted perpendicular beta*, epsilon) with unknown f and Cov(Y, (X-inverted perpendicular beta*)(2)) > 0. For example, MPR encompasses Y = h(vertical bar X-inverted perpendicular beta*vertical bar) + epsilon with increasing h as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of beta*. Furthermore, we prove that our procedure is minimax optimal over the class of MPR models. Interestingly, our minimax analysis characterizes the statistical price of misspecifying the link function in phase retrieval models. Our theory is backed up by thorough numerical results.",,,,,"Wang, Zhaoran/P-7113-2018",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,39,121,,,,,,,,,,,,,,,WOS:000556190200001,0
J,"Park, G",,,,"Park, Gunwoong",,,Identifiability of Additive Noise Models Using Conditional Variances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers a new identifiability condition for additive noise models (ANMs) in which each variable is determined by an arbitrary Borel measurable function of its parents plus an independent error. It has been shown that ANMs are fully recoverable under some identifiability conditions, such as when all error variances are equal. However, this identifiable condition could be restrictive, and hence, this paper focuses on a relaxed identifiability condition that involves not only error variances, but also the influence of parents. This new class of identifiable ANMs does not put any constraints on the form of dependencies, or distributions of errors, and allows different error variances. It further provides a statistically consistent and computationally feasible structure learning algorithm for the identifiable ANMs based on the new identifiability condition. The proposed algorithm assumes that all relevant variables are observed, while it does not assume faithfulness or a sparse graph. Demonstrated through extensive simulated and real multivariate data is that the proposed algorithm successfully recovers directed acyclic graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000027,0
J,"Xing, X; Liu, MM; Ma, P; Zhong, WX",,,,"Xing, Xin; Liu, Meimei; Ma, Ping; Zhong, Wenxuan",,,Minimax Nonparametric Parallelism Test,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Testing the hypothesis of parallelism is a fundamental statistical problem arising from many applied sciences. In this paper, we develop a nonparametric parallelism test for inferring whether the trends are parallel in treatment and control groups. In particular, the proposed nonparametric parallelism test is a Wald type test based on a smoothing spline ANOVA (SSANOVA) model which can characterize the complex patterns of the data. We derive that the asymptotic null distribution of the test statistic is a Chi-square distribution, unveiling a new version of Wilks phenomenon. Notably, we establish the minimax sharp lower bound of the distinguishable rate for the nonparametric parallelism test by using the information theory, and further prove that the proposed test is minimax optimal. Simulation studies are conducted to investigate the empirical performance of the proposed test. DNA methylation and neuroimaging studies are presented to illustrate potential applications of the test. The software is available at https://github.com/BioAlgs/Parallelism.",,,,,"Xing, Xin/AAW-7605-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,94,,,,,,,,,,,,,,,WOS:000545028000001,0
J,"Ahmed, A; Aghasi, A; Hand, P",,,,"Ahmed, Ali; Aghasi, Alireza; Hand, Paul",,,Simultaneous Phase Retrieval and Blind Deconvolution via Convex Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the task of recovering two real or complex m-vectors from phaseless Fourier measurements of their circular convolution. Our method is a novel convex relaxation that is based on a lifted matrix recovery formulation that allows a non-trivial convex relaxation of the bilinear measurements from convolution. We prove that if the two signals belong to known random subspaces of dimensions kappa and n, then they can be recovered up to the inherent scaling ambiguity with m >> (kappa + n) log(2) m phaseless measurements. Our method provides the first theoretical recovery guarantee for this problem by a computationally efficient algorithm and does not require a solution estimate to be computed for initialization. Our proof is based on Rademacher complexity estimates. Additionally, we provide an alternating direction method of multipliers (ADMM) implementation and provide numerical experiments that verify the theory.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,157,,,,,,,,,,,,,,,WOS:000491132200021,0
J,"Ahsen, ME; Vogel, RM; Stolovitzky, GA",,,,"Ahsen, Mehmet Eren; Vogel, Robert M.; Stolovitzky, Gustavo A.",,,Unsupervised Evaluation and Weighted Aggregation of Ranked Classification Predictions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ensemble methods that aggregate predictions from a set of diverse base learners consistently outperform individual classifiers. Many such popular strategies have been developed in a supervised setting, where the sample labels have been provided to the ensemble algorithm. However, with the rising interest in unsupervised algorithms for machine learning and growing amounts of uncurated data, the reliance on labeled data precludes the application of ensemble algorithms to many real world problems. To this end we develop a new theoretical framework for ensemble learning, the Strategy for Unsupervised Multiple Method Aggregation (SUMMA), that estimates the performances of base classifiers and uses these estimates to form an ensemble classifier. SUMMA also generates an ensemble ranking of samples based on the confidence score it assigns to each sample. We illustrate the performance of SUMMA using a synthetic example as well as two real world problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,166,,,,,,,,,,,,,,,WOS:000506403100006,0
J,"Balcan, MF; Liang, YY; Song, Z; Woodruff, DP; Zhang, HY",,,,"Balcan, Maria-Florina; Liang, Yingyu; Song, Zhao; Woodruff, David P.; Zhang, Hongyang",,,Non-Convex Matrix Completion and Related Problems via Strong Duality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work studies the strong duality of non-convex matrix factorization problems: we show that under certain dual conditions, these problems and the dual have the same optimum. This has been well understood for convex optimization, but little was known for non-convex problems. We propose a novel analytical framework and prove that under certain dual conditions, the optimal solution of the matrix factorization program is the same as that of its bi-dual and thus the global optimality of the non-convex program can be achieved by solving its bi-dual which is convex. These dual conditions are satisfied by a wide class of matrix factorization problems, although matrix factorization is hard to solve in full generality. This analytical framework may be of independent interest to non-convex optimization more broadly. We apply our framework to two prototypical matrix factorization problems: matrix completion and robust Principal Component Analysis. These are examples of efficiently recovering a hidden matrix given limited reliable observations. Our framework shows that exact recoverability and strong duality hold with nearly-optimal sample complexity for the two problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,102,,,,,,,,,,,,,,,WOS:000476622000001,0
J,"Bonilla, EV; Krauth, K; Dezfouli, A",,,,"Bonilla, Edwin V.; Krauth, Karl; Dezfouli, Amir",,,Generic Inference in Latent Gaussian Process Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using samples from univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our approach quantitatively and qualitatively with experiments on small datasets, medium-scale datasets and large datasets, showing its competitiveness under different likelihood models and sparsity levels. On the large-scale experiments involving prediction of airline delays and classification of handwritten digits, we show that our method is on par with the state-of-the-art hard-coded approaches for scalable GP regression and classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,117,,,,,,,,,,,,,,,WOS:000487068900001,0
J,"Cotter, A; Jiang, H; Gupta, M; Wang, S; Narayan, T; You, S; Sridharan, K",,,,"Cotter, Andrew; Jiang, Heinrich; Gupta, Maya; Wang, Serena; Narayan, Taman; You, Seungil; Sridharan, Karthik",,,"Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that many machine learning goals can be expressed as rate constraints on a model's predictions. We study the problem of training non-convex models subject to these rate constraints (or other non-convex or non-differentiable constraints). In the non-convex setting, the standard approach of Lagrange multipliers may fail. Furthermore, if the constraints are non-differentiable, then one cannot optimize the Lagrangian with gradient-based methods. To solve these issues, we introduce a new proxy-Lagrangian formulation. This leads to an algorithm that, assuming access to an optimization oracle, produces a stochastic classifier by playing a two-player non-zero-sum game solving for what we call a semi-coarse correlated equilibrium, which in turn corresponds to an approximately optimal and feasible solution to the constrained optimization problem. We then give a procedure that shrinks the randomized solution down to a mixture of at most m + 1 deterministic solutions, given m constraints. This culminates in a procedure that can solve non-convex constrained optimization problems with possibly non-differentiable and non-convex constraints, and enjoys theoretical guarantees. We provide extensive experimental results covering a broad range of policy goals, including various fairness metrics, accuracy, coverage, recall, and churn.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,172,,,,,,,,,,,,,,,WOS:000506403100012,0
J,"Fattahi, S; Sojoudi, S",,,,"Fattahi, Salar; Sojoudi, Somayeh",,,Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphical Lasso (GL) is a popular method for learning the structure of an undirected graphical model, which is based on an l(1) regularization technique. The objective of this paper is to compare the computationally-heavy GL technique with a numerically-cheap heuristic method that is based on simply thresholding the sample covariance matrix. To this end, two notions of sign-consistent and inverse-consistent matrices are developed, and then it is shown that the thresholding and GL methods are equivalent if: (i) the thresholded sample covariance matrix is both sign-consistent and inverse-consistent, and (ii) the gap between the largest thresholded and the smallest un-thresholded entries of the sample covariance matrix is not too small. By building upon this result, it is proved that the GL method-as a conic optimization problem-has an explicit closed-form solution if the thresholded sample covariance matrix has an acyclic structure. This result is then generalized to arbitrary sparse support graphs, where a formula is found to obtain an approximate solution of GL. Furthermore, it is shown that the approximation error of the derived explicit formula decreases exponentially fast with respect to the length of the minimum-length cycle of the sparsity graph. The developed results are demonstrated on synthetic data, functional MRI data, traffic flows for transportation networks, and massive randomly generated data sets. We show that the proposed method can obtain an accurate approximation of the GL for instances with the sizes as large as 80, 000 x 80, 000 (more than 3.2 billion variables) in less than 30 minutes on a standard laptop computer running MATLAB, while other state-of-the-art methods do not converge within 4 hours.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,10,,,,,,,,,,,,,,,WOS:000458664500001,0
J,"Gu, YQ; Xu, GJ",,,,"Gu, Yuqi; Xu, Gongjun",,,Learning Attribute Patterns in High-Dimensional Structured Latent Attribute Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structured latent attribute models (SLAMs) are a special family of discrete latent variable models widely used in social and biological sciences. This paper considers the problem of learning significant attribute patterns from a SLAM with potentially high-dimensional configurations of the latent attributes. We address the theoretical identifiability issue, propose a penalized likelihood method for the selection of the attribute patterns, and further establish the selection consistency in such an overfitted SLAM with a diverging number of latent patterns. The good performance of the proposed methodology is illustrated by simulation studies and two real datasets in educational assessments.",,,,,"Gu, Yuqi/AAB-1252-2021","Gu, Yuqi/0000-0002-4124-113X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,115,,,,,,,,,,,,,,,WOS:000476624900001,0
J,"Kvamme, H; Borgan, O; Scheel, I",,,,"Kvamme, Havard; Borgan, Ornulf; Scheel, Ida",,,Time-to-Event Prediction with Neural Networks and Cox Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"New methods for time-to-event prediction are proposed by extending the Cox proportional hazards model with neural networks. Building on methodology from nested case-control studies, we propose a loss function that scales well to large data sets and enables fitting of both proportional and non-proportional extensions of the Cox model. Through simulation studies, the proposed loss function is verified to be a good approximation for the Cox partial log-likelihood. The proposed methodology is compared to existing methodologies on real-world data sets and is found to be highly competitive, typically yielding the best performance in terms of Brier score and binomial log-likelihood.",,,,,"Scheel, Ida/E-9712-2016","Scheel, Ida/0000-0003-4732-6916; Borgan, Ornulf/0000-0002-4687-1202",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,129,,,,,,,,,,,,,,,WOS:000487068900013,0
J,"Letcher, A; Balduzzi, D; Racaniere, S; Martens, J; Foerster, J; Tuyls, K; Graepel, T",,,,"Letcher, Alistair; Balduzzi, David; Racaniere, Sebastien; Martens, James; Foerster, Jakob; Tuyls, Karl; Graepel, Thore",,,Differentiable Game Mechanics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep learning is built on the foundational guarantee that gradient descent on an objective function converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, that exhibit multiple interacting losses. The behavior of gradient-based methods in games is not well understood - and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new tools to understand and control the dynamics in n-player differentiable games. The key result is to decompose the game Jacobian into two components. The first, symmetric component, is related to potential games, which reduce to gradient descent on an implicit function. The second, antisymmetric component, relates to Hamiltonian games, a new class of games that obey a conservation law akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in differentiable games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs - while at the same time being applicable to, and having guarantees in, much more general cases.",,,,,,"Foerster, Jakob/0000-0001-9688-2498",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,84,,,,,,,,,,,,,,,WOS:000467898200001,0
J,"Li, QX; Tai, C; Weinan, E",,,,"Li, Qianxiao; Tai, Cheng; Weinan, E.",,,Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,40,,,,,,,,,,,,,,,WOS:000463319600001,0
J,"Lyzinski, V; Levin, K; Priebe, CE",,,,"Lyzinski, Vince; Levin, Keith; Priebe, Carey E.",,,On Consistent Vertex Nomination Schemes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a vertex of interest in a network G(1), the vertex nomination problem seeks to fi nd the corresponding vertex of interest (if it exists) in a second network G(2). A vertex nomination scheme produces a list of the vertices in G(2), ranked according to how likely they are judged to be the corresponding vertex of interest in G(2). The vertex nomination problem and related information retrieval tasks have attracted much attention in the machine learning literature, with numerous applications to social and biological networks. However, the current framework has often been con fi ned to a comparatively small class of network models, and the concept of statistically consistent vertex nomination schemes has been only shallowly explored. In this paper, we extend the vertex nomination problem to a very general statistical model of graphs. Further, drawing inspiration from the long- established classi fi cation framework in the pattern recognition literature, we provide de fi nitions for the key notions of Bayes optimality and consistency in our extended vertex nomination framework, including a derivation of the Bayes optimal vertex nomination scheme. In addition, we prove that no universally consistent vertex nomination schemes exist. Illustrative examples are provided throughout.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,2019,,,,,,,,,,,,,,,WOS:000467894700001,0
J,"Maroulas, V; Mike, JL; Oballe, C",,,,"Maroulas, Vasileios; Mike, Joshua L.; Oballe, Christopher",,,Nonparametric Estimation of Probability Density Functions of Random Persistence Diagrams,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Topological data analysis refers to a broad set of techniques that are used to make inferences about the shape of data. A popular topological summary is the persistence diagram. Through the language of random sets, we describe a notion of global probability density function for persistence diagrams that fully characterizes their behavior and in part provides a noise likelihood model. Our approach encapsulates the number of topological features and considers the appearance or disappearance of those near the diagonal in a stable fashion. In particular, the structure of our kernel individually tracks long persistence features, while considering those near the diagonal as a collective unit. The choice to describe short persistence features as a group reduces computation time while simultaneously retaining accuracy. Indeed, we prove that the associated kernel density estimate converges to the true distribution as the number of persistence diagrams increases and the bandwidth shrinks accordingly. We also establish the convergence of the mean absolute deviation estimate, defined according to the bottleneck metric. Lastly, examples of kernel density estimation are presented for typical underlying datasets as well as for virtual electroencephalographic data related to cognition.",,,,,"Maroulas, Vasileios/AAE-5598-2021","Maroulas, Vasileios/0000-0002-3412-6766",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,151,,,,,,,,,,,,,,,WOS:000491132200015,0
J,"Mpoudeu, M; Clarke, B",,,,"Mpoudeu, Merlin; Clarke, Bertrand",,,Model Selection via the VC Dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive an objective function that can be optimized to give an estimator for the Vapnik-Chervonenkis dimension for use in model selection in regression problems. We verify our estimator is consistent. Then, we verify it performs well compared to seven other model selection techniques. We do this for a variety of types of data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,88,,,,,,,,,,,,,,,WOS:000470907300001,0
J,"Ryabko, D",,,,"Ryabko, Daniil",,,On Asymptotic and Finite-Time Optimality of Bayesian Predictors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem is that of sequential probability forecasting for finite-valued time series. The data is generated by an unknown probability distribution over the space of all one-way infinite sequences. Two settings are considered: the realizable and the non-realizable one. Assume first that the probability measure generating the sequence belongs to a given set C (realizable case), but the latter is completely arbitrary (uncountably infinite, without any structure given). It is shown that the minimax asymptotic average loss-which may be positive-is always attainable, and it is attained by a Bayesian predictor whose prior is discrete and concentrated on C. Moreover, the finite-time loss of the Bayesian predictor is also optimal up to an additive log n term (where n is the time step). This upper bound is complemented by a lower bound that goes to infinity but may do so arbitrarily slow. Passing to the non-realizable setting, let the probability measure generating the data be arbitrary, and consider the given set C as a set of experts to compete with. The goal is to minimize the regret with respect to the experts. It is shown that in this setting it is possible that all Bayesian strategies are strictly suboptimal even asymptotically. In other words, a sublinear regret may be attainable but the regret of every Bayesian predictor is linear. A very general recommendation for choosing a model can be made based on these results: it is better to take a model large enough to make sure it includes the process that generates the data, even if it entails positive asymptotic average loss, for otherwise any combination of predictors in the model class may be useless.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,149,,,,,,,,,,,,,,,WOS:000491132200013,0
J,"Wang, DR; Lu, XY; Rinaldo, A",,,,"Wang, Daren; Lu, Xinyang; Rinaldo, Alessandro",,,DBSCAN: Optimal Rates For Density-Based Cluster Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of optimal estimation of the density cluster tree under various smoothness assumptions on the underlying density. Inspired by the seminal work of Chaudhuri et al. (2014), we formulate a new notion of clustering consistency which is better suited to smooth densities, and derive minimax rates for cluster tree estimation under Holder smooth densities of arbitrary degree. We present a computationally efficient, rate optimal cluster tree estimator based on simple extensions of the popular DBSCAN algorithm of Ester et al. (1996). Our procedure relies on kernel density estimators and returns a sequence of nested random geometric graphs whose connected components form a hierarchy of clusters. The resulting optimal rates for cluster tree estimation depend on the degree of smoothness of the underlying density and, interestingly, match the minimax rates for density estimation under the sup-norm loss. Our results complement and extend the analysis of the DBSCAN algorithm in Sriperumbudur and Steinwart (2012). Finally, we consider level set estimation and cluster consistency for densities with jump discontinuities. We demonstrate that the DBSCAN algorithm attains the minimax rate in terms of the jump size and sample size in this setting as well.",,,,,"Wang, Daren/AFQ-9738-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,170,,,,,,,,,,,,,,,WOS:000506403100010,0
J,"Gonen, A; Shalev-Shwartz, S",,,,"Gonen, Alon; Shalev-Shwartz, Shai",,,Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that the average stability notion introduced by Kearns and Ron (1999); Bousquet and Elisseeff (2002) is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We complement the recent bounds of Hardt et al. (2015) on the stability rate of the Stochastic Gradient Descent algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,222,,,,,,,,,,,,,,,WOS:000438189600001,0
J,"Hardt, M; Ma, TY; Recht, B",,,,"Hardt, Moritz; Ma, Tengyu; Recht, Benjamin",,,Gradient Descent Learns Linear Dynamical Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,29,,,,,,,,,,,,,,,WOS:000444401700001,0
J,"Kummerle, C; Sigl, J",,,,"Kuemmerle, Christian; Sigl, Juliane",,,Harmonic Mean Iteratively Reweighted Least Squares for Low-Rank Matrix Recovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new iteratively reweighted least squares (IRLS) algorithm for the recovery of a matrix X is an element of C-d1xd2 of rank r << min(d(1), d(2)) from incomplete linear observations, solving a sequence of low complexity linear problems. The easily implement able algorithm, which we call harmonic mean iteratively reweighted least squares (HM-IRLS), optimizes a non-convex Schatten-p quasi-norm penalization to promote low-rankness and carries three major strengths, in particular for the matrix completion setting. First, we observe a remarkable global convergence behavior of the algorithm's iterates to the low-rank matrix for relevant, interesting cases, for which any other state-of-the-art optimization approach fails the recovery. Secondly, HM-IRLS exhibits an empirical recovery probability close to 1 even for a number of measurements very close to the theoretical lower bound r(d(1)+d(2)-r), i.e., already for significantly fewer linear observations than any other tractable approach in the literature. Thirdly, HM-IRLS exhibits a locally superlinear rate of convergence (of order 2 - p) if the linear observations fulfill a suitable null space property. While for the first two properties we have so far only strong empirical evidence, we prove the third property as our main theoretical result.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,47,,,,,,,,,,,,,,,WOS:000448369300001,0
J,"Needell, D; Saab, R; Woolf, T",,,,"Needell, Deanna; Saab, Rayan; Woolf, Tina",,,Simple Classification Using Binary Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Binary, or one-bit, representations of data arise naturally in many applications, and are appealing in both hardware implementations and algorithm design. In this work, we study the problem of data classification from binary data obtained from the sign pattern of low-dimensional projections and propose a framework with low computation and resource costs. We illustrate the utility of the proposed approach through stylized and realistic numerical experiments, and provide a theoretical analysis for a simple case. We hope that our framework and analysis will serve as a foundation for studying similar types of approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,61,,,,,,,,,,,,,,,WOS:000452044600001,0
J,"Bui, TD; Yan, J; Turner, RE",,,,"Bui, Thang D.; Yan, Josiah; Turner, Richard E.",,,A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian processes (GPs) are flexible distributions over functions that enable highlevel assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are su ffi ciently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that uni fi es a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at ` inference time' rather than at ` modelling time', resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classi fi cation tasks.",,,,,"Bui, Thang/AAZ-5360-2021","Bui, Thang/0000-0002-7878-9748",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000413450200001,0
J,"Durante, D; Mukherjee, N; Steorts, RC",,,,"Durante, Daniele; Mukherjee, Nabanita; Steorts, Rebecca C.",,,Bayesian Learning of Dynamic Multilayer Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction.",,,,,"Durante, Daniele/O-8277-2017","Durante, Daniele/0000-0002-8595-6719",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,29,43,,,,,,,,,,,,,,,WOS:000405962600001,0
J,"Wang, MD; Liu, J; Fang, EX",,,,"Wang, Mengdi; Liu, Ji; Fang, Ethan X.",,,Accelerating Stochastic Composition Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the stochastic nested composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method. This algorithm updates the solution based on noisy gradient queries using a two-timescale iteration. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.",,,,,,"Wang, Mengdi/0000-0002-2101-9507",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,105,,,,,,,,,,,,,,,WOS:000413450400001,0
J,"Biau, G; Bleakley, K; Cadre, B",,,,"Biau, Gerard; Bleakley, Kevin; Cadre, Benoit",,,The Statistical Performance of Collaborative Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical analysis of massive and complex data sets will require the development of algorithms that depend on distributed computing and collaborative inference. Inspired by this, we propose a collaborative framework that aims to estimate the unknown mean theta of a random variable X. In the model we present, a certain number of calculation units, distributed across a communication network represented by a graph, participate in the estimation of theta by sequentially receiving independent data from X while exchanging messages via a stochastic matrix A defined over the graph. We give precise conditions on the matrix A under which the statistical precision of the individual units is comparable to that of a (gold standard) virtual centralized estimate, even though each unit does not have access to all of the data. We show in particular the fundamental role played by both the non-trivial eigenvalues of A and the Ramanujan class of expander graphs, which provide remarkable performance for moderate algorithmic cost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,62,,,,,,,,,,,,,,,WOS:000391522200001,0
J,"Denis, F; Gybels, M; Habrard, A",,,,"Denis, Francois; Gybels, Mattias; Habrard, Amaury",,,Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution p. These methods rely on a singular value decomposition of a matrix H-S, called the empirical Hankel matrix, that records the frequencies of (some of) the observed strings S. The accuracy of the learned distribution depends both on the quantity of information embedded in H-S and on the distance between H-S and its mean H-p. Existing concentration bounds seem to indicate that the concentration over H-p gets looser with its dimensions, suggesting that it might be necessary to bound the dimensions of H-S for learning. We prove new dimension-free concentration bounds for classical Hankel matrices and several variants, based on prefixes or factors of strings, that are useful for learning. Experiments demonstrate that these bounds are tight and that they significantly improve existing (dimension-dependent) bounds. One consequence of these results is that the spectral learning approach remains consistent even if all the observations are recorded within the empirical matrix.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,31,,,,,,,,,,,,,,,WOS:000391479700001,0
J,"Furmston, T; Lever, G; Barber, D",,,,"Furmston, Thomas; Lever, Guy; Barber, David",,,Approximate Newton Methods for Policy Search in Markov Decision Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Approximate Newton methods are standard optimization tools which aim to maintain the bene fits of Newton's method, such as a fast rate of convergence, while alleviating its drawbacks, such as computationally expensive calculation or estimation of the inverse Hessian. In this work we investigate approximate Newton methods for policy optimization in Markov decision processes (MDPs). We first analyse the structure of the Hessian of the total expected reward, which is a standard objective function for MDPs. We show that, like the gradient, the Hessian exhibits useful structure in the context of MDPs and we use this analysis to motivate two Gauss-Newton methods for MDPs. Like the Gauss-Newton method for non-linear least squares, these methods drop certain terms in the Hessian. The approximate Hessians possess desirable properties, such as negative de finiteness, and we demonstrate several important performance guarantees including guaranteed ascent directions, invariance to affine transformation of the parameter space and convergence guarantees. We finally provide a unifying perspective of key policy search algorithms, demonstrating that our second Gauss-Newton algorithm is closely related to both the EM-algorithm and natural gradient ascent applied to MDPs, but performs significantly better in practice on a range of challenging domains.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,227,,,,,,,,,,,,,,,WOS:000391915300001,0
J,"Guo, X; Fan, J; Zhou, DX",,,,"Guo, Xin; Fan, Jun; Zhou, Ding-Xuan",,,Sparsity and Error Analysis of Empirical Feature-Based Regularization Schemes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a learning algorithm generated by a regularization scheme with a concave regularizer for the purpose of achieving sparsity and good learning rates in a least squares regression setting. The regularization is induced for linear combinations of empirical features, constructed in the literatures of kernel principal component analysis and kernel projection machines, based on kernels and samples. In addition to the separability of the involved optimization problem caused by the empirical features, we carry out sparsity and error analysis, giving bounds in the norm of the reproducing kernel Hilbert space, based on a priori conditions which do not require assumptions on sparsity in terms of any basis or system. In particular, we show that as the concave exponent q of the concave regularizer increases to 1, the learning ability of the algorithm improves. Some numerical simulations for both artificial and real MHC-peptide binding data involving the l(q) regularizer and the SCAD penalty are presented to demonstrate the sparsity and error analysis.",,,,,"Fan, Jun/O-4742-2017; Guo, Xin/M-6860-2017","Fan, Jun/0000-0001-8451-3484; Guo, Xin/0000-0002-7465-9356",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,34,89,,,,,,,,,,,,,,,WOS:000391533200001,0
J,"Guo, ZJ; Small, DS",,,,"Guo, Zijian; Small, Dylan S.",,,Control Function Instrumental Variable Estimation of Nonlinear Causal Effect Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The instrumental variable method consistently estimates the effect of a treatment when there is unmeasured confounding and a valid instrumental variable. A valid instrumental variable is a variable that is independent of unmeasured confounders and affects the treatment but does not have a direct effect on the outcome beyond its effect on the treatment. Two commonly used estimators for using an instrumental variable to estimate a treatment effect are the two stage least squares estimator and the control function estimator. For linear causal effect models, these two estimators are equivalent, but for nonlinear causal effect models, the estimators are different. We provide a systematic comparison of these two estimators for nonlinear causal effect models and develop an approach to combing the two estimators that generally performs better than either one alone. We show that the control function estimator is a two stage least squares estimator with an augmented set of instrumental variables. If these augmented instrumental variables are valid, then the control function estimator can be much more efficient than usual two stage least squares without the augmented instrumental variables while if the augmented instrumental variables are not valid, then the control function estimator may be inconsistent while the usual two stage least squares remains consistent. We apply the Hausman test to test whether the augmented instrumental variables are valid and construct a pretest estimator based on this test. The pretest estimator is shown to work well in a simulation study. An application to the effect of exposure to violence on time preference is considered.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,100,,,,,,,,,,,,,,,WOS:000391540900001,0
J,"Maurya, A",,,,"Maurya, Ashwini",,,A Well-Conditioned and Sparse Estimation of Covariance and Inverse Covariance Matrices Using a Joint Penalty,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a method for estimating well-conditioned and sparse covariance and inverse covariance matrices from a sample of vectors drawn from a sub-Gaussian distribution in high dimensional setting. The proposed estimators are obtained by minimizing the quadratic loss function and joint penalty of l(1) norm and variance of its eigenvalues. In contrast to some of the existing methods of covariance and inverse covariance matrix estimation, where often the interest is to estimate a sparse matrix, the proposed method is flexible in estimating both a sparse and well-conditioned covariance matrix simultaneously. The proposed estimators are optimal in the sense that they achieve the mini-max rate of estimation in operator norm for the underlying class of covariance and inverse covariance matrices. We give a very fast algorithm for computation of these covariance and inverse covariance matrices which is easily scalable to large scale data analysis problems. The simulation study for varying sample sizes and variables shows that the proposed estimators performs better than several other estimators for various choices of structured covariance and inverse covariance matrices. We also use our proposed estimator for tumor tissues classification using gene expression data and compare its performance with some other classification methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,130,,,,,,,,,,,,,,,WOS:000391656000001,0
J,"Mokhtari, A; Ribeiro, A",,,,"Mokhtari, Aryan; Ribeiro, Alejandro",,,DSA: Decentralized Double Stochastic Averaging Gradient Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper considers optimization problems where nodes of a network have access to summands of a global objective. Each of these local objectives is further assumed to be an average of a finite set of functions. The motivation for this setup is to solve large scale machine learning problems where elements of the training set are distributed to multiple computational elements. The decentralized double stochastic averaging gradient (DSA) algorithm is proposed as a solution alternative that relies on: (i) The use of local stochastic averaging gradients. (ii) Determination of descent steps as differences of consecutive stochastic averaging gradients. Strong convexity of local functions and Lipschitz continuity of local gradients is shown to guarantee linear convergence of the sequence generated by DSA in expectation. Local iterates are further shown to approach the optimal argument for almost all realizations. The expected linear convergence of DSA is in contrast to the sublinear rate characteristic of existing methods for decentralized stochastic optimization. Numerical experiments on a logistic regression problem illustrate reductions in convergence time and number of feature vectors processed until convergence relative to these other alternatives.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,61,,,,,,,,,,,,,,,WOS:000391522100001,0
J,"Nie, JZ; Kotlowski, W; Warmuth, MK",,,,"Nie, Jiazhong; Kotlowski, Wojciech; Warmuth, Manfred K.",,,Online PCA with Optimal Regret,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the online version of Principle Component Analysis (PCA), where in each trial t the learning algorithm chooses a k-dimensional subspace, and upon receiving the next instance vector x(t), suffers the compression loss, which is the squared Euclidean distance between this instance and its projection into the chosen subspace. When viewed in the right parameterization, this compression loss is linear, i.e. it can be rewritten as tr(W(t)x(t)x(t)(inverted perpendicular)), where W-t is the parameter of the algorithm and the outer product x(t)x(t)(inverted perpendicular) (with parallel to x(t)parallel to <= 1) is the instance matrix. In this paper generalize PCA to arbitrary positive definite instance matrices X-t with the linear loss tr(WtXt). We evaluate online algorithms in terms of their worst-case regret, which is a bound on the additional total loss of the online algorithm on all instances matrices over the compression loss of the best k-dimensional subspace (chosen in hindsight). We focus on two popular online algorithms for generalized PCA: the Gradient Descent (GD) and Matrix Exponentiated Gradient (MEG) algorithms. We show that if the regret is expressed as a function of the number of trials, then both algorithms are optimal to within a constant factor on worst-case sequences of positive definite instances matrices with trace norm at most one (which subsumes the original PCA problem with outer products). This is surprising because MEG is believed be suboptimal in this case. We also show that when considering regret bounds as a function of a loss budget, then MEG remains optimal and strictly outperforms GD when the instance matrices are trace norm bounded. Next, we consider online PCA when the adversary is allowed to present the algorithm with positive semide finite instance matrices whose largest eigenvalue is bounded (rather than their trace which is the sum of their eigenvalues). Again we can show that MEG is optimal and strictly better than GD in this setting.",,,,,"Kotlowski, Wojciech/O-4730-2014",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,49,173,,,,,,,,,,,,,,,WOS:000391678900001,0
J,"Yu, HZ",,,,"Yu, Huizhen",,,Weak Convergence Properties of Constrained Emphatic Temporal-difference Learning with Constant and Slowly Diminishing Stepsize,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the emphatic temporal-difference (TD) algorithm, ETD(lambda), for learning the value functions of stationary policies in a discounted, finite state and action Markov decision process. The ETD(lambda) algorithm was recently proposed by Sutton, Mahmood, and White (2016) to solve a long-standing divergence problem of the standard TD algorithm when it is applied to off-policy training, where data from an exploratory policy are used to evaluate other policies of interest. The almost sure convergence of ETD(lambda) has been proved in our recent work under general off-policy training conditions, but for a narrow range of diminishing stepsize. In this paper we present convergence results for constrained versions of ETD(lambda) with constant stepsize and with diminishing stepsize from a broad range. Our results characterize the asymptotic behavior of the trajectory of iterates produced by those algorithms, and are derived by combining key properties of ETD(lambda) with powerful convergence theorems from the weak convergence methods in stochastic approximation theory. For the case of constant stepsize, in addition to analyzing the behavior of the algorithms in the limit as the stepsize parameter approaches zero, we also analyze their behavior for a fixed stepsize and bound the deviations of their averaged iterates from the desired solution. These results are obtained by exploiting the weak Feller property of the Markov chains associated with the algorithms, and by using ergodic theorems for weak Feller Markov chains, in conjunction with the convergence results we get from the weak convergence methods. Besides ETD(lambda), our analysis also applies to the off-policy TD(lambda) algorithm, when the divergence issue is avoided by setting lambda sufficiently large. It yields, for that case, new results on the asymptotic convergence properties of constrained off-policy TD(lambda) with constant or slowly diminishing stepsize.",,,,,,"Yu, Huizhen/0000-0002-3673-0094",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,220,,,,,,,,,,,,,,,WOS:000391913400001,0
J,"Zhang, J; Sheng, VS; Nicholson, BA; Wu, XD",,,,"Zhang, Jing; Sheng, Victor S.; Nicholson, Bryce A.; Wu, Xindong",,,CEKA: A Tool for Mining the Wisdom of Crowds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"CEKA is a software package for developers and researchers to mine the wisdom of crowds. It makes the entire knowledge discovery procedure much easier, including analyzing qualities of workers, simulating labeling behaviors, inferring true class labels of instances, filtering and correcting mislabeled instances (noise), building learning models and evaluating them. It integrates a set of state-of-the-art inference algorithms, a set of general noise handling algorithms, and abundant functions for model training and evaluation. CEKA is written in Java with core classes being compatible with the well-known machine learning tool WEKA, which makes the utilization of the functions in WEKA much easier.",,,,,"Wu, Xindong/AAB-6713-2022","Wu, Xindong/0000-0003-2396-1704",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2853,2858,,,,,,,,,,,,,,,,WOS:000369888000017,0
J,"Ovcharov, EY",,,,"Ovcharov, Evgeni Y.",,,Existence and Uniqueness of Proper Scoring Rules,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To discuss the existence and uniqueness of proper scoring rules one needs to extend the associated entropy functions as sublinear functions to the conic hull of the prediction set. In some natural function spaces, such as the Lebesgue L-p-spaces over R-d, the positive cones have empty interior. Entropy functions defined on such cones have directional derivatives only, which typically exist on large subspaces and behave similarly to gradients. Certain entropies may be further extended continuously to open cones in normed spaces containing signed densities. The extended entropies are Gateaux differentiable except on a negligible set and have everywhere continuous subgradients due to the supporting hyperplane theorem. We introduce the necessary framework from analysis and algebra that allows us to give an affirmative answer to the titular question of the paper. As a result of this, we give a formal sense in which entropy functions have uniquely associated proper scoring rules. We illustrate our framework by studying the derivatives and subgradients of the following three prototypical entropies: Shannon entropy, Hyvarinen entropy, and quadratic entropy.",,,,,"Ovcharov, Evgeni/ABB-5849-2020",,,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2207,2230,,,,,,,,,,,,,,,,WOS:000369887600003,0
J,"Vapnik, V; Izmailov, R",,,,"Vapnik, Vladimir; Izmailov, Rauf",,,V-Matrix Method of Solving Statistical Inference Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents direct settings and rigorous solutions of the main Statistical Inference problems. It shows that rigorous solutions require solving multidimensional Fredholm integral equations of the first kind in the situation where not only the right-hand side of the equation is an approximation, but the operator in the equation is also de fined approximately. Using Stefanuyk-Vapnik theory for solving such ill-posed operator equations, constructive methods of empirical inference are introduced. These methods are based on a new concept called V-matrix. This matrix captures geometric properties of the observation data that are ignored by classical statistical methods.",,,,,,"Izmailov, Rauf/0000-0002-7326-669X",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1683,1730,,,,,,,,,,,,,,,,WOS:000369887300002,0
J,"Bernstein, A; Shimkin, N",,,,"Bernstein, Andrey; Shimkin, Nahum",,,Response-Based Approachability with Applications to Generalized No-Regret Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Blackwell's theory of approachability provides fundamental results for repeated games with vector-valued payoffs, which have been usefully applied in the theory of learning in games, and in devising online learning algorithms in the adversarial setup. A target set S is approachable by a player (the agent) in such a game if he can ensure that the average payoff vector converges to S, no matter what the opponent does. Blackwell provided two equivalent conditions for a convex set to be approachable. Standard approachability algorithms rely on the primal condition, which is a geometric separation condition, and essentially require to compute at each stage a projection direction from a certain point to S. Here we introduce an approachability algorithm that relies on Blackwell's dual condition, which requires the agent to have a feasible response to each mixed action of the opponent, namely a mixed action such that the expected payoff vector belongs to S. Thus, rather than projections, the proposed algorithm relies on computing the response to a certain action of the opponent at each stage. We demonstrate the utility of the proposed approach by applying it to certain generalizations of the classical regret minimization problem, which incorporate side constraints, reward-to-cost criteria, and so-called global cost functions. In these extensions, computation of the projection is generally complex while the response is readily obtainable.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,747,773,,,,,,,,,,,,,,,,WOS:000369886300004,0
J,"Loh, PL; Wainwright, MJ",,,,"Loh, Po-Ling; Wainwright, Martin J.",,,Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide novel theoretical results regarding local optima of regularized M-estimators, allowing for nonconvexity in both loss and penalty functions. Under restricted strong convexity on the loss and suitable regularity conditions on the penalty, we prove that any stationary point of the composite objective function will lie within statistical precision of the underlying parameter vector. Our theory covers many nonconvex objective functions of interest, including the corrected Lasso for errors-in-variables linear models; regression for generalized linear models with nonconvex penalties such as SCAD, MCP, and capped 1; and high-dimensional graphical model estimation. We quantify statistical accuracy by providing bounds on the l(1)-, l(2)-, and prediction error between stationary points and the population-level optimum. We also propose a simple modification of composite gradient descent that may be used to obtain a near-global optimum within statistical precision epsilon(stat) in log(l/epsilon(stat)) steps, which is the fastest possible rate of any first-order method. We provide simulation studies illustrating the sharpness of our theoretical results.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,559,616,,,,,,,,,,,,,,,,WOS:000369886000009,0
J,"Hansen, TJ; Mahoney, MW",,,,"Hansen, Toke J.; Mahoney, Michael W.",,,Semi-Supervised Eigenvectors for Large-Scale Locally-Biased Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, one has side information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks nearby that prespecified target region. For example, one might be interested in the clustering structure of a data graph near a prespecified seed set of nodes, or one might be interested in finding partitions in an image that are near a prespecified ground truth set of pixels. Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities, thus limiting the applicability of eigenvector-based methods in situations where one is interested in very local properties of the data. In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner. We show that these semi-supervised eigenvectors can be computed quickly as the solution to a system of linear equations; and we also describe several variants of our basic method that have improved scaling properties. We provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning; and we discuss the relationship between our results and recent machine learning algorithms that use global eigenvectors of the graph Laplacian.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3691,3734,,,,,,,,,,,,,,,,WOS:000353126200012,0
J,"Martinez-Cantin, R",,,,"Martinez-Cantin, Ruben",,,"BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization characterized for being sample efficient as it builds a posterior distribution to capture the evidence and prior knowledge of the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.",,,,,"Martinez-Cantin, Ruben/F-8981-2011","Martinez-Cantin, Ruben/0000-0002-6741-844X",,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3735,3739,,,,,,,,,,,,,,,,WOS:000353126200013,0
J,"Shamir, O; Shalev-Shwartz, S",,,,"Shamir, Ohad; Shalev-Shwartz, Shai",,,"Matrix Completion with the Trace Norm: Learning, Bounding, and Transducing",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. However, previous learning guarantees require strong assumptions, such as a uniform distribution over the matrix entries. In this paper, we bridge this gap by providing such guarantees, under much milder assumptions which correspond to matrix completion as performed in practice. In fact, we claim that previous difficulties partially stemmed from a mismatch between the standard learning-theoretic modeling of matrix completion, and its practical application. Our results also shed some light on the issue of matrix completion with bounded models, which enforce predictions to lie within a certain range. In particular, we provide experimental and theoretical evidence that such models lead to a modest yet significant improvement.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3401,3423,,,,,,,,,,,,,,,,WOS:000344638800018,0
J,"Shimizu, S; Bollen, K",,,,"Shimizu, Shohei; Bollen, Kenneth",,,Bayesian Estimation of Causal Direction in Acyclic Structural Equation Models with Individual-specific Confounder Variables and Non-Gaussian Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.",,,,,"Shimizu, Shohei/B-4425-2010","Shimizu, Shohei/0000-0002-1931-0733",,,,,,,,,,,,,1532-4435,,,,,AUG,2014,15,,,,,,2629,2652,,,,,,,,,,,31402848,,,,,WOS:000344638600001,0
J,"Agarwal, S",,,,"Agarwal, Shivani",,,Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. An important consequence is that standard algorithms minimizing a (non-pairwise) strongly proper loss, such as logistic regression and boosting algorithms (assuming a universal function class and appropriate regularization), are in fact consistent for bipartite ranking; moreover, our results allow us to quantify the bipartite ranking regret in terms of the corresponding surrogate regret. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1653,1674,,,,,,,,,,,,,,,,WOS:000344638100002,0
J,"Wang, NY; Meng, F; Chen, L; Madhavan, S; Clarke, R; Hoffman, EP; Xuan, JH; Wang, Y",,,,"Wang, Niya; Meng, Fan; Chen, Li; Madhavan, Subha; Clarke, Robert; Hoffman, Eric P.; Xuan, Jianhua; Wang, Yue",,,The CAM Software for Nonnegative Blind Source Separation in R-Java,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a R-Java CAM (convex analysis of mixtures) package that provides comprehensive analytic functions and a graphic user interface (GUI) for blindly separating mixed nonnegative sources. This open-source multiplatform software implements recent and classic algorithms in the literature including Chan et al. (2008), Wang et al. (2010), Chen et al. (2011a) and Chen et al. (2011b). The CAM package offers several attractive features: (1) instead of using proprietary MATLAB, its analytic functions are written in R, which makes the codes more portable and easier to modify; (2) besides producing and plotting results in R, it also provides a Java GUI for automatic progress update and convenient visual monitoring; (3) multi-thread interactions between the R and Java modules are driven and integrated by a Java GUI, assuring that the whole CAM software runs responsively; (4) the package offers a simple mechanism to allow others to plug-in additional R-functions.",,,,,"Clarke, Robert/A-6485-2008","Clarke, Robert/0000-0002-9278-0854; Hoffman, Eric/0000-0001-6470-5139",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2899,2903,,,,,,,,,,,,,,,,WOS:000327007400013,0
J,"Ruozzi, N; Tatikonda, S",,,,"Ruozzi, Nicholas; Tatikonda, Sekhar",,,Message-Passing Algorithms for Quadratic Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive definite quadratic function. Sufficient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive definite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive definite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive definite whenever the input matrix is positive definite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges.",,,,,,"Ruozzi, Nicholas/0000-0002-4262-2698",,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2287,2314,,,,,,,,,,,,,,,,WOS:000324799600003,0
J,"Wang, C; Blei, DM",,,,"Wang, Chong; Blei, David M.",,,Variational Inference in Nonconjugate Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest-like the correlated topic model and Bayesian logistic regression-are nonconjugate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1005,1031,,,,,,,,,,,,,,,,WOS:000318590500009,0
J,"Lui, YM",,,,"Lui, Yui Man",,,Human Gesture Recognition on Product Manifolds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Action videos are multidimensional data and can be naturally represented as data tensors. While tensor computing is widely used in computer vision, the geometry of tensor space is often ignored. The aim of this paper is to demonstrate the importance of the intrinsic geometry of tensor space which yields a very discriminating structure for action recognition. We characterize data tensors as points on a product manifold and model it statistically using least squares regression. To this aim, we factorize a data tensor relating to each order of the tensor using Higher Order Singular Value Decomposition (HOSVD) and then impose each factorized element on a Grassmann manifold. Furthermore, we account for underlying geometry on manifolds and formulate least squares regression as a composite function. This gives a natural extension from Euclidean space to manifolds. Consequently, classification is performed using geodesic distance on a product manifold where each factor manifold is Grassmannian. Our method exploits appearance and motion without explicitly modeling the shapes and dynamics. We assess the proposed method using three gesture databases, namely the Cambridge hand-gesture, the UMD Keck body-gesture, and the CHALEARN gesture challenge data sets. Experimental results reveal that not only does the proposed method perform well on the standard benchmark data sets, but also it generalizes well on the one-shot-learning gesture challenge. Furthermore, it is based on a simple statistical model and the intrinsic geometry of tensor space.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3297,3321,,,,,,,,,,,,,,,,WOS:000313200200006,0
J,"Hazan, E; Kale, S",,,,"Hazan, Elad; Kale, Satyen",,,Online Submodular Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider an online decision problem over a discrete space in which the loss function is submodular. We give algorithms which are computationally efficient and are Hannan-consistent in both the full information and partial feedback settings.,,,,,,"Hazan, Elad/0000-0002-1566-3216",,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2903,2922,,,,,,,,,,,,,,,,WOS:000313200000003,0
J,"Hernandez-Orallo, J; Flach, P; Ferri, C",,,,"Hernandez-Orallo, Jose; Flach, Peter; Ferri, Cesar",,,A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method.",,,,,"Hernandez-Orallo, Jose/H-9166-2015; Ram√≠rez, Cesar Ferri/H-9181-2015","Hernandez-Orallo, Jose/0000-0001-9746-7632; Ram√≠rez, Cesar Ferri/0000-0002-8975-1120; Flach, Peter/0000-0001-6857-5810",,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2813,2869,,,,,,,,,,,,,,,,WOS:000313200000001,0
J,"Crammer, K; Dredze, M; Pereira, F",,,,"Crammer, Koby; Dredze, Mark; Pereira, Fernando",,,Confidence-Weighted Linear Classification for Text Categorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Confidence-weighted online learning is a generalization of margin-based learning of linear classifiers in which the margin constraint is replaced by a probabilistic constraint based on a distribution over classifier weights that is updated online as examples are observed. The distribution captures a notion of confidence on classifier weights, and in some cases it can also be interpreted as replacing a single learning rate by adaptive per-weight rates. Confidence-weighted learning was motivated by the statistical properties of natural-language classification tasks, where most of the informative features are relatively rare. We investigate several versions of confidence-weighted learning that use a Gaussian distribution over weight vectors, updated at each observed example to achieve high probability of correct classification for the example. Empirical evaluation on a range of text-categorization tasks show that our algorithms improve over other state-of-the-art online and batch methods, learn faster in the online setting, and lead to better classifier combination for a type of distributed training commonly used in cloud computing.",,,,,,"Dredze, Mark/0000-0002-0422-2474",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1891,1926,,,,,,,,,,,,,,,,WOS:000307020700006,0
J,"Minsker, S",,,,"Minsker, Stanislav",,,Plug-in Approach to Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We present a new active learning algorithm based on nonparametric estimators of the regression function. Our investigation provides probabilistic bounds for the rates of convergence of the generalization error achievable by proposed method over a broad class of underlying distributions. We also prove minimax lower bounds which show that the obtained rates are almost tight.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,67,90,,,,,,,,,,,,,,,,WOS:000303045100003,0
J,"Duchi, J; Hazan, E; Singer, Y",,,,"Duchi, John; Hazan, Elad; Singer, Yoram",,,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",,,,,,"Duchi, John/0000-0003-0045-7185; Hazan, Elad/0000-0002-1566-3216",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2121,2159,,,,,,,,,,,,,,,,WOS:000293757900001,0
J,"Cour, T; Sapp, B; Taskar, B",,,,"Cour, Timothee; Sapp, Benjamin; Taskar, Ben",,,Learning from Partial Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of partially-labeled multiclass classification, where instead of a single label per instance, the algorithm is given a candidate set of labels, only one of which is correct. Our setting is motivated by a common scenario in many image and video collections, where only partial access to labels is available. The goal is to learn a classifier that can disambiguate the partially-labeled training instances, and generalize to unseen data. We define an intuitive property of the data distribution that sharply characterizes the ability to learn in this setting and show that effective learning is possible even when all the data is only partially labeled. Exploiting this property of the data, we propose a convex learning formulation based on minimization of a loss function appropriate for the partial label setting. We analyze the conditions under which our loss function is asymptotically consistent, as well as its generalization and transductive performance. We apply our framework to identifying faces culled from web news sources and to naming characters in TV series and movies; in particular, we annotated and experimented on a very large video data set and achieve 6% error for character naming on 16 episodes of the TV series Lost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1501,1536,,,,,,,,,,,,,,,,WOS:000292304000003,0
J,"Griffiths, TL; Ghahramani, Z",,,,"Griffiths, Thomas L.; Ghahramani, Zoubin",,,The Indian Buffet Process: An Introduction and Review,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1185,1224,,,,,,,,,,,,,,,,WOS:000290096100001,0
J,"Huang, JC; Frey, BJ",,,,"Huang, Jim C.; Frey, Brendan J.",,,Cumulative Distribution Networks and the Derivative-sum-product Algorithm: Models and Inference for Cumulative Distribution Functions on Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a class of graphical models for directly representing the joint cumulative distribution function (CDF) of many random variables, called cumulative distribution networks (CDNs). Unlike graphs for probability density and mass functions, for CDFs the marginal probabilities for any subset of variables are obtained by computing limits of functions in the model, and conditional probabilities correspond to computing mixed derivatives. We will show that the conditional independence properties in a CDN are distinct from the conditional independence properties of directed, undirected and factor graphs, but include the conditional independence properties of bi-directed graphs. In order to perform inference in such models, we describe the 'derivative-sum-product' (DSP) message-passing algorithm in which messages correspond to derivatives of the joint CDF. We will then apply CDNs to the problem of learning to rank players in multiplayer team-based games and suggest several future directions for research.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,301,348,,,,,,,,,,,,,,,,WOS:000287938500010,0
J,"Shelton, CR; Fan, Y; Lam, W; Lee, J; Xu, J",,,,"Shelton, Christian R.; Fan, Yu; Lam, William; Lee, Joon; Xu, Jing",,,Continuous Time Bayesian Network Reasoning and Learning Engine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a continuous time Bayesian network reasoning and learning engine (CTBN-RLE). A continuous time Bayesian network (CTBN) provides a compact (factored) description of a continuous-time Markov process. This software provides libraries and programs for most of the algorithms developed for CTBNs. For learning, CTBN-RLE implements structure and parameter learning for both complete and partial data. For inference, it implements exact inference and Gibbs and importance sampling approximate inference for any type of evidence pattern. Additionally, the library supplies visualization methods for graphically displaying CTBNs or trajectories of evidence.",,,,,"Shelton, Christian/GQJ-1146-2022","Shelton, Christian/0000-0001-6698-7838",,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1137,1140,,,,,,,,,,,,,,,,WOS:000277186600004,0
J,"Aliferis, CF; Statnikov, A; Tsamardinos, I; Mani, S; Koutsoukos, XD",,,,"Aliferis, Constantin F.; Statnikov, Alexander; Tsamardinos, Ioannis; Mani, Subramani; Koutsoukos, Xenofon D.",,,Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. Specifically, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for specific domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,235,284,,,,,,,,,,,,,,,,WOS:000277186400008,0
J,"Blanchard, G; Roquain, E",,,,"Blanchard, Gilles; Roquain, Etienne",,,Adaptive False Discovery Rate Control under Independence and Dependence,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the context of multiple hypothesis testing, the proportion p 0 of true null hypotheses in the pool of hypotheses to test often plays a crucial role, although it is generally unknown a priori. A testing procedure using an implicit or explicit estimate of this quantity in order to improve its efficency is called adaptive. In this paper, we focus on the issue of false discovery rate (FDR) control and we present new adaptive multiple testing procedures with control of the FDR. In a first part, assuming independence of the p-values, we present two new procedures and give a unified review of other existing adaptive procedures that have provably controlled FDR. We report extensive simulation results comparing these procedures and testing their robustness when the independence assumption is violated. The new proposed procedures appear competitive with existing ones. The overall best, though, is reported to be Storey's estimator, albeit for a specific parameter setting that does not appear to have been considered before. In a second part, we propose adaptive versions of step-up procedures that have provably controlled FDR under positive dependence and unspecified dependence of the p-values, respectively. In the latter case, while simulations only show an improvement over non-adaptive procedures in limited situations, these are to our knowledge among the first theoretically founded adaptive multiple testing procedures that control the FDR when the p-values are not independent.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2837,2871,,,,,,,,,,,,,,,,WOS:000273877300004,0
J,"Franc, V; Sonnenburg, S",,,,"Franc, Vojtech; Sonnenburg, Soeren",,,Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We have developed an optimized cutting plane algorithm (OCA) for solving large-scale risk minimization problems. We prove that the number of iterations OCA requires to converge to a e precise solution is approximately linear in the sample size. We also derive OCAS, an OCA-based linear binary Support VectorMachine (SVM) solver, and OCAM, a linear multi-class SVM solver. In an extensive empirical evaluation we show that OCAS outperforms current state-of-the-art SVM solvers like SVMlight, SVMperf and BMRM, achieving speedup factor more than 1,200 over SVMlight on some data sets and speedup factor of 29 over SVMperf, while obtaining the same precise support vector solution. OCAS, even in the early optimization steps, often shows faster convergence than the currently prevailing approximative methods in this domain, SGD and Pegasos. In addition, our proposed linear multi-class SVM solver, OCAM, achieves speedups of factor of up to 10 compared to SVMmulti-class. Finally, we use OCAS and OCAM in two real-world applications, the problem of human acceptor splice site detection and malware detection. Effectively parallelizing OCAS, we achieve state-of-the-art results on an acceptor splice site recognition problem only by being able to learn from all the available 50 million examples in a 12-million-dimensional feature space. Source code, data sets and scripts to reproduce the experiments are available at http://cmp.felk.cvut.cz/(similar to)xfrancv/ocas/html/.",,,,,"Sonnenburg, Soeren S/F-2230-2010",,,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2157,2192,,,,,,,,,,,,,,,,WOS:000272346400001,0
J,"Chen, J; Fang, HR; Saad, Y",,,,"Chen, Jie; Fang, Haw-ren; Saad, Yousef",,,Fast Approximate kNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nearest neighbor graphs are widely used in data mining and machine learning. A brute-force method to compute the exact kNN graph takes Theta(dn(2)) time for n data points in the d dimensional Euclidean space. We propose two divide and conquer methods for computing an approximate kNN graph in Theta(dn(t)) time for high dimensional data (large d). The exponent t is an element of (1, 2) is an increasing function of an internal parameter a which governs the size of the common region in the divide step. Experiments show that a high quality graph can usually be obtained with small overlaps, that is, for small values of t. A few of the practical details of the algorithms are as follows. First, the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection. After each conquer step, an additional refinement step is performed to improve the accuracy of the graph. Finally, a hash table is used to avoid repeating distance calculations during the divide and conquer process. The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,1989,2012,,,,,,,,,,,,,,,,WOS:000272346100001,0
J,"Angluin, D; Aspnes, J; Chen, J; Eisenstat, D; Reyzin, L",,,,"Angluin, Dana; Aspnes, James; Chen, Jiang; Eisenstat, David; Reyzin, Lev",,,Learning Acyclic Probabilistic Circuits Using Test Paths,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We define a model of learning probabilistic acyclic circuits using value injection queries, in which fixed values are assigned to an arbitrary subset of the wires and the value on the single output wire is observed. We adapt the approach of using test paths from the Circuit Builder algorithm (Angluin et al., 2009) to show that there is a polynomial time algorithm that uses value injection queries to learn acyclic Boolean probabilistic circuits of constant fan-in and log depth. We establish upper and lower bounds on the attenuation factor for general and transitively reduced Boolean probabilistic circuits of test paths versus general experiments. We give computational evidence that a polynomial time learning algorithm using general value injection experiments may not do much better than one using test paths. For probabilistic circuits with alphabets of size three or greater, we show that the test path lemmas (Angluin et al., 2009, 2008b) fail utterly. To overcome this obstacle, we introduce function injection queries, in which the values on a wire may be mapped to other values rather than just to themselves or constants, and prove a generalized test path lemma for this case.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1881,1911,,,,,,,,,,,,,,,,WOS:000270825200005,0
J,"Corani, G; Zaffalon, M",,,,"Corani, Giorgio; Zaffalon, Marco",,,JNCC2: The Java Implementation Of Naive Credal Classifier 2,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"JNCC2 implements the naive credal classifier 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classifications also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classifications (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license.",,,,,"Zaffalon, Marco/M-7035-2017","Zaffalon, Marco/0000-0001-8908-1502; corani, giorgio/0000-0002-1541-8384",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2695,2698,,,,,,,,,,,,,,,,WOS:000263240700003,0
J,"Balakrishnan, S; Madigan, D",,,,"Balakrishnan, Suhrid; Madigan, David",,,Algorithms for sparse linear classifiers in the massive data setting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classifiers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classifiers, etc., provide competitive methods for classification problems in high dimensions. However, current algorithms for training sparse classifiers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classifiers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,313,337,,,,,,,,,,,,,,,,WOS:000256641800012,0
J,"Neville, J; Jensen, D",,,,"Neville, Jennifer; Jensen, David",,,Relational dependency networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent work on graphical models for relational data has demonstrated significant improvements in classification and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper's references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks ( RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs-namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efficient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efficient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve significant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions.",,,,,,"Jensen, David/0000-0001-5653-3349",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,653,692,,,,,,,,,,,,,,,,WOS:000247002700010,0
J,"Castelo, R; Roverato, A",,,,"Castelo, Robert; Roverato, Alberto",,,A robust procedure for Gaussian graphical model search from microarray data with p larger than n,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning of large-scale networks of interactions from microarray data is an important and challenging problem in bioinformatics. A widely used approach is to assume that the available data constitute a random sample from a multivariate distribution belonging to a Gaussian graphical model. As a consequence, the prime objects of inference are full-order partial correlations which are partial correlations between two variables given the remaining ones. In the context of microarray data the number of variables exceed the sample size and this precludes the application of traditional structure learning procedures because a sampling version of full-order partial correlations does not exist. In this paper we consider limited-order partial correlations, these are partial correlations computed on marginal distributions of manageable size, and provide a set of rules that allow one to assess the usefulness of these quantities to derive the independence structure of the underlying Gaussian graphical model. Furthermore, we introduce a novel structure learning procedure based on a quantity, obtained from limited-order partial correlations, that we call the non-rejection rate. The applicability and usefulness of the procedure are demonstrated by both simulated and real data.",,,,,"Roverato, Alberto/M-1372-2014; Castelo, Robert/A-4679-2010","Castelo, Robert/0000-0003-2229-4508",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2621,2650,,,,,,,,,,,,,,,,WOS:000245390800004,0
J,"Sahbi, H; Geman, D",,,,"Sahbi, Hichem; Geman, Donald",,,A hierarchy of support vector machines for pattern detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a computational design for pattern detection based on a tree-structured network of support vector machines (SVMs). An SVM is associated with each cell in a recursive partitioning of the space of patterns (hypotheses) into increasingly finer subsets. The hierarchy is traversed coarse-to-fine and each chain of positive responses from the root to a leaf constitutes a detection. Our objective is to design and build a network which balances overall error and computation. Initially, SVMs are constructed for each cell with no constraints. This free network is then perturbed, cell by cell, into another network, which is graded in two ways: first, the number of support vectors of each SVM is reduced (by clustering) in order to adjust to a pre-determined, increasing function of cell depth; second, the decision boundaries are shifted to preserve all positive responses from the original set of training data. The limits on the numbers of clusters (virtual support vectors) result from minimizing the mean computational cost of collecting all detections subject to a bound on the expected number of false positives. When applied to detecting faces in cluttered scenes, the patterns correspond to poses and the free network is already faster and more accurate than applying a single pose-specific SVM many times. The graded network promotes very rapid processing of background regions while maintaining the discriminatory power of the free network.",,,,,"Geman, Donald/A-3325-2010",,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2087,2123,,,,,,,,,,,,,,,,WOS:000245390500006,0
J,"Scott, CD; Nowak, RD",,,,"Scott, CD; Nowak, RD",,,Learning minimum volume sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a probability measure P and a reference measure mu, one is often interested in the minimum mu-measure set with P-measure at least a. Minimum volume sets of this type summarize the regions of greatest probability mass of P, and are useful for detecting anomalies and constructing confidence regions. This paper addresses the problem of estimating minimum volume sets based on independent samples distributed according to P. Other than these samples, no other information is available regarding P, but the reference measure mu is assumed to be known. We introduce rules for estimating minimum volume sets that parallel the empirical risk minimization and structural risk minimization principles in classification. As in classification, we show that the performances of our estimators are controlled by the rate of uniform convergence of empirical to true probabilities over the class from which the estimator is drawn. Thus we obtain finite sample size performance bounds in terms of VC dimension and related quantities. We also demonstrate strong universal consistency, an oracle inequality, and rates of convergence. The proposed estimators are illustrated with histogram and decision tree set estimation rules.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2006,7,,,,,,665,704,,,,,,,,,,,,,,,,WOS:000237359100005,0
J,"Silva, R; Scheines, R; Glymour, C; Spirtes, P",,,,"Silva, R; Scheines, R; Glymour, C; Spirtes, P",,,Learning the structure of linear latent variable models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe anytime search procedures that (1) find disjoint subsets of recorded variables for which the members of each subset are d-separated by a single common unrecorded cause, if such exists; (2) return information about the causal relations among the latent factors so identified. We prove the procedure is point-wise consistent assuming (a) the causal relations can be represented by a directed acyclic graph (DAG) satisfying the Markov Assumption and the Faithfulness Assumption; (b) unrecorded variables are not caused by recorded variables; and (c) dependencies are linear. We compare the procedure with standard approaches over a variety of simulated structures and sample sizes, and illustrate its practical value with brief studies of social science data sets. Finally, we consider generalizations for non-linear systems.",,,,,"Spirtes, Peter/GYD-5724-2022",,,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,191,246,,,,,,,,,,,,,,,,WOS:000236331700002,0
J,"Wong, WK; Moore, A; Cooper, G; Wagner, M",,,,"Wong, WK; Moore, A; Cooper, G; Wagner, M",,,What's strange about recent events (WSARE): An algorithm for the early detection of disease outbreaks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Traditional biosurveillance algorithms detect disease outbreaks by looking for peaks in a univariate time series of health-care data. Current health-care surveillance data, however, are no longer simply univariate data streams. Instead, a wealth of spatial, temporal, demographic and symptomatic information is available. We present an early disease outbreak detection algorithm called What's Strange About Recent Events (WSARE), which uses a multivariate approach to improve its timeliness of detection. WSARE employs a rule-based technique that compares recent health-care data against data from a baseline distribution and finds subgroups of the recent data whose proportions have changed the most from the baseline data. In addition, health-care data also pose difficulties for surveillance algorithms because of inherent temporal trends such as seasonal effects and day of week variations. WSARE approaches this problem using a Bayesian network to produce a baseline distribution that accounts for these temporal trends. The algorithm itself incorporates a wide range of ideas, including association rules, Bayesian networks, hypothesis testing and permutation tests to produce a detection algorithm that is careful to evaluate the significance of the alarms that it raises.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,1961,1998,,,,,,,,,,,,,,,,WOS:000236331100004,0
J,"Ong, CS; Smola, AJ; Williamson, RC",,,,"Ong, CS; Smola, AJ; Williamson, RC",,,Learning the kernel with hyperkernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional. We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1043,1071,,,,,,,,,,,,,,,,WOS:000236329900002,0
J,"Fiori, S",,,,"Fiori, S",,,Quasi-geodesic neural learning algorithms over the orthogonal group: A tutorial,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The aim of this contribution is to present a tutorial on learning algorithms for a single neural layer whose connection matrix belongs to the orthogonal group. The algorithms exploit geodesics appropriately connected as piece-wise approximate integrals of the exact differential learning equation. The considered learning equations essentially arise from the Riemannian-gradient-based optimization theory with deterministic and diffusion-type gradient. The paper aims specifically at reviewing the relevant mathematics (and at presenting it in as much transparent way as possible in order to make it accessible to readers that do not possess a background in differential geometry), at bringing together modern optimization methods on manifolds and at comparing the different algorithms on a common machine learning problem. As a numerical case-study, we consider an application to non-negative independent component analysis, although it should be recognized that Riemannian gradient methods give rise to general-purpose algorithms, by no means limited to ICA-related applications.",,,,,"Fiori, Simone/L-5597-2019","Fiori, Simone/0000-0001-5964-7464",,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,743,781,,,,,,,,,,,,,,,,WOS:000236329700002,0
J,"Stracuzzi, DJ; Utgoff, PE",,,,"Stracuzzi, DJ; Utgoff, PE",,,Randomized variable elimination,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variable selection, the process of identifying input variables that are relevant to a particular learning problem, has received much attention in the learning community. Methods that employ a learning algorithm as a part of the selection process ( wrappers) have been shown to outperform methods that select variables independently from the learning algorithm ( filters), but only at great computational expense. We present a randomized wrapper algorithm whose computational requirements are within a constant factor of simply learning in the presence of all input variables, provided that the number of relevant variables is small and known in advance. We then show how to remove the latter assumption, and demonstrate performance on several problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1331,1362,,,,,,,,,,,,,,,,WOS:000236328300005,0
J,"Jebara, T; Kondor, R; Howard, A",,,,"Jebara, T; Kondor, R; Howard, A",,,Probability product kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The advantages of discriminative learning algorithms and kernel machines are combined with generative modeling using a novel kernel between distributions. In the probability product kernel, data points in the input space are mapped to distributions over the sample space and a general inner product is then evaluated as the integral of the product of pairs of distributions. The kernel is straightforward to evaluate for all exponential family models such as multinomials and Gaussians and yields interesting nonlinear kernels. Furthermore, the kernel is computable in closed form for latent distributions such as mixture models, hidden Markov models and linear dynamical systems. For intractable models, such as switching linear dynamical systems, structured mean-field approximations can be brought to bear on the kernel evaluation. For general distributions, even if an analytic expression for the kernel is not feasible, we show a straightforward sampling method to evaluate it. Thus, the kernel permits discriminative learning methods, including support vector machines, to exploit the properties, metrics and invariances of the generative models we infer from each datum. Experiments are shown using multinomial models for text, hidden Markov models for biological data sets and linear dynamical systems for time series data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2004,5,,,,,,819,844,,,,,,,,,,,,,,,,WOS:000236327800004,0
J,"Dybowski, R; Laskey, KB; Myers, JW; Parsons, S",,,,"Dybowski, R; Laskey, KB; Myers, JW; Parsons, S",,,Introduction to the special issue on the fusion of domain knowledge with data for decision support,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,293,294,,10.1162/153244304773633825,0,,,,,,,,,,,,,WOS:000221043900002,0
J,"Gadanho, SC",,,,"Gadanho, SC",,,Learning behavior-selection by emotions and cognition in a multi-goal robot task,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The existence of emotion and cognition as two interacting systems, both with important roles in decision-making, has been recently advocated by neurophysiological research (LeDoux, 1998, Damasio, 1994). Following that idea, this paper presents the ALEC agent architecture which has both emotive and cognitive learning, as well as emotive and cognitive decision-making capabilities to adapt to real-world environments. These two learning mechanisms embody very different properties which can be related to those of natural emotion and cognition systems. The reported experiments test ALEC within a simulated autonomous robot which learns to perform a multi-goal and multi-step survival task when faced with real world conditions, namely continuous time and space, noisy sensors and unreliable actuators. Experimental results show that both systems contribute positively to the learning performance of the agent.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,385,412,,10.1162/153244304773633870,0,,,,,,,,,,,,,WOS:000221043900007,0
J,"Cancedda, N; Gaussier, E; Goutte, C; Renders, JM",,,,"Cancedda, N; Gaussier, E; Goutte, C; Renders, JM",,,Word-sequence kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Machine Learning Methods for Text and Images,2001,"VANCOUVER, CANADA",,,,,"We address the problem of categorising documents using kernel-based methods such as Support Vector Machines. Since the work of Joachims (1998), there is ample experimental evidence that SVM using the standard word frequencies as features yield state-of-the-art performance on a number of benchmark problems. Recently. Lodhi et at. (2002) proposed the use of string kernels, a novel way of computing document similarity based of matching non-consecutive subsequences of characters. In this article, we propose the use of this technique with sequences of words rather than characters. This approach has several advantages, in particular it is more efficient computationally and it ties in closely with standard linguistic pre-processing techniques. We present some extensions to sequence kernels dealing with symbol-dependent and match-dependent decay factors, and present empirical evaluations of these extensions on the Reuters-21578 datasets.",,,,,"Goutte, Cyril/GNH-1837-2022; Goutte, Cyril/A-5824-2009","Goutte, Cyril/0000-0003-4939-6555; Goutte, Cyril/0000-0003-4939-6555",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1059,1082,,10.1162/153244303322533197,0,,,,,,,,,,,,,WOS:000186002400003,0
J,"Szita, I; Takacs, B; Lorincz, A",,,,"Szita, I; Takacs, B; Lorincz, A",,,epsilon-MDPs: Learning in varying environments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper epsilon-MDP-models are introduced and convergence theorems are proven using the generalized MDP framework of Szepesvari and Littman. Using this model family, we show that Q-learning is capable of finding near-optimal policies in varying environments. The potential of this new family of MDP models is illustrated via a reinforcement learning algorithm called event-learning which separates the optimization of decision making from the controller. We show that event-learning augmented by a particular controller, which gives rise to an epsilon-MDP, enables near optimal performance even if considerable and sudden changes may occur in the environment. Illustrations are provided on the two-segment pendulum problem.",,,,,"Lorincz, Andras/H-4125-2012","Lorincz, Andras/0000-0002-1280-3447",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,145,173,,10.1162/153244303768966148,0,,,,,,,,,,,,,WOS:000181462700007,0
J,"Herbster, M; Warmuth, MK",,,,"Herbster, M; Warmuth, MK",,,Tracking the best linear predictor,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In most on-line learning research the total on-line loss of the algorithm is compared to the total loss of the best off-line predictor u from a comparison class of predictors. We call such bounds static bounds. The interesting feature of these bounds is that they hold for an arbitrary sequence of examples. Recently some work has been done where the predictor ut at each trial t is allowed to change with time, and the total on-line loss of the algorithm is compared to the sum of the losses of ut at each trial plus the total cost for shifting to successive predictors. This is to model situations in which the examples change over time, and different predictors from the comparison class are best for different segments of the sequence of examples. We call such bounds shifting bounds. They hold for arbitrary sequences of examples and arbitrary sequences of predictors. Naturally shifting bounds are much harder to prove. The only known bounds are for the case when the comparison class consists of a sequences of experts or boolean disjunctions. In this paper we develop the methodology for lifting known static bounds to the shifting case. In particular we obtain bounds when the comparison class consists of linear neurons (linear combinations of experts). Our essential technique is to project the hypothesis of the static algorithm at the end of each trial into a suitably chosen convex region. This keeps the hypothesis of the algorithm well-behaved and the static bounds can be converted to shifting bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2001,1,4,,,,,281,309,,10.1162/153244301753683726,0,,,,,,,,,,,,,WOS:000173337000002,0
J,"Demirovic, E; Lukina, A; Hebrard, E; Chan, J; Bailey, J; Leckie, C; Ramamohanarao, K; Stuckey, PJ",,,,"Demirovic, Emir; Lukina, Anna; Hebrard, Emmanuel; Chan, Jeffrey; Bailey, James; Leckie, Christopher; Ramamohanarao, Kotagiri; Stuckey, Peter J.",,,MurTree: Optimal Decision Trees via Dynamic Programming and Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy and size. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical use of optimal decision trees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752293400001,0
J,"Ali, M; Berrendorf, M; Hoyt, CT; Vermue, L; Sharifzadeh, S; Tresp, V; Lehmann, J",,,,"Ali, Mehdi; Berrendorf, Max; Hoyt, Charles Tapley; Vermue, Laurent; Sharifzadeh, Sahand; Tresp, Volker; Lehmann, Jens",,,PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, knowledge graph embeddings (KGEs) have received significant attention, and several software libraries have been developed for training and evaluation. While each of them addresses specific needs, we report on a community effort to a re-design and re-implementation of PyKEEN, one of the early KGE libraries. PyKEEN 1.0 enables users to compose knowledge graph embedding models based on a wide range of interaction models, training approaches, loss functions, and permits the explicit modeling of inverse relations. It allows users to measure each component's influence individually on the model's performance. Besides, an automatic memory optimization has been realized in order to optimally exploit the provided hardware. Through the integration of Optuna, extensive hyper-parameter optimization (HPO) functionalities are provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,2021,,,,,,,,,,,,,,,WOS:000656400900001,0
J,"Balasubramanian, K; Li, T; Yuan, M",,,,"Balasubramanian, Krishnakumar; Li, Tong; Yuan, Ming",,,On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The reproducing kernel Hilbert space (RKHS) embedding of distributions offers a general and flexible framework for testing problems in arbitrary domains and has attracted considerable amount of attention in recent years. To gain insights into their operating characteristics, we study here the statistical performance of such approaches within a minimax framework. Focusing on the case of goodness-of-fit tests, our analyses show that a vanilla version of the kernel embedding based test could be minimax suboptimal, when considering chi(2) distance as the separation metric. Hence we suggest a simple remedy by moderating the embedding. We prove that the moderated approach provides optimal tests for a wide range of deviations from the null and can also be made adaptive over a large collection of interpolation spaces. Numerical experiments are presented to further demonstrate the merits of our approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500001,0
J,"Xu, P; Wang, Y; Chen, X; Tian, Z",,,,"Xu, Ping; Wang, Yue; Chen, Xiang; Tian, Zhi",,,COKE: Communication-Censored Decentralized Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function defined over a reproducing kernel Hilbert space by jointly minimizing a global objective function, with access to their own locally observed dataset. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge, we leverage the random feature (RF) approximation approach to enable consensus on the function modeled in the RF space by data-independent parameters across different agents. We then design an iterative algorithm, termed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we further develop a communication-censored kernel learning (COKE) algorithm that reduces the communication load of DKLA by preventing an agent from transmitting at every iteration unless its local updates are deemed informative. Theoretical results in terms of linear convergence guarantee and generalization performance analysis of DKLA and COKE are provided. Comprehensive tests on both synthetic and real datasets are conducted to verify the communication efficiency and learning effectiveness of COKE.(1)",,,,,"Xu, Ping/GLQ-8830-2022","Xu, Ping/0000-0003-4810-7133",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000712030600001,0
J,"Bacciu, D; Errica, F; Micheli, A",,,,"Bacciu, Davide; Errica, Federico; Micheli, Alessio",,,Probabilistic Learning on Graphs via Contextual Architectures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel methodology for representation learning on graph-structured data, in which a stack of Bayesian Networks learns different distributions of a vertex's neighbourhood. Through an incremental construction policy and layer-wise training, we can build deeper architectures with respect to typical graph convolutional neural networks, with benefits in terms of context spreading between vertices. First, the model learns from graphs via maximum likelihood estimation without using target labels. Then, a supervised readout is applied to the learned graph embeddings to deal with graph classification and vertex classification tasks, showing competitive results against neural models for graphs. The computational complexity is linear in the number of edges, facilitating learning on large scale data sets. By studying how depth affects the performances of our model, we discover that a broader context generally improves performances. In turn, this leads to a critical analysis of some benchmarks used in literature.",,,,,"Errica, Federico/AAZ-1840-2021","Errica, Federico/0000-0001-5181-2904; MICHELI, ALESSIO/0000-0001-5764-5238",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,134,,,,,,,,,,,,,,,WOS:000556300100001,0
J,"Bartels, S; Hennig, P",,,,"Bartels, Simon; Hennig, Philipp",,,Conjugate Gradients for Kernel Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularized least-squares (kernel-ridge / Gaussian process) regression is a fundamental algorithm of statistics and machine learning. Because generic algorithms for the exact solution have cubic complexity in the number of datapoints, large datasets require to resort to approximations. In this work, the computation of the least-squares prediction is itself treated as a probabilistic inference problem. We propose a structured Gaussian regression model on the kernel function that uses projections of the kernel matrix to obtain a low-rank approximation of the kernel and the matrix. A central result is an enhanced way to use the method of conjugate gradients for the specific setting of least-squares regression as encountered in machine learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,55,,,,,,,,,,,,,,,WOS:000616873300001,0
J,"Chen, X; Liu, WD; Mao, XJ; Yang, ZY",,,,"Chen, Xi; Liu, Weidong; Mao, Xiaojun; Yang, Zhuoyi",,,Distributed High-dimensional Regression Under a Quantile Loss Function,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies distributed estimation and support recovery for high-dimensional linear regression model with heavy-tailed noise. To deal with heavy-tailed noise whose variance can be infinite, we adopt the quantile regression loss function instead of the commonly used squared loss. However, the non-smooth quantile loss poses new challenges to high-dimensional distributed estimation in both computation and theoretical development. To address the challenge, we transform the response variable and establish a new connection between quantile regression and ordinary linear regression. Then, we provide a distributed estimator that is both computationally and communicationally efficient, where only the gradient information is communicated at each iteration. Theoretically, we show that, after a constant number of iterations, the proposed estimator achieves a near-oracle convergence rate without any restriction on the number of machines. Moreover, we establish the theoretical guarantee for the support recovery. The simulation analysis is provided to demonstrate the effectiveness of our method.",,,,,"Mao, Xiaojun/AAS-9462-2020; Mao, Xiaojun/AEO-8855-2022","Mao, Xiaojun/0000-0002-9362-508X; Liu, Weidong/0000-0002-5449-9180",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,182,,,,,,,,,,,,,,,WOS:000570235500001,0
J,"Eisenach, C; Bunea, F; Ning, Y; Dinicu, C",,,,"Eisenach, Carson; Bunea, Florentina; Ning, Yang; Dinicu, Claudiu",,,High-Dimensional Inference for Cluster-Based Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously, is analytically intractable. Our main contribution is the development and analysis of alternative estimation and inference strategies, for the precision matrix of an unobservable latent vector Z. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used for inference on the graph structure, for instance on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs are proved under the same conditions one would use if the clusters were known in advance. As an illustration of the usage of these newly developed inferential tools, we show that they can be reliably used for recovery of the sparsity pattern of the graphs we study, under FDR control, which is verified via simulation studies and an fMRI data analysis. These experimental results confirm the theoretically established difference between the two graph structures. Furthermore, the data analysis suggests that the latent variable graph, corresponding to the unobserved cluster centers, can help provide more insight into the understanding of the brain connectivity networks relative to the simpler, average-based, graph.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000006,0
J,"Richards, D; Rebeschini, P",,,,"Richards, Dominic; Rebeschini, Patrick",,,Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose graph-dependent implicit regularisation strategies for synchronised distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, single-machine serial statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the single-machine serial SGD setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000009,0
J,"Witte, J; Henckel, L; Maathuis, MH; Didelez, V",,,,"Witte, Janine; Henckel, Leonard; Maathuis, Marloes H.; Didelez, Vanessa",,,On Efficient Adjustment in Causal Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider estimation of a total causal effect from observational data via covariate adjustment. Ideally, adjustment sets are selected based on a given causal graph, reflecting knowledge of the underlying causal structure. Valid adjustment sets are, however, not unique. Recent research has introduced a graphical criterion for an 'optimal' valid adjustment set (O-set). For a given graph, adjustment by the O-set yields the smallest asymptotic variance compared to other adjustment sets in certain parametric and non-parametric models. In this paper, we provide three new results on the O-set. First, we give a novel, more intuitive graphical characterisation: We show that the O-set is the parent set of the outcome node(s) in a suitable latent projection graph, which we call the forbidden projection. An important property is that the forbidden projection preserves all information relevant to total causal effect estimation via covariate adjustment, making it a useful methodological tool in its own right. Second, we extend the existing IDA algorithm to use the O-set, and argue that the algorithm remains semi-local. This is implemented in the R-package pcalg. Third, we present assumptions under which the O-set can be viewed as the target set of popular non-graphical variable selection algorithms such as stepwise backward selection.",,,,,,"Henckel, Leonard/0000-0002-1000-3622",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,246,,,,,,,,,,,,,,,WOS:000608917400001,0
J,"Yu, H; Wu, SW; Xin, LY; Dauwels, J",,,,"Yu, Hang; Wu, Songwei; Xin, Luyin; Dauwels, Justin",,,Fast Bayesian Inference of Sparse Networks with Automatic Sparsity Determination,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structure learning of Gaussian graphical models typically involves careful tuning of penalty parameters, which balance the tradeoff between data fidelity and graph sparsity. Unfortunately, this tuning is often a black art requiring expert experience or brute-force search. It is therefore tempting to develop tuning-free algorithms that can determine the sparsity of the graph adaptively from the observed data in an automatic fashion. In this paper, we propose a novel approach, named BISN (Bayesian inference of Sparse Networks), for automatic Gaussian graphical model selection. Specifically, we regard the off-diagonal entries in the precision matrix as random variables and impose sparse-promoting horseshoe priors on them, resulting in automatic sparsity determination. With the help of stochastic gradients, an efficient variational Bayes algorithm is derived to learn the model. We further propose a decaying recursive stochastic gradient (DRSG) method to reduce the variance of the stochastic gradients and to accelerate the convergence. Our theoretical analysis shows that the time complexity of BISN scales only quadratically with the dimension, whereas the theoretical time complexity of the state-of-the-art methods for automatic graphical model selection is typically a third-order function of the dimension. Furthermore, numerical results show that BISN can achieve comparable or better performance than the state-of-the-art methods in terms of structure recovery, and yet its computational time is several orders of magnitude shorter, especially for large dimensions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,124,,,,,,,,,,,,,,,WOS:000556192100001,0
J,"Ahsen, ME; Vidyasagar, M",,,,"Ahsen, Mehmet Eren; Vidyasagar, Mathukumalli",,,An Approach to One-Bit Compressed Sensing Based on Probably Approximately Correct Learning Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, the problem of one-bit compressed sensing (OBCS) is formulated as a problem in probably approximately correct (PAC) learning. It is shown that the Vapnik-Chervonenkis (VC-) dimension of the set of half-spaces in R-n generated by k-sparse vectors is bounded below by k(left perpendicular lg(n/k) right perpendicular + 1) and above by left perpendicular 2k lg(en) right perpendicular. By coupling this estimate with well-established results in PAC learning theory, we show that a consistent algorithm can recover a k-sparse vector with O(k lg n) measurements, given only the signs of the measurement vector. This result holds for all probability measures on R-n. The theory is also applicable to the case of noisy labels, where the signs of the measurements are flipped with some unknown probability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,11,,,,,,,,,,,,,,,WOS:000458665300001,0
J,"Beimel, A; Nissim, K; Stemmer, U",,,,"Beimel, Amos; Nissim, Kobbi; Stemmer, Uri",,,Characterizing the Sample Complexity of Pure Private Learners,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kasiviswanathan et al. (FOCS 2008) defined private learning as a combination of PAC learning and differential privacy. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. left open the question of characterizing the sample complexity of private learners. We give a combinatorial characterization of the sample size sufficient and necessary to learn a class of concepts under pure differential privacy. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C) = O(m), and that there exists a private learning algorithm with sample complexity m = O(RepDim(C)). We further demonstrate that a similar characterization holds for the database size needed for computing a large class of optimization problems under pure differential privacy, and also for the well studied problem of private data release.",,,,,"BEIMEL, AMOS/F-2033-2012","BEIMEL, AMOS/0000-0002-6572-4195",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,146,,,,,,,,,,,,,,,WOS:000491132200010,0
J,"Bouttier, C; Gavra, I",,,,"Bouttier, Clement; Gavra, Ioana",,,Convergence Rate of a Simulated Annealing Algorithm with Noisy Observations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,4,,,,,,,,,,,,,,,WOS:000458662000001,0
J,"Bravo-Marquez, F; Frank, E; Pfahringer, B; Mohammad, SM",,,,"Bravo-Marquez, Felipe; Frank, Eibe; Pfahringer, Bernhard; Mohammad, Saif M.",,,AffectiveTweets: a Weka Package for Analyzing Affect in Tweets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,AffectiveTweets is a set of programs for analyzing emotion and sentiment of social media messages such as tweets. It is implemented as a package for the Weka machine learning workbench and provides methods for calculating state-of-the-art affect analysis features from tweets that can be fed into machine learning algorithms implemented in Weka. It also implements methods for building affective lexicons and distant supervision methods for training affective models from unlabeled tweets. The package was used by several teams in the shared tasks: Emolnt 2017 and Affect in Tweets SemEval 2018 Task 1.,,,,,"Bravo-Marquez, Felipe/AAG-6488-2019",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,92,,,,,,,,,,,,,,,WOS:000470908000001,0
J,"Fisher, A; Rudin, C; Dominici, F",,,,"Fisher, Aaron; Rudin, Cynthia; Dominici, Francesca",,,"All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f(x) = x(T)beta with a fixed coefficient vector beta) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,177,,,,,,,,,,34335110,,,,,WOS:000506403100017,0
J,"Gao, C; Garber, D; Srebro, N; Wang, JL; Wang, WR",,,,"Gao, Chao; Garber, Dan; Srebro, Nathan; Wang, Jialei; Wang, Weiran",,,Stochastic Canonical Correlation Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the sample complexity of canonical correlation analysis (CCA), i.e., the number of samples needed to estimate the population canonical correlation and directions up to arbitrarily small error. With mild assumptions on the data distribution, we show that in order to achieve epsilon-suboptimality in a properly defined measure of alignment between the estimated canonical directions and the population solution, we can solve the empirical objective exactly with N(epsilon, Delta, gamma) samples, where Delta is the singular value gap of the whitened cross-covariance matrix and 1/gamma is an upper bound of the condition number of auto-covariance matrices. Moreover, we can achieve the same learning accuracy by drawing the same level of samples and solving the empirical objective approximately with a stochastic optimization algorithm; this algorithm is based on the shift-and-invert power iterations and only needs to process the dataset for O (log 1/c) passes. Finally, we show that, given an estimate of the canonical correlation, the streaming version of the shift-and-invert power iterations achieves the same learning accuracy with the same level of sample complexity, by processing the data only once.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,167,,,,,,,,,,,,,,,WOS:000506403100007,0
J,"Shah, NB; Balakrishnan, S; Wainwright, MJ",,,,"Shah, Nihar B.; Balakrishnan, Sivaraman; Wainwright, Martin J.",,,Low Permutation-rank Matrices: Structural Properties and Noisy Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of noisy matrix completion, in which the goal is to reconstruct a structured matrix whose entries are partially observed in noise. Standard approaches to this underdetermined inverse problem are based on assuming that the underlying matrix has low rank, or is well-approximated by a low rank matrix. In this paper, we propose a richer model based on what we term the permutation-rank of a matrix. We first describe how the classical non-negative rank model enforces restrictions that may be undesirable in practice, and how and these restrictions can be avoided by using the richer permutation-rank model. Second, we establish the minimax rates of estimation under the new permutation-based model, and prove that surprisingly, the minimax rates are equivalent up to logarithmic factors to those for estimation under the typical low rank model. Third, we analyze a computationally efficient singular-value-thresholding algorithm, known to be optimal for the low-rank setting, and show that it also simultaneously yields a consistent estimator for the low-permutation rank setting. Finally, we present various structural results characterizing the uniqueness of the permutation-rank decomposition, and characterizing convex approximations of the permutation-rank polytope.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,101,,,,,,,,,,,,,,,WOS:000476621800001,0
J,"Allen-Zhu, Z",,,,"Allen-Zhu, Zeyuan",,,Katyusha: The First Direct Acceleration of Stochastic Gradient Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex and finite-sum. We introduce Katyusha, a direct, primal-only stochastic gradient method to fix this issue. In convex finite-sum stochastic optimization, Katyusha has an optimal accelerated convergence rate, and enjoys an optimal parallel linear speedup in the mini-batch setting. The main ingredient is Katyusha momentum, a novel negative momentum on top of Nesterov's momentum. It can be incorporated into a variance-reduction based algorithm and speed it up, both in terms of sequential and parallel performance. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially try to give Katyusha a hug.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,221,,,,,,,,,,,,,,,WOS:000438189500001,0
J,"Chen, YS; Dwivedi, R; Wainwright, MJ; Yu, B",,,,"Chen, Yuansi; Dwivedi, Raaz; Wainwright, Martin J.; Yu, Bin",,,Fast MCMC Sampling Algorithms on Polytopes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose and analyze two new MCMC sampling algorithms, the Vaidya walk and the John walk, for generating samples from the uniform distribution over a polytope. Both random walks are sampling algorithms derived from interior point methods. The former is based on volumetric-logarithmic barrier introduced by Vaidya whereas the latter uses John's ellipsoids. We show that the Vaidya walk mixes in significantly fewer steps than the logarithmic-barrier based Dikin walk studied in past work. For a polytope in R-d defined by n > d linear constraints, we show that the mixing time from a warm start is bounded as O (n(0.)(5)d(1)(.5)) compared to the O (nd) mixing time bound for the Dikin walk. The cost of each step of the Vaidya walk is of the same order as the Dikin walk, and at most twice as large in terms of constant pre-factors. For the John walk, we prove an O (d(2.5). log(4)(n/d)) bound on its mixing time and conjecture that an improved variant of it could achieve a mixing time of O (d(2).poly-log(n/d)). Additionally, we propose variants of the Vaidya and John walks that mix in polynomial time from a deterministic starting point. The speed-up of the Vaidya walk over the Dikin walk are illustrated in numerical examples.",,,,,"Dwivedi, Raaz/AAZ-2028-2020",,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,86,55,,,,,,,,,,,,,,,WOS:000448378900001,0
J,"Lian, H; Fan, ZY",,,,"Lian, Heng; Fan, Zengyan",,,Divide-and-Conquer for Debiased l(1)-norm Support Vector Machine in Ultra-high Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"1-norm support vector machine (SVM) generally has competitive performance compared to standard 2-norm support vector machine in classification problems, with the advantage of automatically selecting relevant features. We propose a divide-and-conquer approach in the large sample size and high-dimensional setting by splitting the data set across multiple machines, and then averaging the debiased estimators. Extension of existing theoretical studies to SVM is challenging in estimation of the inverse Hessian matrix that requires approximating the Dirac delta function via smoothing. We show that under appropriate conditions the aggregated estimator can obtain the same convergence rate as the central estimator utilizing all observations.",,,,,,"Fan, Zengyan/0000-0003-2948-5331",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,182,,,,,,,,,,,,,,,WOS:000435449000001,0
J,"Rojas-Carulla, M; Scholkopf, B; Turner, R; Peters, J",,,,"Rojas-Carulla, Mateo; Schoelkopf, Bernhard; Turner, Richard; Peters, Jonas",,,Invariant Models for Causal Transfer Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.",,,,,,"Peters, Jonas/0000-0002-1487-7511",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,36,,,,,,,,,,,,,,,WOS:000444668600001,0
J,"Guillame-Bert, M; Dubrawski, A",,,,"Guillame-Bert, Mathieu; Dubrawski, Artur",,,Classification of Time Sequences using Graphs of Temporal Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce two algorithms that learn to classify Symbolic and Scalar Time Sequences (SSTS); an extension of multivariate time series. An SSTS is a set of events and a set of scalars. An event is defined by a symbol and a time-stamp. A scalar is defined by a symbol and a function mapping a number for each possible time stamp of the data. The proposed algorithms rely on temporal patterns called Graph of Temporal Constraints (GTC). A GTC is a directed graph in which vertices express occurrences of specific events, and edges express temporal constraints between occurrences of pairs of events. Additionally, each vertex of a GTC can be augmented with numeric constraints on scalar values. We allow GTCs to be cyclic and/or disconnected. The first of the introduced algorithms extracts sets of co-dependent GTCs to be used in a voting mechanism. The second algorithm builds decision forest like representations where each node is a GTC. In both algorithms, extraction of GTCs and model building are interleaved. Both algorithms are closely related to each other and they exhibit complementary properties including complexity, performance, and interpretability. The main novelties of this work reside in direct building of the model and efficient learning of GTC structures. We explain the proposed algorithms and evaluate their performance against a diverse collection of 59 benchmark data sets. In these experiments, our algorithms come across as highly competitive and in most cases closely match or outperform state-of-the-art alternatives in terms of the computational speed while dominating in terms of the accuracy of classification of time sequences.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,34,,,,,,,,,,,,,,,WOS:000424541200001,0
J,"Sheriff, MR; Chatterjee, D",,,,"Sheriff, Mohammed Rayyan; Chatterjee, Debasish",,,Optimal Dictionary for Least Squares Representation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dictionaries are collections of vectors used for the representation of a class of vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., l(0)-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of a given class of vectors is optimal in an l(2)-sense: optimality of representation is defined as attaining the minimal average l(2)-norm of the coefficients used to represent the vectors in the given class. With the help of recent results on rank-1 decompositions of symmetric positive semidefinite matrices, we provide an explicit description of l(2)-optimal dictionaries as well as their algorithmic constructions in polynomial time.",,,,,"Chatterjee, Debasish/N-2943-2019","Chatterjee, Debasish/0000-0002-1718-653X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,107,,,,,,,,,,,,,,,WOS:000413451000001,0
J,"Wyner, AJ; Olson, M; Bleich, J; Mease, D",,,,"Wyner, Abraham J.; Olson, Matthew; Bleich, Justin; Mease, David",,,Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self averaging, interpolating algorithm which creates what we denote as a spiked-smooth classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000405964900001,0
J,"Abdallah, S; Kaisers, M",,,,"Abdallah, Sherief; Kaisers, Michael",,,Addressing Environment Non-Stationarity by Repeating Q-learning Updates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Q-learning (QL) is a popular reinforcement learning algorithm that is guaranteed to converge to optimal policies in Markov decision processes. However, QL exhibits an artifact: in expectation, the effective rate of updating the value of an action depends on the probability of choosing that action. In other words, there is a tight coupling between the learning dynamics and underlying execution policy. This coupling can cause performance degradation in noisy non-stationary environments. Here, we introduce Repeated Update Q-learning (RUQL), a learning algorithm that resolves the undesirable artifact of Q-learning while maintaining simplicity. We analyze the similarities and differences between RUQL, QL, and the closest state-of-the-art algorithms theoretically. Our analysis shows that RUQL maintains the convergence guarantee of QL in stationary environments, while relaxing the coupling between the execution policy and the learning dynamics. Experimental results confirm the theoretical insights and show how RUQL outperforms both QL and the closest state-of-the-art algorithms in noisy non-stationary environments.",,,,,"Abdallah, Sherief/ABD-3138-2021","Abdallah, Sherief/0000-0002-1213-2014",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,46,,,,,,,,,,,,,,,WOS:000391485300001,0
J,"Chin, WS; Yuan, BW; Yang, MY; Zhuang, Y; Juan, YC; Lin, CJ",,,,"Chin, Wei-Sheng; Yuan, Bo-Wen; Yang, Meng-Yuan; Zhuang, Yong; Juan, Yu-Chin; Lin, Chih-Jen",,,LIBMF: A Library for Parallel Matrix Factorization in Shared-memory Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Matrix factorization (MF) plays a key role in many applications such as recommender systems and computer vision, but MF may take long running time for handling large matrices commonly seen in the big data era. Many parallel techniques have been proposed to reduce the running time, but few parallel MF packages are available. Therefore, we present an open source library, LIBMF, based on recent advances of parallel MF for sharedmemory systems. LIBMF includes easy-to-use command-line tools, interfaces to C/C++ languages, and comprehensive documentation. Our experiments demonstrate that LIBMF outperforms state of the art packages. LIBMF is BSD-licensed, so users can freely use, modify, and redistribute the code.",,,,,"Zhuang, Yong/AAM-1728-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,86,,,,,,,,,,,,,,,WOS:000391528900001,0
J,"Johnson, JK; Oyen, D; Chertkov, M; Netrapalli, P",,,,"Johnson, Jason K.; Oyen, Diane; Chertkov, Michael; Netrapalli, Praneeth",,,Learning Planar Ising Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus on the class of planar Ising models, for which exact inference is tractable using techniques of statistical physics. Based on these techniques and recent methods for planarity testing and planar embedding, we propose a greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in simulations and for two applications: modeling senate voting records and identifying geo-chemical depth trends from Mars rover data.",,,,,"Chertkov, Michael/O-8828-2015","Chertkov, Michael/0000-0002-6758-515X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,215,,,,,,,,,,,,,,,WOS:000391836700001,0
J,"Lin, JH; Rosasco, L; Zhou, DX",,,,"Lin, Junhong; Rosasco, Lorenzo; Zhou, Ding-Xuan",,,Iterative Regularization for Learning with Convex Loss Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of supervised learning with convex loss functions and propose a new form of iterative regularization based on the subgradient method. Unlike other regularization approaches, in iterative regularization no constraint or penalization is considered, and generalization is achieved by (early) stopping an empirical iteration. We consider a nonparametric setting, in the framework of reproducing kernel Hilbert spaces, and prove finite sample bounds on the excess risk under general regularity conditions. Our study provides a new class of efficient regularized learning algorithms and gives insights on the interplay between statistics and optimization in machine learning.",,,,,"Lin, Junhong/M-9045-2016; Zhou, Ding-Xuan/B-3160-2013","Lin, Junhong/0000-0002-4507-9424; Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,77,,,,,,,,,,,,,,,WOS:000391524800001,0
J,"Shah, RD",,,,"Shah, Rajen D.",,,Modelling Interactions in High-dimensional Data with Backtracking,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of high-dimensional regression when there may be interacting variables. Approaches using sparsity-inducing penalty functions such as the Lasso can be useful for producing interpretable models. However, when the number variables runs into the thousands, and so even two-way interactions number in the millions, these methods may become computationally infeasible. Typically variable screening based on model fits using only main effects must be performed first. One problem with screening is that important variables may be missed if they are only useful for prediction when certain interaction terms are also present in the model. To tackle this issue, we introduce a new method we call Backtracking. It can be incorporated into many existing high-dimensional methods based on penalty functions, and works by building increasing sets of candidate interactions iteratively. Models fitted on the main effects and interactions selected early on in this process guide the selection of future interactions. By also making use of previous fits for computation, as well as performing calculations is parallel, the overall run-time of the algorithm can be greatly reduced. The effectiveness of our method when applied to regression and classification problems is demonstrated on simulated and real data sets. In the case of using Backtracking with the Lasso, we also give some theoretical support for our procedure.",,,,,,"Shah, Rajen/0000-0001-9073-3782",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,31,207,,,,,,,,,,,,,,,WOS:000391832500001,0
J,"Yadwadkar, NJ; Hariharan, B; Gonzalez, JE; Katz, R",,,,"Yadwadkar, Neeraja J.; Hariharan, Bharath; Gonzalez, Joseph E.; Katz, Randy",,,Multi-Task Learning for Straggler Avoiding Predictive Job Scheduling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Parallel processing frameworks (Dean and Ghemawat, 2004) accelerate jobs by breaking them into tasks that execute in parallel. However, slow running or straggler tasks can run up to 8 times slower than the median task on a production cluster (Ananthanarayanan et al., 2013), leading to delayed job completion and inefficient use of resources. Existing straggler mitigation techniques wait to detect stragglers and then relaunch them, delaying straggler detection and wasting resources. We built Wrangler (Yadwadkar et al., 2014), a system that predicts when stragglers are going to occur and makes scheduling decisions to avoid such situations. To capture node and workload variability, Wrangler built separate models for every node and workload, requiring the time-consuming collection of substantial training data. In this paper, we propose multi-task learning formulations that share information between the various models, allowing us to use less training data and bring training time down from 4 hours to 40 minutes. Unlike naive multi-task learning formulations, our formulations capture the shared structure in our data, improving generalization performance on limited data. Finally, we extend these formulations using group sparsity inducing norms to automatically discover the similarities between tasks and improve interpretability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,106,,,,,,,,,,,,,,,WOS:000391544200001,0
J,"Lee, W; Liu, YF",,,,"Lee, Wonyul; Liu, Yufeng",,,Joint Estimation of Multiple Precision Matrices with Common Structures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Estimation of inverse covariance matrices, known as precision matrices, is important in various areas of statistical analysis. In this article, we consider estimation of multiple precision matrices sharing some common structures. In this setting, estimating each precision matrix separately can be suboptimal as it ignores potential common structures. This article proposes a new approach to parameterize each precision matrix as a sum of common and unique components and estimate multiple precision matrices in a constrained 11 minimization framework. We establish both estimation and selection consistency of the proposed estimator in the high dimensional setting. The proposed estimator achieves a faster convergence rate for the common structure in certain cases. Our numerical examples demonstrate that our new estimator can perform better than several existing methods in terms of the entropy loss and Frobenius loss. An application to a glioblastoma cancer data set reveals some interesting gene networks across multiple cancer subtypes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2015,16,,,,,,1035,1062,,,,,,,,,,,26568704,,,,,WOS:000369886400003,0
J,"Martins, AFT; Figueiredo, MAT; Aguiar, PMQ; Smith, NA; Xing, EP",,,,"Martins, Andre F. T.; Figueiredo, Mario A. T.; Aguiar, Pedro M. Q.; Smith, Noah A.; Xing, Eric P.",,,AD(3): Alternating Directions Dual Decomposition for MAP Inference in Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present AD(3), a new algorithm for approximate maximum a posteriori (MAP) inference on factor graphs, based on the alternating directions method of multipliers. Like other dual decomposition algorithms, AD(3) has a modular architecture, where local subproblems are solved independently, and their solutions are gathered to compute a global update. The key characteristic of AD(3) is that each local subproblem has a quadratic regularizer, leading to faster convergence, both theoretically and in practice. We provide closed-form solutions for these AD(3) subproblems for binary pairwise factors and factors imposing first-order logic constraints. For arbitrary factors (large or combinatorial), we introduce an active set method which requires only an oracle for computing a local MAP configuration, making AD(3) applicable to a wide range of problems. Experiments on synthetic and real-world problems show that AD(3) compares favorably with the state-of-the-art.",,,,,"Figueiredo, Mario/C-5428-2008","Figueiredo, Mario/0000-0002-0970-7745; Torres Martins, Andre Filipe/0000-0001-8282-625X; Aguiar, Pedro/0000-0002-3809-8416",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,495,545,,,,,,,,,,,,,,,,WOS:000369886000006,0
J,"Chen, X; Lin, QH; Zhou, DY",,,,"Chen, Xi; Lin, Qihang; Zhou, Dengyong",,,Statistical Decision Making for Optimal Budget Allocation in Crowd Labeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level.",,,,,", Gustavo/ABC-1706-2022; Camps-Valls, Gustavo/A-2532-2011","Camps-Valls, Gustavo/0000-0003-1683-2138",,,,,,,,,,,,,1532-4435,,,,,JAN,2015,16,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000369885500001,0
J,"Wipf, D; Zhang, HC",,,,"Wipf, David; Zhang, Haichao",,,Revisiting Bayesian Blind Deconvolution,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that conjoins the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, normalization, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for learning the noise level and choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements and extensions. Moreover, the underlying insights carry over to a wide variety of other bilinear models common in the machine learning literature such as independent component analysis, dictionary learning/sparse coding, and non-negative matrix factorization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3595,3634,,,,,,,,,,,,,,,,WOS:000353126200009,0
J,"Wang, PW; Lin, CJ",,,,"Wang, Po-Wei; Lin, Chih-Jen",,,Iteration Complexity of Feasible Descent Methods for Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many machine learning problems such as the dual form of SVM, the objective function to be minimized is convex but not strongly convex. This fact causes difficulties in obtaining the complexity of some commonly used optimization algorithms. In this paper, we proved the global linear convergence on a wide range of algorithms when they are applied to some non-strongly convex problems. In particular, we are the first to prove O(log(1/is an element of)) time complexity of cyclic coordinate descent methods on dual problems of support vector classification and regression.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1523,1548,,,,,,,,,,,,,,,,WOS:000338420000011,0
J,"Gong, PH; Ye, JP; Zhang, CS",,,,"Gong, Pinghua; Ye, Jieping; Zhang, Changshui",,,Multi-Stage Multi-Task Feature Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an l(0)-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,2979,3010,,,,,,,,,,,24431924,,,,,WOS:000328603600002,0
J,"Lin, BB; He, XF; Zhang, CY; Ji, M",,,,"Lin, Binbin; He, Xiaofei; Zhang, Chiyuan; Ji, Ming",,,Parallel Vector Field Embedding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel local isometry based dimensionality reduction method from the perspective of vector fields, which is called parallel vector field embedding (PFE). We first give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector fields and isometry. The problem of finding an isometry turns out to be equivalent to finding orthonormal parallel vector fields on the data manifold. Therefore, we first find orthonormal parallel vector fields by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient field to be as close to the corresponding parallel vector field as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature.",,,,,"zhang, chi/GRX-3610-2022; liu, bb/GXA-2527-2022",,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,2945,2977,,,,,,,,,,,,,,,,WOS:000328603600001,0
J,"Demsar, J; Curk, T; Erjavec, A; Gorup, C; Hocevar, T; Milutinovic, M; Mozina, M; Polajnar, M; Toplak, M; Staric, A; Stajdohar, M; Umek, L; Zagar, L; Zbontar, J; Zitnik, M; Zupan, B",,,,"Demsar, Janez; Curk, Tomaz; Erjavec, Ales; Gorup, Crt; Hocevar, Tomaz; Milutinovic, Mitar; Mozina, Martin; Polajnar, Matija; Toplak, Marko; Staric, Anze; Stajdohar, Miha; Umek, Lan; Zagar, Lan; Zbontar, Jure; Zitnik, Marinka; Zupan, Blaz",,,Orange: Data Mining Toolbox in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.",,,,,"Hoƒçevar, Toma≈æ/GWM-8593-2022; Curk, Toma≈æ/D-9836-2015; Zupan, Blaz/L-1595-2019; Dem≈°ar, Janez/AAS-2762-2020","Curk, Toma≈æ/0000-0003-4888-7256; Zupan, Blaz/0000-0002-5864-7056; Hocevar, Tomaz/0000-0002-7998-2638",,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2349,2353,,,,,,,,,,,,,,,,WOS:000324799600005,0
J,"Stowell, D; Plumbley, MD",,,,"Stowell, Dan; Plumbley, Mark D.",,,Segregating Event Streams and Noise with a Markov Renewal Process Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a fixed number of sources and/or a fixed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available.",,,,,"Stowell, Dan/AIE-7524-2022; Plumbley, Mark D/A-7298-2008","Stowell, Dan/0000-0001-8068-3769; Plumbley, Mark D/0000-0002-9708-1075",,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2213,2238,,,,,,,,,,,,,,,,WOS:000324799600001,0
J,"Sabato, S; Srebro, N; Tishby, N",,,,"Sabato, Sivan; Srebro, Nathan; Tishby, Naftali",,,Distribution-Dependent Sample Complexity of Large Margin Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L-2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning.",,,,,"Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,2119,2149,,,,,,,,,,,,,,,,WOS:000323367000013,0
J,"Angluin, D; Aspnes, J; Eisenstat, S; Kontorovich, A",,,,"Angluin, Dana; Aspnes, James; Eisenstat, Sarah; Kontorovich, Aryeh",,,On the Learnability of Shuffle Ideals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"PAC learning of unrestricted regular languages is long known to be a difficult problem. The class of shuffle ideals is a very restricted subclass of regular languages, where the shuffle ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shuffle ideals appears quite difficult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP not equal NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efficient algorithm for properly learning shuffle ideals in the statistical query (and therefore also PAC) model under the uniform distribution.",,,,,"KONTOROVICH, ARYEH/F-2375-2012; Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019","Kontorovich, Aryeh/0000-0001-8038-8671; ",,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1513,1531,,,,,,,,,,,,,,,,WOS:000322506400003,0
J,"Parviainen, P; Koivisto, M",,,,"Parviainen, Pekka; Koivisto, Mikko",,,Finding Optimal Bayesian Networks Using Precedence Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of finding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2(n), to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: first, the user may trade space against time; second, the proposed algorithms easily and efficiently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P, which can be significantly less than 2(n). Considering sufficiently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders.",,,,,,"Koivisto, Mikko/0000-0001-9662-3605",,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1387,1415,,,,,,,,,,,,,,,,WOS:000320709300006,0
J,"Bubeck, S; Ernst, D; Garivier, A",,,,"Bubeck, Sebastien; Ernst, Damien; Garivier, Aurelien",,,Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider an original problem that arises from the issue of security analysis of a power system and that we name optimal discovery with probabilistic expert advice. We address it with an algorithm based on the optimistic paradigm and on the Good-Turing missing mass estimator. We prove two different regret bounds on the performance of this algorithm under weak assumptions on the probabilistic experts. Under more restrictive hypotheses, we also prove a macroscopic optimality result, comparing the algorithm both with an oracle strategy and with uniform sampling. Finally, we provide numerical experiments illustrating these theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,601,623,,,,,,,,,,,,,,,,WOS:000315981900009,0
J,"Han, F; Zhao, T; Liu, H",,,,"Han, Fang; Zhao, Tuo; Liu, Han",,,CODA: High Dimensional Copula Discriminant Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a high dimensional classification method, named the Copula Discriminant Analysis (CODA). The CODA generalizes the normal-based linear discriminant analysis to the larger Gaussian Copula models (or the nonparanormal) as proposed by Liu et al. (2009). To simultaneously achieve estimation efficiency and robustness, the nonparametric rank-based methods including the Spearman's rho and Kendall's tau are exploited in estimating the covariance matrix. In high dimensional settings, we prove that the sparsity pattern of the discriminant features can be consistently recovered with the parametric rate, and the expected misclassification error is consistent to the Bayes risk. Our theory is backed up by careful numerical experiments, which show that the extra flexibility gained by the CODA method incurs little efficiency loss even when the data are truly Gaussian. These results suggest that the CODA method can be an alternative choice besides the normal-based high dimensional linear discriminant analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,629,671,,,,,,,,,,,,,,,,WOS:000315981900011,0
J,"Mahoney, MW; Orecchia, L; Vishnoi, NK",,,,"Mahoney, Michael W.; Orecchia, Lorenzo; Vishnoi, Nisheeth K.",,,A Local Spectral Method for Graphs: With Applications to Improving Graph Partitions and Exploring Data Graphs Locally,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The second eigenvalue of the Laplacian matrix and its associated eigenvector are fundamental features of an undirected graph, and as such they have found widespread use in scientific computing, machine learning, and data analysis. In many applications, however, graphs that arise have several local regions of interest, and the second eigenvector will typically fail to provide information fine-tuned to each local region. In this paper, we introduce a locally-biased analogue of the second eigenvector, and we demonstrate its usefulness at highlighting local properties of data graphs in a semi-supervised manner. To do so, we first view the second eigenvector as the solution to a constrained optimization problem, and we incorporate the local information as an additional constraint; we then characterize the optimal solution to this new problem and show that it can be interpreted as a generalization of a Personalized PageRank vector; and finally, as a consequence, we show that the solution can be computed in nearly-linear time. In addition, we show that this locally-biased vector can be used to compute an approximation to the best partition near an input seed set in a manner analogous to the way in which the second eigenvector of the Laplacian can be used to obtain an approximation to the best partition in the entire input graph. Such a primitive is useful for identifying and refining clusters locally, as it allows us to focus on a local region of interest in a semi-supervised manner. Finally, we provide a detailed empirical evaluation of our method by showing how it can applied to finding locally-biased sparse cuts around an input vertex seed set in social and information networks.",,,,,,"Orecchia, Lorenzo/0000-0001-7558-1397",,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2339,2365,,,,,,,,,,,,,,,,WOS:000308795200004,0
J,"Kakade, SM; Shalev-Shwartz, S; Tewari, A",,,,"Kakade, Sham M.; Shalev-Shwartz, Shai; Tewari, Ambuj",,,Regularization Techniques for Learning with Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is growing body of learning problems for which it is natural to organize the parameters into a matrix. As a result, it becomes easy to impose sophisticated prior knowledge by appropriately regularizing the parameters under some matrix norm. This work describes and analyzes a systematic method for constructing such matrix-based regularization techniques. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate. Our methodology is based on a known duality phenomenon: a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and multiple kernel learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1865,1890,,,,,,,,,,,,,,,,WOS:000307020700005,0
J,"May, BC; Korda, N; Lee, A; Leslie, DS",,,,"May, Benedict C.; Korda, Nathan; Lee, Anthony; Leslie, David S.",,,Optimistic Bayesian Sampling in Contextual-Bandit Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout.",,,,,,"Lee, Anthony/0000-0001-7765-0616; Leslie, David Stuart/0000-0001-5253-7676",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,2069,2106,,,,,,,,,,,,,,,,WOS:000307020700013,0
J,"Xue, L; Qu, AN",,,,"Xue, Lan; Qu, Annie",,,Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients. It is important to identify significant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L-0 penalty, and we investigate the global optimality properties of the varying-coefficient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefficient modeling could be infinite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefficient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efficient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches.",,,,,"Qu, Annie/Q-9314-2019",,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1973,1998,,,,,,,,,,,,,,,,WOS:000307020700009,0
J,"Konidaris, G; Scheidwasser, I; Barto, AG",,,,"Konidaris, George; Scheidwasser, Ilya; Barto, Andrew G.",,,Transfer in Reinforcement Learning via Shared Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a framework for transfer in reinforcement learning based on the idea that related tasks share some common features, and that transfer can be achieved via those shared features. The framework attempts to capture the notion of tasks that are related but distinct, and provides some insight into when transfer can be usefully applied to a problem sequence and when it cannot. We apply the framework to the knowledge transfer problem, and show that an agent can learn a portable shaping function from experience in a sequence of tasks to significantly improve performance in a later related task, even given a very brief training period. We also apply the framework to skill transfer, to show that agents can learn portable skills across a sequence of tasks that significantly improve performance on later related tasks, approaching the performance of agents given perfectly learned problem-specific skills.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1333,1371,,,,,,,,,,,,,,,,WOS:000305456600003,0
J,"Liu, J; Wonka, P; Ye, JP",,,,"Liu, Ji; Wonka, Peter; Ye, Jieping",,,A Multi-Stage Framework for Dantzig Selector and LASSO,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the following sparse signal recovery (or feature selection) problem: given a design matrix X is an element of R-nxm (m >> n) and a noisy observation vector y is an element of R-n satisfying y = X beta* + epsilon where e is the noise vector following a Gaussian distribution N(0, sigma I-2), how to recover the signal (or parameter vector) beta* when the signal is sparse? The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal beta*. We show that if X obeys a certain condition, then with a large probability the difference between the solution (beta) over cap estimated by the proposed method and the true solution beta* measured in terms of the l(p) norm (p >= 1) is bounded as parallel to(beta) over cap-beta*parallel to p <= (C(s-N)(1/p) root logm+Delta)sigma, where C is a constant, s is the number of nonzero entries in beta*, the risk of the oracle estimator Delta is independent of m and is much smaller than the first term, and N is the number of entries of beta* larger than a certain value in the order of O(sigma root logm). The proposed method improves the estimation bound of the standard Dantzig selector approximately from Cs-1/p root logm sigma to C(s-N)(1/p) root logm sigma where the value N depends on the number of large entries in beta*. When N = s, the proposed algorithm achieves the oracle solution with a high probability, where the oracle solution is the projection of the observation vector y onto true features. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector. Finally, we extend this multi-stage procedure to the LASSO case.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1189,1219,,,,,,,,,,,,,,,,WOS:000303773100010,0
J,"Benbouzid, D; Busa-Fekete, R; Casagrande, N; Collin, FD; Kegl, B",,,,"Benbouzid, Djalel; Busa-Fekete, Robert; Casagrande, Norman; Collin, Francois-David; Kegl, Balazs",,,MULTIBOOST: A Multi-purpose Boosting Package,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The MULTIBOOST package provides a fast C++ implementation of multi-class/multi-label/multi-task boosting algorithms. It is based on ADABOOST.MH but it also implements popular cascade classifiers and FILTERBOOST. The package contains common multi-class base learners (stumps, trees, products, Haar filters). Further base learners and strong learners following the boosting paradigm can be easily implemented in a flexible framework.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,549,553,,,,,,,,,,,,,,,,WOS:000303772100002,0
J,"Gretton, A; Borgwardt, KM; Rasch, MJ; Scholkopf, B; Smola, A",,,,"Gretton, Arthur; Borgwardt, Karsten M.; Rasch, Malte J.; Schoelkopf, Bernhard; Smola, Alexander",,,A Kernel Two-Sample Test,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Gretton, Arthur/0000-0003-3169-7624; Borgwardt, Karsten/0000-0001-7221-2393",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,723,773,,,,,,,,,,,,,,,,WOS:000303772100009,0
J,"Theis, L; Gerwinn, S; Sinz, F; Bethge, M",,,,"Theis, Lucas; Gerwinn, Sebastian; Sinz, Fabian; Bethge, Matthias",,,"In All Likelihood, Deep Belief Is Not Enough",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Statistical models of natural images provide an important tool for researchers in the fields of machine learning and computational neuroscience. The canonical measure to quantitatively assess and compare the performance of statistical models is given by the likelihood. One class of statistical models which has recently gained increasing popularity and has been applied to a variety of complex data is formed by deep belief networks. Analyses of these models, however, have often been limited to qualitative analyses based on samples due to the computationally intractable nature of their likelihood. Motivated by these circumstances, the present article introduces a consistent estimator for the likelihood of deep belief networks which is computationally tractable and simple to apply in practice. Using this estimator, we quantitatively investigate a deep belief network for natural image patches and compare its performance to the performance of other models for natural image patches. We find that the deep belief network is outperformed with respect to the likelihood even by very simple mixture models.",,,,,"Bethge, Matthias/B-1554-2008; Sinz, Fabian/E-6708-2010","Sinz, Fabian/0000-0002-1348-9736",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3071,3096,,,,,,,,,,,,,,,,WOS:000298103700001,0
J,"Pedregosa, F; Varoquaux, G; Gramfort, A; Michel, V; Thirion, B; Grisel, O; Blondel, M; Prettenhofer, P; Weiss, R; Dubourg, V; Vanderplas, J; Passos, A; Cournapeau, D; Brucher, M; Perrot, M; Duchesnay, E",,,,"Pedregosa, Fabian; Varoquaux, Gaeel; Gramfort, Alexandre; Michel, Vincent; Thirion, Bertrand; Grisel, Olivier; Blondel, Mathieu; Prettenhofer, Peter; Weiss, Ron; Dubourg, Vincent; Vanderplas, Jake; Passos, Alexandre; Cournapeau, David; Brucher, Matthieu; Perrot, Matthieu; Duchesnay, Edouard",,,Scikit-learn: Machine Learning in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",,,,,"DUCHESNAY, EDOUARD/AAS-4046-2020; Louppe, Gilles/D-1923-2017; Pedregosa, Fabian/U-3477-2019","DUCHESNAY, EDOUARD/0000-0002-4073-3490; Louppe, Gilles/0000-0002-2082-3106; Varoquaux, Gael/0000-0003-1076-5122; Gramfort, Alexandre/0000-0001-9791-4404",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2825,2830,,,,,,,,,,,,,,,,WOS:000298103200003,0
J,"Goldwater, S; Griffiths, TL; Johnson, M",,,,"Goldwater, Sharon; Griffiths, Thomas L.; Johnson, Mark",,,Producing Power-Law Distributions and Damping Word Frequencies with Two-Stage Language Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that can generically produce power laws, breaking generative models into two stages. The first stage, the generator, can be any standard probabilistic model, while the second stage, the adaptor, transforms the word frequencies of this model to provide a closer match to natural language. We show that two commonly used Bayesian models, the Dirichlet-multinomial model and the Dirichlet process, can be viewed as special cases of our framework. We discuss two stochastic processes-the Chinese restaurant process and its two-parameter generalization based on the Pitman-Yor process-that can be used as adaptors in our framework to produce power-law distributions over word frequencies. We show that these adaptors justify common estimation procedures based on logarithmic or inverse-power transformations of empirical frequencies. In addition, taking the Pitman-Yor Chinese restaurant process as an adaptor justifies the appearance of type frequencies in formal analyses of natural language and improves the performance of a model for unsupervised learning of morphology.",,,,,,"Johnson, Mark/0000-0003-4809-8441",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2335,2382,,,,,,,,,,,,,,,,WOS:000293757900008,0
J,"Wang, LW",,,,"Wang, Liwei",,,"Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study pool-based active learning in the presence of noise, that is, the agnostic setting. It is known that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space. Although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have an advantage. Previous works have shown that the label complexity of active learning relies on the disagreement coefficient which often characterizes the intrinsic difficulty of the learning problem. In this paper, we study the disagreement coefficient of classification problems for which the classification boundary is smooth and the data distribution has a density that can be bounded by a smooth function. We prove upper and lower bounds for the disagreement coefficients of both finitely and infinitely smooth problems. Combining with existing results, it shows that active learning is superior to passive supervised learning for smooth problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2269,2292,,,,,,,,,,,,,,,,WOS:000293757900005,0
J,"Cseke, B; Heskes, T",,,,"Cseke, Botond; Heskes, Tom",,,Approximate Marginals in Latent Gaussian Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of improving the Gaussian approximate posterior marginals computed by expectation propagation and the Laplace method in latent Gaussian models and propose methods that are similar in spirit to the Laplace approximation of Tierney and Kadane (1986). We show that in the case of sparse Gaussian models, the computational complexity of expectation propagation can be made comparable to that of the Laplace method by using a parallel updating scheme. In some cases, expectation propagation gives excellent estimates where the Laplace approximation fails. Inspired by bounds on the correct marginals, we arrive at factorized approximations, which can be applied on top of both expectation propagation and the Laplace method. The factorized approximations can give nearly indistinguishable results from the non-factorized approximations and their computational complexity scales linearly with the number of variables. We experienced that the expectation propagation based marginal approximations we introduce are typically more accurate than the methods of similar complexity proposed by Rue et al. (2009).",,,,,"Heskes, Tom/A-1443-2010","Heskes, Tom/0000-0002-3398-5235",,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,417,454,,,,,,,,,,,,,,,,WOS:000288896800003,0
J,"Gillenwater, J; Ganchev, K; Graca, J; Pereira, F; Taskar, B",,,,"Gillenwater, Jennifer; Ganchev, Kuzman; Graca, Joao; Pereira, Fernando; Taskar, Ben",,,Posterior Sparsity in Unsupervised Dependency Parsing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A strong inductive bias is essential in unsupervised grammar induction. In this paper, we explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. We use part-of-speech (POS) tags to group dependencies by parent-child types and investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graca et al. (2007). In experiments with 12 different languages, we achieve significant gains in directed attachment accuracy over the standard expectation maximization (EM) baseline, with an average accuracy improvement of 6.5%, outperforming EM by at least 1% for 9 out of 12 languages. Furthermore, the new method outperforms models based on standard Bayesian sparsity-inducing parameter priors with an average improvement of 5% and positive gains of at least 1% for 9 out of 12 languages. On English text in particular, we show that our approach improves performance over other state-of-the-art techniques.",,,,,"Graca, Joao/E-6329-2011",,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,455,490,,,,,,,,,,,,,,,,WOS:000288896800004,0
J,"Bifet, A; Holmes, G; Kirkby, R; Pfahringer, B",,,,"Bifet, Albert; Holmes, Geoff; Kirkby, Richard; Pfahringer, Bernhard",,,MOA: Massive Online Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Massive Online Analysis (MOA) is a software environment for implementing algorithms and running experiments for online learning from evolving data streams. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Naive Bayes classifiers at the leaves. MOA supports bi-directional interaction with WEKA, the Waikato Environment for Knowledge Analysis, and is released under the GNU GPL license.",,,,,"Bifet, Albert/E-4984-2017","Bifet, Albert/0000-0002-8339-7773; Holmes, Geoffrey/0000-0003-0433-8925",,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1601,1604,,,,,,,,,,,,,,,,WOS:000282522000001,0
J,"Ghiasi-Shirazi, K; Safabakhsh, R; Shamsi, M",,,,"Ghiasi-Shirazi, Kamaledin; Safabakhsh, Reza; Shamsi, Mostafa",,,Learning Translation Invariant Kernels for Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Appropriate selection of the kernel function, which implicitly defines the feature space of an algorithm, has a crucial role in the success of kernel methods. In this paper, we consider the problem of optimizing a kernel function over the class of translation invariant kernels for the task of binary classification. The learning capacity of this class is invariant with respect to rotation and scaling of the features and it encompasses the set of radial kernels. We show that how translation invariant kernel functions can be embedded in a nested set of sub-classes and consider the kernel learning problem over one of these sub-classes. This allows the choice of an appropriate sub-class based on the problem at hand. We use the criterion proposed by Lanckriet et al. (2004) to obtain a functional formulation for the problem. It will be proven that the optimal kernel is a finite mixture of cosine functions. The kernel learning problem is then formulated as a semi-infinite programming (SIP) problem which is solved by a sequence of quadratically constrained quadratic programming (QCQP) sub-problems. Using the fact that the cosine kernel is of rank two, we propose a formulation of a QCQP sub-problem which does not require the kernel matrices to be loaded into memory, making the method applicable to large-scale problems. We also address the issue of including other classes of kernels, such as individual kernels and isotropic Gaussian kernels, in the learning process. Another interesting feature of the proposed method is that the optimal classifier has an expansion in terms of the number of cosine kernels, instead of support vectors, leading to a remarkable speedup at run-time. As a by-product, we also generalize the kernel trick to complex-valued kernel functions. Our experiments on artificial and real-world benchmark data sets, including the USPS and the MNIST digit recognition data sets, show the usefulness of the proposed method.",,,,,", Mostafa/H-7646-2019; Ghiasi-Shirazi, Sayed Kamaledin/A-6046-2017; Safabakhsh, Reza/K-9687-2018",", Mostafa/0000-0002-3806-9316; Ghiasi-Shirazi, Sayed Kamaledin/0000-0001-6043-1820; Safabakhsh, Reza/0000-0002-4937-8026",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1353,1390,,,,,,,,,,,,,,,,WOS:000282521500006,0
J,"Rodriguez-Lujan, I; Huerta, R; Elkan, C; Cruz, CS",,,,"Rodriguez-Lujan, Irene; Huerta, Ramon; Elkan, Charles; Santa Cruz, Carlos",,,Quadratic Programming Feature Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Identifying a subset of features that preserves classification accuracy is a problem of growing importance, because of the increasing size and dimensionality of real-world data sets. We propose a new feature selection method, named Quadratic Programming Feature Selection (QPFS), that reduces the task to a quadratic optimization problem. In order to limit the computational complexity of solving the optimization problem, QPFS uses the Nystrom method for approximate matrix diagonalization. QPFS is thus capable of dealing with very large data sets, for which the use of other methods is computationally expensive. In experiments with small and medium data sets, the QPFS method leads to classification accuracy similar to that of other successful techniques. For large data sets, QPFS is superior in terms of computational efficiency.",,,,,"Rodriguez-Lujan, Irene/E-8619-2016; Santa Cruz, Carlos/B-7200-2014; Huerta, Ramon/J-4316-2012; Huerta, Ramon/C-9296-2013","Rodriguez-Lujan, Irene/0000-0001-9512-9162; Santa Cruz, Carlos/0000-0002-3236-1906; Huerta, Ramon/0000-0003-3925-5169; Huerta, Ramon/0000-0003-3925-5169",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1491,1516,,,,,,,,,,,,,,,,WOS:000282521500008,0
J,"Zhang, T",,,,"Zhang, Tong",,,Analysis of Multi-stage Convex Relaxation for Sparse Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider learning formulations with non-convex objective functions that often occur in practical applications. There are two approaches to this problem: Heuristic methods such as gradient descent that only find a local minimum. A drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution. Convex relaxation such as L1-regularization that solves the problem under some conditions. However it often leads to a sub-optimal solution in reality. This paper tries to remedy the above gap between theory and practice. In particular, we present a multi-stage convex relaxation scheme for solving problems with non-convex objective functions. For learning formulations with sparse regularization, we analyze the behavior of a specific multistage relaxation scheme. Under appropriate conditions, we show that the local solution obtained by this procedure is superior to the global solution of the standard L-1 convex relaxation for learning sparse targets.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1081,1107,,,,,,,,,,,,,,,,WOS:000277186600002,0
J,"Goedertier, S; Martens, D; Vanthienen, J; Baesens, B",,,,"Goedertier, Stijn; Martens, David; Vanthienen, Jan; Baesens, Bart",,,Robust Process Discovery with Artificial Negative Events,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and specificity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a configurable technique that deals with these challenges by representing process discovery as a multi-relational classification problem on event logs supplemented with Artificially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias.",,,,,"Martens, David/A-8158-2013; Vanthienen, Jan/L-5375-2019; Vanthienen, Jan/P-7425-2019; Vanthienen, Jan J/A-1668-2018","Vanthienen, Jan/0000-0002-3867-7055; Vanthienen, Jan J/0000-0002-3867-7055",,,,,,,,,,,,,1532-4435,,,,,JUN,2009,10,,,,,,1305,1340,,,,,,,,,,,,,,,,WOS:000270824900004,0
J,"Hofling, H; Tibshirani, R",,,,"Hoefling, Holger; Tibshirani, Robert",,,Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problems of estimating the parameters as well as the structure of binary-valued Markov networks. For maximizing the penalized log-likelihood, we implement an approximate procedure based on the pseudo-likelihood of Besag (1975) and generalize it to a fast exact algorithm. The exact algorithm starts with the pseudo-likelihood solution and then adjusts the pseudo-likelihood criterion so that each additional iterations moves it closer to the exact solution. Our results show that this procedure is faster than the competing exact method proposed by Lee, Ganapathi, and Koller (2006a). However, we also find that the approximate pseudo-likelihood as well as the approaches of Wainwright et al. (2006), when implemented using the coordinate descent procedure of Friedman, Hastie, and Tibshirani (2008b), are much faster than the exact methods, and only slightly less accurate.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,883,906,,,,,,,,,,,21857799,,,,,WOS:000270824600003,0
J,"Perrier, E; Imoto, S; Miyano, S",,,,"Perrier, Eric; Imoto, Seiya; Miyano, Satoru",,,Finding Optimal Bayesian Network Given a Super-Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(gamma(n)(m)), where gamma(m) < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree <(m)over tilde> and sparse structures allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even finds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; significance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to significantly outperform other heuristic searches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2251,2286,,,,,,,,,,,,,,,,WOS:000262637300008,0
J,"Chhabra, M; Jacobs, RA",,,,"Chhabra, Manu; Jacobs, Robert A.",,,Learning to combine motor primitives via greedy additive regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefficients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of local features-each sequence in the library is a solution to a single training task-and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm's dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are specified in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artificial intelligence and cognitive science communities.",,,,,"Jacobs, Robert/B-7670-2013","Jacobs, Robert/0000-0001-6607-908X",,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1535,1558,,,,,,,,,,,,,,,,WOS:000258646800008,0
J,"d'Aspremont, A; Bach, F; El Ghaoui, L",,,,"d'Aspremont, Alexandre; Bach, Francis; El Ghaoui, Laurent",,,Optimal solutions for sparse principal component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n(3)), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O ( n3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1269,1294,,,,,,,,,,,,,,,,WOS:000258646800001,0
J,"Bennett, KP",,,,"Bennett, Kristin P.",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,157,164,,,,,,,,,,,,,,,,WOS:000256641800002,0
J,"Franc, V; Savchynskyy, B",,,,"Franc, Vojtech; Savchynskyy, Bogdan",,,Discriminative learning of max-sum classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The max-sum classifier predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions defined over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classifier. Learning parameters of the max-sum classifier is a challenging problem because even computing the response of such classifier is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classifiers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classifiers, have been extended for learning some subclasses of the max-sum classifier. Besides the max-sum classifiers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulfill some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classifiers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classifiers whose response can be computed in polynomial time: (i) the max-sum classifiers with supermodular quality functions and (ii) the max-sum classifiers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classifier.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2008,9,,,,,,67,104,,,,,,,,,,,,,,,,WOS:000256641400004,0
J,"Hussain, Z; Laviolette, F; Marchand, M; Shawe-Taylor, J; Brubaker, SC; Mullin, MD",,,,"Hussain, Zakria; Laviolette, Francois; Marchand, Mario; Shawe-Taylor, John; Brubaker, Spencer Charles; Mullin, Matthew D.",,,Revised loss bounds for the set covering machine and sample-compression loss bounds for imbalanced data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classifier achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classifier achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassifications.",,,,,,"Marchand, Mario/0000-0002-7078-7393; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,NOV,2007,8,,,,,,2533,2549,,,,,,,,,,,,,,,,WOS:000252744900001,0
J,"Hein, M; Audibert, JY; von Luxburg, U",,,,"Hein, Matthias; Audibert, Jean-Yves; von Luxburg, Ulrike",,,Graph laplacians and their convergence on random neighborhood graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2007,8,,,,,,1325,1368,,,,,,,,,,,,,,,,WOS:000248351800005,0
J,"Tewari, A; Bartlett, PL",,,,"Tewari, Ambuj; Bartlett, Peter L.",,,On the consistency of multiclass classification methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Binary classification is a well studied special case of the classification problem. Statistical properties of binary classifiers, such as consistency, have been investigated in a variety of settings. Binary classification methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classification method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufficient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature.",,,,,,"Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1007,1025,,,,,,,,,,,,,,,,WOS:000248351700004,0
J,"Dasgupta, S; Schulman, L",,,,"Dasgupta, Sanjoy; Schulman, Leonard",,,"A probabilistic analysis of EM for mixtures of separated, spherical Gaussians",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that, given data from a mixture of k well-separated spherical Gaussians in R-d, a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to near-optimal precision, if the dimension is high (d >> lnk). We relate this to previous theoretical and empirical work on the EM algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,203,226,,,,,,,,,,,,,,,,WOS:000247002600001,0
J,"Barber, D",,,,"Barber, David",,,Expectation correction for smoothed inference in switching linear dynamical systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a method for approximate smoothed inference in a class of switching linear dynamical systems, based on a novel form of Gaussian Sum smoother. This class includes the switching Kalman 'Filter' and the more general case of switch transitions dependent on the continuous latent state. The method improves on the standard Kim smoothing approach by dispensing with one of the key approximations, thus making fuller use of the available future information. Whilst the central assumption required is projection to a mixture of Gaussians, we show that an additional conditional independence assumption results in a simpler but accurate alternative. Our method consists of a single Forward and Backward Pass and is reminiscent of the standard smoothing 'correction' recursions in the simpler linear dynamical system. The method is numerically stable and compares favourably against alternative approximations, both in cases where a single mixture component provides a good posterior approximation, and where a multimodal approximation is required.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2515,2540,,,,,,,,,,,,,,,,WOS:000245390700009,0
J,"Jonsson, A; Barto, A",,,,"Jonsson, Anders; Barto, Andrew",,,Causal graph based decomposition of factored MDPs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present Variable Influence Structure Analysis, or VISA, an algorithm that performs hierarchical decomposition of factored Markov decision processes. VISA uses a dynamic Bayesian network model of actions, and constructs a causal graph that captures relationships between state variables. In tasks with sparse causal graphs VISA exploits structure by introducing activities that cause the values of state variables to change. The result is a hierarchy of activities that together represent a solution to the original task. VISA performs state abstraction for each activity by ignoring irrelevant state variables and lower-level activities. In addition, we describe an algorithm for constructing compact models of the activities introduced. State abstraction and compact activity models enable VISA to apply efficient algorithms to solve the stand-alone subtask associated with each activity. Experimental results show that the decomposition introduced by VISA can significantly accelerate construction of an optimal, or near-optimal, policy.",,,,,"Jonsson, Anders/B-2996-2016","Jonsson, Anders/0000-0002-5756-7847",,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2259,2301,,,,,,,,,,,,,,,,WOS:000245390700001,0
J,"Scheinberg, K",,,,"Scheinberg, Katya",,,An efficient implementation of an active set method for SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose an active set algorithm to solve the convex quadratic programming (QP) problem which is the core of the support vector machine (SVM) training. The underlying method is not new and is based on the extensive practice of the Simplex method and its variants for convex quadratic problems. However, its application to large-scale SVM problems is new. Until recently the traditional active set methods were considered impractical for large SVM problems. By adapting the methods to the special structure of SVM problems we were able to produce an efficient implementation. We conduct an extensive study of the behavior of our method and its variations on SVM problems. We present computational results comparing our method with Joachims' SVMlight (see Joachims, 1999). The results show that our method has overall better performance on many SVM problems. It seems to have a particularly strong advantage on more difficult problems. In addition this algorithm has better theoretical properties and it naturally extends to the incremental mode. Since the proposed method solves the standard SVM formulation, as does SVMlight, the generalization properties of these two approaches are identical and we do not discuss them in the paper.",,,,,,", Katya/0000-0003-3547-1841",,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2237,2257,,,,,,,,,,,,,,,,WOS:000245390500011,0
J,"Shivaswamy, PK; Bhattacharyya, C; Smola, AJ",,,,"Shivaswamy, Pannagadatta K.; Bhattacharyya, Chiranjib; Smola, Alexander J.",,,Second order cone programming approaches for handling missing and uncertain data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel second order cone programming formulation for designing robust classifiers which can handle uncertainty in observations. Similar formulations are also derived for designing regression functions which are robust to uncertainties in the regression setting. The proposed formulations are independent of the underlying distribution, requiring only the existence of second order moments. These formulations are then specialized to the case of missing values in observations for both classification and regression problems. Experiments show that the proposed formulations outperform imputation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1283,1314,,,,,,,,,,,,,,,,WOS:000245388800006,0
J,"Mukherjee, S; Zhou, DX",,,,"Mukherjee, S; Zhou, DX",,,Learning coordinate covariances via gradients,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efficient implementation with respect to both memory and time.,,,,,"Zhou, Ding-Xuan/B-3160-2013","Zhou, Ding-Xuan/0000-0003-0224-9216; Mukherjee, Sayan/0000-0002-6715-3920",,,,,,,,,,,,,1532-4435,,,,,MAR,2006,7,,,,,,519,549,,,,,,,,,,,,,,,,WOS:000237359000002,0
J,"Ando, RK; Zhang, T",,,,"Ando, RK; Zhang, T",,,A framework for learning predictive structures from multiple tasks and unlabeled data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2005,6,,,,,,1817,1853,,,,,,,,,,,,,,,,WOS:000236330700003,0
J,"Bordes, A; Ertekin, S; Weston, J; Bottou, L",,,,"Bordes, A; Ertekin, S; Weston, J; Bottou, L",,,Fast kernel classifiers with online and active learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention? This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.",,,,,"Ertekin, Seyda/AAP-5301-2021; Ertekin, Seyda/N-9066-2013",,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1579,1619,,,,,,,,,,,,,,,,WOS:000236330100012,0
J,"Ernst, D; Geurts, P; Wehenkel, L",,,,"Ernst, D; Geurts, P; Wehenkel, L",,,Tree-based batch mode reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system. In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (x(t), u(t), r(t), x(t+1)) where x(t) denotes the system state at time t, u(t) the control action taken, r(t) the instantaneous reward obtained and x(t+1) the successor state of the system, and by determining the control policy from this Q-function. The Q-function approximation may be obtained from the limit of a sequence of (batch mode) supervised learning problems. Within this framework we describe the use of several classical tree-based supervised learning methods (CART, Kd-tree, tree bagging) and two newly proposed ensemble algorithms, namely extremely and totally randomized trees. We study their performances on several examples and find that the ensemble methods based on regression trees perform well in extracting relevant information about the optimal control policy from sets of four-tuples. In particular, the totally randomized trees give good results while ensuring the convergence of the sequence, whereas by relaxing the convergence constraint even better accuracy results are provided by the extremely randomized trees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,503,556,,,,,,,,,,,,,,,,WOS:000236329600006,0
J,"Evgeniou, T; Micchelli, CA; Pontil, M",,,,"Evgeniou, T; Micchelli, CA; Pontil, M",,,Learning multiple tasks with kernel methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,615,637,,,,,,,,,,,,,,,,WOS:000236329600009,0
J,"Cardoso, JF",,,,"Cardoso, JF",,,"Dependence, correlation and gaussianity in independent component analysis",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Independent component analysis (ICA) is the decomposition of a random vector in linear components which are as independent as possible. Here, independence should be understood in its strong statistical sense: it goes beyond (second-order) decorrelation and thus involves the nonGaussianity of the data. The ideal measure of independence is the mutual information and is known to be related to the entropy of the components when the search for components is restricted to uncorrelated components. This paper explores the connections between mutual information, entropy and non-Gaussianity in a larger framework, without resorting to a somewhat arbitrary decorrelation constraint. A key result is that the mutual information can be decomposed, under linear transforms, as the sum of two terms: one term expressing the decorrelation of the components and one expressing their non-Gaussianity. Our results extend the previous understanding of these connections and explain them in the light of information geometry. We also describe the local geometry of ICA by re-expressing all our results via a Gram-Charlier expansion by which all quantities of interest are obtained in terms of cumulants.",,,,,"cardoso, jean francois/L-4252-2018","cardoso, jean francois/0000-0002-3388-6608",,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1177,1203,,,,,,,,,,,,,,,,WOS:000224808300002,0
J,"De Vito, E; Rosasco, L; Caponnetto, A; Piana, M; Verri, A",,,,"De Vito, E; Rosasco, L; Caponnetto, A; Piana, M; Verri, A",,,Some properties of regularized Kernel methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In regularized kernel methods, the solution of a learning problem is found by minimizing functionals consisting of the sum of a data and a complexity term. In this paper we investigate some properties of a more general form of the above functionals in which the data term corresponds to the expected risk. First, we prove a quantitative version of the representer theorem holding for both regression and classification, for both differentiable and non-differentiable loss functions, and for arbitrary offset terms. Second, we show that the case in which the offset space is non trivial corresponds to solving a standard problem of regularization in a Reproducing Kernel Hilbert Space in which the penalty term is given by a seminorm. Finally, we discuss the issues of existence and uniqueness of the solution. From the specialization of our analysis to the discrete setting it is immediate to establish a connection between the solution properties of sparsity and coefficient boundedness and some properties of the loss function. For the case of Support Vector Machines for classification, we also obtain a complete characterization of the whole method in terms of the Khun-Tucker conditions with no need to introduce the dual formulation.",,,,,"De Vito, Ernesto/K-6354-2015; De Vito, Ernesto/AAX-5125-2021; piana, michele/H-9376-2015","De Vito, Ernesto/0000-0002-4320-3292; De Vito, Ernesto/0000-0002-4320-3292; PIANA, MICHELE/0000-0003-1700-991X; CAPONNETTO, Andrea/0000-0002-6311-0667",,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1363,1390,,,,,,,,,,,,,,,,WOS:000236328300006,0
J,"Lagoudakis, MG; Parr, R",,,,"Lagoudakis, MG; Parr, R",,,Least-squares policy iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new approach to reinforcement learning for control problems which combines value-function approximation with linear architectures and approximate policy iteration. This new approach is motivated by the least-squares temporal-difference learning algorithm (LSTD) for prediction problems, which is known for its efficient use of sample experiences compared to pure temporal-difference algorithms. Heretofore, LSTD has not had a straightforward application to control problems mainly because LSTD learns the state value function of a fixed policy which cannot be used for action selection and control without a model of the underlying process. Our new algorithm, least-squares policy iteration (LSPI), learns the state-action value function which allows for action selection without a model and for incremental policy improvement within a policy-iteration framework. LSPI is a model-free, off-policy method which can use efficiently (and reuse in each iteration) sample experiences collected in any manner. By separating the sample collection method, the choice of the linear approximation architecture, and the solution method, LSPI allows for focused attention on the distinct elements that contribute to practical reinforcement learning. LSPI is tested on the simple task of balancing an inverted pendulum and the harder task of balancing and riding a bicycle to a target location. In both cases, LSPI learns to control the pendulum or the bicycle by merely observing a relatively small number of trials where actions are selected randomly. LSPI is also compared against Q-learning (both with and without experience replay) using the same value function architecture. While LSPI achieves good performance fairly consistently on the difficult bicycle task, Q-learning variants were rarely able to balance for more than a small fraction of the time needed to reach the target location.",,,,,"Lagoudakis, Michail/C-5145-2008",,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,1107,1149,,10.1162/1532443041827907,0,,,,,,,,,,,,,WOS:000231002600007,0
J,"Kauchak, D; Smarr, J; Elkan, C",,,,"Kauchak, D; Smarr, J; Elkan, C",,,Sources of success for Boosted Wrapper Induction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we examine an important recent rule-based information extraction (IE) technique named Boosted Wrapper Induction (BWI) by conducting experiments on a wider variety of tasks than previously studied, including tasks using several collections of natural text documents. We investigate systematically how each algorithmic component of BWI, in particular boosting, contributes to its success. We show that the benefit of boosting arises from the ability to reweight examples to learn specific rules (resulting in high precision) combined with the ability to continue learning rules after all positive examples have been covered (resulting in high recall). As a quantitative indicator of the regularity of an extraction task, we propose a new measure that we call the SWI ratio. We show that this measure is a good predictor of IE success and a useful tool for analyzing IE tasks. Based on these results, we analyze the strengths and limitations of BWI. Specifically, we explain limitations in the information made available, and in the representations used. We also investigate the consequences of the fact that confidence values returned during extraction are not true probabilities. Next, we investigate the benefits of including grammatical and semantic information for natural text documents, as well as parse tree and attribute-value information for XML and HTML documents. We show experimentally that incorporating even limited grammatical information can increase the regularity of natural text extraction tasks, resulting in improved performance. We conclude with proposals for enriching the representational power of BWI and other IE methods to exploit these and other types of regularities.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2004,5,,,,,,499,527,,,,,,,,,,,,,,,,WOS:000236327500003,0
J,"Langford, J; McAllester, D",,,,"Langford, J; McAllester, D",,,Computable shell decomposition bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Haussler, Kearns, Seung and Tishby introduced the notion of a shell decomposition of the union bound as a means of understanding certain empirical phenomena in learning curves such as phase transitions. Here we use a variant of their ideas to derive an upper bound on the generalization error of a hypothesis computable from its training error and the histogram of training errors for the hypotheses in the class. In most cases this new bound is significantly tighter than traditional bounds computed from the training error and the cardinality of the class. Our results can also be viewed as providing a rigorous foundation for a model selection algorithm proposed by Scheffer and Joachims.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2004,5,,,,,,529,547,,,,,,,,,,,,,,,,WOS:000236327500004,0
J,"Bakker, B; Heskes, T",,,,"Bakker, B; Heskes, T",,,Task clustering and gating for Bayesian multitask learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modeling a collection of similar regression or classification tasks can be improved by making the tasks 'learn from each other'. In machine learning, this subject is approached through 'multitask learning', where parallel tasks are modeled as multiple outputs of the same network. In multilevel analysis this is generally implemented through the mixed-effects linear model where a distinction is made between 'fixed effects', which are the same for all tasks, and 'random effects', which may vary between tasks. In the present article we will adopt a Bayesian approach in which some of the model parameters are shared (the same for all tasks) and others more loosely connected through a joint prior distribution that can be learned from the data. We seek in this way to combine the best parts of both the statistical multilevel approach and the neural network machinery. The standard assumption expressed in both approaches is that each task can learn equally well from any other task. In this article we extend the model by allowing more differentiation in the similarities between tasks. One such extension is to make the prior mean depend on higher-level task characteristics. More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians. This can be further generalized to a mixture of experts architecture with the gates depending on task characteristics. All three extensions are demonstrated through application both on an artificial data set and on two real-world problems, one a school problem and the other involving single-copy newspaper sales.",,,,,"Heskes, Tom/A-1443-2010","Heskes, Tom/0000-0002-3398-5235",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,83,99,,10.1162/153244304322765658,0,,,,,,,,,,,,,WOS:000221043500005,0
J,"Nair, PB; Choudhury, A; Keane, AJ",,,,"Nair, PB; Choudhury, A; Keane, AJ",,,Some greedy learning algorithms for sparse regression and classification with Mercer kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"We present some greedy learning algorithms for building sparse nonlinear regression and classification models from observational data using Mercer kernels. Our objective is to develop efficient numerical schemes for reducing the training and runtime complexities of kernel-based algorithms applied to large datasets. In the spirit of Natarajan's greedy algorithm (Natarajan, 1995), we iteratively minimize the L-2 loss function subject to a specified constraint on the degree of sparsity required of the final model until a specified stopping criterion is reached. We discuss various greedy criteria for basis selection and numerical schemes for improving the robustness and computational efficiency. Subsequently, algorithms based on residual minimization and thin QR factorization are presented for constructing sparse regression and classification models. During the course of the incremental model construction, the algorithms are terminated using model selection principles such as the minimum descriptive length (MDL) and Akaike's information criterion (AIC). Finally, experimental results on benchmark data are presented to demonstrate the competitiveness of the algorithms developed in this paper.",,,,,"Nair, Prasanth/C-5820-2015","Keane, Andy/0000-0001-7993-1569",,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,781,801,,10.1162/jmlr.2003.3.4-5.781,0,,,,,,,,,,,,,WOS:000184926200008,0
J,"Brafman, RI; Tennenholtz, M",,,,"Brafman, RI; Tennenholtz, M",,,R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E-3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the optimism under uncertainty bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Termenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.",,,,,,"Brafman, Ronen/0000-0001-8227-5646",,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,213,231,,10.1162/153244303765208377,0,,,,,,,,,,,,,WOS:000182488500002,0
J,"Gers, FA; Schraudolph, NN; Schmidhuber, J",,,,"Gers, FA; Schraudolph, NN; Schmidhuber, J",,,Learning precise timing with LSTM recurrent networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by peephole connections from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,115,143,,10.1162/153244303768966139,0,,,,,,,,,,,,,WOS:000181462700006,0
J,"Hammerton, J; Osborne, M; Armstrong, S; Daelemans, W",,,,"Hammerton, J; Osborne, M; Armstrong, S; Daelemans, W",,,Introduction to special issue on machine learning approaches to shallow parsing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article introduces the problem of partial or shallow parsing (assigning partial syntactic structure to sentences) and explains why it is an important natural language processing (NLP) task. The complexity of the task makes Machine Learning an attractive option in comparison to the handcrafting of rules. On the other hand, because of the same task complexity, shallow parsing makes an excellent benchmark problem for evaluating machine learning algorithms. We sketch the origins of shallow parsing as a specific task for machine learning of language, and introduce the articles accepted for this special issue, a representative sample of current research in this area. Finally, future directions for machine learning of shallow parsing are suggested.",,,,,"Daelemans, Walter/N-5785-2014","Daelemans, Walter/0000-0002-9832-7890",,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,551,558,,10.1162/153244302320884533,0,,,,,,,,,,,,,WOS:000179542800001,0
J,"Molina, A; Pla, F",,,,"Molina, A; Pla, F",,,Shallow parsing using specialized HMMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a unified technique to solve different shallow parsing tasks as a tagging problem using a Hidden Markov Model-based approach (HMM). This technique consists of the incorporation of the relevant information for each task into the models. To do this, the training corpus is transformed to take into account this information. In this way, no change is necessary for either the training or tagging process, so it allows for the use of a standard HMM approach. Taking into account this information, we-construct a Specialized HMM which gives more complete contextual models. We have tested our system on chunking and clause identification tasks using different specialization criteria. The results obtained are in line with the results reported for most of the relevant state-of-the-art approaches.",,,,,"Pla, Ferran/K-5468-2014","Pla, Ferran/0000-0003-4822-8808",,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,595,613,,10.1162/153244302320884551,0,,,,,,,,,,,,,WOS:000179542800003,0
J,"Rimella, L; Whiteley, N",,,,"Rimella, Lorenzo; Whiteley, Nick",,,Exploiting locality in high-dimensional Factorial hidden Markov models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose algorithms for approximate filtering and smoothing in high-dimensional Factorial hidden Markov models. The approximation involves discarding, in a principled way, likelihood factors according to a notion of locality in a factor graph associated with the emission distribution. This allows the exponential-in-dimension cost of exact filtering and smoothing to be avoided. We prove that the approximation accuracy, measured in a local total variation norm, is dimension-free in the sense that as the overall dimension of the model increases the error bounds we derive do not necessarily degrade. A key step in the analysis is to quantify the error introduced by localizing the likelihood function in a Bayes' rule update. The factorial structure of the likelihood function which we exploit arises naturally when data have known spatial or network structure. We demonstrate the new algorithms on synthetic examples and a London Underground passenger flow problem, where the factor graph is effectively given by the train network.",,,,,,"Rimella, Lorenzo/0000-0001-8846-1482",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766875300001,0
J,"Atamturk, A; Gomez, A; Han, SN",,,,"Atamturk, Alper; Gomez, Andres; Han, Shaoning",,,Sparse and Smooth Signal Estimation: Convexification of l(0)-Formulations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l(0)-norm constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on l(1)-norm relaxations. In this paper, we propose new iterative (convex) conic quadratic relaxations that exploit not only the l(0)-norm terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the l(0)-norm and its l(1) surrogate. These stronger relaxations lead to significantly better estimators than l(1)-norm approaches and also allow one to utilize affine sparsity priors. In addition, the parameters of the model and the resulting estimators are easily interpretable. Experiments with a tailored Lagrangian decomposition method indicate that the proposed iterative convex relaxations yield solutions within 1% of the exact l(0)-approach, and can tackle instances with up to 100,000 variables under one minute.",,,,,,"Gomez, Andres/0000-0003-3668-0653",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656352500001,0
J,"Chen, YS; Buhlmann, P",,,,"Chen, Yuansi; Buehlmann, Peter",,,Domain adaptation under structural causal models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Domain adaptation (DA) arises as an important problem in statistical machine learning when the source data used to train a model is different from the target data used to test the model. Recent advances in DA have mainly been application-driven and have largely relied on the idea of a common subspace for source and target data. To understand the empirical successes and failures of DA methods, we propose a theoretical framework via structural causal models that enables analysis and comparison of the prediction performance of DA methods. This framework also allows us to itemize the assumptions needed for the DA methods to have a low target error. Additionally, with insights from our theory, we propose a new DA method called CIRM that outperforms existing DA methods when both the covariates and label distributions are perturbed in the target data. We complement the theoretical analysis with extensive simulations to show the necessity of the devised assumptions. Reproducible synthetic and real data experiments are also provided to illustrate the strengths and weaknesses of DA methods when parts of the assumptions in our theory are violated.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,80,,,,,,,,,,,,,,,,WOS:000726706200001,0
J,"Chen, YX; Ying, ZL; Zhang, HR",,,,"Chen, Yunxiao; Ying, Zhiliang; Zhang, Haoran",,,"Unfolding-Model-Based Visualization: Theory, Method and Applications",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multidimensional unfolding methods are widely used for visualizing item response data. Such methods project respondents and items simultaneously onto a low-dimensional Euclidian space, in which respondents and items are represented by ideal points, with person-person, item-item, and person-item similarities being captured by the Euclidian distances between the points. In this paper, we study the visualization of multidimensional unfolding from a statistical perspective. We cast multidimensional unfolding into an estimation problem, where the respondent and item ideal points are treated as parameters to be estimated. An estimator is then proposed for the simultaneous estimation of these parameters. Asymptotic theory is provided for the recovery of the ideal points, shedding lights on the validity of model-based visualization. An alternating projected gradient descent algorithm is proposed for the parameter estimation. We provide two illustrative examples, one on users' movie rating and the other on senate roll call voting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500011,0
J,"Covert, IC; Lundberg, S; Lee, SI",,,,"Covert, Ian C.; Lundberg, Scott; Lee, Su-In",,,Explaining by Removing: A Unified Framework for Model Explanation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706446200001,0
J,"Ergen, T; Pilanci, M",,,,"Ergen, Tolga; Pilanci, Mert",,,Convex Geometry and Duality of Over-parameterized Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a convex analytic approach to analyze finite width two-layer ReLU networks. We first prove that an optimal solution to the regularized training problem can be char-acterized as extreme points of a convex set, where simple solutions are encouraged via its convex geometrical properties. We then leverage this characterization to show that an op-timal set of parameters yield linear spline interpolation for regression problems involving one dimensional or rank-one data. We also characterize the classification decision regions in terms of a kernel matrix and minimum .21-norm solutions. This is in contrast to Neu-ral Tangent Kernel which is unable to explain predictions of finite width networks. Our convex geometric characterization also provides intuitive explanations of hidden neurons as auto-encoders. In higher dimensions, we show that the training problem can be cast as a finite dimensional convex problem with infinitely many constraints. Then, we apply certain convex relaxations and introduce a cutting-plane algorithm to globally optimize the network. We further analyze the exactness of the relaxations to provide conditions for the convergence to a global optimum. Our analysis also shows that optimal network param-eters can be also characterized as interpretable closed-form formulas in some practically relevant special cases.",,,,,,"Ergen, Tolga/0000-0003-4806-0224",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706451500001,0
J,"Lange, H; Brunton, SL; Kutz, JN",,,,"Lange, Henning; Brunton, Steven L.; Kutz, J. Nathan",,,From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose spectral methods for long-term forecasting of temporal signals stemming from linear and nonlinear quasi-periodic dynamical systems. For linear signals, we introduce an algorithm with similarities to the Fourier transform but which does not rely on periodicity assumptions, allowing for forecasting given potentially arbitrary sampling intervals. We then extend this algorithm to handle nonlinearities by leveraging Koopman theory. The resulting algorithm performs a spectral decomposition in a nonlinear, data-dependent basis. The optimization objective for both algorithms is highly non-convex. However, expressing the objective in the frequency domain allows us to compute global optima of the error surface in a scalable and efficient manner, partially by exploiting the computational properties of the Fast Fourier Transform. Because of their close relation to Bayesian Spectral Analysis, uncertainty quantification metrics are a natural by product of the spectral forecasting methods. We extensively benchmark these algorithms against other leading forecasting methods on a range of synthetic experiments as well as in the context of real-world power systems and fluid flows.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500041,0
J,"Li, JJY; Chen, YEL; Tong, X",,,,"Li, Jingyi Jessica; Chen, Yiling Elaine; Tong, Xin",,,A flexible model-free prediction-based framework for feature ranking,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Despite the availability of numerous statistical and machine learning tools for joint feature modeling, many scientists investigate features marginally, i.e., one feature at a time. This is partly due to training and convention but also roots in scientists' strong interests in simple visualization and interpretability. As such, marginal feature ranking for some predictive tasks, e.g., prediction of cancer driver genes, is widely practiced in the process of scientific discoveries. In this work, we focus on marginal ranking for binary classification, one of the most common predictive tasks. We argue that the most widely used marginal ranking criteria, including the Pearson correlation, the two-sample t test, and two-sample Wilcoxon rank-sum test, do not fully take feature distributions and prediction objectives into account. To address this gap in practice, we propose two ranking criteria corresponding to two prediction objectives: the classical criterion (CC) and the Neyman-Pearson criterion (NPC), both of which use model-free nonparametric implementations to accommodate diverse feature distributions. Theoretically, we show that under regularity conditions, both criteria achieve sample-level ranking that is consistent with their population-level counterpart with high probability. Moreover, NPC is robust to sampling bias when the two class proportions in a sample deviate from those in the population. This property endows NPC good potential in biomedical research where sampling biases are ubiquitous. We demonstrate the use and relative advantages of CC and NPC in simulation and real data studies. Our model-free objective-based ranking idea is extendable to ranking feature subsets and generalizable to other prediction tasks and learning objectives.",,,,,"Chen, Yi/HIR-2608-2022; Li, Jingyi Jessica/I-2779-2019","Li, Jingyi Jessica/0000-0002-9288-5648",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,124,,,,,,,,,,35321091,,,,,WOS:000663175600001,0
J,"Liu, LT; Ruan, F; Mania, H; Jordan, MI",,,,"Liu, Lydia T.; Ruan, Feng; Mania, Horia; Jordan, Michael I.",,,Bandit Learning in Decentralized Matching Markets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study two-sided matching markets in which one side of the market (the players) does not have a priori knowledge about its preferences for the other side (the arms) and is required to learn its preferences from experience. Also, we assume the players have no direct means of communication. This model extends the standard stochastic multi-armed bandit framework to a decentralized multiple player setting with competition. We introduce a new algorithm for this setting that, over a time horizon T, attains O(log(T)) stable regret when preferences of the arms over players are shared, and O(log(T)(2)) regret when there are no assumptions on the preferences on either side. Moreover, in the setting where a single player may deviate, we show that the algorithm is incentive compatible whenever the arms' preferences are shared, but not necessarily so when preferences are fully general.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000712030300001,0
J,"Lu, B; Hardin, J",,,,"Lu, Benjamin; Hardin, Johanna",,,A Unified Framework for Random Forest Prediction Error Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a unified framework for random forest prediction error estimation based on a novel estimator of the conditional prediction error distribution function. Our framework enables simple plug-in estimation of key prediction uncertainty metrics, including conditional mean squared prediction errors, conditional biases, and conditional quantiles, for random forests and many variants. Our approach is especially well-adapted for prediction interval estimation; we show via simulations that our proposed prediction intervals are competitive with, and in some settings outperform, existing methods. To establish theoretical grounding for our framework, we prove pointwise uniform consistency of a more stringent version of our estimator of the conditional prediction error distribution function. The estimators introduced here are implemented in the R package forestError.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500008,0
J,"Ornik, M; Topcu, U",,,,"Ornik, Melkior; Topcu, Ufuk",,,Learning and Planning for Time-Varying MDPs Using Maximum Likelihood Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes a formal approach to online learning and planning for agents operating in a priori unknown, time-varying environments. The proposed method computes the maximally likely model of the environment, given the observations about the environment made by an agent earlier in the system run and assuming knowledge of a bound on the maximal rate of change of system dynamics. Such an approach generalizes the estimation method commonly used in learning algorithms for unknown Markov decision processes with time-invariant transition probabilities, but is also able to quickly and correctly identify the system dynamics following a change. Based on the proposed method, we generalize the exploration bonuses used in learning for time-invariant Markov decision processes by introducing a notion of uncertainty in a learned time-varying model, and develop a control policy for time-varying Markov decision processes based on the exploitation and exploration trade-off. We demonstrate the proposed methods on four numerical examples: a patrolling task with a change in system dynamics, a two-state MDP with periodically changing outcomes of actions, a wind flow estimation task, and a multi-armed bandit problem with periodically changing probabilities of different rewards.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,35002545,,,,,WOS:000628529500035,0
J,"Peluchetti, S; Favaro, S",,,,"Peluchetti, Stefano; Favaro, Stefano",,,Doubly infinite residual neural networks: a diffusion process approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modern neural networks featuring a large number of layers (depth) and units per layer (width) have achieved a remarkable performance across many domains. While there exists a vast literature on the interplay between infinitely wide neural networks and Gaussian processes, a little is known about analogous interplays with respect to infinitely deep neural networks. Neural networks with independent and identically distributed (i.i.d.) initializations exhibit undesirable forward and backward propagation properties as the number of layers increases, e.g., vanishing dependency on the input, and perfectly correlated outputs for any two inputs. To overcome these drawbacks, Peluchetti and Favaro (2020) considered fully-connected residual networks (ResNets) with network's parameters initialized by means of distributions that shrink as the number of layers increases, thus establishing an interplay between infinitely deep ResNets and solutions to stochastic differential equations, i.e. diffusion processes, and showing that infinitely deep ResNets does not suffer from undesirable forward-propagation properties. In this paper, we review the results of Peluchetti and Favaro (2020), extending them to convolutional ResNets, and we establish analogous backward-propagation results, which directly relate to the problem of training fully-connected deep ResNets. Then, we investigate the more general setting of doubly infinite neural networks, where both network's width and network's depth grow unboundedly. We focus on doubly infinite fully-connected ResNets, for which we consider i.i.d. initializations. Under this setting, we show that the dynamics of quantities of interest converge, at initialization, to deterministic limits. This allow us to provide analytical expressions for inference, both in the case of weakly trained and fully trained ResNets. Our results highlight a limited expressive power of doubly infinite ResNets when the unscaled network's parameters are i.i.d. and the residual blocks are shallow.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700316200001,0
J,"Pineau, J; Vincent-Lamarre, P; Sinha, K; Lariviere, V; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Larochelle, H",,,,"Pineau, Joelle; Vincent-Lamarre, Philippe; Sinha, Koustuv; Lariviere, Vincent; Beygelzimer, Alina; d'Alche-Buc, Florence; Fox, Emily; Larochelle, Hugo",,,Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.",,,,,,"Fox, Emily/0000-0003-3188-9685",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687303300001,0
J,"Wang, T; Lin, QH",,,,"Wang, Tong; Lin, Qihang",,,Hybrid Predictive Models: When an Interpretable Model Collaborates with a Black-box Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Interpretable machine learning has become a strong competitor for black-box models. However, the possible loss of the predictive performance for gaining understandability is often inevitable, especially when it needs to satisfy users with diverse backgrounds or high standards for what is considered interpretable. This tension puts practitioners in a dilemma of choosing between high accuracy (black-box models) and interpretability (interpretable models). In this work, we propose a novel framework for building a Hybrid Predictive Model that integrates an interpretable model with any pre-trained black-box model to combine their strengths. The interpretable model substitutes the black-box model on a subset of data where the interpretable model is most competent, gaining transparency at a low cost of the predictive accuracy. We design a principled objective function that considers predictive accuracy, model interpretability, and model transparency (defined as the percentage of data processed by the interpretable substitute.) Under this framework, we propose two hybrid models, one substituting with association rules and the other with linear models, and design customized training algorithms for both models. We test the hybrid models on structured data and text data where interpretable models collaborate with various state-of-the-art black-box models. Results show that hybrid models obtain an efficient trade-off between transparency and predictive performance, characterized by pareto frontiers. Finally, we apply the proposed model on a real-world patients dataset for predicting cardiovascular disease and propose multi-model Pareto frontiers to assist model selection in real applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687178800001,0
J,"Zhong, ZX; Chueng, WC; Tan, VYF",,,,"Zhong, Zixin; Chueng, Wang Chi; Tan, Vincent Y. F.",,,Thompson Sampling Algorithms for Cascading Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by the important and urgent need for efficient optimization in online recommender systems, we revisit the cascading bandit model proposed by Kveton et al. (2015a). While Thompson sampling (TS) algorithms have been shown to be empirically superior to Upper Confidence Bound (UCB) algorithms for cascading bandits, theoretical guarantees are only known for the latter. In this paper, we first provide a problem-dependent upper bound on the regret of a TS algorithm with Beta-Bernoulli updates; this upper bound is tighter than a recent derivation under a more general setting by Huyuk and Tekin (2019). Next, we design and analyze another TS algorithm with Gaussian updates, TS-CASCADE. TS-CASCADE achieves the state-of-the-art problem-independent regret bound for cascading bandits. Complementarily, we consider a linear generalization of the cascading bandit model, which allows efficient learning in large-scale cascading bandit problem instances. We introduce and analyze a TS algorithm, which enjoys a regret bound that depends on the dimension of the linear model but not the number of items. Finally, by using information theoretic techniques and a judicious construction of cascading bandit instances, we derive a nearly-matching lower bound on the expected regret for the standard model. Our paper establishes the first theoretical guarantees on TS algorithms for a stochastic combinatorial bandit problem model with partial feedback. Numerical experiments demonstrate the superiority of the proposed TS algorithms compared to existing UCB-based ones.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706882300001,0
J,"Elgabli, A; Park, J; Bedi, AS; Bennis, M; Aggarwal, V",,,,"Elgabli, Anis; Park, Jihong; Bedi, Amrit S.; Bennis, Mehdi; Aggarwal, Vaneet",,,GADMM: Fast and Communication Efficient Framework for Distributed Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When the data is distributed across multiple servers, lowering the communication cost between the servers (or workers) while solving the distributed learning problem is an important problem and is the focus of this paper. In particular, we propose a fast, and communication-efficient decentralized framework to solve the distributed machine learning (DML) problem. The proposed algorithm, Group Alternating Direction Method of Multipliers (GADMM) is based on the Alternating Direction Method of Multipliers (ADMM) framework. The key novelty in GADMM is that it solves the problem in a decentralized topology where at most half of the workers are competing for the limited communication resources at any given time. Moreover, each worker exchanges the locally trained model only with two neighboring workers, thereby training a global model with a lower amount of communication overhead in each exchange. We prove that GADMM converges to the optimal solution for convex loss functions, and numerically show that it converges faster and more communication-efficient than the state-of-the-art communication-efficient algorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in linear and logistic regression tasks on synthetic and real datasets. Furthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove its convergence under the time-varying network topology of the workers.",,,,,"Bennis, Mehdi/ABE-5838-2020; Park, Jihong/ABC-7334-2020; Aggarwal, Vaneet/A-4843-2017","Bennis, Mehdi/0000-0003-0261-0171; Park, Jihong/0000-0001-7623-6552; Aggarwal, Vaneet/0000-0001-9131-4723; Elgabli, Anis/0000-0001-5012-2370",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000028,0
J,"Jaiswal, P; Rao, V; Honnappa, H",,,,"Jaiswal, Prateek; Rao, Vinayak; Honnappa, Harsha",,,Asymptotic Consistency of alpha-Renyi-Approximate Posteriors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the asymptotic consistency properties of alpha-Renyi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the alpha-Renyi divergence from the true posterior. Unique to our work is that we consider settings with alpha > 1, resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a 'good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where alpha equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods.",,,,,"Jaiswal, Prateek/HII-8294-2022","Jaiswal, Prateek/0000-0002-7637-2754",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,156,,,,,,,,,,,,,,,WOS:000570145200001,0
J,"Kumar, S; Ying, JX; Cardoso, JVD; Palomar, DP",,,,"Kumar, Sandeep; Ying, Jiaxi; Cardoso, Jose Vincius de M.; Palomar, Daniel P.",,,A Unified Framework for Structured Graph Learning via Spectral Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph learning from data is a canonical problem that has received substantial attention in the literature. Learning a structured graph is essential for interpretability and identification of the relationships among data. In general, learning a graph with a specific structure is an NP-hard combinatorial problem and thus designing a general tractable algorithm is challenging. Some useful structured graphs include connected, sparse, multi-component, bipartite, and regular graphs. In this paper, we introduce a unified framework for structured graph learning that combines Gaussian graphical model and spectral graph theory. We propose to convert combinatorial structural constraints into spectral constraints on graph matrices and develop an optimization framework based on block majorization-minimization to solve structured graph learning problem. The proposed algorithms are provably convergent and practically amenable for a number of graph based applications such as data clustering. Extensive numerical experiments with both synthetic and real data sets illustrate the effectiveness of the proposed algorithms. An open source R package containing the code for all the experiments is available at https://CRAN.R-project.org/package=spectralGraphTopology",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300022,0
J,"Mokhtari, A; Koppel, A; Takac, M; Ribeiro, A",,,,"Mokhtari, Aryan; Koppel, Alec; Takac, Martin; Ribeiro, Alejandro",,,A Class of Parallel Doubly Stochastic Algorithms for Large-Scale Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider learning problems over training sets in which both, the number of training examples and the dimension of the feature vectors, are large. To solve these problems we propose the random parallel stochastic algorithm (RAPSA). We call the algorithm random parallel because it utilizes multiple parallel processors to operate on a randomly chosen subset of blocks of the feature vector. RAPSA is doubly stochastic since each processor utilizes a random set of functions to compute the stochastic gradient associated with a randomly chosen sets of variable coordinates. Algorithms that are parallel in either of these dimensions exist, but RAPSA is the first attempt at a methodology that is parallel in both the selection of blocks and the selection of elements of the training set. In RAPSA, processors utilize the randomly chosen functions to compute the stochastic gradient component associated with a randomly chosen block. The technical contribution of this paper is to show that this minimally coordinated algorithm converges to the optimal classifier when the training objective is strongly convex. Moreover, we present an accelerated version of RAPSA (ARAPSA) that incorporates the objective function curvature information by premultiplying the descent direction by a Hessian approximation matrix. We further extend the results for asynchronous settings and show that if the processors perform their updates without any coordination the algorithms are still convergent to the optimal argument. RAPSA and its extensions are then numerically evaluated on a linear estimation problem and a binary image classification task using the MNIST handwritten digit dataset.",,,,,"Takac, Martin/AAA-8564-2022; Koppel, Alec/ABC-5438-2020","Takac, Martin/0000-0001-7455-2025; Koppel, Alec/0000-0003-2447-2873",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,120,,,,,,,,,,,,,,,WOS:000556189600001,0
J,"Rotnitzky, A; Smucler, E",,,,"Rotnitzky, Andrea; Smucler, Ezequiel",,,Efficient Adjustment Sets for Population Average Causal Treatment Effect Estimation in Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The method of covariate adjustment is often used for estimation of total treatment effects from observational studies. Restricting attention to causal linear models, a recent article (Henckel et al., 2019) derived two novel graphical criteria: one to compare the asymptotic variance of linear regression treatment effect estimators that control for certain distinct adjustment sets and another to identify the optimal adjustment set that yields the least squares estimator with the smallest asymptotic variance. In this paper we show that the same graphical criteria can be used in non-parametric causal graphical models when treatment effects are estimated using non-parametrically adjusted estimators of the interventional means. We also provide a new graphical criterion for determining the optimal adjustment set among the minimal adjustment sets and another novel graphical criterion for comparing time dependent adjustment sets. We show that uniformly optimal time dependent adjustment sets do not always exist. For point interventions, we provide a sound and complete graphical criterion for determining when a non-parametric optimally adjusted estimator of an interventional mean, or of a contrast of interventional means, is semiparametric efficient under the non-parametric causal graphical model. In addition, when the criterion is not met, we provide a sound algorithm that checks for possible simplifications of the efficient influence function of the parameter. Finally, we find an interesting connection between identification and efficient covariate adjustment estimation. Specifically, we show that if there exists an identifying formula for an interventional mean that depends only on treatment, outcome and mediators, then the non-parametric optimally adjusted estimator can never be globally efficient under the causal graphical model.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,188,,,,,,,,,,,,,,,WOS:000570237200001,0
J,"Agrell, C",,,,"Agrell, Christian",,,Gaussian Processes with Linear Operator Inequality Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an approach for constrained Gaussian Process ( GP) regression where we assume that a set of linear transformations of the process are bounded. It is motivated by machine learning applications for high-consequence engineering systems, where this kind of information is often made available from phenomenological knowledge. We consider a GP f over functions on X subset of R-n taking values in R, where the process Lf is still Gaussian when L is a linear operator. Our goal is to model f under the constraint that realizations of Lf are confined to a convex set of functions. In particular, we require that a <= Lf <= b, given two functions a and b where a < b pointwise. This formulation provides a consistent way of encoding multiple linear constraints, such as shape-constraints based on e.g. boundedness, monotonicity or convexity. We adopt the approach of using a sufficiently dense set of virtual observation locations where the constraint is required to hold, and derive the exact posterior for a conjugate likelihood. The results needed for stable numerical implementation are derived, together with an efficient sampling scheme for estimating the posterior process. Keywords: Gaussian processes, Linear constraints, Virtual observations, Uncertainty Quantification, Computer code emulation",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,135,,,,,,,,,,,,,,,WOS:000487068900019,0
J,"El-Bachir, Y; Davison, AC",,,,"El-Bachir, Yousra; Davison, Anthony C.",,,Fast Automatic Smoothing for Generalized Additive Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Generalized additive models (GAMs) are regression models wherein parameters of probability distributions depend on input variables through a sum of smooth functions, whose degrees of smoothness are selected by L-2 regularization. Such models have become the de-facto standard nonlinear regression models when interpretability and flexibility are required, but reliable and fast methods for automatic smoothing in large data sets are still lacking. We develop a general methodology for automatically learning the optimal degree of L-2 regularization for GAMs using an empirical Bayes approach. The smooth functions are penalized by hyper-parameters that are learned simultaneously by maximization of a marginal likelihood using an approximate expectation-maximization algorithm. The latter involves a double Laplace approximation at the E-step, and leads to an efficient M-step. Empirical analysis shows that the resulting algorithm is numerically stable, faster than the best existing methods and achieves state-of-the-art accuracy. For illustration, we apply it to an important and challenging problem in the analysis of extremal data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,173,,,,,,,,,,,,,,,WOS:000506403100013,0
J,"Katz-Samuels, J; Blanchard, G; Scott, C",,,,"Katz-Samuels, Julian; Blanchard, Gilles; Scott, Clayton",,,Decontamination of Mutual Contamination Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many machine learning problems can be characterized by mutual contamination models. In these problems, one observes several random samples from different convex combinations of a set of unknown base distributions and the goal is to infer these base distributions. This paper considers the general setting where the base distributions are defined on arbitrary probability spaces. We examine three popular machine learning problems that arise in this general setting: multiclass classification with label noise, demixing of mixed membership models, and classification with partial labels. In each case, we give sufficient conditions for identifiability and present algorithms for the infinite and finite sample settings, with associated performance guarantees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,41,,,,,,,,,,,,,,,WOS:000463319900001,0
J,"Letham, B; Bakshy, E",,,,"Letham, Benjamin; Bakshy, Eytan",,,Bayesian Optimization for Policy Search via Online-Offline Experimentation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Online field experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces-such as those found in recommendation and ranking problems-is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an offline simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased offline experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We find that improved kernel inference is a significant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization efficiently tuning a live machine learning system by combining offline and online experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,145,,,,,,,,,,,,,,,WOS:000491132200009,0
J,"Maunu, T; Zhang, T; Lerman, G",,,,"Maunu, Tyler; Zhang, Teng; Lerman, Gilad",,,A Well-Tempered Landscape for Non-convex Robust Subspace Recovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a mathematical analysis of a non-convex energy landscape for robust subspace recovery. We prove that an underlying subspace is the only stationary point and local minimizer in a specified neighborhood under a deterministic condition on a dataset. If the deterministic condition is satisfied, we further show that a geodesic gradient descent method over the Grassmannian manifold can exactly recover the underlying subspace when the method is properly initialized. Proper initialization by principal component analysis is guaranteed with a simple deterministic condition. Under slightly stronger assumptions, the gradient descent method with a piecewise constant step-size scheme achieves linear convergence. The practicality of the deterministic condition is demonstrated on some statistical models of data, and the method achieves almost state-of-the-art recovery guarantees on the Haystack Model for different regimes of sample size and ambient dimension. In particular, when the ambient dimension is fixed and the sample size is large enough, we show that our gradient method can exactly recover the underlying subspace for any fixed fraction of outliers (less than 1).",,,,,,"Lerman, Gilad/0000-0003-4624-3115; Maunu, Tyler/0000-0001-9747-4461",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,37,,,,,,,,,,,,,,,WOS:000463318800001,0
J,"Yu, SQ; Drton, M; Shojaie, A",,,,"Yu, Shiqing; Drton, Mathias; Shojaie, Ali",,,Generalized Score Matching for Non-Negative Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common challenge in estimating parameters of probability density functions is the intractability of the normalizing constant. While in such cases maximum likelihood estimation may be implemented using numerical integration, the approach becomes computationally intensive. The score matching method of Hyvarinen (2005) avoids direct calculation of the normalizing constant and yields closed-form estimates for exponential families of continuous distributions over R-m. Hyvarinen (2007) extended the approach to distributions supported on the non-negative orthant, R-+(m). In this paper, we give a generalized form of score matching for non-negative data that improves estimation e ffi ciency. As an example, we consider a general class of pairwise interaction models. Addressing an overlooked inexistence problem, we generalize the regularized score matching method of Lin et al. (2016) and improve its theoretical guarantees for non-negative Gaussian graphical models.",,,,,,"Yu, Shiqing/0000-0002-2719-7558; Drton, Mathias/0000-0001-5614-3025",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,76,,,,,,,,,,34290571,,,,,WOS:000467896000001,0
J,"Fang, Y; Xu, JF; Yang, L",,,,"Fang, Yixin; Xu, Jinfeng; Yang, Lei",,,Online Bootstrap Confidence Intervals for the Stochastic Gradient Descent Estimator,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications involving large dataset or online learning, stochastic gradient descent (SGD) is a scalable algorithm to compute parameter estimates and has gained increasing popularity due to its numerical convenience and memory efficiency. While the asymptotic properties of SGD-based estimators have been well established, statistical inference such as interval estimation remains much unexplored. The classical bootstrap is not directly applicable if the data are not stored in memory. The plug-in method is not applicable when there is no explicit formula for the covariance matrix of the estimator. In this paper, we propose an online bootstrap procedure for the estimation of confidence intervals, which, upon the arrival of each observation, updates the SGD estimate as well as a number of randomly perturbed SGD estimates. The proposed method is easy to implement in practice. We establish its theoretical properties for a general class of models that includes linear regressions, generalized linear models, M-estimators and quantile regressions as special cases. The finite-sample performance and numerical utility is evaluated by simulation studies and real data applications.",,,,,"Xu, Jinfeng/AAC-5368-2020","Fang, Yixin/0000-0003-0104-9140; Xu, Jinfeng/0000-0002-3165-2015",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,78,,,,,,,,,,,,,,,WOS:000454479400001,0
J,"Jain, P; Netrapalli, P; Kakade, SM; Kidambi, R; Sidford, A",,,,"Jain, Prateek; Netrapalli, Praneeth; Kakade, Sham M.; Kidambi, Rahul; Sidford, Aaron",,,"Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work presents a sharp analysis of: (1) minibatching, a method of averaging many samples of a stochastic gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents sharp finite sample generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. This characterization is used to understand the relationship between learning rate versus batch size when considering the excess risk of the final iterate of an SGD procedure. Next, this mini-batching characterization is utilized in providing a highly parallelizable SGD method that achieves the minimax risk with nearly the same number of serial updates as batch gradient descent, improving significantly over existing SGD-style methods. Following this, a non-asymptotic excess risk bound for model averaging (which is a communication efficient parallelization scheme) is provided. Finally, this work sheds light on fundamental differences in SGD's behavior when dealing with mis-specified models in the non-realizable least squares problem. This paper shows that maximal stepsizes ensuring minimax risk for the mis-specified case must depend on the noise properties. The analysis tools used by this paper generalize the operator view of averaged SGD (Defossez and Bach, 2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques are of broader interest in analyzing various computational aspects of stochastic approximation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,223,,,,,,,,,,,,,,,WOS:000438190100001,0
J,"Mahajan, D; Agrawal, N; Keerthi, SS; Sellamanickam, S; Bottou, L",,,,"Mahajan, Dhruv; Agrawal, Nikunj; Keerthi, S. Sathiya; Sellamanickam, Sundararajan; Bottou, Leon",,,An efficient distributed learning algorithm based on effective local functional approximations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Scalable machine learning over big data is an important problem that is receiving a lot of attention in recent years. On popular distributed environments such as Hadoop running on a cluster of commodity machines, communication costs are substantial and algorithms need to be designed suitably considering those costs. In this paper we give a novel approach to the distributed training of linear classifiers (involving smooth losses and L-2 regularization) that is designed to reduce the total communication costs. At each iteration, the nodes minimize locally formed approximate objective functions; then the resulting minimizers are combined to form a descent direction to move. Our approach gives a lot of freedom in the formation of the approximate objective function as well as in the choice of methods to solve them. The method is shown to have O (log(1/epsilon)) time convergence. The method can be viewed as an iterative parameter mixing method. A special instantiation yields a parallel stochastic gradient descent method with strong convergence. When communication times between nodes are large, our method is much faster than the Terascale method (Agarwal et al., 2011), which is a state of the art distributed solver based on the statistical query model (Chu et al., 2006) that computes function and gradient values in a distributed fashion. We also evaluate against other recent distributed methods and demonstrate superior performance of our method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454478000001,0
J,"Shah, NB; Wainwright, MJ",,,,"Shah, Nihar B.; Wainwright, Martin J.",,,"Simple, Robust and Optimal Ranking from Pairwise Comparisons",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider data in the form of pairwise comparisons of n items, with the goal of identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Borda counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) it is an optimal method achieving the information-theoretic limits up to constant factors; (b) it is robust in that its optimality holds without imposing conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) its computational efficiency leads to speed-ups of several orders of magnitude. We address the problem of exact recovery, and for the top-k recovery problem we also extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. In doing so, we introduce a general framework that allows us to treat a variety of problems in the literature in an unified manner.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,199,,,,,,,,,,,,,,,WOS:000435455600001,0
J,"Ahsen, ME; Challapalli, N; Vidyasagar, M",,,,"Ahsen, Mehmet Eren; Challapalli, Niharika; Vidyasagar, Mathukumalli",,,Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce a new optimization formulation for sparse regression and compressed sensing, called CLOT (Combined L-One and Two), wherein the regularizer is a convex combination of the l(1)- and l(2)-norms. This formulation differs from the Elastic Net (EN) formulation, in which the regularizer is a convex combination of the l(1)- and l(2)-norm squared. It is shown that, in the context of compressed sensing, the EN formulation does not achieve robust recovery of sparse vectors, whereas the new CLOT formulation achieves robust recovery. Also, like EN but unlike LASSO, the CLOT formulation achieves the grouping effect, wherein coefficients of highly correlated columns of the measurement (or design) matrix are assigned roughly comparable values. It is already known LASSO does not have the grouping effect. Therefore the CLOT formulation combines the best features of both LASSO (robust sparse recovery) and EN (grouping effect). The CLOT formulation is a special case of another one called SGL (Sparse Group LASSO) which was introduced into the literature previously, but without any analysis of either the grouping effect or robust sparse recovery. It is shown here that SGL achieves robust sparse recovery, and also achieves a version of the grouping effect in that coefficients of highly correlated columns belonging to the same group of the measurement (or design) matrix are assigned roughly comparable values.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,24,,,,,,,,,,,,,,,,WOS:000405980500001,0
J,"Coretto, P; Hennig, C",,,,"Coretto, Pietro; Hennig, Christian",,,"Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The robust improper maximum likelihood estimator (RIMLE) is a new method for robust multivariate clustering finding approximately Gaussian clusters. It maximizes a pseudo-likelihood defined by adding a component with improper constant density for accommodating outliers to a Gaussian mixture. A special case of the RIMLE is MLE for multivariate finite Gaussian mixture models. In this paper we treat existence, consistency, and breakdown theory for the RIMLE comprehensively. RIMLE's existence is proved under non-smooth covariance matrix constraints. It is shown that these can be implemented via a computationally feasible Expectation-Conditional Maximization algorithm.",,,,,"Hennig, Christian/GXG-8354-2022; Coretto, Pietro/D-2140-2018","Hennig, Christian/0000-0003-1550-5637; Coretto, Pietro/0000-0002-6972-9671",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,142,,,,,,,,,,,,,,,WOS:000424548800001,0
J,"Javanmard, A",,,,"Javanmard, Adel",,,Perishability of Data: Dynamic Pricing under Varying-Coefficient Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a firm that sells a large number of products to its customers in an online fashion. Each product is described by a high dimensional feature vector, and the market value of a product is assumed to be linear in the values of its features. Parameters of the valuation model are unknown and can change over time. The firm sequentially observes a product's features and can use the historical sales data (binary sale/no sale feedbacks) to set the price of current product, with the objective of maximizing the collected revenue. We measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on projected stochastic gradient descent (PSGD) and characterize its regret in terms of time T, features dimension d, and the temporal variability in the model parameters, delta(t). We consider two settings. In the first one, feature vectors are chosen antagonistically by nature and we prove that the regret of PSGD pricing policy is of order O(root T + Sigma(T)(t-1)root t delta(t)). In the second setting (referred to as stochastic features model), the feature vectors are drawn independently from an unknown distribution. We show that in this case, the regret of PSGD pricing policy is of order O(d(2) logT + Sigma(T)(t=1)t delta(t)/d).",,,,,"Javanmard, Adel/ABB-5000-2020",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,31,,,,,,,,,,,,,,,,WOS:000405980400001,0
J,"Yu, G; Bien, J",,,,"Yu, Guo; Bien, Jacob",,,Learning Local Dependence In Ordered Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive de finite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,60,42,,,,,,,,,,,,,,,WOS:000405962100001,0
J,"Babbar, R; Partalas, I; Gaussier, E; Amini, MR; Amblard, C",,,,"Babbar, Rohit; Partalas, Ioannis; Gaussier, Eric; Amini, Massih-Reza; Amblard, Cecile",,,Learning Taxonomy Adaptation in Large-scale Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study flat and hierarchical classification strategies in the context of large-scale taxonomies. Addressing the problem from a learning-theoretic point of view, we first propose a multi-class, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. Based on this bound, we also propose a technique for modifying a given taxonomy through pruning, that leads to a lower value of the upper bound as compared to the original taxonomy. We then present another method for hierarchy pruning by studying approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",,,,,,"Babbar, Rohit/0000-0002-3787-8971",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,98,,,,,,,,,,,,,,,WOS:000391540200001,0
J,"Chen, YT; Bornn, L; de Freitas, N; Eskelin, M; Fang, J; Welling, M",,,,"Chen, Yutian; Bornn, Luke; de Freitas, Nando; Eskelin, Mareija; Fang, Jing; Welling, Max",,,Herded Gibbs Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Gibbs sampler is one of the most popular algorithms for inference in statistical models. In this paper, we introduce a herding variant of this algorithm, called herded Gibbs, that is entirely deterministic. We prove that herded Gibbs has an O(1/T) convergence rate for models with independent variables and for fully connected probabilistic graphical models. Herded Gibbs is shown to outperform Gibbs in the tasks of image denoising with MRFs and named entity recognition with CRFs. However, the convergence for herded Gibbs for sparsely connected probabilistic graphical models is still an open problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,29,,,,,,,,,,,,,,,WOS:000391467400001,0
J,"Couillet, R; Wainrib, G; Sevi, H; Ali, HT",,,,"Couillet, Romain; Wainrib, Gilles; Sevi, Harry; Ali, Hafiz Tiomoko",,,The Asymptotic Performance of Linear Echo State Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, a study of the mean-square error (MSE) performance of linear echo-state neural networks is performed, both for training and testing tasks. Considering the realistic setting of noise present at the network nodes, we derive deterministic equivalents for the aforementioned MSE in the limit where the number of input data T and network size n both grow large. Specializing then the network connectivity matrix to specific random settings, we further obtain simple formulas that provide new insights on the performance of such networks.",,,,,,"Couillet, Romain/0000-0001-5755-2090",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,178,,,,,,,,,,,,,,,WOS:000391681400001,0
J,"Daniel, C; Neumann, G; Kroemer, O; Peters, J",,,,"Daniel, Christian; Neumann, Gerhard; Kroemer, Oliver; Peters, Jan",,,Hierarchical Relative Entropy Policy Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many reinforcement learning (RL) tasks, especially in robotics, consist of multiple sub-tasks that are strongly structured. Such task structures can be exploited by incorporating hierarchical policies that consist of gating networks and sub-policies. However, this concept has only been partially explored for real world settings and complete methods, derived from first principles, are needed. Real world settings are challenging due to large and continuous state-action spaces that are prohibitive for exhaustive sampling methods. We define the problem of learning sub-policies in continuous state action spaces as finding a hierarchical policy that is composed of a high-level gating policy to select the low-level sub-policies for execution by the agent. In order to efficiently share experience with all sub-policies, also called inter-policy learning, we treat these sub-policies as latent variables which allows for distribution of the update information between the sub-policies. We present three different variants of our algorithm, designed to be suitable for a wide variety of real world robot learning tasks and evaluate our algorithms in two real robot learning scenarios as well as several simulations and comparisons.",,,,,"Peters, Jan R/D-5068-2009; Peters, Jan/P-6027-2019","Peters, Jan R/0000-0002-5266-8091; Peters, Jan/0000-0002-5266-8091; Neumann, Gerhard/0000-0002-5483-4225",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,93,,,,,,,,,,,,,,,WOS:000391538900001,0
J,"Gupta, M; Cotter, A; Pfeifer, J; Voevodski, K; Canini, K; Mangylov, A; Moczydlowski, W; van Esbroeck, A",,,,"Gupta, Maya; Cotter, Andrew; Pfeifer, Jan; Voevodski, Konstantin; Canini, Kevin; Mangylov, Alexander; Moczydlowski, Wojciech; van Esbroeck, Alexander",,,Monotonic Calibrated Interpolated Look-Up Tables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Real-world machine learning applications may have requirements beyond accuracy, such as fast evaluation times and interpretability. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical for user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.",,,,,,"Voevodski, Konstantin/0000-0002-7518-8242",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,109,,,,,,,,,,,,,,,WOS:000391547300001,0
J,"He, R; Tian, J; Wu, HQ",,,,"He, Ru; Tian, Jin; Wu, Huaiqing",,,Structure Learning in Bayesian Networks of a Moderate Size by Efficient Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the Bayesian model averaging approach to learning Bayesian network structures (DAGs) from data. We develop new algorithms including the first algorithm that is able to efficiently sample DAGs of a moderate size (with up to about 25 variables) according to the exact structure posterior. The DAG samples can then be used to construct estimators for the posterior of any feature. We theoretically prove good properties of our estimators and empirically show that our estimators considerably outperform the estimators from the previous state-of-the-art methods.,,,,,"Tian, Jin/GZM-3191-2022","Tian, Jin/0000-0001-5313-1600",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,101,,,,,,,,,,,,,,,WOS:000391541200001,0
J,"Kouw, WM; van der Maaten, LJP; Krijthe, JH; Loog, M",,,,"Kouw, Wouter M.; van der Maaten, Laurens J. P.; Krijthe, Jesse H.; Loog, Marco",,,Feature-Level Domain Adaptation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Domain adaptation is the supervised learning setting in which the training and test data are sampled from different distributions: training data is sampled from a source domain, whilst test data is sampled from a target domain. This paper proposes and studies an approach, called feature-level domain adaptation (FLDA), that models the dependence between the two domains by means of a feature-level transfer model that is trained to describe the transfer from source to target domain. Subsequently, we train a domain-adapted classifier by minimizing the expected loss under the resulting transfer model. For linear classifiers and a large family of loss functions and transfer models, this expected loss can be computed or approximated analytically, and minimized efficiently. Our empirical evaluation of FLDA focuses on problems comprising binary and count data in which the transfer can be naturally modeled via a dropout distribution, which allows the classifier to adapt to differences in the marginal probability of features in the source and the target domain. Our experiments on several real-world problems show that FLDA performs on par with state-of-the-art domain adaptation techniques.",,,,,"Kouw, Wouter/T-9432-2019","Kouw, Wouter/0000-0002-0547-4817",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,171,,,,,,,,,,,,,,,WOS:000391676100001,0
J,"Maggioni, M; Minsker, S; Strawn, N",,,,"Maggioni, Mauro; Minsker, Stanislav; Strawn, Nate",,,Multiscale Dictionary Learning: Non-Asymptotic Bounds and Robustness,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High-dimensional datasets are well-approximated by low-dimensional structures. Over the past decade, this empirical observation motivated the investigation of detection, measurement, and modeling techniques to exploit these low-dimensional intrinsic structures, yielding numerous implications for high-dimensional statistics, machine learning, and signal processing. Manifold learning (where the low-dimensional structure is a manifold) and dictionary learning (where the low-dimensional structure is the set of sparse linear combinations of vectors from a fi nite dictionary) are two prominent theoretical and computational frameworks in this area. Despite their ostensible distinction, the recently-introduced Geometric Multi-Resolution Analysis (GMRA) provides a robust, computationally efficient, multiscale procedure for simultaneously learning manifolds and dictionaries. In this work, we prove non-asymptotic probabilistic bounds on the approximation error of GMRA for a rich class of data-generating statistical models that includes noisy manifolds, thereby establishing the theoretical robustness of the procedure and con firming empirical observations. In particular, if a dataset aggregates near a low-dimensional manifold, our results show that the approximation error of the GMRA is completely independent of the ambient dimension. Our work therefore establishes GMRA as a provably fast algorithm for dictionary learning with approximation and sparsity guarantees. We include several numerical experiments con fi rming these theoretical results, and our theoretical framework provides new tools for assessing the behavior of manifold learning and dictionary learning procedures on a large class of interesting models.",,,,,"Maggioni, Martina Anna A/M-2931-2016","Maggioni, Martina Anna A/0000-0002-6319-8566; Maggioni, Mauro/0000-0003-3258-9297; Minsker, Stanislav/0000-0002-1452-2956",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,2,,,,,,,,,,,,,,,WOS:000391462300001,0
J,"Neu, G; Bartok, G",,,,"Neu, Gergely; Bartok, Gabor",,,Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a sample-efficient alternative for importance weighting for situations where one only has sample access to the probability distribution that generates the observations. Our new method, called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial optimization under semi-bandit feedback, where a learner sequentially selects its actions from a combinatorial decision set so as to minimize its cumulative loss. In particular, we show that the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric Resampling yields the first computationally efficient reduction from offline to online optimization in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that its performance is on par with previous, inefficient solutions. Our main contribution is showing that, despite the relatively large variance induced by the GR procedure, our performance guarantees hold with high probability rather than only in expectation. As a side result, we also improve the best known regret bounds for FPL in online combinatorial optimization with full feedback, closing the perceived performance gap between FPL and exponential weights in this setting.",,,,,"Neu, Gergely/B-4044-2017","Neu, Gergely/0000-0001-6287-3796",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,21,154,,,,,,,,,,,,,,,WOS:000391664700001,0
J,"Wang, YX; Xu, H",,,,"Wang, Yu-Xiang; Xu, Huan",,,Noisy Sparse Subspace Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabeled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,12,,,,,,,,,,,,,,,WOS:000391470700001,0
J,"Wang, YX; Lei, J; Fienberg, SE",,,,"Wang, Yu-Xiang; Lei, Jing; Fienberg, Stephen E.",,,"Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While machine learning has proven to be a powerful data-driven solution to many real life problems, its use in sensitive domains has been limited due to privacy concerns. A popular approach known as differential privacy offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm) under Vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable if an only if there is a private algorithm that asymptotically minimizes the empirical risk (AERM). In contrast, for non-private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. Lastly, we extend some of the results to the more practical (epsilon,delta)-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small delta needs to be.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,183,,,,,,,,,,,,,,,WOS:000391824600001,0
J,"Yuan, K; Ying, BC; Sayed, AH",,,,"Yuan, Kun; Ying, Bicheng; Sayed, Ali H.",,,On the Influence of Momentum Acceleration on Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general strongly convex and smooth risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the adaptive online setting when small constant step-sizes are used to enable continuous adaptation and learning in the presence of persistent gradient noise. From simulations, the equivalence between momentum and standard stochastic gradient methods is also observed for non-differentiable and non-convex problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,192,,,,,,,,,,,,,,,WOS:000391826600001,0
J,"Varando, G; Bielza, C; Larranaga, P",,,,"Varando, Gherardo; Bielza, Concha; Larranaga, Pedro",,,Decision Boundary for Discrete Bayesian Network Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian network classifiers are a powerful machine learning tool. In order to evaluate the expressive power of these models, we compute families of polynomials that sign-represent decision functions induced by Bayesian network classifiers. We prove that those families are linear combinations of products of Lagrange basis polynomials. In absence of V-structures in the predictor sub-graph, we are also able to prove that this family of polynomials does indeed characterize the specific classifier considered. We then use this representation to bound the number of decision functions representable by Bayesian network classifiers with a given structure.",,,,,"Varando, Gherardo/AAJ-9024-2020; Larranaga, Pedro/F-9293-2013; Bielza, Concha/F-9277-2013","Varando, Gherardo/0000-0002-6708-1103; Larranaga, Pedro/0000-0003-0652-9872; Bielza, Concha/0000-0001-7109-2668",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2725,2749,,,,,,,,,,,,,,,,WOS:000369888000013,0
J,"Wang, WW; Lin, L",,,,"Wang, WenWu; Lin, Lu",,,Derivative Estimation Based on Difference Sequence via Locally Weighted Least Squares Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new method is proposed for estimating derivatives of a nonparametric regression function. By applying Taylor expansion technique to a derived symmetric difference sequence, we obtain a sequence of approximate linear regression representation in which the derivative is just the intercept term. Using locally weighted least squares, we estimate the derivative in the linear regression model. The estimator has less bias in both valleys and peaks of the true derivative function. For the special case of a domain with equispaced design points, the asymptotic bias and variance are derived; consistency and asymptotic normality are established. In simulations our estimators have less bias and mean square error than its main competitors, especially second order derivative estimator.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2617,2641,,,,,,,,,,,,,,,,WOS:000369888000010,0
J,"Gyorfi, L; Walk, H",,,,"Gyoerfi, Laszlo; Walk, Harro",,,On the Asymptotic Normality of an Estimate of a Regression Functional,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An estimate of the second moment of the regression function is introduced. Its asymptotic normality is proved such that the asymptotic variance depends neither on the dimension of the observation vector, nor on the smoothness properties of the regression function. The asymptotic variance is given explicitly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1863,1877,,,,,,,,,,,,,,,,WOS:000369887300006,0
J,"Nikulin, V",,,,"Nikulin, Vladimir",,,Strong Consistency of the Prototype Based Clustering in Probabilistic Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we formulate in general terms an approach to prove strong consistency of the Empirical Risk Minimisation inductive principle applied to the prototype or distance based clustering. This approach was motivated by the Divisive Information-Theoretic Feature Clustering model in probabilistic space with Kullback-Leibler divergence, which may be regarded as a special case within the Clustering Minimisation framework.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,775,785,,,,,,,,,,,,,,,,WOS:000369886300005,0
J,"Goussies, NA; Ubalde, S; Mejail, M",,,,"Goussies, Norberto A.; Ubalde, Sebastian; Mejail, Marta",,,Transfer Learning Decision Forests for Gesture Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Decision forests are an increasingly popular tool in computer vision problems. Their advantages include high computational efficiency, state-of-the-art accuracy and multi-class support. In this paper, we present a novel method for transfer learning which uses decision forests, and we apply it to recognize gestures and characters. We introduce two mechanisms into the decision forest framework in order to transfer knowledge from the source tasks to a given target task. The first one is mixed information gain, which is a data-based regularizer. The second one is label propagation, which infers the manifold structure of the feature space. We show that both of them are important to achieve higher accuracy. Our experiments demonstrate improvements over traditional decision forests in the ChaLearn Gesture Challenge and MNIST data set. They also compare favorably against other state-of-the-art classifiers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3667,3690,,,,,,,,,,,,,,,,WOS:000353126200011,0
J,"Van Moffaert, K; Nowe, A",,,,"Van Moffaert, Kristof; Nowe, Ann",,,Multi-Objective Reinforcement Learning using Sets of Pareto Dominating Policies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many real-world problems involve the optimization of multiple, possibly conflicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal difference learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as epsilon-greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is sufficiently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.",,,,,,"Nowe, Ann/0000-0001-6346-4564",,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3483,3512,,,,,,,,,,,,,,,,WOS:000353126200005,0
J,"van der Maaten, L",,,,"van der Maaten, Laurens",,,Accelerating t-SNE using Tree-Based Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper investigates the acceleration of t-SNE an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots using two treebased algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE embeddings in 0(N log N). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3221,3245,,,,,,,,,,,,,,,,WOS:000344638800013,0
J,"Doppa, JR; Fern, A; Tadepalli, P",,,,"Doppa, Janardhan Rao; Fern, Alan; Tadepalli, Prasad",,,Structured Prediction via Out put Space Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we describe a novel approach to automatically defining an effective search space over structured outputs, which is able to leverage the availability of powerful classification learning algorithms. In particular, we define the limited-discrepancy search space and relate the quality of that space to the quality of learned classifiers. We also define a sparse version of the search space to improve the efficiency of our overall approach. Second, we give a generic cost function learning approach that is applicable to a wide range of search procedures. The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains show that a small amount of search in limited discrepancy search space is often sufficient for significantly improving on state-of-the-art structured-prediction performance. We also demonstrate significant speed improvements for our approach using sparse search spaces with little or no loss in accuracy.",,,,,,"Tadepalli, Prasad/0000-0003-2736-3912",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1317,1350,,,,,,,,,,,,,,,,WOS:000338420000004,0
J,"Tan, MK; Tsang, IW; Wang, L",,,,"Tan, Mingkui; Tsang, Ivor W.; Wang, Li",,,Towards Ultrahigh Dimensional Feature Selection for Big Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(10(14)) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training efficiency.",,,,,"Tan, Mingkui/T-2667-2019",,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1371,1429,,,,,,,,,,,,,,,,WOS:000338420000006,0
J,"Geist, M; Scherrer, B",,,,"Geist, Matthieu; Scherrer, Bruno",,,Off-policy Learning With Eligibility Traces: A Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the framework of Markov Decision Processes, we consider linear off-policy learning, that is the problem of learning a linear approximation of the value function of some fixed policy from one trajectory possibly generated by some other policy. We briefly review on-policy learning algorithms of the literature (gradient-based and least-squares-based), adopting a unified algorithmic view. Then, we highlight a systematic approach for adapting them to off-policy learning with eligibility traces. This leads to some known algorithms-off-policy LSTD(lambda), LSPE(lambda), TD(lambda), TDC/GQ(lambda)-and suggests new extensions-off-policy FPKF(lambda), BRM(lambda), gBRM(lambda), GTD2(lambda). We describe a comprehensive algorithmic derivation of all algorithms in a recursive and memory-efficent form, discuss their known convergence properties and illustrate their relative empirical behavior on Garnet problems. Our experiments suggest that the most standard algorithms on and off-policy LSTD(lambda)/LSPE(lambda)-and TD(lambda) if the feature space dimension is too large for a least-squares approach-perform the best.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,289,333,,,,,,,,,,,,,,,,WOS:000335457400010,0
J,"Szabo, Z",,,,"Szabo, Zoltan",,,Information Theoretical Estimators Toolbox,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present ITE (information theoretical estimators) a free and open source, multi-platform, Matlab/Octave toolbox that is capable of estimating many different variants of entropy, mutual information, divergence, association measures, cross quantities, and kernels on distributions. Thanks to its highly modular design, ITE supports additionally (i) the combinations of the estimation techniques, (ii) the easy construction and embedding of novel information theoretical estimators, and (iii) their immediate application in information theoretical optimization problems. ITE also includes a prototype application in a central problem class of signal processing, independent subspace analysis and its extensions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,283,287,,,,,,,,,,,,,,,,WOS:000335457400009,0
J,"Gilad-Bachrach, R; Burgs, CJC",,,,"Gilad-Bachrach, Ran; Burgs, Christopher J. C.",,,Classifier Selection using Predicate Depth,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Typically, one approaches a supervised machine learning problem by writing down an objective function and finding a hypothesis that minimizes it. This is equivalent to finding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by defining a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efficient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we define: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work.",,,,,"Gilad-Bachrach, Ran/ABG-6990-2020","Gilad-Bachrach, Ran/0000-0002-4001-8307",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3591,3618,,,,,,,,,,,,,,,,WOS:000335457100004,0
J,"He, YJ; She, YY; Wu, DP",,,,"He, Yuejia; She, Yiyuan; Wu, Dapeng",,,Stationary-Sparse Causality Network Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identification. The main purpose of this paper is to tackle these challenging issues. We propose the S-2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efficient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efficiency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems.",,,,,,"Wu, Dapeng/0000-0003-1755-0183",,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3073,3104,,,,,,,,,,,,,,,,WOS:000328603600005,0
J,"Salleb-Aouissi, A; Vrain, C; Nortet, C; Kong, XR; Rathod, V; Cassard, D",,,,"Salleb-Aouissi, Ansaf; Vrain, Christel; Nortet, Cyril; Kong, Xiangrong; Rathod, Vivek; Cassard, Daniel",,,QuantMiner for Mining Quantitative Association Rules,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose QUANTMINER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers good intervals in association rules by optimizing both the support and the confidence. The experiments on real and artificial databases have shown the usefulness of QUANTMINER as an interactive, exploratory data mining tool.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3153,3157,,,,,,,,,,,,,,,,WOS:000328603600008,0
J,"Mairal, J; Yu, B",,,,"Mairal, Julien; Yu, Bin",,,Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called path coding penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2449,2485,,,,,,,,,,,,,,,,WOS:000324799600009,0
J,"Su, XG; Kang, J; Fan, JJ; Levine, RA; Yan, X",,,,"Su, Xiaogang; Kang, Joseph; Fan, Juanjuan; Levine, Richard A.; Yan, Xin",,,Facilitating Score and Causal Inference Trees for Large Observational Studies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Assessing treatment effects in observational studies is a multifaceted problem that not only involves heterogeneous mechanisms of how the treatment or cause is exposed to subjects, known as propensity, but also differential causal effects across sub-populations. We introduce a concept termed the facilitating score to account for both the confounding and interacting impacts of covariates on the treatment effect. Several approaches for estimating the facilitating score are discussed. In particular, we put forward a machine learning method, called causal inference tree (CIT), to provide a piecewise constant approximation of the facilitating score. With interpretable rules, CIT splits data in such a way that both the propensity and the treatment effect become more homogeneous within each resultant partition. Causal inference at different levels can be made on the basis of CIT. Together with an aggregated grouping procedure, CIT stratifies data into strata where causal effects can be conveniently assessed within each. Besides, a feasible way of predicting individual causal effects (ICE) is made available by aggregating ensemble CIT models. Both the stratified results and the estimated ICE provide an assessment of heterogeneity of causal effects and can be integrated for estimating the average causal effect (ACE). Mean square consistency of CIT is also established. We evaluate the performance of proposed methods with simulations and illustrate their use with the NSW data in Dehejia and Wahba (1999) where the objective is to assess the impact of a labor training program, the National Supported Work (NSW) demonstration, on post-intervention earnings.",,,,,"Levine, Richard A./AAA-5606-2021; Su, Xiaogang/B-7340-2009",,,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2955,2994,,,,,,,,,,,,,,,,WOS:000313200000005,0
J,"Kim, J; Scott, CD",,,,"Kim, JooSeuk; Scott, Clayton D.",,,Robust Kernel Density Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2529,2565,,,,,,,,,,,,,,,,WOS:000309580600002,0
J,"Cooper, H; Ong, EJ; Pugeault, N; Bowden, R",,,,"Cooper, Helen; Ong, Eng-Jon; Pugeault, Nicolas; Bowden, Richard",,,Sign Language Recognition using Sub-Units,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classifier; here, two options are presented. The first uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54% achieved by the Markov Chains to 76%.",,,,,"Bowden, Richard/AAF-8283-2019; Pugeault, Nicolas/AAF-9768-2019; Pugeault, Nicolas/I-1873-2015","Bowden, Richard/0000-0003-3285-8020; Pugeault, Nicolas/0000-0002-3455-6280; Pugeault, Nicolas/0000-0002-3455-6280",,,,,,,,,,,,,1532-4435,,,,,JUL,2012,13,,,,,,2205,2231,,,,,,,,,,,,,,,,WOS:000307496000004,0
J,"Zwiernik, P",,,,"Zwiernik, Piotr",,,An Asymptotic Behaviour of the Marginal Likelihood for General Markov Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisfied in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold.,,,,,"Zwiernik, Piotr/H-4107-2015","Zwiernik, Piotr/0000-0003-3431-131X",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3283,3310,,,,,,,,,,,,,,,,WOS:000298103700008,0
J,"Hazan, E; Kale, S",,,,"Hazan, Elad; Kale, Satyen",,,Better Algorithms for Benign Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The online multi-armed bandit problem and its generalizations are repeated decision making problems, where the goal is to select one of several possible decisions in every round, and incur a cost associated with the decision, in such a way that the total cost incurred over all iterations is close to the cost of the best fixed decision in hindsight. The difference in these costs is known as the regret of the algorithm. The term bandit refers to the setting where one only obtains the cost of the decision used in a given iteration and no other information. A very general form of this problem is the non-stochastic bandit linear optimization problem, where the set of decisions is a convex set in some Euclidean space, and the cost functions are linear. Only recently an efficient algorithm attaining (O) over tilde (root T) regret was discovered in this setting. In this paper we propose a new algorithm for the bandit linear optimization problem which obtains a tighter regret bound of (O) over tilde (root Q), where Q is the total variation in the cost functions. This regret bound, previously conjectured to hold in the full information case, shows that it is possible to incur much less regret in a slowly changing environment even in the bandit setting. Our algorithm is efficient and applies several new ideas to bandit optimization such as reservoir sampling.",,,,,,"Hazan, Elad/0000-0002-1566-3216",,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1287,1311,,,,,,,,,,,,,,,,WOS:000290096100004,0
J,"Choi, JD; Kim, KE",,,,"Choi, Jaedeug; Kim, Kee-Eung",,,Inverse Reinforcement Learning in Partially Observable Environments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,691,730,,,,,,,,,,,,,,,,WOS:000289635000002,0
J,"Watanabe, S",,,,"Watanabe, Sumio",,,Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2 lambda/n, where lambda is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.",,,,,"Watanabe, Sumio/C-3880-2015; Watanabe, Sumio/M-7370-2019","Watanabe, Sumio/0000-0001-8341-5639; ",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3571,3594,,,,,,,,,,,,,,,,WOS:000286637200010,0
J,"Songsiri, J; Vandenberghe, L",,,,"Songsiri, Jitkomut; Vandenberghe, Lieven",,,Topology Selection in Graphical Models of Autoregressive Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,An algorithm is presented for topology selection in graphical models of autoregressive Gaussian time series. The graph topology of the model represents the sparsity pattern of the inverse spectrum of the time series and characterizes conditional independence relations between the variables. The method proposed in the paper is based on an l1-type nonsmooth regularization of the conditional maximum likelihood estimation problem. We show that this reduces to a convex optimization problem and describe a large-scale algorithm that solves the dual problem via the gradient projection method. Results of experiments with randomly generated and real data sets are also included.,,,,,"Songsiri, Jitkomut/AAI-2926-2021",,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2671,2705,,,,,,,,,,,,,,,,WOS:000284040000004,0
J,"Sun, SL; Shawe-Taylor, J",,,,"Sun, Shiliang; Shawe-Taylor, John",,,Sparse Semi-supervised Learning Using Conjugate Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a general framework for sparse semi-supervised learning, which concerns using a small portion of unlabeled data and a few labeled data to represent target functions and thus has the merit of accelerating function evaluations when predicting the output of a new example. This framework makes use of Fenchel-Legendre conjugates to rewrite a convex insensitive loss involving a regularization with unlabeled data, and is applicable to a family of semi-supervised learning methods such as multi-view co-regularized least squares and single-view Laplacian support vector machines (SVMs). As an instantiation of this framework, we propose sparse multi-view SVMs which use a squared epsilon-insensitive loss. The resultant optimization is an inf-sup problem and the optimal solutions have arguably saddle-point properties. We present a globally optimal iterative algorithm to optimize the problem. We give the margin bound on the generalization error of the sparse multi-view SVMs, and derive the empirical Rademacher complexity for the induced function class. Experiments on artificial and real-world data show their effectiveness. We further give a sequential training approach to show their possibility and potential for uses in large-scale problems and provide encouraging experimental results indicating the efficacy of the margin bound and empirical Rademacher complexity on characterizing the roles of unlabeled data for semi-supervised learning.",,,,,,"Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,SEP,2010,11,,,,,,2423,2455,,,,,,,,,,,,,,,,WOS:000282523400002,0
J,"Raskutti, G; Wainwright, MJ; Yu, B",,,,"Raskutti, Garvesh; Wainwright, Martin J.; Yu, Bin",,,Restricted Eigenvalue Properties for Correlated Gaussian Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Methods based on l(1)-relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisfies the restricted nullspace property, and (2) the squared l(2)-error of a Lasso estimate decays at the minimax optimal rate klogp/n, where k is the sparsity of the p-dimensional regression problem with additive Gaussian noise, whenever the design satisfies a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisfies these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisfied when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on l(1)-relaxations to a much broader class of problems than the case of completely independent or unitary designs.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2241,2259,,,,,,,,,,,,,,,,WOS:000282523300008,0
J,"Xu, YH; Fern, A; Yoon, S",,,,"Xu, Yuehua; Fern, Alan; Yoon, Sungwook",,,Learning Linear Ranking Functions for Beam Search with Application to Planning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Beam search is commonly used to help maintain tractability in large search spaces at the expense of completeness and optimality. Here we study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search, and hence gain computational efficiency without seriously sacrificing optimality. In this paper, we develop theoretical aspects of this learning problem and investigate the application of this framework to learning in the context of automated planning. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem, giving insight into the problem structure. Next, we analyze the convergence of recently proposed and modified online learning algorithms, where we introduce several notions of problem margin that imply convergence for the various algorithms. Finally, we present empirical results in automated planning, where ranking functions are learned to guide beam search in a number of benchmark planning domains. The results show that our approach is often able to outperform an existing state-of-the-art planning heuristic as well as a recent approach to learning such heuristics.",,,,,"Xu, yue/HGE-1737-2022",,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1571,1610,,,,,,,,,,,,,,,,WOS:000270825000010,0
J,"Paquet, U; Winther, O; Opper, M",,,,"Paquet, Ulrich; Winther, Ole; Opper, Manfred",,,Perturbation Corrections in Approximate Inference: Mixture Modelling Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian inference is intractable for many interesting models, making deterministic algorithms for approximate inference highly desirable. Unlike stochastic methods, which are exact in the limit, the accuracy of these approaches cannot be reasonably judged. In this paper we show how low order perturbation corrections to an expectation-consistent (EC) approximation can provide the necessary tools to ameliorate inference accuracy, and to give an indication of the quality of approximation without having to resort to Monte Carlo methods. Further comparisons are given with variational Bayes and parallel tempering (PT) combined with thermodynamic integration on a Gaussian mixture model. To obtain practical results we further generalize PT to temper from arbitrary distributions rather than a prior in Bayesian inference.",,,,,,"Winther, Ole/0000-0002-1966-3205",,,,,,,,,,,,,1532-4435,,,,,JUN,2009,10,,,,,,1263,1304,,,,,,,,,,,,,,,,WOS:000270824900003,0
J,"Silva, R; Ghahramani, Z",,,,"Silva, Ricardo; Ghahramani, Zoubin",,,The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Directed acyclic graphs (DAGs) have been widely used as a representation of conditional independence in machine learning and statistics. Moreover, hidden or latent variables are often an important component of graphical models. However, DAG models suffer from an important limitation: the family of DAGs is not closed under marginalization of hidden variables. This means that in general we cannot use a DAG to represent the independencies over a subset of variables in a larger DAG. Directed mixed graphs (DMGs) are a representation that includes DAGs as a special case, and overcomes this limitation. This paper introduces algorithms for performing Bayesian inference in Gaussian and probit DMG models. An important requirement for inference is the specification of the distribution over parameters of the models. We introduce a new distribution for covariance matrices of Gaussian DMGs. We discuss and illustrate how several Bayesian machine learning tasks can benefit from the principle presented here: the power to model dependencies that are generated from hidden variables, but without necessarily modeling such variables explicitly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2009,10,,,,,,1187,1238,,,,,,,,,,,,,,,,WOS:000270824900001,0
J,"Kontorovich, L; Nadler, B",,,,"Kontorovich, Leonid (Aryeh); Nadler, Boaz",,,Universal Kernel-Based Learning with Applications to Regular Languages,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel framework for supervised learning of discrete concepts. Since the 1970's, the standard computational primitive has been to find the most consistent hypothesis in a given complexity class. In contrast, in this paper we propose a new basic operation: for each pair of input instances, count how many concepts of bounded complexity contain both of them. Our approach maps instances to a Hilbert space, whose metric is induced by a universal kernel coinciding with our computational primitive, and identifies concepts with half-spaces. We prove that all concepts are linearly separable under this mapping. Hence, given a labeled sample and an oracle for evaluating the universal kernel, we can efficiently compute a linear classifier (via SVM, for example) and use margin bounds to control its generalization error. Even though exact evaluation of the universal kernel may be infeasible, in various natural situations it is efficiently approximable. Though our approach is general, our main application is to regular languages. Our approach presents a substantial departure from current learning paradigms and in particular yields a novel method for learning this fundamental concept class. Unlike existing techniques, we make no structural assumptions on the corresponding unknown automata, the string distribution or the completeness of the training set. Instead, given a labeled sample our algorithm outputs a classifier with guaranteed distribution-free generalization bounds; to our knowledge, the proposed framework is the only one capable of achieving the latter. Along the way, we touch upon several fundamental questions in complexity, automata, and machine learning.",,,,,"Nadler, Boaz/C-7217-2008; Kontorovich, Aryeh/AAB-4744-2020; KONTOROVICH, ARYEH/F-2375-2012; Nadler, Boaz/N-3234-2019","Kontorovich, Aryeh/0000-0001-8038-8671; Nadler, Boaz/0000-0002-9777-4576",,,,,,,,,,,,,1532-4435,,,,,MAY,2009,10,,,,,,1095,1129,,,,,,,,,,,,,,,,WOS:000270824800003,0
J,"Novak, PK; Lavrac, N; Webb, GI",,,,"Novak, Petra Kralj; Lavrac, Nada; Webb, Geoffrey I.",,,"Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task definitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a unified terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods.",,,,,"Novak, Petra Kralj/AAE-5852-2020; Webb, Geoff i/R-9967-2017; Webb, Geoffrey/Q-7270-2019","Webb, Geoff i/0000-0001-9963-5169; Webb, Geoffrey/0000-0001-9963-5169; Kralj Novak, Petra/0000-0003-3385-6430",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,377,403,,,,,,,,,,,,,,,,WOS:000270824200010,0
J,"Csaji, BC; Monostori, L",,,,"Csaji, Balazs Csanad; Monostori, Laszlo",,,Value Function Based Reinforcement Learning in Changing Markovian Environments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (epsilon, delta)-MDPs is introduced, which is a generalization of MDPs and epsilon-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented.",,,,,"Csanad, Csaji Balazs/AAB-5771-2020","Monostori, Laszlo/0000-0001-8692-8640",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1679,1709,,,,,,,,,,,,,,,,WOS:000262636800002,0
J,"Chechik, G; Heitz, G; Elidan, G; Abbeel, P; Koller, D",,,,"Chechik, Gal; Heitz, Geremy; Elidan, Gal; Abbeel, Pieter; Koller, Daphne",,,Max-margin classification of data with absent features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning classifiers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to first complete their unknown values, and then use a standard classification procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classified directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efficiently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images.",,,,,"Elidan, GAl/A-7380-2009","Elidan, Gal/0000-0001-5365-599X",,,,,,,,,,,,,1532-4435,,,,,JAN,2008,9,,,,,,1,21,,,,,,,,,,,,,,,,WOS:000256641400001,0
J,"Li, P; Hastie, TJ; Church, KW",,,,"Li, Ping; Hastie, Trevor J.; Church, Kenneth W.",,,Nonlinear estimators and tail bounds for dimension reduction in l(1) using Cauchy random projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For(1) dimension reduction in the l(1) norm, the method of Cauchy random projections multiplies the original data matrix A epsilon R-nxD with a random matrix R epsilon R-Dxk (k<<D) whose entries are i.i.d. samples of the standard Cauchy C (0, 1). Because of the impossibility result, one can not hope to recover the pairwise l(1) distances in A from B = AxR epsilon R-nxk, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O (logn/epsilon(2)) suffices with the constants explicitly given. Asymptotically (as k ->infinity), both the sample median and the geometric mean estimators are about 80% efficient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian.",,,,,"Church, Kenneth/AAV-9667-2021; Church, Kenneth/G-3167-2010; Church, Kenneth/GYR-1624-2022","Church, Kenneth/0000-0001-8378-6069; Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2497,2532,,,,,,,,,,,,,,,,WOS:000252744800010,0
J,"Chariatis, A",,,,"Chariatis, Aggelos",,,Very fast online learning of highly non linear problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The experimental investigation on the efficient learning of highly non-linear problems by online training, using ordinary feed forward neural networks and stochastic gradient descent on the errors computed by back-propagation, gives evidence that the most crucial factors for efficient training are the hidden units' differentiation, the attenuation of the hidden units' interference and the selective attention on the parts of the problems where the approximation error remains high. In this report, we present global and local selective attention techniques and a new hybrid activation function that enables the hidden units to acquire individual receptive fields which may be global or local depending on the problem's local complexities. The presented techniques enable very efficient training on complex classification problems with embedded subproblems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,2017,2045,,,,,,,,,,,,,,,,WOS:000252744600002,0
J,"Banerjee, A; Dhillon, I; Ghosh, J; Merugu, S; Modha, DS",,,,"Banerjee, Arindam; Dhillon, Inderjit; Ghosh, Joydeep; Merugu, Srujana; Modha, Dharmendra S.",,,A generalized maximum entropy approach to Bregman co-clustering and matrix approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Co-clustering, or simultaneous clustering of rows and columns of a two-dimensional data matrix, is rapidly becoming a powerful data analysis technique. Co-clustering has enjoyed wide success in varied application domains such as text clustering, gene-microarray analysis, natural language processing and image, speech and video analysis. In this paper, we introduce a partitional co-clustering formulation that is driven by the search for a good matrix approximation-every co-clustering is associated with an approximation of the original data matrix and the quality of co-clustering is determined by the approximation error. We allow the approximation error to be measured using a large class of loss functions called Bregman divergences that include squared Euclidean distance and KL-divergence as special cases. In addition, we permit multiple structurally different co-clustering schemes that preserve various linear statistics of the original data matrix. To accomplish the above tasks, we introduce a new minimum Bregman information (MBI) principle that simultaneously generalizes the maximum entropy and standard least squares principles, and leads to a matrix approximation that is optimal among all generalized additive models in a certain natural parameter space. Analysis based on this principle yields an elegant meta algorithm, special cases of which include most previously known alternate minimization based clustering algorithms such as kmeans and co-clustering algorithms such as information theoretic (Dhillon et al., 2003b) and minimum sum-squared residue co-clustering (Cho et al., 2004). To demonstrate the generality and flexibility of our co-clustering framework, we provide examples and empirical evidence on a variety of problem domains and also describe novel co-clustering applications such as missing value prediction and compression of categorical data matrices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1919,1986,,,,,,,,,,,,,,,,WOS:000252744400009,0
J,"Clark, A; Eyraud, R",,,,"Clark, Alexander; Eyraud, Remi",,,Polynomial identification in the limit of substitutable context-free languages,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper formalises the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free languages. We show that there is a polynomial characteristic set, and thus prove polynomial identification in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference. It transpires that it is not necessary to identify constituents in order to learn a context-free language-it is sufficient to identify the syntactic congruence, and the operations of the syntactic monoid can be converted into a context-free grammar. We also discuss modifications to the algorithm that produces a reduction system rather than a context-free grammar, that will be much more compact. We discuss the relationship to Angluin's notion of reversibility for regular languages. We also demonstrate that an implementation of this algorithm is capable of learning a classic example of structure dependent syntax in English: this constitutes a refutation of an argument that has been used in support of nativist theories of language.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1725,1745,,,,,,,,,,,,,,,,WOS:000252744400002,0
J,"Johnson, R; Zhang, T",,,,"Johnson, Rie; Zhang, Tong",,,On the effectiveness of laplacian normalization for graph semi-supervised learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Specifically, by introducing a definition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary significantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments confirm the superiority of the normalization scheme motivated by learning theory on artificial and real-world data sets.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1489,1517,,,,,,,,,,,,,,,,WOS:000249353700005,0
J,"Takeuchi, I; Le, QV; Sears, TD; Smola, AJ",,,,"Takeuchi, Ichiro; Le, Quoc V.; Sears, Timothy D.; Smola, Alexander J.",,,Nonparametric quantile estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In regression, the desired estimate of y vertical bar x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, tau, of y vertical bar x, will be below the estimate. For tau = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1231,1264,,,,,,,,,,,,,,,,WOS:000245388800004,0
J,"Blanchard, G; Kawanabe, M; Sugiyama, M; Spokoiny, V; Muller, KR",,,,"Blanchard, G; Kawanabe, M; Sugiyama, M; Spokoiny, V; Muller, KR",,,In search of non-Gaussian components of a high-dimensional distribution,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finding non-Gaussian components of high-dimensional data is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the '' non-Gaussian subspace '' within a very general semi-parametric framework. Our proposed method, called NGCA (non-Gaussian component analysis), is based on a linear operator which, to any arbitrary nonlinear (smooth) function, associates a vector belonging to the low dimensional non-Gaussian target subspace, up to an estimation error. By applying this operator to a family of different nonlinear functions, one obtains a family of different vectors lying in a vicinity of the target space. As a final step, the target space itself is estimated by applying PCA to this family of vectors. We show that this procedure is consistent in the sense that the estimaton error tends to zero at a parametric rate, uniformly over the family, Numerical examples demonstrate the usefulness of our method.",,,,,"Spokoiny, Vladimir/AAF-4942-2021; Muller, Klaus R/C-3196-2013; Mueller, Klaus-Robert/Y-3547-2019; Sugiyama, Masashi/AEO-1176-2022; Spokoiny, Vladimir G./L-5441-2015","Spokoiny, Vladimir/0000-0002-2040-3427; Mueller, Klaus-Robert/0000-0002-3861-7685; Sugiyama, Masashi/0000-0001-6658-6743; Spokoiny, Vladimir G./0000-0002-2040-3427",,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,247,282,,,,,,,,,,,,,,,,WOS:000236331700003,0
J,"Lawrence, N",,,,"Lawrence, N",,,Probabilistic non-linear principal component analysis with Gaussian process latent variable models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be nonlinearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artificially generated data sets.",,,,,,"Lawrence, Neil/0000-0001-9258-1030",,,,,,,,,,,,,1532-4435,,,,,NOV,2005,6,,,,,,1783,1816,,,,,,,,,,,,,,,,WOS:000236330700002,0
J,"Eibl, G; Pfeiffer, KP",,,,"Eibl, G; Pfeiffer, KP",,,Multiclass boosting for weak classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"AdaBoost.M2 is a boosting algorithm designed for multiclass problems with weak base classifiers. The algorithm is designed to minimize a very loose bound on the training error. We propose two alternative boosting algorithms which also minimize bounds on performance measures. These performance measures are not as strongly connected to the expected error as the training error, but the derived bounds are tighter than the bound on the training error of AdaBoost.M2. In experiments the methods have roughly the same performance in minimizing the training and test error rates. The new algorithms have the advantage that the base classifier should minimize the confidence-rated error, whereas for AdaBoost.M2 the base classifier should minimize the pseudo-loss. This makes them more easily applicable to already existing base classifiers. The new algorithms also tend to converge faster than AdaBoost.M2.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2005,6,,,,,,189,210,,,,,,,,,,,,,,,,WOS:000236329000001,0
J,"Parra, L; Sajda, P",,,,"Parra, L; Sajda, P",,,Blind source separation via generalized Eigenvalue decomposition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this short note we highlight the fact that linear blind source separation can be formulated as a generalized eigenvalue decomposition under the assumptions of non-Gaussian, non-stationary, or non-white independent sources. The solution for the unmixing matrix is given by the generalized eigenvectors that simultaneously diagonalize the covariance matrix of the observations and an additional symmetric matrix whose form depends upon the particular assumptions. The method critically determines the mixture coefficients and is therefore not robust to estimation errors. However it provides a rather general and unified solution that summarizes the conditions for successful blind source separation. To demonstrate the method, which can be implemented in two lines of matlab code, we present results for artificial mixtures of speech and real mixtures of electroencephalography (EEG) data, showing that the same sources are recovered under the various assumptions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1261,1269,,,,,,,,,,,,,,,,WOS:000224808300005,0
J,"Mannor, S; Meir, R; Zhang, T",,,,"Mannor, S; Meir, R; Zhang, T",,,"Greedy algorithms for classification - Consistency, convergence rates, and adaptivity",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many regression and classification algorithms proposed over the years can be described as greedy procedures for the stagewise minimization of an appropriate cost function. Some examples include additive models, matching pursuit, and boosting. In this work we focus on the classification problem, for which many recent algorithms have been proposed and applied successfully. For a specific regularized form of greedy stagewise optimization, we prove consistency of the approach under rather general conditions. Focusing on specific classes of problems we provide conditions under which our greedy procedure achieves the (nearly) minimax. rate of convergence, implying that the procedure cannot be improved in a worst case setting. We also construct a fully adaptive procedure, which, without knowing the smoothness parameter of the decision boundary, converges at the same rate as if the smoothness parameter were known.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,713,742,,10.1162/153244304773936108,0,,,,,,,,,,,,,WOS:000221345700012,0
J,"Klautau, A; Jevtic, N; Orlitsky, A",,,,"Klautau, A; Jevtic, N; Orlitsky, A",,,On nearest-neighbor error-correcting output codes with application to all-pairs multiclass support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common way of constructing a multiclass classifier is by combining the outputs of several binary ones, according to an error-correcting output code (ECOC) scheme. The combination is typically done via a simple nearest-neighbor rule that finds the class that is closest in some sense to the outputs of the binary classifiers. For these nearest-neighbor ECOCs, we improve existing bounds on the error rate of the multiclass classifier given the average binary distance. The new bounds provide insight into the one-versus-rest and all-pairs matrices, which are compared through experiments with standard datasets. The results also show why elimination (also known as DAGSVM) and Hamming decoding often achieve the same accuracy.",,,,,"Klautau, Aldebaro/A-8696-2008","Klautau, Aldebaro/0000-0001-7773-2080",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,1,15,,10.1162/153244304322765612,0,,,,,,,,,,,,,WOS:000221043500001,0
J,"Dooly, DR; Zhang, Q; Goldman, SA; Amar, RA",,,,"Dooly, DR; Zhang, Q; Goldman, SA; Amar, RA",,,Multiple-instance learning of real-valued data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MASSACHUSETTS",,,,,"The multiple-instance learning model has received much attention recently with a primary application area being that of drug activity prediction. Most prior work on multiple-instance learning has been for concept learning, yet for drug activity prediction, the label is a real-valued affinity measurement giving the binding strength. We present extensions of k-nearest neighbors (k-NN), Citation-kNN, and the diverse density algorithm for the real-valued setting and study their performance on Boolean and real-valued data. We also provide a method for generating chemically realistic artificial data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,651,678,,10.1162/jmlr.2003.3.4-5.651,0,,,,,,,,,,,,,WOS:000184926200003,0
J,"Antos, A; Kegl, B; Linder, T; Lugosi, G",,,,"Antos, A; Kegl, B; Linder, T; Lugosi, G",,,Data-dependent margin-based generalization bounds for classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,14th Annual Conference on Computational Learning Theory (COLT 2001)/5th European Conference on Computational Learning Theory (EuroCOLT 2001),"JUL 16-19, 2001","AMSTERDAM, NETHERLANDS","Natl Res Inst Math & Comp Sci,Amsterdam Hist Museum,Netherlands Org Sci Res",,,,"We derive new margin-based inequalities for the probability of error of classifiers. The main feature of these bounds is that they can be calculated using the training data and therefore may be effectively used for model selection purposes. In particular, the bounds involve empirical complexities measured on the training data (such as the empirical fat-shattering dimension) as opposed to their worst-case counterparts traditionally used in such analyses. Also, our bounds appear to be sharper and more general than recent results involving empirical complexity measures. In addition, we develop an alternative data-based bound for the generalization error of classes of convex combinations of classifiers involving an empirical complexity measure that is easier to compute than the empirical covering number or fat-shattering dimension. We also show examples of efficient computation of the new bounds.",,,,,,"Lugosi, Gabor/0000-0003-1614-5901",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,73,98,,10.1162/153244303768966111,0,,,,,,,,,,,,,WOS:000181462700004,0
J,"Bshouty, NH; Eiron, N",,,,"Bshouty, NH; Eiron, N",,,Learning monotone DNF from a teacher that almost does not answer membership queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present results concerning the learning of Monotone DNF (MDNF) from Incomplete Membership Queries and Equivalence Queries. Our main result is a new algorithm that allows efficient learning of MDNF using Equivalence Queries and Incomplete Membership Queries with probability of p = 1 - 1/poly (n, t) of failing. Our algorithm is expected to make o((tn/1-p)(2)) queries, when learning a MDNF formula with t terms over n variables. Note that this is polynomial for any failure probability p = 1 - 1/poly(n, t). The algorithm's running time is also polynomial in I, n, and 1/(1 - p). In a sense this is the best possible, as learning with p = 1 - 1/w(poly(n, t)) would imply learning MDNF, and thus also DNF, from equivalence queries alone. 1",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,49,57,,10.1162/153244303768966094,0,,,,,,,,,,,,,WOS:000181462700002,0
J,"Osborne, M",,,,"Osborne, M",,,Shallow parsing using noisy and non-stationary training material,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Shallow parsers are usually assumed to be trained on noise-free material, drawn from the same distribution as the testing material. However, when either the training set is noisy or else drawn from a different distributions, performance may be degraded. Using the parsed Wall Street Journal, we investigate the performance of four shallow parsers (maximum entropy; memory-based learning, N-grams and ensemble learning) trained using various types of artificially noisy material. Our first set of results show that shallow parsers are surprisingly robust to synthetic noise, with performance gradually decreasing as the rate of noise increases. Further results show that no single shallow parser performs best in all noise situations. Final results show that simple, parser-specific extensions can improve noise-tolerance. Our second set of results addresses the question of whether naturally occurring disfluencies undermines performance more than does a change in distribution. Results using the parsed Switchboard corpus suggest that, although naturally occurring disfluencies might harm performance, differences in distribution between the training set and the testing set are more significant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,695,718,,10.1162/153244302320884597,0,,,,,,,,,,,,,WOS:000179542800007,0
J,"Chickering, DM",,,,"Chickering, DM",,,Learning equivalence classes of Bayesian-network structures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Two Bayesian-network structures are said to be equivalent if the set of distributions that can be represented with one of those structures is identical to the set of distributions that can be represented with the other. Many scoring criteria that are used to learn Bayesian-network structures from data are score equivalent; that is, these criteria do not distinguish among networks that are equivalent. In this paper, we consider using a score equivalent criterion in conjunction with a heuristic search algorithm to perform model selection or model averaging. We argue that it is often appropriate to search among equivalence classes of network structures as opposed to the more common approach of searching among individual Bayesian-network structures. We describe a convenient graphical representation for an equivalence class of structures, and introduce a set of operators that can be applied to that representation by a search algorithm to move among equivalence classes. We show that our equivalence-class operators can be scored locally, and thus share the computational efficiency of traditional operators defined for individual structures. We show experimentally that a greedy model-selection algorithm using our representation yields slightly higher-scoring structures than the traditional approach without any additional time overhead, and we argue that more sophisticated search algorithms are likely to benefit much more.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,445,498,,10.1162/153244302760200696,0,,,,,,,,,,,,,WOS:000178101500006,0
J,"Manevitz, LM; Yousef, M",,,,"Manevitz, LM; Yousef, M",,,One-class SVMs for document classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, COLORADO",,,,,"We implemented versions of the SVM appropriate for one-class classification in the context of information retrieval. The experiments were conducted on the standard Reuters data set. For the SVM implementation we used both a version of Scholkopf et al. and a somewhat different version of one-class SVM based on identifying outlier data as representative of the second-class. We report on experiments with different kernels for both of these implementations and with different representations of the data, including binary vectors, tf-idf representation and a modification called Hadamard representation. Then we compared it with one-class versions of the algorithms prototype (Rocchio), nearest neighbor, naive Bayes, and finally a natural one-class neural network classification method based on bottleneck compression generated filters. The SVM approach as represented by Scholk-opf was superior to all the methods except the neural network one, where it was, although occasionally worse, essentially comparable. However, the SVM methods turned out to be quite sensitive to the choice of representation and kernel in ways which are not well understood; therefore, for the time being leaving the neural network approach as the most robust.",,,,,"Manevitz, Larry Michael/ACA-4453-2022; Manevitz, Larry Michael/AAG-3353-2022",,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,139,154,,10.1162/15324430260185574,0,,,,,,,,,,,,,WOS:000176055300004,0
J,"Bunea, F; Strimas-Mackey, S; Wegkamp, M",,,,"Bunea, Florentina; Strimas-Mackey, Seth; Wegkamp, Marten",,,Interpolating Predictors in High-Dimensional Factor Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work studies finite-sample properties of the risk of the minimum-norm interpolating predictor in high-dimensional regression models. If the effective rank of the covariance matrix sigma of the p regression features is much larger than the sample size n, we show that the min-norm interpolating predictor is not desirable, as its risk approaches the risk of trivially predicting the response by 0. However, our detailed finite-sample analysis reveals, surprisingly, that this behavior is not present when the regression response and the features are jointly low-dimensional, following a widely used factor regression model. Within this popular model class, and when the effective rank of sigma is smaller than n, while still allowing for p >> n, both the bias and the variance terms of the excess risk can be controlled, and the risk of the minimum-norm interpolating predictor approaches optimal benchmarks. Moreover, through a detailed analysis of the bias term, we exhibit model classes under which our upper bound on the excess risk approaches zero, while the corresponding upper bound in the recent work Bartlett et al. (2020) diverges. Furthermore, we show that the minimum-norm interpolating predictor analyzed under the factor regression model, despite being model-agnostic and devoid of tuning parameters, can have similar risk to predictors based on principal components regression and ridge regression, and can improve over LASSO based predictors, in the high-dimensional regime.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752380600001,0
J,"Levin, K; Lodhia, A; Levina, E",,,,"Levin, Keith; Lodhia, Asad; Levina, Elizaveta",,,Recovering shared structure from multiple networks with unknown edge distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In increasingly many settings, data sets consist of multiple samples from a population of networks, with vertices aligned across networks; for example, brain connectivity networks in neuroscience. We consider the setting where the observed networks have a shared expectation, but may differ in the noise structure on their edges. Our approach exploits the shared mean structure to denoise edge-level measurements of the observed networks and estimate the underlying population-level parameters. We also explore the extent to which edge-level errors influence estimation and downstream inference. In the process, we establish a finite-sample concentration inequality for the low-rank eigenvalue truncation of a random weighted adjacency matrix, which may be of independent interest. The proposed approach is illustrated on synthetic networks and on data from an fMRI study of schizophrenia.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,48,,,,,,,,,,,,,,,,WOS:000752296300001,0
J,"Lv, SG; Lian, H",,,,"Lv, Shaogao; Lian, Heng",,,Debiased Distributed Learning for Sparse Partial Linear Models in High Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Although various distributed machine learning schemes have been proposed recently for purely linear models and fully nonparametric models, little attention has been paid to distributed optimization for semi-parametric models with multiple structures (e.g. sparsity, linearity and nonlinearity). To address these issues, the current paper proposes a new communication-efficient distributed learning algorithm for sparse partially linear models with an increasing number of features. The proposed method is based on the classical divide and conquer strategy for handling big data and the computation on each subsample consists of a debiased estimation of the doubly regularized least squares approach. With the proposed method, we theoretically prove that our global parametric estimator can achieve the optimal parametric rate in our semi-parametric model given an appropriate partition on the total data. Specifically, the choice of data partition relies on the underlying smoothness of the nonparametric component, and it is adaptive to the sparsity parameter. Finally, some simulated experiments are carried out to illustrate the empirical performances of our debiased technique under the distributed setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752299100001,0
J,"O'Connor, K; McGoff, K; Nobel, AB",,,,"O'Connor, Kevin; McGoff, Kevin; Nobel, Andrew B.",,,Optimal Transport for Stationary Markov Chains via Policy Iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the optimal transport problem for pairs of stationary finite-state Markov chains, with an emphasis on the computation of optimal transition couplings. Transition couplings are a constrained family of transport plans that capture the dynamics of Markov chains. Solutions of the optimal transition coupling (OTC) problem correspond to alignments of the two chains that minimize long-term average cost. We establish a connection between the OTC problem and Markov decision processes, and show that solutions of the OTC problem can be obtained via an adaptation of policy iteration. For settings with large state spaces, we develop a fast approximate algorithm based on an entropy-regularized version of the OTC problem, and provide bounds on its per-iteration complexity. We establish a stability result for both the regularized and unregularized algorithms, from which a statistical consistency result follows as a corollary. We validate our theoretical results empirically through a simulation study, demonstrating that the approximate algorithm exhibits faster overall runtime with low error. Finally, we extend the setting and application of our methods to hidden Markov models, and illustrate the potential use of the proposed algorithms in practice with an application to computer-generated music.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752289500001,0
J,"Silverman, JD; Roche, K; Holmes, ZC; David, LA; Mukherjee, S",,,,"Silverman, Justin D.; Roche, Kimberly; Holmes, Zachary C.; David, Lawrence A.; Mukherjee, Sayan",,,Bayesian Multinomial Logistic Normal Models through Marginally Latent Matrix-T Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian multinomial logistic-normal (MLN) models are popular for the analysis of sequence count data (e.g., microbiome or gene expression data) due to their ability to model multivariate count data with complex covariance structure. However, existing implementations of MLN models are limited to small datasets due to the non-conjugacy of the multinomial and logistic-normal distributions. Motivated by the need to develop efficient inference for Bayesian MLN models, we develop two key ideas. First, we develop the class of Marginally Latent Matrix-T Process (Marginally LTP) models. We demonstrate that many popular MLN models, including those with latent linear, non-linear, and dynamic linear structure are special cases of this class. Second, we develop an efficient inference scheme for Marginally LTP models with specific accelerations for the MLN subclass. Through application to MLN models, we demonstrate that our inference scheme are both highly accurate and often 4-5 orders of magnitude faster than MCMC.",,,,,"David, Lawrence Anthony/HCJ-0062-2022","David, Lawrence Anthony/0000-0002-3570-4767",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000819482400001,0
J,"Bouchard-Cote, A; Roth, A",,,,"Bouchard-Cote, Alexandre; Roth, Andrew",,,Particle-Gibbs Sampling for Bayesian Feature Allocation Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian feature allocation models are a popular tool for modelling data with a combinatorial latent structure. Exact inference in these models is generally intractable and so practitioners typically apply Markov Chain Monte Carlo (MCMC) methods for posterior inference. The most widely used MCMC strategies rely on a single variable Gibbs update of the feature allocation matrix. These updates can be inefficient as features are typically strongly correlated. To overcome this problem we have developed a block sampler that can update an entire row of the feature allocation matrix in a single move. In the context of feature allocation models, naive block Gibbs sampling is impractical for models with a large number of features as the computational complexity scales exponentially in the number of features. We develop a Particle Gibbs (PG) sampler that targets the same distribution as the row wise Gibbs updates, but has computational complexity that only grows linearly in the number of features. We compare the performance of our proposed methods to the standard Gibbs sampler using synthetic and real data from a range of feature allocation models. Our results suggest that row wise updates using the PG methodology can significantly improve the performance of samplers for feature allocation models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706447800001,0
J,"Ha, W; Fountoulakis, K; Mahoney, MW",,,,"Ha, Wooseok; Fountoulakis, Kimon; Mahoney, Michael W.",,,Statistical guarantees for local graph clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Local graph clustering methods aim to find small clusters in very large graphs. These methods take as input a graph and a seed node, and they return as output a good cluster in a running time that depends on the size of the output cluster but that is independent of the size of the input graph. In this paper, we adopt a statistical perspective on local graph clustering, and we analyze the per-formance of the l(1)-regularized PageRank method (Fountoulakis et al., 2019) for the recovery of a single target cluster, given a seed node inside the cluster. Assuming the target cluster has been generated by a random model, we present two results. In the first, we show that the optimal sup-port of l(1)-regularized PageRank recovers the full target cluster, with bounded false positives. In the second, we show that if the seed node is connected solely to the target cluster then the optimal sup-port of l(1)-regularized PageRank recovers exactly the target cluster. We also show empirically that l(1)-regularized PageRank has a state-of-the-art performance on many real graphs, demonstrating the superiority of the method. From a computational perspective, we show that the solution path of l(1)-regularized PageRank is monotonic. This allows for the application of the forward stagewise algorithm, which approximates the entire solution path in running time that does not depend on the size of the whole graph. Finally, we show that l(1)-regularized PageRank and approximate per-sonalized PageRank (APPR) (Andersen et al., 2006), another very popular method for local graph clustering, are equivalent in the sense that we can lower and upper bound the output of one with the output of the other. Based on this relation, we establish for APPR similar results to those we establish for l(1)-regularized PageRank.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687226300001,0
J,"Huusari, R; Kadri, H",,,,"Huusari, Riikka; Kadri, Hachem",,,Entangled Kernels - Beyond Separability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of operator-valued kernel learning and investigate the possibility of going beyond the well-known separable kernels. Borrowing tools and concepts from the field of quantum computing, such as partial trace and entanglement, we propose a new view on operator-valued kernels and define a general family of kernels that encompasses previously known operator-valued kernels, including separable and transformable kernels. Within this framework, we introduce another novel class of operator-valued kernels called entangled kernels that are not separable. We propose an efficient two-step algorithm for this framework, where the entangled kernel is learned based on a novel extension of kernel alignment to operator-valued kernels. We illustrate our algorithm with an application to supervised dimensionality reduction, and demonstrate its effectiveness with both artificial and real data for multi-output regression.",,,,,,"Huusari, Riikka/0000-0001-7821-0313",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500024,0
J,"Khalife, S; Goncalves, D; Allouah, Y; Liberti, L",,,,"Khalife, Sammy; Goncalves, Douglas; Allouah, Youssef; Liberti, Leo",,,Further results on latent discourse models and word embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discuss some properties of generative models for word embeddings. Namely, (Arora et al., 2016) proposed a latent discourse model implying the concentration of the partition function of the word vectors. This concentration phenomenon led to an asymptotic linear relation between the pointwise mutual information (PMI) of pairs of words and the scalar product of their vectors. Here, we first revisit this concentration phenomenon and prove it under slightly weaker assumptions, for a set of random vectors symmetrically distributed around the origin. Second, we empirically evaluate the relation between PMI and scalar products of word vectors satisfying the concentration property. Our empirical results indicate that, in practice, this relation does not hold with arbitrarily small error. This observation is further supported by two theoretical results: (i) the error cannot be exactly zero because the corresponding shifted PMI matrix cannot be positive semidefinite; (ii) under mild assumptions, there exist pairs of words for which the error cannot be close to zero. We deduce that either natural language does not follow the assumptions of the considered generative model, or the current word vector generation methods do not allow the construction of the hypothesized word embeddings.",,,,,,"Allouah, Youssef/0000-0003-1048-7548",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000765311400001,0
J,"Liu, Y; Fan, T; Chen, TJ; Xu, Q; Yang, Q",,,,"Liu, Yang; Fan, Tao; Chen, Tianjian; Xu, Qian; Yang, Qiang",,,FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Collaborative and federated learning has become an emerging solution to many industrial applications where data values from different sites are exploit jointly with privacy protection. We introduce FATE, an industrial-grade project that supports enterprises and institutions to build machine learning models collaboratively at large-scale in a distributed manner. FATE supports a variety of secure computation protocols and machine learning algorithms, and features out-of-box usability with end-to-end building modules and visualization tools. Documentations are available at https ://github.com/FederatedAI/FATE. Case studies and other information are available at https ://www. fedai .org.",,,,,"yang, qiang/GYJ-0971-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706886600001,0
J,"Raffin, A; Hi, A; Gleave, A; Kanervisto, A; Ernestus, M; Dormann, N",,,,"Raffin, Antonin; Hi, Ashley; Gleave, Adam; Kanervisto, Anssi; Ernestus, Maximilian; Dormann, Noah",,,Stable-Baselines3: Reliable Reinforcement Learning Implementations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"STABLE-BASELINES3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,8,,,,,,,,,,,,,,,,WOS:000726706300001,0
J,"Tong, T; Ma, C; Chi, YJ",,,,"Tong, Tian; Ma, Cong; Chi, Yuejie",,,Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Low-rank matrix estimation is a canonical problem that finds numerous applications in signal processing, machine learning and imaging science. A popular approach in practice is to factorize the matrix into two compact low-rank factors, and then optimize these factors directly via simple iterative methods such as gradient descent and alternating minimization. Despite nonconvexity, recent literatures have shown that these simple heuristics in fact achieve linear convergence when initialized properly for a growing number of problems of interest. However, upon closer examination, existing approaches can still be computationally expensive especially for ill-conditioned matrices: the convergence rate of gradient descent depends linearly on the condition number of the low-rank matrix, while the per-iteration cost of alternating minimization is often prohibitive for large matrices. The goal of this paper is to set forth a competitive algorithmic approach dubbed Scaled Gradient Descent (ScaledGD) which can be viewed as preconditioned or diagonally-scaled gradient descent, where the preconditioners are adaptive and iteration-varying with a minimal computational overhead. With tailored variants for low-rank matrix sensing, robust principal component analysis and matrix completion, we theoretically show that ScaledGD achieves the best of both worlds: it converges linearly at a rate independent of the condition number of the low-rank matrix similar as alternating minimization, while maintaining the low per-iteration cost of gradient descent. Our analysis is also applicable to general loss functions that are restricted strongly convex and smooth over low-rank matrices. To the best of our knowledge, ScaledGD is the first algorithm that provably has such properties over a wide range of low-rank matrix estimation tasks. At the core of our analysis is the introduction of a new distance function that takes account of the preconditioners when measuring the distance between the iterates and the ground truth. Finally, numerical examples are provided to demonstrate the effectiveness of ScaledGD in accelerating the convergence rate of ill-conditioned low-rank matrix estimation in a wide number of applications. Keywords: low-rank matrix factorization, scaled gradient descent, ill-conditioned matrix recovery, matrix sensing, robust PCA, matrix completion, general losses",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000687204100001,0
J,"Huynh, V; Ho, N; Dam, N; Nguyen, X; Yurochkin, M; Bui, H; Phung, D",,,,"Viet Huynh; Nhat Ho; Nhan Dam; XuanLong Nguyen; Yurochkin, Mikhail; Hung Bui; Dinh Phung",,,On Efficient Multilevel Clustering via Wasserstein Distances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose several variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters. Consistency properties are established for the estimates of both local and global clusters. Finally, experimental results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687210700001,0
J,"Almodovar-Rivera, IA; Maitra, R",,,,"Almodovar-Rivera, Israel A.; Maitra, Ranjan",,,Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Commonly-used clustering algorithms usually find ellipsoidal, spherical or other regular-structured clusters, but are more challenged when the underlying groups lack formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial clustering algorithm that can be used with k-means and other algorithms. Our approach estimates the cumulative distribution function of the normed residuals from an appropriately fit k-groups model and calculates the estimated nonparametric overlap between each pair of clusters. Groups with high pairwise overlap are merged as long as the estimated generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in several datasets and can be applied to datasets with scatter or incomplete records. The approach is also used to identify the distinct kinds of gamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and the distinct kinds of activation in a functional Magnetic Resonance Imaging study.",,,,,,"Maitra, Ranjan/0000-0002-3515-8532",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,122,,,,,,,,,,,,,,,WOS:000556190700001,0
J,"Arya, V; Bellamy, RKE; Chen, PY; Dhurandhar, A; Hind, M; Hoffman, SC; Houde, S; Liao, QV; Luss, R; Mojsilovic, A; Mourad, S; Pedemonte, P; Raghavendra, R; Richards, JT; Sattigeri, P; Shanmugam, K; Singh, M; Varshney, KR; Wei, D; Zhang, YF",,,,"Arya, Vijay; Bellamy, Rachel K. E.; Chen, Pin-Yu; Dhurandhar, Amit; Hind, Michael; Hoffman, Samuel C.; Houde, Stephanie; Liao, Q. Vera; Luss, Ronny; Mojsilovic, Aleksandra; Mourad, Sami; Pedemonte, Pablo; Raghavendra, Ramya; Richards, John T.; Sattigeri, Prasanna; Shanmugam, Karthikeyan; Singh, Moninder; Varshney, Kush R.; Wei, Dennis; Zhang, Yunfeng",,,AI Explainability 360: An Extensible Toolkit for Understanding Data and Machine Learning Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As artificial intelligence algorithms make further inroads in high-stakes societal applications, there are increasing calls from multiple stakeholders for these algorithms to explain their outputs. To make matters more challenging, different personas of consumers of explanations have different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360, an open-source Python toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics (http://aix360.mybluemix.net). Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of interpretation and explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. The toolkit is not only the software, but also guidance material, tutorials, and an interactive web demo to introduce AI explainability to different audiences. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,130,,,,,,,,,,,,,,,WOS:000556297000001,0
J,"Chen, X; Wang, YN; Zhou, Y",,,,"Chen, Xi; Wang, Yining; Zhou, Yuan",,,Dynamic Assortment Optimization with Changing Contextual Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the dynamic assortment optimization problem over a finite selling season of length T. At each time period, the seller offers an arriving customer an assortment of substitutable products under a cardinality constraint, and the customer makes the purchase among offered products according to a discrete choice model. Most existing work associates each product with a real-valued fixed mean utility and assumes a multinomial logit choice (MNL) model. In many practical applications, feature/contextual information of products is readily available. In this paper, we incorporate the feature information by assuming a linear relationship between the mean utility and the feature. In addition, we allow the feature information of products to change over time so that the underlying choice model can also be non-stationary. To solve the dynamic assortment optimization under this changing contextual MNL model, we need to simultaneously learn the underlying unknown coefficient and make the decision on the assortment. To this end, we develop an upper confidence bound (UCB) based policy and establish the regret bound on the order of (O) over tilde (d root T), where d is the dimension of the feature and (O) over tilde suppresses logarithmic dependence. We further establish a lower bound Omega(d root T/K), where K is the cardinality constraint of an offered assortment, which is usually small. When K is a constant, our policy is optimal up to logarithmic factors. In the exploitation phase of the UCB algorithm, we need to solve a combinatorial optimization problem for assortment optimization based on the learned information. We further develop an approximation algorithm and an efficient greedy heuristic. The effectiveness of the proposed policy is further demonstrated by our numerical studies.",,,,,,"Wang, Yining/0000-0001-9410-0392",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,216,,,,,,,,,,,,,,,WOS:000590026200001,0
J,"de Vazelhes, W; Carey, CJ; Tang, Y; Vauquier, N; Bellet, A",,,,"de Vazelhes, William; Carey, C. J.; Tang, Yuan; Vauquier, Nathalie; Bellet, Aurelien",,,metric-learn: Metric Learning Algorithms in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT license.",,,,,,"Bellet, Aurelien/0000-0003-3440-1251",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,138,,,,,,,,,,,,,,,WOS:000558791200001,0
J,"Haider, H; Hoehn, B; Davis, S; Greiner, R",,,,"Haider, Humza; Hoehn, Bret; Davis, Sarah; Greiner, Russell",,,Effective Ways to Build and Evaluate Individual Survival Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An accurate model of a patient's individual survival distribution can help determine the appropriate treatment for terminal patients. Unfortunately, risk scores (for example from Cox Proportional Hazard models) do not provide survival probabilities, single-time probability models (for instance the Gail model, predicting 5 year probability) only provide for a single time point, and standard Kaplan-Meier survival curves provide only population averages for a large class of patients, meaning they are not specific to individual patients. This motivates an alternative class of tools that can learn a model that provides an individual survival distribution for each subject, which gives survival probabilities across all times, such as extensions to the Cox model, Accelerated Failure Time, an extension to Random Survival Forests, and Multi-Task Logistic Regression. This paper first motivates such individual survival distribution (ISD) models, and explains how they differ from standard models. It then discusses ways to evaluate such models namely Concordance, 1-Calibration, Inte- grated Brier score, and versions of L1-loss then motivates and defines a novel approach, D-Calibration, which determines whether a model's probability estimates are meaningful. We also discuss how these measures differ, and use them to evaluate several ISD prediction tools over a range of survival data sets. We also provide a code base for all of these survival models and evaluation measures, at https : //github. com/haiderstats/ISDEvaluation.",,,,,,"Davis, Sacha/0000-0003-0295-162X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,85,,,,,,,,,,,,,,,WOS:000545026300001,0
J,"Hamalainen, J; Alencar, ASC; Karkkainen, T; Mattos, CLC; Souza, AH; Gomes, JPP",,,,"Hamalainen, Joonas; Alencar, Alisson S. C.; Karkkainen, Tommi; Mattos, Cesar L. C.; Souza Junior, Amauri H.; Gomes, Joao P. P.",,,Minimal Learning Machine: Theoretical Results and Clustering-Based Reference Point Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Minimal Learning Machine (MLM) is a nonlinear, supervised approach based on learning linear mapping between distance matrices computed in input and output data spaces, where distances are calculated using a subset of points called reference points. Its simple formulation has attracted several recent works on extensions and applications. In this paper, we aim to address some open questions related to the MLM. First, we detail the theoretical aspects that assure the MLM's interpolation and universal approximation capabilities, which had previously only been empirically verified. Second, we identify the major importance of the task of selecting reference points for the MLM's generalization capability. Several clustering-based methods for reference point selection in regression scenarios are then proposed and analyzed. Based on an extensive empirical evaluation, we conclude that the evaluated methods are both scalable and useful. Specifically, for a small number of reference points, the clustering-based methods outperform the standard random selection of the original MLM formulation.",,,,,"H√§m√§l√§inen, Joonas/AAF-4231-2019","Karkkainen, Tommi/0000-0003-0327-1167",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,239,,,,,,,,,,,,,,,WOS:000608910700001,0
J,"Hector, EC; Song, PXK",,,,"Hector, Emily C.; Song, Peter X-K",,,Doubly Distributed Supervised Learning and Inference with High-Dimensional Correlated Outcomes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a unified framework for supervised learning and inference procedures using the divide-and-conquer approach for high-dimensional correlated outcomes. We propose a general class of estimators that can be implemented in a fully distributed and parallelized computational scheme. Modeling, computational and theoretical challenges related to high-dimensional correlated outcomes are overcome by dividing data at both outcome and subject levels, estimating the parameter of interest from blocks of data using a broad class of supervised learning procedures, and combining block estimators in a closed-form meta-estimator asymptotically equivalent to estimates obtained by Hansen (1982)'s generalized method of moments (GMM) that does not require the entire data to be reloaded on a common server. We provide rigorous theoretical justifications for the use of distributed estimators with correlated outcomes by studying the asymptotic behaviour of the combined estimator with fixed and diverging number of data divisions. Simulations illustrate the finite sample performance of the proposed method, and we provide an R package for ease of implementation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,173,,,,,,,,,,,,,,,WOS:000570206200001,0
J,"Kalainathan, D; Goudet, O; Dutta, R",,,,"Kalainathan, Diviyan; Goudet, Olivier; Dutta, Ritik",,,Causal Discovery Toolbox: Uncovering causal relationships in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The CDT package implements an end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'BNLEARN' (Scutari, 2018) and 'PCALG' (Kalisch et al., 2018) packages, together with algorithms for pairwise causal discovery such as ANM (Hover et al., 2009). CDT is available under the MIT License at https://github.com/FenTechSolutions/CausalDiscoveryToolbox.",,,,,"Goudet, Olivier/AAD-7802-2021","Goudet, Olivier/0000-0001-7040-5052",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000012,0
J,"Schreiber, J; Bilmes, J; Noble, WS",,,,"Schreiber, Jacob; Bilmes, Jeffrey; Noble, William Stafford",,,apricot: Submodular selection for data summarization in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present apricot, an open source Python package for selecting representative subsets from large data sets using submodular optimization. The package implements several efficient greedy selection algorithms that offer strong theoretical guarantees on the quality of the selected set. Additionally, several submodular set functions are implemented, including facility location, which is broadly applicable but requires memory quadratic in the number of examples in the data set, and a feature-based function that is less broadly applicable but can scale to millions of examples. Apricot is extremely efficient, using both algorithmic speedups such as the lazy greedy algorithm and memoization as well as code optimization using numba. We demonstrate the use of subset selection by training machine learning models to comparable accuracy using either the full data set or a representative subset thereof. This paper presents an explanation of submodular selection, an overview of the features in apricot, and applications to two data sets. The code and tutorial Jupyter notebooks are available at https://github.com/jmschrei/apricot",,,,,"Schreiber, Jacob/GWC-3164-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,161,,,,,,,,,,,,,,,WOS:000570149800001,0
J,"Sifaou, H; Kammoun, A; Alouini, MS",,,,"Sifaou, Houssem; Kammoun, Abla; Alouini, Mohamed-Slim",,,High-dimensional Linear Discriminant Analysis Classifier for Spiked Covariance Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear discriminant analysis (LDA) is a popular classifier that is built on the assumption of common population covariance matrix across classes. The performance of LDA depends heavily on the quality of estimating the mean vectors and the population covariance matrix. This issue becomes more challenging in high-dimensional settings where the number of features is of the same order as the number of training samples. Several techniques for estimating the covariance matrix can be found in the literature. One of the most popular approaches are estimators based on using a regularized sample covariance matrix, giving the name regularized LDA (R-LDA) to the corresponding classifier. These estimators are known to be more resilient to the sample noise than the traditional sample covariance matrix estimator. However, the main challenge of the regularization approach is the choice of the optimal regularization parameter, as an arbitrary choice could lead to severe degradation of the classifier performance. In this work, we propose an improved LDA classifier based on the assumption that the covariance matrix follows a spiked covariance model. The main principle of our proposed technique is the design of a parametrized inverse covariance matrix estimator, the parameters of which are shown to be easily optimized. Numerical simulations, using both real and synthetic data, show that the proposed classifier yields better classification performance than the classical R-LDA while requiring lower computational complexity.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,112,,,,,,,,,,,,,,,WOS:000546629900001,0
J,"Wang, HR; Zariphopoulou, T; Zhou, XY",,,,"Wang, Haoran; Zariphopoulou, Thaleia; Zhou, Xun Yu",,,Reinforcement Learning in Continuous Time and Space: A Stochastic Control Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider reinforcement learning (RL) in continuous time with continuous feature and action spaces. We motivate and devise an exploratory formulation for the feature dynamics that captures learning under exploration, with the resulting optimization problem being a revitalization of the classical relaxed stochastic control. We then study the problem of achieving the best trade-off between exploration and exploitation by considering an entropy-regularized reward function. We carry out a complete analysis of the problem in the linear-quadratic (LQ) setting and deduce that the optimal feedback control distribution for balancing exploitation and exploration is Gaussian. This in turn interprets the widely adopted Gaussian exploration in RL, beyond its simplicity for sampling. Moreover, the exploitation and exploration are captured respectively by the mean and variance of the Gaussian distribution. We characterize the cost of exploration, which, for the LQ case, is shown to be proportional to the entropy regularization weight and inversely proportional to the discount rate. Finally, as the weight of exploration decays to zero, we prove the convergence of the solution of the entropy-regularized LQ problem to the one of the classical LQ problem.",,,,,"Zariphopoulou, Thaleia/C-6407-2019",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,198,,,,,,,,,,,,,,,WOS:000590003900001,0
J,"Azulay, A; Weiss, Y",,,,"Azulay, Aharon; Weiss, Yair",,,Why do deep convolutional networks generalize so poorly to small image transformations?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,184,,,,,,,,,,,,,,,WOS:000506403100024,0
J,"Bartlett, PL; Harvey, N; Liaw, C; Mehrabian, A",,,,"Bartlett, Peter L.; Harvey, Nick; Liaw, Christopher; Mehrabian, Abbas",,,Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting W be the number of weights and L be the number of layers, we prove that the VC-dimension is O(WL log(W)), and provide examples with VC-dimension Omega(WL log(W/L)). This improves both the previously known upper bounds and lower. bounds. In terms of the number U of non-linear units, we prove a tight bound Theta(WU) on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,1,17,63,,,,,,,,,,,,,,,WOS:000467879800001,0
J,"Biessmann, F; Rukat, T; Schmidt, P; Naidu, P; Schelter, S; Taptunov, A; Lange, D; Salinas, D",,,,"Biessmann, Felix; Rukat, Tammo; Schmidt, Phillipp; Naidu, Prathik; Schelter, Sebastian; Taptunov, Andrey; Lange, Dustin; Salinas, David",,,DataWig: Missing Value Imputation for Tables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the growing importance of machine learning (ML) algorithms for practical applications, reducing data quality problems in ML pipelines has become a major focus of research. In many cases missing values can break data pipelines which makes completeness one of the most impactful data quality challenges. Current missing value imputation methods are focusing on numerical or categorical data and can be difficult to scale to datasets with millions of rows. We release DataWig, a robust and scalable approach for missing value imputation that can be applied to tables with heterogeneous data types, including unstructured text. DataWig combines deep learning feature extractors with automatic hyperparameter tuning. This enables users without a machine learning background, such as data engineers, to impute missing values with minimal effort in tables with more heterogeneous data types than supported in existing libraries, while requiring less glue code for feature engineering and offering more flexible modelling options. We demonstrate that DataWig compares favourably to existing imputation packages. Source code, documentation, and unit tests for this package are available at: github.com/awslabs/datawig",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,175,,,,,,,,,,,,,,,WOS:000506403100015,0
J,"Eftekhari, A; Ongie, G; Balzano, L; Wakin, MB",,,,"Eftekhari, Armin; Ongie, Gregory; Balzano, Laura; Wakin, Michael B.",,,Streaming Principal Component Analysis From Incomplete Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear subspace models are pervasive in computational sciences and particularly used for large datasets which are often incomplete due to privacy issues or sampling constraints. Therefore, a critical problem is developing an efficient algorithm for detecting low-dimensional linear structure from incomplete data efficiently, in terms of both computational complexity and storage. In this paper we propose a streaming subspace estimation algorithm called Subspace Navigation via Interpolation from Partial Entries (SNIPE) that efficiently processes blocks of incomplete data to estimate the underlying subspace model. In every iteration, SNIPE finds the subspace that best fits the new data block but remains close to the previous estimate. We show that SNIPE is a streaming solver for the underlying nonconvex matrix completion problem, that it converges globally to a stationary point of this program regardless of initialization, and that the convergence is locally linear with high probability. We also find that SNIPE shows state-of-the-art performance in our numerical simulations.",,,,,"Wakin, Michael/G-1582-2012","Wakin, Michael/0000-0002-2165-4586",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,86,,,,,,,,,,,,,,,WOS:000470907100001,0
J,"Glimsdal, S; Granmo, OC",,,,"Glimsdal, Sondre; Granmo, Ole-Christoffer",,,Thompson Sampling Guided Stochastic Searching on the Line for Deceptive Environments with Applications to Root-Finding Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The multi-armed bandit problem forms the foundation for solving a wide range of online stochastic optimization problems through a simple, yet effective mechanism. One simply casts the problem as a gambler who repeatedly pulls one out of N slot machine arms, eliciting random rewards. Learning of reward probabilities is then combined with reward maximization, by carefully balancing reward exploration against reward exploitation. In this paper, we address a particularly intriguing variant of the multi-armed bandit problem, referred to as the Stochastic Point Location (SPL) problem. The gambler is here only told whether the optimal arm (point) lies to the left or to the right of the arm pulled, with the feedback being erroneous with probability 1 - pi. This formulation thus targets optimization in continuous action spaces with both informative and deceptive feedback. To tackle this class of problems, we formulate a compact and scalable Bayesian representation of the solution space that simultaneously captures both the location of the optimal arm as well as the probability of receiving correct feedback. We further introduce the accompanying Thompson Sampling guided Stochastic Point Location (TS-SPL) scheme for balancing exploration against exploitation. By learning pi, TS-SPL also supports deceptive environments that are lying about the direction of the optimal arm. This, in turn, allows us to address the fundamental Stochastic Root Finding (SRF) problem. Empirical results demonstrate that our scheme deals with both deceptive and informative environments, significantly outperforming competing algorithms both for SRF and SPL.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,52,,,,,,,,,,,,,,,WOS:000463323300001,0
J,"Szymanski, P; Kajdanowicz, T",,,,"Szymanski, Piotr; Kajdanowicz, Tomasz",,,scikit-multilearn: A scikit-based Python environment for performing multi-label classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The scikit-multilearn is a Python library for performing multi-label classification. It is compatible with the scikit-learn and scipy ecosystems and uses sparse matrices for all internal operations; provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division and includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. The library provides Python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from http://scikit.m1 and also via pip. The project is BSD-licensed.",,,,,,"Szymanski, Piotr/0000-0002-7733-3239",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,6,,,,,,,,,,,,,,,WOS:000458662900001,0
J,"Vasiloudis, T; Morales, GD; Bostrom, H",,,,"Vasiloudis, Theodore; Morales, Gianmarco De Francisci; Bostrom, Henrik",,,Quantifying Uncertainty in Online Regression Forests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Accurately quantifying uncertainty in predictions is essential for the deployment of machine learning algorithms in critical applications where mistakes are costly. Most approaches to quantifying prediction uncertainty have focused on settings where the data is static, or bounded. In this paper, we investigate methods that quantify the prediction uncertainty in a streaming setting, where the data is potentially unbounded. We propose two meta-algorithms that produce prediction intervals for online regression forests of arbitrary tree models; one based on conformal prediction, and the other based on quantile regression. We show that the approaches are able to maintain specified error rates, with constant computational cost per example and bounded memory usage. We provide empirical evidence that the methods outperform the state-of-the-art in terms of maintaining error guarantees, while being an order of magnitude faster. We also investigate how the algorithms are able to recover from concept drift.",,,,,"Bostr√∂m, Henrik/AAF-6105-2020; De Francisci Morales, Gianmarco/AAK-2941-2021","De Francisci Morales, Gianmarco/0000-0002-2415-494X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,155,,,,,,,,,,,,,,,WOS:000491132200019,0
J,"Zhang, LJ; Yang, TB; Jin, R; Zhou, ZH",,,,"Zhang, Lijun; Yang, Tianbao; Jin, Rong; Zhou, Zhi-Hua",,,Relative Error Bound Analysis for Nuclear Norm Regularized Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we develop a relative error bound for nuclear norm regularized matrix completion, with the focus on the completion of full-rank matrices. Under the assumption that the top eigenspaces of the target matrix are incoherent, we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix. Although multiple works have been devoted to analyzing the recovery error of full-rank matrix completion, their error bounds are usually additive, making it impossible to obtain the perfect recovery case and more generally difficult to leverage the skewed distribution of eigenvalues. Our analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion. To the best of our knowledge, this is the first relative bound that has been proved for the regularized formulation of matrix completion.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,97,,,,,,,,,,,,,,,WOS:000476621100001,0
J,"Achille, A; Soatto, S",,,,"Achille, Alessandro; Soatto, Stefano",,,Emergence of Invariance and Disentanglement in Deep Representations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,50,,,,,,,,,,,,,,,WOS:000448375600001,0
J,"Baydin, AG; Pearlmutter, BA; Radul, AA; Siskind, JM",,,,"Baydin, Atilim Gunes; Pearlmutter, Barak A.; Radul, Alexey Andreyevich; Siskind, Jeffrey Mark",,,Automatic Differentiation in Machine Learning: a Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply autodiff, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names dynamic computational graphs and differentiable programming. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms autodiff, automatic differentiation, and symbolic differentiation as these are encountered more and more in machine learning settings.",,,,,"Baydin, Atilim Gunes/M-7029-2014; Pearlmutter, Barak A./AAL-8999-2020; Pearlmutter, Barak A/M-8791-2014; Baydin, Atƒ±lƒ±m G√ºne≈ü/N-5247-2019","Baydin, Atilim Gunes/0000-0001-9854-8100; Pearlmutter, Barak A/0000-0003-0521-4553; Baydin, Atƒ±lƒ±m G√ºne≈ü/0000-0001-9854-8100",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,153,,,,,,,,,,,,,,,WOS:000431709400001,0
J,"Cowan, W; Honda, J; Katehakis, MN",,,,"Cowan, Wesley; Honda, Junya; Katehakis, Michael N.",,,Normal Bandits of Unknown Means and Variances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider the problem of sampling sequentially from a finite number of N >= 2 populations, specified by random variables X-k(i), i = 1; ..., N; and k = 1; 2, ... ; where X-k(i) denotes the outcome from population i the kth time it is sampled. It is assumed that for each fixed i, {X-k(i)} k >= 1 is a sequence of i. i. d. normal random variables, with unknown mean m i and unknown variance sigma(2)(i). The objective is to have a policy pi for deciding from which of the N populations to sample from at any time t = 1, 2, ...so as to maximize the expected sum of outcomes of n total samples or equivalently to minimize the regret due to lack on information of the parameters mu(i) and sigma(2)(i). In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from Burnetas and Katehakis (1996b). Additionally, finite horizon regret bounds are given.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,154,,,,,,,,,,,,,,,WOS:000433250600001,0
J,"George, CP; Doss, H",,,,"George, Clint P.; Doss, Hani",,,Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,162,,,,,,,,,,,,,,,WOS:000433254400001,0
J,"Lintusaari, J; Vuollekoski, H; Kangasraasio, A; Skyten, K; Jarvenpaa, M; Marttinen, P; Gutmann, MU; Vehtari, A; Corander, J; Kaski, S",,,,"Lintusaari, Jarno; Vuollekoski, Henri; Kangasraasio, Antti; Skyten, Kusti; Jarvenpaa, Marko; Marttinen, Pekka; Gutmann, Michael U.; Vehtari, Aki; Corander, Jukka; Kaski, Samuel",,,ELFI: Engine for Likelihood-Free Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.",,,,,"Kaski, Samuel/B-6684-2008; Marttinen, Pekka E/N-6234-2015","Kaski, Samuel/0000-0003-1925-9154; Marttinen, Pekka E/0000-0001-7078-7927; Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,16,,,,,,,,,,,,,,,WOS:000443226500001,0
J,"Mai, XY; Couillet, R",,,,"Mai, Xiaoyi; Couillet, Romain",,,A Random Matrix Analysis and Improvement of Semi-Supervised Learning for Large Dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article provides an original understanding of the behavior of a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data. It is demonstrated that the intuition at the root of these methods collapses in this limit and that, as a result, most of them become inconsistent. Corrective measures and a new data-driven parametrization scheme are proposed along with a theoretical analysis of the asymptotic performances of the resulting approach. A surprisingly close behavior between theoretical performances on Gaussian mixture models and on real data sets is also illustrated throughout the article, thereby suggesting the importance of the proposed analysis for dealing with practical data. As a result, significant performance gains are observed on practical data classification using the proposed parametrization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,79,,,,,,,,,,,,,,,WOS:000454479600001,0
J,"Padilla, OHM; Sharpnack, J; Scott, JG; Tibshirani, RJ",,,,"Padilla, Oscar Hernan Madrid; Sharpnack, James; Scott, James G.; Tibshirani, Ryan J.",,,The DFS Fused Lasso: Linear-Time Denoising over General Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso or simply, ld fused lasso-can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph. This result leads to several interesting theoretical and computational conclusions. Letting m and n denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation t over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of t(2/3) n(-2/3). Moreover, precisely the same mean squared error rate is achieved by running the ld fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring O(m) operations to construct the DFS-induced chain and 0(n) operations to compute the ld fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of t(2/3) n(-2/3) cannot be improved, in the sense that it is the minimax rate for signals that have total variation t over the tree. Finally, several related results also hold for example, the analogous result holds for a roughness measure defined by the to norm of differences across edges in place of the total variation metric.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,176,,,,,,,,,,,,,,,WOS:000435443100001,0
J,"E, CHEN; Avron, H; Sindhwani, V",,,,"Chen, Jie; Avron, Haim; Sindhwani, Vikas",,,Hierarchically Compositional Kernels for Scalable Nonparametric Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the Nystrom method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off-diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from O(n(2)) and O(n(3)) to O(nr) and O (nr(2)), respectively, where n is the number of training examples and r is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller r. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions.",,,,,"cai, jie/HHS-0606-2022","Avron, Haim/0000-0002-1688-9030",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,42,66,,,,,,,,,,,,,,,WOS:000412057400001,0
J,"Lin, JH; Rosasco, L",,,,"Lin, Junhong; Rosasco, Lorenzo",,,Optimal Rates for Multi-pass Stochastic Gradient Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. We study how regularization properties are controlled by the step-size, the number of passes and the mini-batch size. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. As a byproduct, we derive optimal convergence results for batch gradient methods (even in the non-attainable cases.",,,,,"Lin, Junhong/M-9045-2016","Lin, Junhong/0000-0002-4507-9424",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,97,,,,,,,,,,,,,,,WOS:000412486000001,0
J,"Ollivier, Y; Arnold, L; Auger, A; Hansen, N",,,,"Ollivier, Yann; Arnold, Ludovic; Auger, Anne; Hansen, Nikolaus",,,Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space.. into a continuous-time black-box optimization method on.., the information-geometric optimization (IGO) method. Invariance as a major design principle keeps the number of arbitrary choices to a minimum. The resulting IGO flow is the flow of an ordinary differential equation conducting the natural gradient ascent of an adaptive, time-dependent transformation of the objective function. It makes no particular assumptions on the objective function to be optimized. The IGO method produces explicit IGO algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. In continuous search spaces, IGO algorithms take a form related to natural evolution strategies (NES). The cross-entropy method is recovered in a particular case with a large time step, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML). When applied to the family of Gaussian distributions on R-d, the IGO framework recovers a version of the well-known CMA-ES algorithm and of xNES. For the family of Bernoulli distributions on {0, 1}(d), we recover the seminal PBIL algorithm and cGA. For the distributions of restricted Boltzmann machines, we naturally obtain a novel algorithm for discrete optimization on {0, 1}(d). All these algorithms are natural instances of, and unified under, the single information-geometric optimization framework. The IGO method achieves, thanks to its intrinsic formulation, maximal invariance properties: invariance under reparametrization of the search space.., under a change of parameters of the probability distribution, and under increasing transformation of the function to be optimized. The latter is achieved through an adaptive, quantile-based formulation of the objective. Theoretical considerations strongly suggest that IGO algorithms are essentially characterized by a minimal change of the distribution over time. Therefore they have minimal loss in diversity through the course of optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. As a simple consequence, IGO seems to provide, from information theory, an elegant way to simultaneously explore several valleys of a fitness landscape in a single run.",,,,,"Hansen, Nikolaus/U-6988-2019","Hansen, Nikolaus/0000-0001-7788-4906",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,18,,,,,,,,,,,,,,,WOS:000399839000001,0
J,"Yu, YL; Zhang, XH; Schuurmans, D",,,,"Yu, Yaoliang; Zhang, Xinhua; Schuurmans, Dale",,,Generalized Conditional Gradient for Sparse Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving sparse optimization problems-demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After studying the convergence properties of GCG for general convex composite problems, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in learning low-rank matrices, instantiated with detailed examples on matrix completion and dictionary learning. A further improvement is achieved by interleaving GCG with fixed-rank local subspace optimization. A series of experiments on matrix completion, multi-class classification, and multi-view dictionary learning shows that the proposed method can significantly reduce the training cost of current alternatives.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,144,,,,,,,,,,,,,,,WOS:000424549400001,0
J,"Zhang, HS; Zhou, Y; Liang, YB; Chi, YJ",,,,"Zhang, Huishuai; Zhou, Yi; Liang, Yingbin; Chi, Yuejie",,,A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of solving a quadratic system of equations, i.e., recovering a vector signal x is an element of R-n from its magnitude measurements y(i) = |< a(i), x(i)>|, i = 1,..., m. We develop a gradient descent algorithm (referred to as RWF for reshaped Wirtinger flow) by minimizing the quadratic loss of the magnitude measurements. Comparing with Wirtinger flow (WF) (Candes et al., 2015), the loss function of RWF is nonconvex and nonsmooth, but better resembles the least-squares loss when the phase information is also available. We show that for random Gaussian measurements, RWF enjoys linear convergence to the true signal as long as the number of measurements is O(n). This improves the sample complexity of WF (O(n log n)), and achieves the same sample complexity as truncated Wirtinger flow (TWF) (Chen and Candes, 2015), but without any sophisticated truncation in the gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop an incremental (stochastic) version of RWF (IRWF) and connect it with the randomized Kaczmarz method for phase retrieval. We demonstrate that IRWF outperforms existing incremental as well as batch algorithms with experiments.",,,,,"Chi, Yuejie/AAG-5084-2019",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,141,,,,,,,,,,,,,,,WOS:000424548700001,0
J,"Ieva, F; Paganoni, AM; Tarabelloni, N",,,,"Ieva, Francesca; Paganoni, Anna Maria; Tarabelloni, Nicholas",,,Covariance-based Clustering in Multivariate and Functional Data Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose a new algorithm to perform clustering of multivariate and functional data. We study the case of two populations different in their covariances, rather than in their means. The algorithm relies on a proper quantification of distance between the estimated covariance operators of the populations, and subdivides data in two groups maximising the distance between their induced covariances. The naive implementation of such an algorithm is computationally forbidding, so we propose a heuristic formulation with a much lighter complexity and we study its convergence properties, along with its computational cost. We also propose to use an enhanced estimator for the estimation of discrete covariances of functional data, namely a linear shrinkage estimator, in order to improve the precision of the clustering. We establish the effectiveness of our algorithm through applications to both synthetic data and a real data set coming from a biomedical context, showing also how the use of shrinkage estimation may lead to substantially better results.",,,,,,"Ieva, Francesca/0000-0003-0165-1983",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,21,143,,,,,,,,,,,,,,,WOS:000391661200001,0
J,"Oates, CJ; Smith, JQ; Mukherjee, S",,,,"Oates, Chris J.; Smith, Jim Q.; Mukherjee, Sach",,,Estimating Causal Structure Using Conditional DAG Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers inference of causal structure in a class of graphical models called conditional DAGs. These are directed acyclic graph (DAG) models with two kinds of variables, primary and secondary. The secondary variables are used to aid in the estimation of the structure of causal relationships between the primary variables. We prove that, under certain assumptions, such causal structure is identifiable from the joint observational distribution of the primary and secondary variables. We give causal semantics for the model class, put forward a score-based approach for estimation and establish consistency results. Empirical results demonstrate gains compared with formulations that treat all variables on an equal footing, or that ignore secondary variables. The methodology is motivated by applications in biology that involve multiple data types and is illustrated here using simulated data and in an analysis of molecular data from the Cancer Genome Atlas.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,54,,,,,,,,,,,,,,,WOS:000391488400001,0
J,"Richtarik, P; Takac, M",,,,"Richtarik, Peter; Takac, Martin",,,Distributed Coordinate Descent Method for Learning with Big Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.",,,,,"Takac, Martin/AAA-8564-2022; Richtarik, Peter/O-5797-2018","Takac, Martin/0000-0001-7455-2025; ",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,75,,,,,,,,,,,,,,,WOS:000391524600001,0
J,"Szabo, Z; Sriperumbudur, BK; Poczos, B; Gretton, A",,,,"Szabo, Zoltan; Sriperumbudur, Bharath K.; Poczos, Barnabas; Gretton, Arthur",,,Learning Theory for Distribution Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate (Caponnetto and De Vito, 2007; Steinwart et al., 2009). This result answers a 17-year-old open question, establishing the consistency of the classical set kernel (Haussler, 1999; Gartner et al., 2002) in regression. We also cover consistency for more recent kernels on distributions, including those due to Christmann and Steinwart (2010).",,,,,,"Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,152,,,,,,,,,,,,,,,WOS:000391663600001,0
J,"Townsend, J; Koep, N; Weichwald, S",,,,"Townsend, James; Koep, Niklas; Weichwald, Sebastian",,,Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. We introduce PYMANOPT (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that-similarly to the Manopt Matlab toolbox-implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,137,,,,,,,,,,,,,,,WOS:000391659000001,0
J,"Pati, D; Bhattacharya, A; Cheng, G",,,,"Pati, Debdeep; Bhattacharya, Anirban; Cheng, Guang",,,Optimal Bayesian Estimation in Random Covariate Design with a Rescaled Gaussian Process Prior,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In Bayesian nonparametric models, Gaussian processes provide a popular prior choice for regression function estimation. Existing literature on the theoretical investigation of the resulting posterior distribution almost exclusively assume a fixed design for covariates. The only random design result we are aware of (van der Vaart and van Zanten, 2011) assumes the assigned Gaussian process to be supported on the smoothness class specified by the true function with probability one. This is a fairly restrictive assumption as it essentially rules out the Gaussian process prior with a squared exponential kernel when modeling rougher functions. In this article, we show that an appropriate rescaling of the above Gaussian process leads to a rate-optimal posterior distribution even when the covariates are independently realized from a known density on a compact set. The proofs are based on deriving sharp concentration inequalities for frequentist kernel estimators; the results might be of independent interest.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2837,2851,,,,,,,,,,,,,,,,WOS:000369888000016,0
J,"Shamir, O",,,,"Shamir, Ohad",,,The Sample Complexity of Learning Linear Predictors with the Squared Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide a tight sample complexity bound for learning bounded-norm linear predictors with respect to the squared loss. Our focus is on an agnostic PAC-style setting, where no assumptions are made on the data distribution beyond boundedness. This contrasts with existing results in the literature, which rely on other distributional assumptions, refer to specific parameter settings, or use other performance measures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3475,3486,,,,,,,,,,,,,,,,WOS:000369888000037,0
J,"Alain, G; Bengio, Y",,,,"Alain, Guillaume; Bengio, Yoshua",,,What Regularized Auto-Encoders Learn from the Data-Generating Distribution,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3563,3593,,,,,,,,,,,,,,,,WOS:000353126200008,0
J,"Yamazaki, K",,,,"Yamazaki, Keisuke",,,Asymptotic Accuracy of Distribution-Based Estimation of Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hierarchical statistical models are widely employed in information science and data engineering. The models consist of two types of variables: observable variables that represent the given data and latent variables for the unobservable labels. An asymptotic analysis of the models plays an important role in evaluating the learning process; the result of the analysis is applied not only to theoretical but also to practical situations, such as optimal model selection and active learning. There are many studies of generalization errors, which measure the prediction accuracy of the observable variables. However, the accuracy of estimating the latent variables has not yet been elucidated. For a quantitative evaluation of this, the present paper formulates distribution-based functions for the errors in the estimation of the latent variables. The asymptotic behavior is analyzed for both the maximum likelihood and the Bayes methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3541,3562,,,,,,,,,,,,,,,,WOS:000353126200007,0
J,"Couckuyt, I; Dhaene, T; Demeester, P",,,,"Couckuyt, Ivo; Dhaene, Tom; Demeester, Piet",,,ooDACE Toolbox: A Flexible Object-Oriented Kriging Implementation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When analyzing data from computationally expensive simulation codes, surrogate modeling methods are firmly established as facilitators for design space exploration, sensitivity analysis, visualization and optimization. Kriging is a popular surrogate modeling technique used for the Design and Analysis of Computer Experiments (DACE). Hence, the past decade Kriging has been the subject of extensive research and many extensions have been proposed, e.g., co-Kriging, stochastic Kriging, blind Kriging, etc. However, few Kriging implementations are publicly available and tailored towards scientists and engineers. Furthermore, no Kriging toolbox exists that unifies several Kriging flavors. This paper addresses this need by presenting an efficient object-oriented Kriging implementation and several Kriging extensions, providing a flexible and easily extendable framework to test and implement new Kriging flavors while reusing as much code as possible.",,,,,"Dhaene, Tom/A-4541-2009; Couckuyt, Ivo/V-6435-2019","Dhaene, Tom/0000-0003-2899-4636; Couckuyt, Ivo/0000-0002-9524-4205",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3183,3186,,,,,,,,,,,,,,,,WOS:000344638800011,0
J,"Zhu, J; Chen, N; Xing, EP",,,,"Zhu, Jun; Chen, Ning; Xing, Eric P.",,,Bayesian Inference with Posterior Regularization and Applications to Infinite Latent SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1799,1847,,,,,,,,,,,,,,,,WOS:000344638100006,0
J,"de Rooij, S; van Erven, T; Grunwald, PD; Koolen, WM",,,,"de Rooij, Steven; van Erven, Tim; Grunwald, Peter D.; Koolen, Wouter M.",,,"Follow the Leader If You Can, Hedge If You Must",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has poor performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As a stepping stone for our analysis, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour, and Stoltz (2007), yielding improved worst-case guarantees. By interleaving AdaHedge and FTL, FlipFlop achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1281,1316,,,,,,,,,,,,,,,,WOS:000338420000003,0
J,"Hoi, SCH; Wang, JL; Zhao, PL",,,,"Hoi, Steven C. H.; Wang, Jialei; Zhao, Peilin",,,LIBOL: A Library for Online Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.",,,,,"HOI, Steven C. H./A-3736-2011","Hoi, Steven/0000-0002-4584-3453",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,495,499,,,,,,,,,,,,,,,,WOS:000335457700004,0
J,"Pang, HT; Liu, H; Vanderbei, R",,,,"Pang, Haotian; Liu, Han; Vanderbei, Robets",,,The FASTCLIME Package for Linear Programming and Large-Scale Precision Matrix Estimation in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop an R package FASTCLIME for solving a family of regularized linear programming (LP) problems. Our package efficiently implements the parametric simplex algorithm, which provides a scalable and sophisticated tool for solving large-scale linear programs. As an illustrative example, one use of our LP solver is to implement an important sparse precision matrix estimation method called CLIME (Constrained L-1 Minimization Estimator). Compared with existing packages for this problem such as CLIME and FLARE, our package has three advantages: (1) it efficiently calculates the full piecewise-linear regularization path; (2) it provides an accurate dual certificate as stopping criterion; (3) it is completely coded in C and is highly portable. This package is designed to be useful to statisticians and machine learning researchers for solving a wide range of problems.",,,,,"Vanderbei, Robert J/A-9779-2009",,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,489,493,,,,,,,,,,,25620890,,,,,WOS:000335457700003,0
J,"Raskutti, G; Wainwright, MJ; Yu, B",,,,"Raskutti, Garvesh; Wainwright, Martin J.; Yu, Bin",,,Early Stopping and Non-parametric Regression: An Optimal Data-dependent Stopping Rule,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Early stopping is a form of regularization based on choosing when to stop running an iterative algorithm. Focusing on non-parametric regression in a reproducing kernel Hilbert space, we analyze the early stopping strategy for a form of gradient-descent applied to the least-squares loss function. We propose a data-dependent stopping rule that does not involve hold-out or cross-validation data, and we prove upper bounds on the squared error of the resulting function estimate, measured in either the L-2(P) and L-2(P-n) norm. These upper bounds lead to minimax-optimal rates for various kernel classes, including Sobolev smoothness classes and other forms of reproducing kernel Hilbert spaces. We show through simulation that our stopping rule compares favorably to two other stopping rules, one based on hold-out data and the other based on Stein's unbiased risk estimate. We also establish a tight connection between our early stopping strategy and the solution path of a kernel ridge regression estimator.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,335,366,,,,,,,,,,,,,,,,WOS:000335457400011,0
J,"Niu, G; Dai, B; Shang, L; Sugiyama, M",,,,"Niu, Gang; Dai, Bo; Shang, Lin; Sugiyama, Masashi",,,Maximum Volume Clustering: A New Discriminative Clustering Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-definite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hard-label MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method possesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed k-means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to infinity. Experiments on several artificial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2641,2687,,,,,,,,,,,,,,,,WOS:000327007400006,0
J,"Lisitsyn, S; Widmer, C; Garcia, FJI",,,,"Lisitsyn, Sergey; Widmer, Christian; Garcia, Fernando J. Iglesias",,,Tapkee: An Efficient Dimension Reduction Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present Tapkee, a C++ template library that provides efficient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced Barnes-Hut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and flexibility. For performance, we combine efficient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve flexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2355,2359,,,,,,,,,,,,,,,,WOS:000324799600006,0
J,"Arora, R; Gupta, MR; Kapila, A; Fazel, M",,,,"Arora, Raman; Gupta, Maya R.; Kapila, Amol; Fazel, Maryam",,,Similarity-based Clustering by Left-Stochastic Matrix Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efficient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efficient hierarchical variant performs surprisingly well.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1715,1746,,,,,,,,,,,,,,,,WOS:000323367000002,0
J,"Clemencon, S; Depecker, M; Vayatis, N",,,,"Clemencon, Stephan; Depecker, Marine; Vayatis, Nicolas",,,Ranking Forests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The present paper examines how the aggregation and feature randomization principles underlying the algorithm RANDOM FOREST (Breiman, 2001) can be adapted to bipartite ranking. The approach taken here is based on nonparametric scoring and ROC curve optimization in the sense of the AUC criterion. In this problem, aggregation is used to increase the performance of scoring rules produced by ranking trees, as those developed in Clemencon and Vayatis (2009c). The present work describes the principles for building median scoring rules based on concepts from rank aggregation. Consistency results are derived for these aggregated scoring rules and an algorithm called RANKING FOREST is presented. Furthermore, various strategies for feature randomization are explored through a series of numerical experiments on artificial data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,39,73,,,,,,,,,,,,,,,,WOS:000314530200002,0
J,"Chai, KMA",,,,"Chai, Kian Ming A.",,,Variational Multinomial Logit Gaussian Process,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian process prior with an appropriate likelihood function is a flexible non-parametric model for a variety of learning tasks. One important and standard task is multi-class classification, which is the categorization of an item into one of several fixed classes. A usual likelihood function for this is the multinomial logistic likelihood function. However, exact inference with this model has proved to be difficult because high-dimensional integrations are required. In this paper, we propose a variational approximation to this model, and we describe the optimization of the variational parameters. Experiments have shown our approximation to be tight. In addition, we provide data-independent bounds on the marginal likelihood of the model, one of which is shown to be much tighter than the existing variational mean-field bound in the experiments. We also derive a proper lower bound on the predictive likelihood that involves the Kullback-Leibler divergence between the approximating and the true posterior. We combine our approach with a recently proposed sparse approximation to give a variational sparse approximation to the Gaussian process multi-class model. We also derive criteria which can be used to select the inducing set, and we show the effectiveness of these criteria over random selection in an experiment.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1745,1808,,,,,,,,,,,,,,,,WOS:000307020700002,0
J,"Subramanya, A; Bilmes, J",,,,"Subramanya, Amarnag; Bilmes, Jeff",,,Semi-Supervised Learning with Measure Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a new objective for graph-based semi-supervised learning based on minimizing the Kullback-Leibler divergence between discrete probability measures that encode class membership probabilities. We show how the proposed objective can be efficiently optimized using alternating minimization. We prove that the alternating minimization procedure converges to the correct optimum and derive a simple test for convergence. In addition, we show how this approach can be scaled to solve the semi-supervised learning problem on very large data sets, for example, in one instance we use a data set with over 10(8) samples. In this context, we propose a graph node ordering algorithm that is also applicable to other graph-based semi-supervised learning approaches. We compare the proposed approach against other standard semi-supervised learning algorithms on the semi-supervised learning benchmark data sets (Chapelle et al., 2007), and other real-world tasks such as text classification on Reuters and WebKB, speech phone classification on TIMIT and Switchboard, and linguistic dialog-act tagging on Dihana and Switchboard. In each case, the proposed approach outperforms the state-of-the-art. Lastly, we show that our objective can be generalized into a form that includes the standard squared-error loss, and we prove a geometric rate of convergence in that case.",,,,,"Zhang, JinYuan/C-1542-2010",,,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3311,3370,,,,,,,,,,,,,,,,WOS:000298103700009,0
J,"Yu, SP; Krishnapuram, B; Rosales, R; Rao, RB",,,,"Yu, Shipeng; Krishnapuram, Balaji; Rosales, Romer; Rao, R. Bharat",,,Bayesian Co-Training,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clarifies the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classifiers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classification on all the samples. Experiments on toy data and several real world data sets illustrate the benefits of this approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2649,2680,,,,,,,,,,,,,,,,WOS:000298102900004,0
J,"Blei, DM; Frazier, PI",,,,"Blei, David M.; Frazier, Peter I.",,,Distance Dependent Chinese Restaurant Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop the distance dependent Chinese restaurant process, a flexible class of distributions over partitions that allows for dependencies between the elements. This class can be used to model many kinds of dependencies between data in infinite clustering models, including dependencies arising from time, space, and network connectivity. We examine the properties of the distance dependent CRP, discuss its connections to Bayesian nonparametric mixture models, and derive a Gibbs sampler for both fully observed and latent mixture settings. We study its empirical performance with three text corpora. We show that relaxing the assumption of exchangeability with distance dependent CRPs can provide a better fit to sequential data and network data. We also show that the distance dependent CRP representation of the traditional CRP mixture leads to a faster-mixing Gibbs sampling algorithm than the one based on the original formulation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2011,12,,,,,,2461,2488,,,,,,,,,,,,,,,,WOS:000298102200001,0
J,"Cohn, T; Blunsom, P; Goldwater, S",,,,"Cohn, Trevor; Blunsom, Phil; Goldwater, Sharon",,,Inducing Tree-Substitution Grammars,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inducing a grammar from text has proven to be a notoriously challenging learning task despite decades of research. The primary reason for its difficulty is that in order to induce plausible grammars, the underlying model must be capable of representing the intricacies of language while also ensuring that it can be readily learned from data. The majority of existing work on grammar induction has favoured model simplicity (and thus learnability) over representational capacity by using context free grammars and first order dependency grammars, which are not sufficiently expressive to model many common linguistic constructions. We propose a novel compromise by inferring a probabilistic tree substitution grammar, a formalism which allows for arbitrarily large tree fragments and thereby better represent complex linguistic structures. To limit the model's complexity we employ a Bayesian non-parametric prior which biases the model towards a sparse grammar with shallow productions. We demonstrate the model's efficacy on supervised phrase-structure parsing, where we induce a latent segmentation of the training treebank, and on unsupervised dependency grammar induction. In both cases the model uncovers interesting latent linguistic structures while producing competitive results.",,,,,"Cohn, Trevor/E-8053-2014","Cohn, Trevor/0000-0003-4363-1673",,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3053,3096,,,,,,,,,,,,,,,,WOS:000285643600004,0
J,"Zhang, HZ; Xu, YS; Zhang, J",,,,"Zhang, Haizhang; Xu, Yuesheng; Zhang, Jun",,,Reproducing Kernel Banach Spaces for Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semi-inner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established.",,,,,"Xu, yue/HGE-1737-2022; Zhang, Haizhang/GPX-1222-2022","Zhang, Haizhang/0000-0002-8241-3145",,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2741,2775,,,,,,,,,,,,,,,,WOS:000273877300002,0
J,"Kanamori, T; Hido, S; Sugiyama, M",,,,"Kanamori, Takafumi; Hido, Shohei; Sugiyama, Masashi",,,A Least-squares Approach to Direct Importance Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks such as covariate shift adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally highly efficient and simple to implement. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bounds. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1391,1445,,,,,,,,,,,,,,,,WOS:000270825000004,0
J,"Vural, V; Fung, G; Krishnapuram, B; Dy, JG; Rao, B",,,,"Vural, Volkan; Fung, Glenn; Krishnapuram, Balaji; Dy, Jennifer G.; Rao, Bharat",,,Using Local Dependencies within Batches to Improve Large Margin Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. This paper introduces three algorithms for classifying all the samples in a batch jointly: one based on a probabilistic formulation, and two based on mathematical programming analysis. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significantly more accurate than a naive support vector machine which ignores the correlations among the samples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,183,206,,,,,,,,,,,,,,,,WOS:000270824200004,0
J,"Larochelle, H; Bengio, Y; Louradour, J; Lamblin, P",,,,"Larochelle, Hugo; Bengio, Yoshua; Louradour, Jerome; Lamblin, Pascal",,,Exploring Strategies for Training Deep Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.",,,,,,"Larochelle, Hugo/0000-0002-2960-9288",,,,,,,,,,,,,1532-4435,,,,,JAN,2009,10,,,,,,1,40,,,,,,,,,,,,,,,,WOS:000270824100001,0
J,"Ricci, E; De Bie, T; Cristianini, N",,,,"Ricci, Elisa; De Bie, Tijl; Cristianini, Nello",,,Magic Moments for Structured Output Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efficiently computing the first two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction.",,,,,"De Bie, Tijl/B-2920-2013","De Bie, Tijl/0000-0002-2692-7504; Ricci, Elisa/0000-0002-0228-1147",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2803,2846,,,,,,,,,,,,,,,,WOS:000263240700007,0
J,"van der Maaten, L; Hinton, G",,,,"van der Maaten, Laurens; Hinton, Geoffrey",,,Visualizing Data using t-SNE,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new technique called t-SNE that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2579,2605,,,,,,,,,,,,,,,,WOS:000262637600007,0
J,"Bartlett, PL; Wegkamp, MH",,,,"Bartlett, Peter L.; Wegkamp, Marten H.",,,Classification with a Reject Option using a Hinge Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of binary classification where the classifier can, for a particular cost, choose not to classify an observation. Just as in the conventional classification problem, minimization of the sample average of the cost is a difficult optimization problem. As an alternative, we propose the optimization of a certain convex loss function phi, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efficiently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss-the phi-risk-also minimizes the risk. We also study the rate at which the phi-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1 vertical bar X) is unlikely to be close to certain critical values.",,,,,,"Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1823,1840,,,,,,,,,,,,,,,,WOS:000262636800007,0
J,"Fan, RE; Chang, KW; Hsieh, CJ; Wang, XR; Lin, CJ",,,,"Fan, Rong-En; Chang, Kai-Wei; Hsieh, Cho-Jui; Wang, Xiang-Rui; Lin, Chih-Jen",,,LIBLINEAR: A Library for Large Linear Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.,,,,,"Chang, Kai-Wei/M-6055-2016","Chang, Kai-Wei/0000-0001-5365-0072; Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1871,1874,,,,,,,,,,,,,,,,WOS:000262636800009,0
J,"Goldberg, Y; Zakai, A; Kushnir, D; Ritov, Y",,,,"Goldberg, Yair; Zakai, Alon; Kushnir, Dan; Ritov, Ya'acov",,,Manifold Learning: The Price of Normalization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the performance of a class of manifold-learning algorithms that find their output by minimizing a quadratic form under some normalization constraints. This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps. We present and prove conditions on the manifold that are necessary for the success of the algorithms. Both the finite sample case and the limit case are analyzed. We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds. Finally, we present numerical results that demonstrate our claims.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1909,1939,,,,,,,,,,,,,,,,WOS:000262636800011,0
J,"Srinivasan, A; King, RD",,,,"Srinivasan, Ashwin; King, Ross D.",,,Incremental identification of qualitative models of biological systems using inductive logic programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identification from first principles can be extremely difficult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identification technique must be powerful enough to identify substantially complex models; and (3) There may not be sufficient data to obtain both the model's structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of first-order logic. Specifically, they are expressed as logic programs; (2) System identification is done using techniques developed in Inductive Logic Programming (ILP). This allows the identification of first-order logic models from data. Specifically, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to qualitative models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identification of models at four scales of biological organisation, namely: (a) a predator-prey model at the ecosystem level; (b) a model for the human lung at the organ level; (c) a model for regulation of glucose by insulin in the human body at the extra-cellular level; and (d) a model for the glycolysis metabolic pathway at the cellular level.",,,,,,"King, Ross/0000-0001-7208-4387",,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1475,1533,,,,,,,,,,,,,,,,WOS:000258646800007,0
J,"Lucke, J; Sahani, M",,,,"Lucke, Jorg; Sahani, Maneesh",,,Maximal causes for non-linear component extraction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a generative model in which hidden causes combine competitively to produce observations. Multiple active causes combine to determine the value of an observed variable through a max function, in the place where algorithms such as sparse coding, independent component analysis, or non-negative matrix factorization would use a sum. This max rule can represent a more realistic model of non-linear interaction between basic components in many settings, including acoustic and image data. While exact maximum-likelihood learning of the parameters of this model proves to be intractable, we show that efficient approximations to expectation-maximization (EM) can be found in the case of sparsely active hidden causes. One of these approximations can be formulated as a neural network model with a generalized softmax activation function and Hebbian learning. Thus, we show that learning in recent softmax-like neural networks may be interpreted as approximate maximization of a data likelihood. We use the bars benchmark test to numerically verify our analytical results and to demonstrate the competitiveness of the resulting algorithms. Finally, we show results of learning model parameters to fit acoustic and visual data sets in which max-like component combinations arise naturally.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1227,1267,,,,,,,,,,,,,,,,WOS:000258646300009,0
J,"Seeger, MW",,,,"Seeger, Matthias W.",,,Cross-validation optimization for large scale structured classification kernel methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a highly efficient framework for penalized likelihood kernel methods applied to multi-class models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the fitting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classification tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1147,1178,,,,,,,,,,,,,,,,WOS:000258646300007,0
J,"Krupka, E; Tishby, N",,,,"Krupka, Eyal; Tishby, Naftali",,,Generalization from observed to unobserved features by clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We argue that when objects are characterized by many attributes, clustering them on the basis of a random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove finite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. We use our framework to analyze generalization to unobserved features of two well-known clustering algorithms: k-means and the maximum likelihood multinomial mixture model. The scheme is demonstrated for collaborative filtering of users with movie ratings as attributes and document clustering with words as attributes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,339,370,,,,,,,,,,,,,,,,WOS:000256642000001,0
J,"Hue, C; Boulle, M",,,,"Hue, Carine; Boulle, Marc",,,A new probabilistic approach in rank regression with optimal Bayesian partitioning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we define a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A first set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overfitting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2007,8,,,,,,2727,2754,,,,,,,,,,,,,,,,WOS:000252745100003,0
J,"Nunez, M; Fidalgo, R; Morales, R",,,,"Nunez, Marlon; Fidalgo, Raul; Morales, Rafael",,,Learning in environments with unknown dynamics: Towards more robust concept learners,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the field of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain ( virtual drift).",,,,,"N√∫√±ez, Marlon/P-1334-2014","N√∫√±ez, Marlon/0000-0001-5374-5231",,,,,,,,,,,,,1532-4435,,,,,NOV,2007,8,,,,,,2595,2628,,,,,,,,,,,,,,,,WOS:000252744900003,0
J,"Melvin, I; Ie, E; Wetson, J; Noble, WS; Leslie, C",,,,"Melvin, Iain; Ie, Eugene; Wetson, Jason; Noble, William Stafford; Leslie, Christina",,,Multi-class protein classification using adaptive codes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predicting a protein's structural class from its amino acid sequence is a fundamental problem in computational biology. Recent machine learning work in this domain has focused on developing new input space representations for protein sequences, that is, string kernels, some of which give state-of-the-art performance for the binary prediction task of discriminating between one class and all the others. However, the underlying protein classification problem is in fact a huge multiclass problem, with over 1000 protein folds and even more structural subcategories organized into a hierarchy. To handle this challenging many-class problem while taking advantage of progress on the binary problem, we introduce an adaptive code approach in the output space of one-vs-the-rest prediction scores. Specifically, we use a ranking perceptron algorithm to learn a weighting of binary classifiers that improves multi-class prediction with respect to a fixed set of output codes. We use a cross-validation set-up to generate output vectors for training, and we define codes that capture information about the protein structural hierarchy. Our code weighting approach significantly improves on the standard one-vs-all method for two difficult multi-class protein classification problems: remote homology detection and fold recognition. Our algorithm also outperforms a previous code learning approach due to Crammer and Singer, trained here using a perceptron, when the dimension of the code vectors is high and the number of classes is large. Finally, we compare against PSI-BLAST, one of the most widely used methods in protein sequence analysis, and find that our method strongly outperforms it on every structure classification problem that we consider.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1557,1581,,,,,,,,,,,,,,,,WOS:000249353700007,0
J,"Saar-Tsechansky, M; Provost, F",,,,"Saar-Tsechansky, Maytal; Provost, Foster",,,Handling missing values when applying classification models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper first compares several different methods-predictive value imputation, the distribution-based imputation used by C4.5, and using reduced models-for applying classification trees to instances with missing values ( and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its ( perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1625,1657,,,,,,,,,,,,,,,,WOS:000249353700009,0
J,"Zhao, P; Yu, B",,,,"Zhao, Peng; Yu, Bin",,,On model selection consistency of Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are irrepresentable (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2541,2563,,,,,,,,,,,,,,,,WOS:000245390700010,0
J,"Raghavan, H; Madani, O; Jones, R",,,,"Raghavan, Hema; Madani, Omid; Jones, Rosie",,,Active learning with feedback on both features and instances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We extend the traditional active learning framework to include feedback on features in addition to labeling instances, and we execute a careful study of the effects of feature selection and human feedback on features in the setting of text categorization. Our experiments on a variety of categorization tasks indicate that there is significant potential in improving classifier performance by feature re-weighting, beyond that achieved via membership queries alone (traditional active learning) if we have access to an oracle that can point to the important (most predictive) features. Our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion of the most relevant features (over 50% in our experiments). We find that on average, labeling a feature takes much less time than labeling a document. We devise an algorithm that interleaves labeling features and documents which significantly accelerates standard active learning in our simulation experiments. Feature feedback can complement traditional active learning in applications such as news filtering, e-mail classification, and personalization, where the human teacher can have significant knowledge on the relevance of features.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2006,7,,,,,,1655,1686,,,,,,,,,,,,,,,,WOS:000245389200001,0
J,"Heiler, M; Schnorr, C",,,,"Heiler, Matthias; Schnoerr, Christoph",,,Learning sparse representations by non-negative matrix factorization and sequential cone programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We exploit the biconvex nature of the Euclidean non-negative matrix factorization (NMF) optimization problem to derive optimization schemes based on sequential quadratic and second order cone programming. We show that for ordinary NMF, our approach performs as well as existing state-of-the-art algorithms, while for sparsity-constrained NMF, as recently proposed by P.O. Hoyer in JMLR 5 (2004), it outperforms previous methods. In addition, we show how to extend NMF learning within the same optimization framework in order to make use of class membership information in supervised learning problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1385,1407,,,,,,,,,,,,,,,,WOS:000245388800010,0
J,"Mangasarian, OL",,,,"Mangasarian, Olvi L.",,,Exact 1-norm support vector machines via unconstrained convex differentiable minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines utilizing the 1-norm, typically set up as linear programs (Mangasarian, 2000; Bradley and Mangasarian, 1998), are formulated here as a completely unconstrained minimization of a convex differentiable piecewise-quadratic objective function in the dual space. The objective function, which has a Lipschitz continuous gradient and contains only one additional finite parameter, can be minimized by a generalized Newton method and leads to an exact solution of the support vector machine problem. The approach here is based on a formulation of a very general linear program as an unconstrained minimization problem and its application to support vector machine classification problems. The present approach which generalizes both (Mangasarian, 2004) and (Fung and Mangasarian, 2004) is also applied to nonlinear approximation where a minimal number of nonlinear kernel functions are utilized to approximate a function from a given number of function values.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1517,1530,,,,,,,,,,,,,,,,WOS:000245388800015,0
J,"Cesa-Bianchi, N; Gentile, C; Zaniboni, L",,,,"Cesa-Bianchi, N; Gentile, C; Zaniboni, L",,,Incremental algorithms for hierarchical classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of classifying data in a given taxonomy when classifications associated with multiple and/or partial paths are allowed. We introduce a new algorithm that incrementally learns a linear-threshold classifier for each node of the taxonomy. A hierarchical classification is obtained by evaluating the trained node classifiers in a top-down fashion. To evaluate classifiers in our multipath framework, we define a new hierarchical loss function, the H-loss, capturing the intuition that whenever a classification mistake is made on a node of the taxonomy, then no loss should be charged for any additional mistake occurring in the subtree of that node. Making no assumptions on the mechanism generating the data instances, and assuming a linear noise model for the labels, we bound the H-loss of our on-line algorithm in terms of the H-loss of a reference classifier knowing the true parameters of the label-generating process. We show that, in expectation, the excess cumulative H-loss grows at most logarithmically in the length of the data sequence. Furthermore, our analysis reveals the precise dependence of the rate of convergence on the eigenstructure of the data each node observes. Our theoretical results are complemented by a number of experiments on texual corpora. In these experiments we show that, after only one epoch of training, our algorithm performs much better than Perceptron-based hierarchical classifiers, and reasonably close to a hierarchical support vector machine.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,31,54,,,,,,,,,,,,,,,,WOS:000236331400002,0
J,"Mohammadi, L; van de Geer, S",,,,"Mohammadi, L; van de Geer, S",,,Asymptotics in empirical risk minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"xIn this paper, we study a two-category classification problem. We indicate the categories by labels Y = 1 and Y = -1. We observe a covariate, or feature, X is an element of X subset of R-d. Consider a collection {h(a)} of classifiers indexed by a finite-dimensional parameter a, and the classifier h(a*) that minimizes the prediction error over this class. The parameter a* is estimated by the empirical risk minimizer (a) over cap (n) over the class, where the empirical risk is calculated on a training sample of size n. We apply the Kim Pollard Theorem to show that under certain differentiability assumptions, (a) over cap (n) converges to a* with rate n(-1/3), and also present the asymptotic distribution of the renormalized estimator. For example, let V-0 denote the set of x on which, given X = x, the label Y = 1 is more likely (than the label Y = -1). If X is one-dimensional, the set V-0 is the union of disjoint intervals. The problem is then to estimate the thresholds of the intervals. We obtain the asymptotic distribution of the empirical risk minimizer when the classifiers have K thresholds, where K is fixed. We furthermore consider an extension to higher-dimensional X, assuming basically that V-0 has a smooth boundary in some given parametric class. We also discuss various rates of convergence when the differentiability conditions are possibly violated. Here, we again restrict ourselves to one-dimensional X. We show that the rate is n- 1 in certain cases, and then also obtain the asymptotic distribution for the empirical prediction error.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2027,2047,,,,,,,,,,,,,,,,WOS:000236331100006,0
J,"Cowell, RG",,,,"Cowell, RG",,,Local propagation in conditional Gaussian Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes a scheme for local computation in conditional Gaussian Bayesian networks that combines the approach of Lauritzen and Jensen (2001) with some elements of Shachter and Kenley (1989). Message passing takes place on an elimination tree structure rather than the more compact (and usual) junction tree of cliques. This yields a local computation scheme in which all calculations involving the continuous variables are performed by manipulating univariate regressions, and hence matrix operations are avoided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1517,1550,,,,,,,,,,,,,,,,WOS:000236330100010,0
J,"Dekel, O; Shalev-Shwartz, S; Singer, Y",,,,"Dekel, O; Shalev-Shwartz, S; Singer, Y",,,Smooth epsilon-insensitive regression by loss symmetrization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe new loss functions for regression problems along with an accompanying algorithmic framework which utilizes these functions. These loss functions are derived by symmetrization of margin-based losses commonly used in boosting algorithms, namely, the logistic loss and the exponential loss. The resulting symmetric logistic loss can be viewed as a smooth approximation to the epsilon-insensitive hinge loss used in support vector regression. We describe and analyze two parametric families of batch learning algorithms for minimizing these symmetric losses. The first family employs an iterative log-additive update which can be viewed as a regression counterpart to recent boosting algorithms. The second family utilizes an iterative additive update step. We also describe and analyze online gradient descent (GD) and exponentiated gradient (EG) algorithms for the symmetric logistic loss. A byproduct of our work is a new simple form of regularization for boosting-based classification and regression algorithms. Our regression framework also has implications on classification algorithms, namely, a new additive update boosting algorithm for classification. We demonstrate the merits of our algorithms in a series of experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,711,741,,,,,,,,,,,,,,,,WOS:000236329700001,0
J,"Chechik, G; Globerson, A; Tishby, N; Weiss, Y",,,,"Chechik, G; Globerson, A; Tishby, N; Weiss, Y",,,Information bottleneck for Gaussian variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another-relevance-variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Sigma(x vertical bar y)Sigma(-1)(x), which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the information-curve), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.",,,,,,"Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,165,188,,,,,,,,,,,,,,,,WOS:000236328800006,0
J,"Teh, YW; Welling, M; Osindero, S; Hinton, GE",,,,"Teh, YW; Welling, M; Osindero, S; Hinton, GE",,,Energy-based models for sparse overcomplete representations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.",,,,,"Teh, Yee Whye/C-3400-2008",,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1235,1260,,,,,,,,,,,,,,,,WOS:000224808300004,0
J,"Tsitsiklis, JN",,,,"Tsitsiklis, JN",,,On the convergence of optimistic policy iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a finite-state Markov decision problem and establish the convergence of a special case of optimistic policy iteration that involves Monte Carlo estimation of Q-values, in conjunction with greedy policy selection. We provide convergence results for a number of algorithmic variations, including one that involves temporal difference learning (bootstrapping) instead of Monte Carlo estimation. We also indicate some extensions that either fail or are unlikely to go through.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,59,72,,10.1162/153244303768966102,0,,,,,,,,,,,,,WOS:000181462700003,0
J,"Li, Z; Han, JQ; E, WN; Li, QX",,,,"Li, Zhong; Han, Jiequn; E, Weinan; Li, Qianxiao",,,Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We perform a systematic study of the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. On the approximation side, we prove a direct and an inverse approximation theorem of linear functionals using RNNs, which reveal the intricate connections between memory structures in the target and the corresponding approximation efficiency. In particular, we show that temporal relationships can be effectively approximated by RNNs if and only if the former possesses sufficient memory decay. On the optimization front, we perform detailed analysis of the optimization dynamics, including a precise understanding of the difficulty that may arise in learning relationships with long-term memory. The term curse of memory is coined to describe the uncovered phenomena, akin to the curse of dimension that plagues high dimensional function approximation. These results form a relatively complete picture of the interaction of memory and recurrent structures in the linear dynamical setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,85,,,,,,,,,,,,,,,,WOS:000752365400001,0
J,"Peruzzi, M; Dunson, DB",,,,"Peruzzi, Michele; Dunson, David B.",,,Spatial Multivariate Trees for Big Data Bayesian Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High resolution geospatial data are challenging because standard geostatistical models based on Gaussian processes are known to not scale to large data sizes. While progress has been made towards methods that can be computed more efficiently, considerably less attention has been devoted to methods for large scale data that allow the description of complex relationships between several outcomes recorded at high resolutions by different sensors. Our Bayesian multivariate regression models based on spatial multivariate trees (SPAMTREES) achieve scalability via conditional independence assumptions on latent random effects following a treed directed acyclic graph. Information-theoretic arguments and considerations on computational efficiency guide the construction of the tree and the related efficient sampling algorithms in imbalanced multivariate settings. In addition to simulated data examples, we illustrate SPAMTREES using a large climate data set which combines satellite data with land-based station data. Software and source code are available on CRAN at https://CRAN.R-project.org/package=spaintree.",,,,,,"Peruzzi, Michele/0000-0003-4242-8059",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,35891979,,,,,WOS:000752281500001,0
J,"Agarwal, A; Kakade, SM; Lee, JD; Mahajan, G",,,,"Agarwal, Alekh; Kakade, Sham M.; Lee, Jason D.; Mahajan, Gaurav",,,"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution or how they cope with approximation error due to using a restricted class of parametric policies. This work provides provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: tabular policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy; and parametric policy classes (considering both log-linear and neural policy classes), which may not contain the optimal policy and where we provide agnostic learning results. One central contribution of this work is in providing approximation guarantees that are average case - which avoid explicit worst-case dependencies on the size of state space - by making a formal connection to supervised learning under distribution shift. This characterization shows an important interplay between estimation error, approximation error, and exploration (as characterized through a precisely defined condition number).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000663150200001,0
J,"Bhatia, JS",,,,"Bhatia, Jagdeep Singh",,,Simple and Fast Algorithms for Interactive Machine Learning with Random Counter-examples,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work describes simple and efficient algorithms for interactively learning non-binary concepts in the learning from random counter-examples (LRC) model. Here, learning takes place from random counter-examples that the learner receives in response to their proper equivalence queries, and the learning time is the number of counter-examples needed by the learner to identify the target concept. Such learning is particularly suited for online ranking, classification, clustering, etc., where machine learning models must be used before they are fully trained. We provide two simple LRC algorithms, deterministic and randomized, for exactly learning concepts from any concept class H. We show that both these algorithms have an O(log vertical bar H vertical bar) asymptotically optimal average learning time. This solves an open problem on the existence of an efficient LRC randomized algorithm while also simplifying previous results and improving their computational efficiency. We also show that the expected learning time of any Arbitrary LRC algorithm can be upper bounded by O(1/epsilon log vertical bar H vertical bar/delta), where epsilon and delta are the allowed learning error and failure probability respectively. This shows that LRC interactive learning is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning. Our simulations also show that these algorithms outperform their theoretical bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500015,0
J,"Binder, M; Pfisterer, F; Lang, M; Schneider, L; Kotthoff, L; Bischl, B",,,,"Binder, Martin; Pfisterer, Florian; Lang, Michel; Schneider, Lennart; Kotthoff, Lars; Bischl, Bernd",,,mlr3pipelines-Flexible Machine Learning Pipelines in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent years have seen a proliferation of ML frameworks. Such systems make ML accessible to non-experts, especially when combined with powerful parameter tuning and AutoML techniques. Modern, applied ML extends beyond direct learning on clean data, however, and needs an expressive language for the construction of complex ML workflows beyond simple pre-and post-processing. We present mlr3pipelines, an R framework which can be used to define linear and complex non-linear ML workflows as directed acyclic graphs. The framework is part of the mlr3 ecosystem, leveraging convenient resampling, benchmarking, and tuning components.",,,,,"Kotthoff, Lars/AFV-6526-2022","Lang, Michel/0000-0001-9754-0393",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700315600001,0
J,"Flamary, R; Courty, N; Gramfort, A; Alaya, MZ; Boisbunon, A; Chambon, S; Chapel, L; Corenflos, A; Fatras, K; Fournier, N; Gautheron, L; Gayraud, NTH; Janati, H; Rakotomamonjy, A; Redko, I; Rolet, A; Schutz, A; Seguy, V; Sutherland, DJ; Tavenard, R; Tong, A; Vayer, T",,,,"Flamary, Remi; Courty, Nicolas; Gramfort, Alexandre; Alaya, Mokhtar Z.; Boisbunon, Aurelie; Chambon, Stanislas; Chapel, Laetitia; Corenflos, Adrien; Fatras, Kilian; Fournier, Nemo; Gautheron, Leo; Gayraud, Nathalie T. H.; Janati, Hicham; Rakotomamonjy, Alain; Redko, Ievgen; Rolet, Antoine; Schutz, Antony; Seguy, Vivien; Sutherland, Danica J.; Tavenard, Romain; Tong, Alexander; Vayer, Titouan",,,POT: Python Optimal Transport,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.",,,,,"Flamary, R√©mi/AAC-1958-2022; Alaya, Mokhtar Z./AAM-9242-2021","Flamary, R√©mi/0000-0002-4212-6627; Alaya, Mokhtar Z./0000-0002-1103-6944",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,2021,,,,,,,,,,,,,,,WOS:000656391000001,0
J,"Krishnamurthy, V; Yin, G",,,,"Krishnamurthy, Vikram; Yin, George",,,Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inverse reinforcement learning (IRL) aims to estimate the reward function of optimizing agents by observing their response (estimates or actions). This paper considers IRL when noisy estimates of the gradient of a reward function generated by multiple stochastic gradient agents are observed. We present a generalized Langevin dynamics algorithm to estimate the reward function R(theta); specifically, the resulting Langevin algorithm asymptotically generates samples from the distribution proportional to exp(R(theta)). The proposed adaptive IRL algorithms use kernel-based passive learning schemes. We also construct multi-kernel passive Langevin algorithms for IRL which are suitable for high dimensional data and achieve variance reduction. The performance of the proposed IRL algorithms are illustrated on examples in adaptive Bayesian learning, logistic regression (high dimensional problem) and constrained Markov decision processes. We prove weak convergence of the proposed IRL algorithms using martingale averaging methods. We also analyze the tracking performance of the IRL algorithms in non-stationary environments where the utility function R(theta) has a hyper-parameter that jump changes over time as a slow Markov chain which is not known to the inverse learner. In this case, martingale averaging yields a Markov switched diffusion limit as the asymptotic behavior of the IRL algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,49,121,,,,,,,,,,,,,,,WOS:000663172400001,0
J,"Lei, Y; Ying, Y",,,,"Lei, Yunwen; Ying, Yiming",,,Stochastic Proximal AUC Maximization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we consider the problem of maximizing the Area under the ROC curve (AUC) which is a widely used performance metric in imbalanced classification and anomaly detection. Due to the pairwise nonlinearity of the objective function, classical SGD algorithms do not apply to the task of AUC maximization. We propose a novel stochastic proximal algorithm for AUC maximization which is scalable to large scale streaming data. Our algorithm can accommodate general penalty terms and is easy to implement with favorable O(d) space and per-iteration time complexities. We establish a high-probability convergence rate O (1/root T) for the general convex setting, and improve it to a fast convergence rate O(1/T) for the cases of strongly convex regularizers and no regularization term (without strong convexity). Our proof does not need the uniform boundedness assumption on the loss function or the iterates which is more fidelity to the practice. Finally, we perform extensive experiments over various benchmark data sets from real-world application domains which show the superior performance of our algorithm over the existing AUC maximization algorithms.",,,,,"Ying, Yiming/AGD-7246-2022; Lei, Yunwen/V-2782-2018","Ying, Yiming/0000-0001-7345-6672; Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,45,,,,,,,,,,,,,,,,WOS:000656361300001,0
J,"Li, WW; Hannig, J; Mukherjee, S",,,,"Li, Weiwei; Hannig, Jan; Mukherjee, Sayan",,,Subspace Clustering through Sub-Clusters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of dimension reduction is of increasing importance in modern data analysis. In this paper, we consider modeling the collection of points in a high dimensional space as a union of low dimensional subspaces. In particular we propose a highly scalable sampling based algorithm that clusters the entire data via first spectral clustering of a small random sample followed by classifying or labeling the remaining out-of-sample points. The key idea is that this random subset borrows information across the entire dataset and that the problem of clustering points can be replaced with the more efficient problem of clustering sub-clusters. We provide theoretical guarantees for our procedure. The numerical results indicate that for large datasets the proposed algorithm outperforms other state-of-the-art subspace clustering algorithms with respect to accuracy and speed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,53,,,,,,,,,,,,,,,WOS:000656353300001,0
J,"Matsubara, T; Oates, CJ; Briol, FX",,,,"Matsubara, Takuo; Oates, Chris J.; Briol, Francois-Xavier",,,The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian neural networks attempt to combine the strong predictive performance of neural networks with formal quantification of uncertainty associated with the predictive output in the Bayesian framework. However, it remains unclear how to endow the parameters of the network with a prior distribution that is meaningful when lifted into the output space of the network. A possible solution is proposed that enables the user to posit an appropriate Gaussian process covariance function for the task at hand. Our approach constructs a prior distribution for the parameters of the network, called a ridgelet prior, that approximates the posited Gaussian process in the output space of the network. In contrast to existing work on the connection between neural networks and Gaussian processes, our analysis is non-asymptotic, with finite sample-size error bounds provided. This establishes the universality property that a Bayesian neural network can approximate any Gaussian process whose covariance function is sufficiently regular. Our experimental assessment is limited to a proof-of-concept, where we demonstrate that the ridgelet prior can out-perform an unstructured prior on regression problems for which a suitable Gaussian process prior can be provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687044400001,0
J,"Qian, X; Qu, Z; Richtarik, P",,,,"Qian, Xun; Qu, Zheng; Richtarik, Peter",,,L-SVRG and L-Katyusha with Arbitrary Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop and analyze a new family of nonaccelerated and accelerated loopless variance-reduced methods for finite-sum optimization problems. Our convergence analysis relies on a novel expected smoothness condition which upper bounds the variance of the stochastic gradient estimation by a constant times a distance-like function. This allows us to handle with ease arbitrary sampling schemes as well as the nonconvex case. We perform an indepth estimation of these expected smoothness parameters and propose new importance samplings which allow linear speedup when the expected minibatch size is in a certain range. Furthermore, a connection between these expected smoothness parameters and expected separable overapproximation (ESO) is established, which allows us to exploit data sparsity as well. Our general methods and results recover as special cases the loopless SVRG (Hofmann et al., 2015) and loopless Katyusha (Kovalev et al., 2019) methods.",,,,,,"Qu, Zheng/0000-0003-0883-2277; Qian, Xun/0000-0002-6072-2684",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,112,,,,,,,,,,,,,,,WOS:000663167800001,0
J,"Sarkar, P; Wang, YXR; Mukherjee, SS",,,,"Sarkar, Purnamrita; Wang, Y. X. Rachel; Mukherjee, Soumendu Sundar",,,When random initializations help: a study of variational inference for community detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variational approximation has been widely used in large-scale Bayesian inference recently, the simplest kind of which involves imposing a mean field assumption to approximate complicated latent structures. Despite the computational scalability of mean field, theoretical studies of its loss function surface and the convergence behavior of iterative updates for optimizing the loss are far from complete. In this paper, we focus on the problem of community detection for a simple two-class Stochastic Blockmodel (SBM) with equal class sizes. Using batch co-ordinate ascent (BCAVI) for updates, we show different convergence behavior with respect to different initializations. When the parameters are known or estimated within a reasonable range and held fixed, we characterize conditions under which an initialization can converge to the ground truth. On the other hand, when the parameters need to be estimated iteratively, a random initialization will converge to an uninformative local optimum.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500022,0
J,"Sarkar, T; Rakhlin, A; Dahleh, MA",,,,"Sarkar, Tuhin; Rakhlin, Alexander; Dahleh, Munther A.",,,Finite Time LTI System Identification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of learning the parameters of a stable linear time invariant (LTI) system with unknown latent space dimension, or order, from a single time-series of noisy input-output data. We focus on learning the best lower order approximation allowed by finite data. Motivated by subspace algorithms in systems theory, where the doubly infinite system Hankel matrix captures both order and good lower order approximations, we construct a Hankel-like matrix from noisy finite data using ordinary least squares. This circumvents the non-convexities that arise in system identification, and allows accurate estimation of the underlying LTI system. Our results rely on careful analysis of self-normalized martingale difference terms that helps bound identification error up to logarithmic factors of the lower bound. We provide a data-dependent scheme for order selection and find an accurate realization of system parameters, corresponding to that order, by an approach that is closely related to the Ho-Kalman subspace algorithm. We demonstrate that the proposed model order selection procedure is not overly conservative, i.e., for the given data length it is not possible to estimate higher order models or find higher order approximations with reasonable accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500026,0
J,"Agrawal, D; Papamarkou, T; Hinkle, J",,,,"Agrawal, Devanshu; Papamarkou, Theodore; Hinkle, Jacob",,,Wide Neural Networks with Bottlenecks are Deep Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There has recently been much work on the wide limit of neural networks, where Bayesian neural networks (BNNs) are shown to converge to a Gaussian process (GP) as all hidden layers are sent to infinite width. However, these results do not apply to architectures that require one or more of the hidden layers to remain narrow. In this paper, we consider the wide limit of BNNs where some hidden layers, called bottlenecks, are held at finite width. The result is a composition of GPs that we term a bottleneck neural network Gaussian process (bottleneck NNGP). Although intuitive, the subtlety of the proof is in showing that the wide limit of a composition of networks is in fact the composition of the limiting GPs. We also analyze theoretically a single-bottleneck NNGP, finding that the bottleneck induces dependence between the outputs of a multi-output network that persists through extreme post-bottleneck depths, and prevents the kernel of the network from losing discriminative power at extreme post-bottleneck depths.",,,,,"Hinkle, Jacob/AAM-6795-2021","Hinkle, Jacob/0000-0002-7751-1760",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,175,,,,,,,,,,,,,,,WOS:000570207100001,0
J,"Burt, DR; Rasmussen, CE; van der Wilk, M",,,,"Burt, David R.; Rasmussen, Carl Edward; van der Wilk, Mark",,,Convergence of Sparse Variational Inference in Gaussian Processes Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian processes are distributions over functions that are versatile and mathematically convenient priors in Bayesian modelling. However, their use is often impeded for data with large numbers of observations, N, due to the cubic (in N) cost of matrix operations used in exact inference. Many solutions have been proposed that rely on M << N inducing variables to form an approximation at a cost of O(NM2). While the computational cost appears linear in N, the true complexity depends on how M must scale with N to ensure a certain quality of the approximation. In this work, we investigate upper and lower bounds on how M needs to grow with N to ensure high quality approximations. We show that we can make the KL-divergence between the approximate model and the exact posterior arbitrarily small for a Gaussian-noise regression model with M << N. Specifically, for the popular squared exponential kernel and D-dimensional Gaussian distributed covariates, M = O ((log N)(D)) suffice and a method with an overall computational cost of O N (log N)(2D)(log log N)(2)) can be used to perform inference.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,131,,,,,,,,,,,,,,,WOS:000556297300001,0
J,"Faouzi, J; Janati, H",,,,"Faouzi, Johann; Janati, Hicham",,,pyts: A Python Package for Time Series Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, job1ib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples. Source code and documentation can be downloaded from https://github.com/johannf aouzi/pyts.",,,,,,"Faouzi, Johann/0000-0003-0542-9968",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000021,0
J,"Gu, MY; Shen, WN",,,,"Gu, Mengyang; Shen, Weining",,,Generalized probabilistic principal component analysis of correlated data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated. In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA) by providing the closed-form maximum marginal likelihood estimator of the factor loadings and other parameters. Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience. Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.",,,,,,"Shen, Weining/0000-0003-3137-1085",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300013,0
J,"Lai, ZR; Tan, LM; Wu, XT; Fang, LD",,,,"Lai, Zhao-Rong; Tan, Liming; Wu, Xiaotian; Fang, Liangda",,,Loss Control with Rank-one Covariance Estimate for Short-term Portfolio Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In short-term portfolio optimization (SPO), some financial characteristics like the expected return and the true covariance might be dynamic. Then there are only a small window size w of observations that are sufficiently close to the current moment and reliable to make estimations. w is usually much smaller than the number of assets d, which leads to a typical undersampled problem. Worse still, the asset price relatives are not likely subject to any proper distributions. These facts violate the statistical assumptions of the traditional covariance estimates and invalidate their statistical efficiency and consistency in risk measurement. In this paper, we propose to reconsider the function of covariance estimates in the perspective of operators, and establish a rank-one covariance estimate in the principal rank-one tangent space at the observation matrix. Moreover, we propose a loss control scheme with this estimate, which effectively catches the instantaneous risk structure and avoids extreme losses. We conduct extensive experiments on 7 real-world benchmark daily or monthly data sets with stocks, funds and portfolios from diverse regional markets to show that the proposed method achieves state-of-the-art performance in comprehensive downside risk metrics and gains good investing incomes as well. It offers a novel perspective of rank-related approaches for undersampled estimations in SPO.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,97,,,,,,,,,,,,,,,WOS:000545031000001,0
J,"Li, TX; Qian, C; Levina, E; Zhu, J",,,,"Li, Tianxi; Qian, Cheng; Levina, Elizaveta; Zhu, Ji",,,High-dimensional Gaussian graphical models on network-linked data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphical models are commonly used to represent conditional dependence relationships between variables. There are multiple methods available for exploring them from high-dimensional data, but almost all of them rely on the assumption that the observations are independent and identically distributed. At the same time, observations connected by a network are becoming increasingly common, and tend to violate these assumptions. Here we develop a Gaussian graphical model for observations connected by a network with potentially different mean vectors, varying smoothly over the network. We propose an efficient estimation algorithm and demonstrate its effectiveness on both simulated and real data, obtaining meaningful and interpretable results on a statistics coauthorship network. We also prove that our method estimates both the inverse covariance matrix and the corresponding graph structure correctly under the assumption of network cohesion, which refers to the empirically observed phenomenon of network neighbors sharing similar traits.",,,,,"zhu, ji/HDM-1538-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000026,0
J,"Marschall, O; Cho, K; Savin, C",,,,"Marschall, Owen; Cho, Kyunghyun; Savin, Cristina",,,A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing these algorithms on two parametric task families shows that performances cluster according to our criteria. Although a similar clustering is also observed for pairwise gradient alignment, alignment with exact methods does not explain ultimate performance. This suggests the need for better comparison metrics.",,,,,"Savin, Cristina/ABI-4570-2020","Savin, Cristina/0000-0002-3414-8244",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,135,,,,,,,,,,,,,,,WOS:000556301100001,0
J,"Raffel, C; Shazeer, N; Roberts, A; Lee, K; Narang, S; Matena, M; Zhou, YQ; Li, W; Liu, PJ",,,,"Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J.",,,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,140,,,,,,,,,,,,,,,WOS:000558791600001,0
J,"Siglidis, G; Nikolentzos, G; Limnios, S; Giatsidis, C; Skianis, K; Vazirgiannis, M",,,,"Siglidis, Giannis; Nikolentzos, Giannis; Limnios, Stratis; Giatsidis, Christos; Skianis, Konstantinos; Vazirgiannis, Michalis",,,GraKeL: A Graph Kernel Library in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/GraKeL",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000007,0
J,"Vehtari, A; Gelman, A; Sivula, T; Jylanki, P; Tran, D; Sahai, S; Blomstedt, P; Cunningham, JP; Schiminovich, D; Robert, CP",,,,"Vehtari, Aki; Gelman, Andrew; Sivula, Tuomas; Jylanki, Pasi; Tran, Dustin; Sahai, Swupnil; Blomstedt, Paul; Cunningham, John P.; Schiminovich, David; Robert, Christian P.",,,Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.",,,,,"Robert, Christian/D-4439-2013; Robert, Christian/AAL-2415-2020","Robert, Christian/0000-0001-6635-3261; Robert, Christian/0000-0001-6635-3261; Vehtari, Aki/0000-0003-2164-9469",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300017,0
J,"Wang, D; Gaboardi, M; Smith, A; Xu, JH",,,,"Wang, Di; Gaboardi, Marco; Smith, Adam; Xu, Jinhui",,,Empirical Risk Minimization in the Non-interactive Local Model of Differential Privacy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the Empirical Risk Minimization (ERM) problem in the non-interactive Local Differential Privacy (LDP) model. Previous research on this problem (Smith et al., 2017) indicates that the sample complexity, to achieve error alpha, needs to be exponentially depending on the dimensionality p for general loss functions. In this paper, we make two attempts to resolve this issue by investigating conditions on the loss functions that allow us to remove such a limit. In our first attempt, we show that if the loss function is (infinity, T)-smooth, by using the Bernstein polynomial approximation we can avoid the exponential dependency in the term of alpha. We then propose player-efficient algorithms with 1-bit communication complexity and O(1) computation cost for each player. The error bound of these algorithms is asymptotically the same as the original one. With some additional assumptions, we also give an algorithm which is more efficient for the server. In our second attempt, we show that for any 1-Lipschitz generalized linear convex loss function, there is an (epsilon, delta)-LDP algorithm whose sample complexity for achieving error alpha is only linear in the dimensionality p. Our results use a polynomial of inner product approximation technique. Finally, motivated by the idea of using polynomial approximation and based on different types of polynomial approximations, we propose (efficient) non-interactive locally differentially private algorithms for learning the set of k-way marginal queries and the set of smooth queries.",,,,,"Smith, Adam/GPS-8322-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,200,,,,,,,,,,,,,,,WOS:000590005400001,0
J,"Berkenkamp, F; Schoellig, AP; Krause, A",,,,"Berkenkamp, Felix; Schoellig, Angela P.; Krause, Andreas",,,No-Regret Bayesian Optimization with Unknown Hyperparameters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,50,,,,,,,,,,,,,,,WOS:000463322800001,0
J,"Bhadra, A; Datta, J; Li, YF; Poison, NG; Willard, B",,,,"Bhadra, Anindya; Datta, Jyotishka; Li, Yunfan; Poison, Nicholas G.; Willard, Brandon",,,Prediction Risk for the Horseshoe Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that prediction performance for global-local shrinkage regression can overcome two major difficulties of global shrinkage regression: (i) the amount of relative shrinkage is monotone in the singular values of the design matrix and (ii) the shrinkage is determined by a single tuning parameter. Specifically, we show that the horseshoe regression, with heavy-tailed component-specific local shrinkage parameters, in conjunction with a global parameter providing shrinkage towards zero, alleviates both these difficulties and consequently, results in an improved risk for prediction. Numerical demonstrations of improved prediction over competing approaches in simulations and in a pharmacogenomics data set confirm our theoretical findings.",,,,,"willard, brandon/AAR-6254-2021","willard, brandon/0000-0002-0951-3289; Bhadra, Anindya/0000-0003-0636-5273; Datta, Jyotishka/0000-0001-5991-5182",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,78,,,,,,,,,,,,,,,WOS:000467896400001,0
J,"Borboudakis, G; Tsamardinos, I",,,,"Borboudakis, Giorgos; Tsamardinos, Ioannis",,,Forward-Backward Selection with Early Dropping,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Forward-backward selection is one of the most basic and commonly-used feature selection algorithms available. It is also general and conceptually applicable to many different types of data. In this paper, we propose a heuristic that significantly improves its running time, while preserving predictive performance. The idea is to temporarily discard the variables that are conditionally independent with the outcome given the selected variable set. Depending on how those variables are reconsidered and reintroduced, this heuristic gives rise to a family of algorithms with increasingly stronger theoretical guarantees. In distributions that can be faithfully represented by Bayesian networks or maximal ancestral graphs, members of this algorithmic family are able to correctly identify the Markov blanket in the sample limit. In experiments we show that the proposed heuristic increases computational efficiency by about 1-2 orders of magnitude, while selecting fewer or the same number of variables and retaining predictive performance. Furthermore, we show that the proposed algorithm and feature selection with LASSO perform similarly when restricted to select the same number of variables, making the proposed algorithm an attractive alternative for problems where no (efficient) algorithm for LASSO exists.",,,,,"Borboudakis, Giorgos/GWM-9234-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,8,,,,,,,,,,,,,,,WOS:000458663500001,0
J,"Ghosh, S; Yao, JY; Doshi-Velez, F",,,,"Ghosh, Soumya; Yao, Jiayu; Doshi-Velez, Finale",,,Model Selection in Bayesian Neural Networks via Horseshoe Priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The promise of augmenting accurate predictions provided by modern neural networks with well-calibrated predictive uncertainties has reinvigorated interest in Bayesian neural networks. However, model selection even choosing the number of nodes remains an open question. Poor choices can severely affect the quality of the produced uncertainties. In this paper, we explore continuous shrinkage priors, the horseshoe, and the regularized horseshoe distributions, for model selection in Bayesian neural networks. When placed over node pre-activations and coupled with appropriate variational approximations, we find that the strong shrinkage provided by the horseshoe is effective at turning off nodes that do not help explain the data. We demonstrate that our approach finds compact network structures even when the number of nodes required is grossly over-estimated. Moreover, the model selection over the number of nodes does not come at the expense of predictive or computational performance; in fact, we learn smaller networks with comparable predictive performance to current approaches. These effects are particularly apparent in sample-limited settings, such as small data sets and reinforcement learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,182,,,,,,,,,,,,,,,WOS:000506403100022,0
J,"Kiraly, FJ; Oberhauser, H",,,,"Kiraly, Franz J.; Oberhauser, Harald",,,Kernels for Sequentially Ordered Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel framework for learning with sequential data of any kind, such as multivariate time series, strings, or sequences of graphs. The main result is a sequentialization that transforms any kernel on a given domain into a kernel for sequences in that domain. This procedure preserves properties such as positive definiteness, the associated kernel feature map is an ordered variant of sample (cross-)moments, and this sequentialized kernel is consistent in the sense that it converges to a kernel for paths if sequences converge to paths (by discretization). Further, classical kernels for sequences arise as special cases of this method. We use dynamic programming and low-rank techniques for tensors to provide efficient algorithms to compute this sequentialized kernel.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,31,,,,,,,,,,,,,,,WOS:000463316500001,0
J,"Liao, WJ; Maggioni, M",,,,"Liao, Wenjing; Maggioni, Mauro",,,Adaptive Geometric Multiscale Approximations for Intrinsically Low-dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of efficiently approximating and encoding high-dimensional data sampled from a probability distribution rho in R-D, that is nearly supported on a d-dimensional set M - for example supported on a d-dimensional manifold. Geometric Multi-Resolution Analysis (GMRA) provides a robust and computationally efficient procedure to construct low-dimensional geometric approximations of M at varying resolutions. We introduce GMRA approximations that adapt to the unknown regularity of M, by introducing a thresholding algorithm on the geometric wavelet coefficients. We show that these data-driven, empirical geometric approximations perform well, when the threshold is chosen as a suitable universal function of the number of samples n, on a large class of measures rho, that are allowed to exhibit different regularity at different scales and locations, thereby efficiently encoding data from more complex measures than those supported on manifolds. These GMRA approximations are associated to a dictionary, together with a fast transform mapping data to d-dimensional coefficients, and an inverse of such a map, all of which are data-driven. The algorithms for both the dictionary construction and the transforms have complexity CDn log n with the constant C exponential in d. Our work therefore establishes Adaptive GMRA as a fast dictionary learning algorithm, with approximation guarantees, for intrinsically low-dimensional data. We include several numerical experiments on both synthetic and real data, confirming our theoretical results and demonstrating the effectiveness of Adaptive GMRA.",,,,,,"Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,98,,,,,,,,,,,,,,,WOS:000476621300001,0
J,"Sutter, T; Sutter, D; Esfahani, PM; Lygeros, J",,,,"Sutter, Tobias; Sutter, David; Esfahani, Peyman Mohajerin; Lygeros, John",,,Generalized maximum entropy estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating a probability distribution that maximizes the entropy while satisfying a finite number of moment constraints, possibly corrupted by noise. Based on duality of convex programming, we present a novel approximation scheme using a smoothed fast gradient method that is equipped with explicit bounds on the approximation error. We further demonstrate how the presented scheme can be used for approximating the chemical master equation through the zero-information moment closure method, and for an approximate dynamic programming approach in the context of constrained Markov decision processes with uncountable state and action spaces.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,138,,,,,,,,,,,,,,,WOS:000491132200002,0
J,"Zhang, MM; Williamson, SA",,,,"Zhang, Michael Minyi; Williamson, Sinead A.",,,Embarrassingly Parallel Inference for Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Training Gaussian process-based models typically involves an O(N-3) computational bottleneck due to inverting the covariance matrix. Popular methods for overcoming this matrix inversion problem cannot adequately model all types of latent functions, and are often not parallelizable. However, judicious choice of model structure can ameliorate this problem. A mixture-of-experts model that uses a mixture of K Gaussian processes offers modeling flexibility and opportunities for scalable inference. Our embarrassingly parallel algorithm combines low-dimensional matrix inversions with importance sampling to yield a flexible, scalable mixture-of-experts model that offers comparable performance to Gaussian process regression at a much lower computational cost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,169,,,,,,,,,,,,,,,WOS:000506403100009,0
J,"Angelino, E; Larus-Stone, N; Alabi, D; Seltzer, M; Rudin, C",,,,"Angelino, Elaine; Larus-Stone, Nicholas; Alabi, Daniel; Seltzer, Margo; Rudin, Cynthia",,,Learning Certifiably Optimal Rule Lists for Categorical Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling.",,,,,"Alabi, Daniel/AAF-4233-2019",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,234,,,,,,,,,,,,,,,WOS:000440886700001,0
J,"de Bruin, T; Kober, J; Tuyls, K; Babuska, R",,,,"de Bruin, Tim; Kober, Jens; Tuyls, Karl; Babuska, Robert",,,Experience Selection in Deep Reinforcement Learning for Control,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about, the characteristics of the control problem at hand to choose the appropriate experience replay strategy.",,,,,"Kober, Jens/I-9119-2017; Babuska, Robert/D-6332-2012; Tuyls, Karl P/Q-7328-2018","Kober, Jens/0000-0001-7257-5434; Babuska, Robert/0000-0001-9578-8598; Tuyls, Karl P/0000-0001-7929-1944",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,9,,,,,,,,,,,,,,,WOS:000443224900001,0
J,"Dunlop, MM; Girolami, MA; Stuart, AM; Teckentrup, AL",,,,"Dunlop, Matthew M.; Girolami, Mark A.; Stuart, Andrew M.; Teckentrup, Aretha L.",,,How Deep Are Deep Gaussian Processes?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent research has shown the potential utility of deep Gaussian processes. These deep structures are probability distributions, designed through hierarchical construction, which are conditionally Gaussian. In this paper, the current published body of work is placed in a common framework and, through recursion, several classes of deep Gaussian processes are defined. The resulting samples generated from a deep Gaussian process have a Markovian structure with respect to the depth parameter, and the effective depth of the resulting process is interpreted in terms of the ergodicity, or non-ergodicity, of the resulting Markov chain. For the classes of deep Gaussian processes introduced, we provide results concerning their ergodicity and hence their effective depth. We also demonstrate how these processes may be used for inference; in particular we show how a Metropolis-within-Gibbs construction across the levels of the hierarchy can be used to derive sampling tools which are robust to the level of resolution used to represent the functions on a computer. For illustration, we consider the effect of ergodicity in some simple numerical examples.",,,,,,"Girolami, Mark/0000-0003-3008-253X",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,54,,,,,,,,,,,,,,,WOS:000448378300001,0
J,"Freitag, M; Amiriparian, S; Pugachevskiy, S; Cummins, N; Schuller, B",,,,"Freitag, Michael; Amiriparian, Shahin; Pugachevskiy, Sergey; Cummins, Nicholas; Schuller, Bjoern",,,auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"AUDEEP is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that AUDEEP features are competitive with state-of-the art audio classification.",,,,,"Amiriparian, Shahin/AFN-4830-2022; Cummins, Nicholas/AAC-6431-2019","Amiriparian, Shahin/0000-0002-1129-8223; Cummins, Nicholas/0000-0002-1178-917X; Schuller, Bjorn/0000-0002-6478-8699",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,173,,,,,,,,,,,,,,,WOS:000435442400001,0
J,"Hao, BT; Sun, WW; Liu, YF; Cheng, G",,,,"Hao, Botao; Sun, Will Wei; Liu, Yufeng; Cheng, Guang",,,Simultaneous Clustering and Estimation of Heterogeneous Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,217,,,,,,,,,,30662373,,,,,WOS:000435629200001,0
J,"Jammalamadaka, SR; Qiu, JW; Ning, N",,,,"Jammalamadaka, S. Rao; Qiu, Jinwen; Ning, Ning",,,Multivariate Bayesian Structural Time Series Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper deals with inference and prediction for multiple correlated time series, where one also has the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for time series, we use Bayesian tools for model fitting, prediction and feature selection, thus extending some recent works along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overfitting, as well as captures correlations among multiple target time series with various state components. The model provides needed flexibility in selecting a different set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. Extensive simulations were run to investigate properties such as estimation accuracy and performance in forecasting. This was followed by an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading financial institutions. Both the simulation studies and the extensive empirical study confirm that this multivariate model outperforms three other benchmark models, viz, a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.",,,,,,"Ning, Ning/0000-0002-8950-1279",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,68,,,,,,,,,,,,,,,WOS:000452055000001,0
J,"Lamprier, S; Gisselbrecht, T; Gallinari, P",,,,"Lamprier, Sylvain; Gisselbrecht, Thibault; Gallinari, Patrick",,,Profile-Based Bandit with Unknown Profiles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic bandits have been widely studied since decades. A very large panel of settings have been introduced, some of them for the inclusion of some structure between actions. If actions are associated with feature vectors that underlie their usefulness, the discovery of a mapping parameter between such profiles and rewards can help the exploration process of the bandit strategies. This is the setting studied in this paper, but in our case the action profiles (constant feature vectors) are unknown beforehand. Instead, the agent is only given sample vectors, with mean centered on the true profiles, for a subset of actions at each step of the process. In this new bandit instance, policies have thus to deal with a doubled uncertainty, both on the profile estimators and the reward mapping parameters learned so far. We propose a new algorithm, called SampLinUCB, specifically designed for this case. Theoretical convergence guarantees are given for this strategy, according to various profile samples delivery scenarios. Finally, experiments are conducted on both artificial data and a task of focused data capture from online social networks. Obtained results demonstrate the relevance of the approach in various settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,53,,,,,,,,,,,,,,,WOS:000448378100001,0
J,"Painsky, A; Tishby, N",,,,"Painsky, Amichai; Tishby, Naftali",,,Gaussian Lower Bound for the Information Bottleneck Limit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its Gaussian part, on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data-sets and solve complex problems by linear methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,18,,,,,,,,,,,,,,,WOS:000435628700001,0
J,"Spantini, A; Bigoni, D; Marzouk, Y",,,,"Spantini, Alessio; Bigoni, Daniele; Marzouk, Youssef",,,Inference via Low-Dimensional Couplings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the low-dimensional structure of deterministic transformations between random variables, i.e., transport maps between probability measures. In the context of statistics and machine learning, these transformations can be used to couple a tractable reference measure (e.g., a standard Gaussian) with a target measure of interest. Direct simulation from the desired measure can then be achieved by pushing forward reference samples through the map. Yet characterizing such a map-e.g., representing and evaluating it-grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of low-dimensional couplings, induced by transport maps that are sparse and/or decomposable. Our analysis not only facilitates the construction of transformations in high-dimensional settings, but also suggests new inference methodologies for continuous non-Gaussian graphical models. For instance, in the context of nonlinear state-space models, we describe new variational algorithms for filtering, smoothing, and sequential parameter inference. These algorithms can be understood as the natural generalization-to the non-Gaussian case-of the square-root Rauch-Tung-Striebel Gaussian smoother.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,66,,,,,,,,,,,,,,,WOS:000452053900001,0
J,"Vaughan, JW",,,,"Vaughan, Jennifer Wortman",,,Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,193,,,,,,,,,,,,,,,WOS:000435453400001,0
J,"Ashraphijuo, M; Wang, XD",,,,"Ashraphijuo, Morteza; Wang, Xiaodong",,,Fundamental Conditions for Low-CP-Rank Tensor Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability p and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability p, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability p. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of-magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,29,,,,,,,,,,,,,,,,WOS:000405992000001,0
J,"Atchade, YF; Fort, G; Moulines, E",,,,"Atchade, Yves F.; Fort, Gersende; Moulines, Eric",,,On Perturbed Proximal Gradient Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non-asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models.",,,,,,"FORT, Gersende/0000-0001-5400-1058",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,33,10,,,,,,,,,,,,,,,WOS:000399835100001,0
J,"Bilodeau, M; Nangue, AG",,,,"Bilodeau, Martin; Nangue, Aurelien Guetsop",,,Tests of Mutual or Serial Independence of Random Vectors with Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The problem of testing mutual independence between many random vectors is addressed. The closely related problem of testing serial independence of a multivariate stationary sequence is also considered. The Mobius transformation of characteristic functions is used to characterize independence. A generalization to p vectors of distance covariance and Hilbert-Schmidt independence criterion (HSIC) tests with the translation invariant kernel of a stable probability distribution is proposed. Both test statistics can be expressed in a simple form as a sum over all elements of a componentwise product of p doubly-centered matrices. It is shown that an HSIC statistic with sufficiently small scale parameters is equivalent to a distance covariance statistic. Consistency and weak convergence of both types of statistics are established. Approximation of p - values is made by randomization tests without recomputing interpoint distances for each randomized sample. The dependogram is adapted to the proposed tests for the graphical identification of sources of dependencies. Empirical rejection rates obtained through extensive simulations confirm both the applicability of the testing procedures in small samples and the high level of competitiveness in terms of power. Applications to meteorological and financial data provide some interesting interpretations of dependencies revealed by dependograms.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,74,,,,,,,,,,,,,,,WOS:000412059900001,0
J,"Brockmeier, AJ; Mu, TT; Ananiadou, S; Goulermas, JY",,,,"Brockmeier, Austin J.; Mu, Tingting; Ananiadou, Sophia; Goulermas, John Y.",,,Quantifying the Informativeness of Similarity Measurements,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we describe an unsupervised measure for quantifying the 'informativeness' of correlation matrices formed from the pairwise similarities or relationships among data instances. The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. This non-parametric notion generalizes existing test statistics for equality of correlation coefficients by allowing for alternative distance metrics, such as the Bures and other distances from quantum information theory. For several distance and dissimilarity metrics, we derive closed-form expressions of informativeness, which can be applied as objective functions for machine learning applications. Empirically, we demonstrate that informativeness is a useful criterion for selecting kernel parameters, choosing the dimension for kernel-based nonlinear dimensionality reduction, and identifying structured graphs. We also consider the problem of finding a maximally informative correlation matrix around a target matrix, and explore parameterizing the optimization in terms of the coordinates of the sample or through a lower-dimensional embedding. In the latter case, we find that maximizing the Bures-based informativeness measure, which is maximal for centered rank-1 correlation matrices, is equivalent to minimizing a specific matrix norm, and present an algorithm to solve the minimization problem using the norm's proximal operator. The proposed correlation denoising algorithm consistently improves spectral clustering. Overall, we find informativeness to be a novel and useful criterion for identifying non-trivial correlation structure.",,,,,"Mu, Tingting/AAV-4795-2020; Brockmeier, Austin Jay/Y-3262-2018","Mu, Tingting/0000-0001-6315-3432; Brockmeier, Austin Jay/0000-0002-7293-8140",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,76,,,,,,,,,,,,,,,WOS:000412061300001,0
J,"Darnell, G; Georgiev, S; Mukherjee, S; Engelhardt, BE",,,,"Darnell, Gregory; Georgiev, Stoyan; Mukherjee, Sayan; Engelhardt, Barbara E.",,,Adaptive Randomized Dimension Reduction on Massive Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The scalability of statistical estimators is of increasing importance in modern applications. One approach to implementing scalable algorithms is to compress data into a low dimensional latent space using dimension reduction methods. In this paper, we develop an approach for dimension reduction that exploits the assumption of low rank structure in high dimensional data to gain both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide an efficient solution to principal component analysis (PCA), and we use this efficient solver to improve estimation in large-scale linear mixed models (LMM) for association mapping in statistical genomics. A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance by implicitly regularizing the covariance matrix estimate of the random effect in an LMM. These statistical and computational advantages are highlighted in our experiments on simulated data and large-scale genomic studies.",,,,,,"Darnell, Gregory/0000-0003-0425-940X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,140,,,,,,,,,,,,,,,WOS:000424548500001,0
J,"Melnyk, I; Banerjee, A",,,,"Melnyk, Igor; Banerjee, Arindam",,,A Spectral Algorithm for Inference in Hidden semi-Markov Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,35,,,,,,,,,,,,,,,WOS:000400517600001,0
J,"Adamskiy, D; Koolen, WM; Chernov, A; Vovk, V",,,,"Adamskiy, Dmitry; Koolen, Wouter M.; Chernov, Alexey; Vovk, Vladimir",,,A Closer Look at Adaptive Regret,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For the prediction with expert advice setting, we consider methods to construct algorithms that have low adaptive regret. The adaptive regret of an algorithm on a time interval [t(1); t(2)] is the loss of the algorithm minus the loss of the best expert over that interval. Adaptive regret measures how well the algorithm approximates the best expert locally, and so is different from, although closely related to, both the classical regret, measured over an initial time interval [1,t], and the tracking regret, where the algorithm is compared to a good sequence of experts over [1, t]. We investigate two existing intuitive methods for deriving algorithms with low adaptive regret, one based on specialist experts and the other based on restarts. Quite surprisingly, we show that both methods lead to the same algorithm, namely Fixed Share, which is known for its tracking regret. We provide a thorough analysis of the adaptive regret of Fixed Share. We obtain the exact worst-case adaptive regret for Fixed Share, from which the classical tracking bounds follow. We prove that Fixed Share is optimal for adaptive regret: the worst-case adaptive regret of any algorithm is at least that of an instance of Fixed Share.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,23,,,,,,,,,,,,,,,WOS:000391474500001,0
J,"Blaser, R; Fryzlewicz, P",,,,"Blaser, Rico; Fryzlewicz, Piotr",,,Random Rotation Ensembles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In machine learning, ensemble methods combine the predictions of multiple base learners to construct more accurate aggregate predictions. Established supervised learning algorithms inject randomness into the construction of the individual base learners in an effort to promote diversity within the resulting ensembles. An undesirable side effect of this approach is that it generally also reduces the accuracy of the base learners. In this paper, we introduce a method that is simple to implement yet general and effective in improving ensemble diversity with only modest impact on the accuracy of the individual base learners. By randomly rotating the feature space prior to inducing the base learners, we achieve favorable aggregate predictions on standard data sets compared to state of the art ensemble methods, most notably for tree-based ensembles, which are particularly sensitive to rotation.",,,,,"Fryzlewicz, Piotr/P-3532-2018","Fryzlewicz, Piotr/0000-0002-9676-902X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,4,,,,,,,,,,,,,,,WOS:000391463200001,0
J,"Collier, O; Dalalyan, AS",,,,"Collier, Olivier; Dalalyan, Arnak S.",,,Minimax Rates in Permutation Estimation for Feature Matching,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension of the features. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension of the features is of the order of the logarithm of the number of features n. For d = O (log n), the rate is dimension free and equals sigma (log n) 1 = 2, where sigma is the noise level. In contrast, when d is larger than c log n for some constant c > 0, the minimax rate increases with d and is of the order of sigma (d log n)(1/4). We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria.",,,,,"Dalalyan, Arnak S/G-7853-2011","Dalalyan, Arnak S/0000-0003-4337-9500",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,6,,,,,,,,,,,,,,,WOS:000391464300001,0
J,"Damianou, AC; Titsias, MK; Lawrence, ND",,,,"Damianou, Andreas C.; Titsias, Michalis K.; Lawrence, Neil D.",,,Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i. i. d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a bene fit of the variational Bayesian procedure is its robustness to over fitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,62,42,,,,,,,,,,,,,,,WOS:000391483100001,0
J,"Elibol, HM; Nguyen, V; Linderman, S; Johnson, M; Hashmi, A; Doshi-Velez, F",,,,"Elibol, Huseyin Melih; Nguyen, Vincent; Linderman, Scott; Johnson, Matthew; Hashmi, Amna; Doshi-Velez, Finale",,,Cross-Corpora Unsupervised Learning of Trajectories in Autism Spectrum Disorders,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Patients with developmental disorders, such as autism spectrum disorder (ASD), present with symptoms that change with time even if the named diagnosis remains fixed. For example, language impairments may present as delayed speech in a toddler and difficulty reading in a school-age child. Characterizing these trajectories is important for early treatment. However, deriving these trajectories from observational sources is challenging: electronic health records only reflect observations of patients at irregular intervals and only record what factors are clinically relevant at the time of observation. Meanwhile, caretakers discuss daily developments and concerns on social media. In this work, we present a fully unsupervised approach for learning disease trajectories from incomplete medical records and social media posts, including cases in which we have only a single observation of each patient. In particular, we use a dynamic topic model approach which embeds each disease trajectory as a path in R-D. A Polya-gamma augmentation scheme is used to efficiently perform inference as well as incorporate multiple data sources. We learn disease trajectories from the electronic health records of 13,435 patients with ASD and the forum posts of 13,743 caretakers of children with ASD, deriving interesting clinical insights as well as good predictions.",,,,,"nguyen, vincent/GSN-9355-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,133,,,,,,,,,,,,,,,WOS:000391657500001,0
J,"Fan, J; Wu, YR; Yuan, M; Page, D; Liu, J; Ong, IM; Peissig, P; Burnside, E",,,,"Fan, Jun; Wu, Yirong; Yuan, Ming; Page, David; Liu, Jie; Ong, Irene M.; Peissig, Peggy; Burnside, Elizabeth",,,Structure-Leveraged Methods in Breast Cancer Risk Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predicting breast cancer risk has long been a goal of medical research in the pursuit of precision medicine. The goal of this study is to develop novel penalized methods to improve breast cancer risk prediction by leveraging structure information in electronic health records. We conducted a retrospective case-control study, garnering 49 mammography descriptors and 77 high-frequency/low-penetrance single-nucleotide polymorphisms (SNPs) from an existing personalized medicine data repository. Structured mammography reports and breast imaging features have long been part of a standard electronic health record (EHR), and genetic markers likely will be in the near future. Lasso and its variants are widely used approaches to integrated learning and feature selection, and our methodological contribution is to incorporate the dependence structure among the features into these approaches. More specifically, we propose a new methodology by combining group penalty and l(p) (1 <= p <= 2) fusion penalty to improve breast cancer risk prediction, taking into account structure information in mammography descriptors and SNPs. We demonstrate that our method provides benefits that are both statistically significant and potentially significant to people's lives.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,85,,,,,,,,,,,,,,,WOS:000391919000001,0
J,"Gittens, A; Mahoney, MW",,,,"Gittens, Alex; Mahoney, Michael W.",,,Revisiting the Nystrom Method for Improved Large-scale Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds-e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error-and they point to future directions to make these algorithms useful in even larger-scale machine learning applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,117,,,,,,,,,,,,,,,WOS:000391552200001,0
J,"Hummel, P; McAfee, RP",,,,"Hummel, Patrick; McAfee, R. Preston",,,Machine Learning in an Auction Environment,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration in an auction environment is exceedingly small.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,197,,,,,,,,,,,,,,,WOS:000391827800001,0
J,"Kairouz, P; Oh, S; Viswanath, P",,,,"Kairouz, Peter; Oh, Sewoong; Viswanath, Pramod",,,Extremal Mechanisms for Local Differential Privacy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where both the data providers and data analysts want to maximize the utility of statistical analyses performed on the released data, we study the fundamental trade-off between local differential privacy and utility. This trade-off is formulated as a constrained optimization problem: maximize utility subject to local differential privacy constraints. We introduce a combinatorial family of extremal privatization mechanisms, which we call staircase mechanisms, and show that it contains the optimal privatization mechanisms for a broad class of information theoretic utilities such as mutual information and f-divergences. We further prove that for any utility function and any privacy level, solving the privacy-utility maximization problem is equivalent to solving a finite-dimensional linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the size of the alphabet the data lives in. To account for this, we show that two simple privatization mechanisms, the binary and randomized response mechanisms, are universally optimal in the low and high privacy regimes, and well approximate the intermediate regime.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,17,,,,,,,,,,,,,,,WOS:000391472600001,0
J,"Lefakis, L; Fleuret, F",,,,"Lefakis, Leonidas; Fleuret, Francois",,,Jointly Informative Feature Selection Made Tractable by Gaussian Modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,39,182,,,,,,,,,,,,,,,WOS:000391824400001,0
J,"Qian, W; Yang, YH",,,,"Qian, Wei; Yang, Yuhong",,,Kernel Estimation and Model Combination in A Bandit Problem with Covariates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multi-armed bandit problem is an important optimization game that requires an exploration-exploitation tradeoff to achieve optimal total reward. Motivated from industrial applications such as online advertising and clinical research, we consider a setting where the rewards of bandit machines are associated with covariates, and the accurate estimation of the corresponding mean reward functions plays an important role in the performance of allocation rules. Under a flexible problem setup, we establish asymptotic strong consistency and perform a finite-time regret analysis for a sequential randomized allocation strategy based on kernel estimation. In addition, since many nonparametric and parametric methods in supervised learning may be applied to estimating the mean reward functions but guidance on how to choose among them is generally unavailable, we propose a model combining allocation strategy for adaptive performance. Simulations and a real data evaluation are conducted to illustrate the performance of the proposed allocation strategy.",,,,,"Yang, Yuhong/J-6596-2019","Yang, Yuhong/0000-0003-3618-3083",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,149,,,,,,,,,,,,,,,WOS:000391662700001,0
J,"Tang, L; Song, PXK",,,,"Tang, Lu; Song, Peter X. K.",,,Fused Lasso Approch in Regression Coefficients Clustering - Learning Parameter Heterogeneity in Data Integration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As data sets of related studies become more easily accessible, combining data sets of similar studies is often undertaken in practice to achieve a larger sample size and higher power. A major challenge arising from data integration pertains to data heterogeneity in terms of study population, study design, or study coordination. Ignoring such heterogeneity in data analysis may result in biased estimation and misleading inference. Traditional techniques of remedy to data heterogeneity include the use of interactions and random effects, which are inferior to achieving desirable statistical power or providing a meaningful interpretation, especially when a large number of smaller data sets are combined. In this paper, we propose a regularized fusion method that allows us to identify and merge inter-study homogeneous parameter clusters in regression analysis, without the use of hypothesis testing approach. Using the fused lasso, we establish a computationally efficient procedure to deal with large-scale integrated data. Incorporating the estimated parameter ordering in the fused lasso facilitates computing speed with no loss of statistical power. We conduct extensive simulation studies and provide an application example to demonstrate the performance of the new method with a comparison to the conventional methods.",,,,,"Tang, Lu/AAM-1941-2021","Tang, Lu/0000-0001-6143-9314",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,113,,,,,,,,,,29056876,,,,,WOS:000391550300001,0
J,"Triantafillou, S; Tsamardinos, I",,,,"Triantafillou, Sofia; Tsamardinos, Ioannis",,,Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Scientific practice typically involves repeatedly studying a system, each time trying to unravel a different perspective. In each study, the scientist may take measurements under different experimental conditions (interventions, manipulations, perturbations) and measure different sets of quantities (variables). The result is a collection of heterogeneous data sets coming from different data distributions. In this work, we present algorithm COmbINE, which accepts a collection of data sets over overlapping variable sets under different experimental conditions; COmbINE then outputs a summary of all causal models indicating the invariant and variant structural characteristics of all models that simultaneously fit all of the input data sets. COmbINE converts estimated dependencies and independencies in the data into path constraints on the data-generating causal model and encodes them as a SAT instance. The algorithm is sound and complete in the sample limit. To account for conflicting constraints arising from statistical errors, we introduce a general method for sorting constraints in order of confidence, computed as a function of their corresponding p-values. In our empirical evaluation, COmbINE outperforms in terms of efficiency the only pre-existing similar algorithm; the latter additionally admits feedback cycles, but does not admit conflicting constraints which hinders the applicability on real data. As a proof-of-concept, COmbINE is employed to co-analyze 4 real, mass-cytometry data sets measuring phosphorylated protein concentrations of overlapping protein sets under 3 different interventions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2147,2205,,,,,,,,,,,,,,,,WOS:000369887600002,0
J,"Gammerman, A; Vovk, V",,,,"Gammerman, Alex; Vovk, Vladimir",,,Learning using privileged information: Similarity control and knowledge transfer,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1677,1680,,,,,,,,,,,,,,,,WOS:000369887300001,0
J,"Qiao, XY; Zhang, LS",,,,"Qiao, Xingye; Zhang, Lingsong",,,Flexible High-Dimensional Classification Machines and Their Asymptotic Properties,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classification is an important topic in statistics and machine learning with great potential in many real applications. In this paper, we investigate two popular large-margin classification methods, Support Vector Machine (SVM) and Distance Weighted Discrimination (DWD), under two contexts: the high-dimensional, low-sample size data and the imbalanced data. A unified family of classification machines, the FLexible Assortment MachinE (FLAME) is proposed, within which DWD and SVM are special cases. The FLAME family helps to identify the similarities and differences between SVM and DWD. It is well known that many classifiers overfit the data in the high-dimensional setting; and others are sensitive to the imbalanced data, that is, the class with a larger sample size overly influences the classifier and pushes the decision boundary towards the minority class. SVM is resistant to the imbalanced data issue, but it overfits high-dimensional data sets by showing the undesired data-piling phenomenon. The DWD method was proposed to improve SVM in the high dimensional setting, but its decision boundary is sensitive to the imbalanced ratio of sample sizes. Our FLAME family helps to understand an intrinsic connection between SVM and DWD, and provides a trade-off between sensitivity to the imbalanced data and overfitting the high-dimensional data. Several asymptotic properties of the FLAME classifiers are studied. Simulations and real data applications are investigated to illustrate theoretical findings.",,,,,"Qiao, Xingye/P-6321-2019","Qiao, Xingye/0000-0003-0937-9822",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1547,1572,,,,,,,,,,,,,,,,WOS:000369887100007,0
J,"Lu, T; Boutilier, C",,,,"Lu, Tyler; Boutilier, Craig",,,Effective Sampling and Learning for Mallows Models with Pairwise-Preference Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases and have provable bounds with pairwise evidence and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2014,15,,,,,,3783,3829,,,,,,,,,,,,,,,,WOS:000354999700001,0
J,"Fabisch, A; Metzen, JH",,,,"Fabisch, Alexander; Metzen, Jan Hendrik",,,Active Contextual Policy Search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider the problem of learning skills that are versatilely applicable. One popular approach for learning such skills is contextual policy search in which the individual tasks are represented as context vectors. We are interested in settings in which the agent is able to actively select the tasks that it examines during the learning process. We argue that there is a better way than selecting each task equally often because some tasks might be easier to learn at the beginning and the knowledge that the agent can extract from these tasks can be transferred to similar but more difficult tasks. The methods that we propose for addressing the task-selection problem model the learning process as a non-stationary multi-armed bandit problem with custom intrinsic reward heuristics so that the estimated learning progress will be maximized. This approach does neither make any assumptions about the underlying contextual policy search algorithm nor about the policy representation. We present empirical results on an artificial benchmark problem and a ball throwing problem with a simulated Mitsubishi PA-10 robot arm which show that active context selection can improve the learning of skills considerably.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3371,3399,,,,,,,,,,,,,,,,WOS:000344638800017,0
J,"Henniges, M; Turner, RE; Sahani, M; Eggert, J; Lucke, J",,,,"Henniges, Marc; Turner, Richard E.; Sahani, Maneesh; Eggert, Julian; Luecke, Joerg",,,Efficient Occlusive Components Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study unsupervised learning in a probabilistic generative model for occlusion. The model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth. This depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image. We show that the object parameters can be learned from an unlabeled set of images in which objects occlude one another. Exact maximum-likelihood learning is intractable. Tractable approximations can be derived, however, by applying a truncated variational approach to Expectation Maximization (EM). In numerical experiments it is shown that these approximations recover the underlying set of object parameters including data noise and sparsity. Experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating components. The studied approach demonstrates that the multiple-causes generative approach can be generalized to extract occluding components, which links research on occlusion to the field of sparse coding approaches.",,,,,,"Turner, Richard/0000-0003-0066-0984",,,,,,,,,,,,,1532-4435,,,,,AUG,2014,15,,,,,,2689,2722,,,,,,,,,,,,,,,,WOS:000344638600003,0
J,"Ailon, N; Begleiter, R; Ezra, E",,,,"Ailon, Nir; Begleiter, Ron; Ezra, Esther",,,Active Learning Using Smooth Relative Regret Approximations with Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The disagreement coefficient of Hanneke has become a central data independent invariant in proving active learning rates. It has been shown in various ways that a concept class with low complexity together with a bound on the disagreement coefficient at an optimal solution allows active learning rates that are superior to passive learning ones. We present a different tool for pool based active learning which follows from the existence of a certain uniform version of low disagreement coefficient, but is not equivalent to it. In fact, we present two fundamental active learning problems of significant interest for which our approach allows nontrivial active learning bounds. However, any general purpose method relying on the disagreement coefficient bounds only, fails to guarantee any useful bounds for these problems. The applications of interest are: Learning to rank from pairwise preferences, and clustering with side information (a.k.a. semi-supervised clustering). The tool we use is based on the learner's ability to compute an estimator of the difference between the loss of any hypothesis and some fixed pivotal hypothesis to within an absolute error of at most epsilon times the disagreement measure (l(1) distance) between the two hypotheses. We prove that such an estimator implies the existence of a learning algorithm which, at each iteration, reduces its in-class excess risk to within a constant factor. Each iteration replaces the current pivotal hypothesis with the minimizer of the estimated loss difference function with respect to the previous pivotal hypothesis. The label complexity essentially becomes that of computing this estimator.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,885,920,,,,,,,,,,,,,,,,WOS:000335458100002,0
J,"Parrish, N; Anderson, HS; Gupta, MR; Hsiao, DY",,,,"Parrish, Nathan; Anderson, Hyrum S.; Gupta, Maya R.; Hsiao, Dun Yu",,,Classifying With Confidence From Incomplete Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of classifying a test sample given incomplete information. This problem arises naturally when data about a test sample is collected over time, or when costs must be incurred to compute the classification features. For example, in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time, and additional time, power, and bandwidth is needed to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability-the probability that a label assigned given incomplete data would be the same as the label assigned given the complete data, and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classification decision, rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time-series data sets, where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3561,3589,,,,,,,,,,,,,,,,WOS:000335457100003,0
J,"Sun, W; Wang, JH; Fang, YX",,,,"Sun, Wei; Wang, Junhui; Fang, Yixin",,,Consistent Selection of Tuning Parameters via Variable Selection Stability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data.",,,,,,"WANG, Junhui/0000-0002-9165-5664",,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3419,3440,,,,,,,,,,,,,,,,WOS:000329786900007,0
J,"Aravkin, AY; Burke, JV; Pillonetto, G",,,,"Aravkin, Aleksandr Y.; Burke, James V.; Pillonetto, Gianluigi",,,"Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the l(2), Huber, l(1) and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with specified means and variances from simple scalar building blocks. The main contribution of this paper is a flexible statistical modeling framework for a variety of learning applications, together with a toolbox of efficient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efficiency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaussian errors are assumed both in the process and measurement models for such problems. We show that the extended framework allows arbitrary PLQ densities to be used, and that the proposed IP approach solves the generalized Kalman smoothing problem while maintaining the linear complexity in the size of the time series, just as in the Gaussian case. This extends the computational efficiency of the Mayne-Fraser and Rauch-Tung-Striebel algorithms to a much broader nonsmooth setting, and includes many recently proposed robust and sparse smoothers as special cases.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2689,2728,,,,,,,,,,,,,,,,WOS:000327007400007,0
J,"Challis, E; Barber, D",,,,"Challis, Edward; Barber, David",,,Gaussian Kullback-Leibler Approximate Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufficient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2239,2286,,,,,,,,,,,,,,,,WOS:000324799600002,0
J,"Mahdavi, M; Jin, R; Yang, TB",,,,"Mahdavi, Mehrdad; Jin, Rong; Yang, Tianbao",,,Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose efficient algorithms for solving constrained online convex optimization problems. Our motivation stems from the observation that most algorithms proposed for online convex optimization require a projection onto the convex set K from which the decisions are made. While the projection is straightforward for simple shapes (e.g., Euclidean ball), for arbitrary complex sets it is the main computational challenge and may be inefficient in practice. In this paper, we consider an alternative online convex optimization problem. Instead of requiring that decisions belong to K for all rounds, we only require that the constraints, which define the set K, be satisfied in the long run. By turning the problem into an online convex-concave optimization problem, we propose an efficient algorithm which achieves O(root T) regret bound and O(T-3/4) bound on the violation of constraints. Then, we modify the algorithm in order to guarantee that the constraints are satisfied in the long run. This gain is achieved at the price of getting O(T-3/4) regret bound. Our second algorithm is based on the mirror prox method (Nemirovski, 2005) to solve variational inequalities which achieves O(T-2/3) bound for both regret and the violation of constraints when the domain K can be described by a finite number of linear constraints. Finally, we extend the results to the setting where we only have partial access to the convex set K and propose a multipoint bandit feedback algorithm with the same bounds in expectation as our first algorithm.",,,,,"Mahdavi, Mehrdad/A-2975-2013",,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2503,2528,,,,,,,,,,,,,,,,WOS:000309580600001,0
J,"Montavon, G; Braun, ML; Muller, KR",,,,"Montavon, Gregoire; Braun, Mikio L.; Mueller, Klaus-Robert",,,Kernel Analysis of Deep Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,When training deep networks it is common knowledge that an efficient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer.,,,,,"Montavon, Gr√©goire/Q-1836-2016; Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2563,2581,,,,,,,,,,,,,,,,WOS:000298102900002,0
J,"Meyer, G; Bonnabel, S; Sepulchre, R",,,,"Meyer, Gilles; Bonnabel, Silvere; Sepulchre, Rodolphe",,,Regression on Fixed-Rank Positive Semidefinite Matrices: A Riemannian Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.",,,,,,"Sepulchre, Rodolphe/0000-0002-7047-3124",,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,593,625,,,,,,,,,,,,,,,,WOS:000288896800008,0
J,"Chang, F; Guo, CY; Lin, XR; Lu, CJ",,,,"Chang, Fu; Guo, Chien-Yang; Lin, Xiao-Rong; Lu, Chi-Jen",,,Tree Decomposition for Large-Scale SVM Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To handle problems created by large data sets, we propose a method that uses a decision tree to decompose a given data space and train SVMs on the decomposed regions. Although there are other means of decomposing a data space, we show that the decision tree has several merits for large-scale SVM training. First, it can classify some data points by its own means, thereby reducing the cost of SVM training for the remaining data points. Second, it is efficient in determining the parameter values that maximize the validation accuracy, which helps maintain good test accuracy. Third, the tree decomposition method can derive a generalization error bound for the classifier. For data sets whose size can be handled by current non-linear, or kernel-based, SVM training techniques, the proposed method can speed up the training by a factor of thousands, and still achieve comparable test accuracy.",,,,,"Lu, Chi-Jen/AAQ-3728-2021",,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2935,2972,,,,,,,,,,,,,,,,WOS:000284040000011,0
J,"Yuan, M; Wegkamp, M",,,,"Yuan, Ming; Wegkamp, Marten",,,Classification Methods with Reject Option Based on Convex Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we investigate the problem of binary classification with a reject option in which one can withhold the decision of classifying an observation at a cost lower than that of misclassification. Since the natural loss function is non-convex so that empirical risk minimization easily becomes infeasible, the paper proposes minimizing convex risks based on surrogate convex loss functions. A necessary and sufficient condition for infinite sample consistency ( both risks share the same minimizer) is provided. Moreover, we show that the excess risk can be bounded through the excess surrogate risk under appropriate conditions. These bounds can be tightened by a generalized margin condition. The impact of the results is illustrated on several commonly used surrogate loss functions.",,,,,"Yuan, Ming/J-6153-2019","Yuan, Ming/0000-0002-4415-8606",,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,111,130,,,,,,,,,,,,,,,,WOS:000277186400005,0
J,"Jain, BJ; Obermayer, K",,,,"Jain, Brijnesh J.; Obermayer, Klaus",,,Structure Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finite structures such as point patterns, strings, trees, and graphs occur as natural representations of structured data in different application areas of machine learning. We develop the theory of structure spaces and derive geometrical and analytical concepts such as the angle between structures and the derivative of functions on structures. In particular, we show that the gradient of a differentiable structural function is a well-defined structure pointing in the direction of steepest ascent. Exploiting the properties of structure spaces, it will turn out that a number of problems in structural pattern recognition such as central clustering or learning in structured output spaces can be formulated as optimization problems with cost functions that are locally Lipschitz. Hence, methods from nonsmooth analysis are applicable to optimize those cost functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2667,2714,,,,,,,,,,,,,,,,WOS:000272346600010,0
J,"Rudin, C; Schapire, RE",,,,"Rudin, Cynthia; Schapire, Robert E.",,,Margin-based Ranking and an Equivalence between AdaBoost and RankBoost,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study boosting algorithms for learning to rank. We give a general margin-based bound for ranking based on covering numbers for the hypothesis space. Our bound suggests that algorithms that maximize the ranking margin will generalize well. We then describe a new algorithm, smooth margin ranking, that precisely converges to a maximum ranking-margin solution. The algorithm is a modification of RankBoost, analogous to approximate coordinate ascent boosting. Finally, we prove that AdaBoost and RankBoost are equally good for the problems of bipartite ranking and classification in terms of their asymptotic behavior on the training set. Under natural conditions, AdaBoost achieves an area under the ROC curve that is equally as good as RankBoost's; furthermore, RankBoost, when given a specific intercept, achieves a misclassification error that is as good as AdaBoost's. This may help to explain the empirical observations made by Cortes and Mohri, and Caruana and Niculescu-Mizil, about the excellent performance of AdaBoost as a bipartite ranking algorithm, as measured by the area under the ROC curve.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2193,2232,,,,,,,,,,,,,,,,WOS:000272346400002,0
J,"Raeder, T; Chawla, NV",,,,"Raeder, Troy; Chawla, Nitesh V.",,,"Model Monitor (M-2): Evaluating, Comparing, and Monitoring Models",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents Model Monitor (M-2), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M-2 provides a simple and intuitive framework in which users can evaluate classifiers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M-2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classifiers can be used if desired.",,,,,"Chawla, Nitesh/F-2690-2016","Chawla, Nitesh/0000-0003-3932-5956",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1387,1390,,,,,,,,,,,,,,,,WOS:000270825000003,0
J,"Pena, JM; Nilsson, R; Bjorkegren, J; Tegner, J",,,,"Pena, Jose M.; Nilsson, Roland; Bjorkegren, Johan; Tegner, Jesper",,,An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a sound and complete graphical criterion for reading dependencies from the minimal undirected independence map G of a graphoid M that satisfies weak transitivity. Here, complete means that it is able to read all the dependencies in M that can be derived by applying the graphoid properties and weak transitivity to the dependencies used in the construction of G and the independencies obtained from G by vertex separation. We argue that assuming weak transitivity is not too restrictive. As an intermediate step in the derivation of the graphical criterion, we prove that for any undirected graph G there exists a strictly positive discrete probability distribution with the prescribed sample spaces that is faithful to G. We also report an algorithm that implements the graphical criterion and whose running time is considered to be at most O(n(2)(e + n)) for n nodes and e edges. Finally, we illustrate how the graphical criterion can be used within bioinformatics to identify biologically meaningful gene dependencies.",,,,,"tegner, jesper N/R-5095-2017; Nilsson, Roland/AAJ-3519-2021","tegner, jesper N/0000-0002-9568-5588; ",,,,,,,,,,,,,1532-4435,,,,,MAY,2009,10,,,,,,1071,1094,,,,,,,,,,,,,,,,WOS:000270824800002,0
J,"Abernethy, J; Bach, F; Evgeniou, T; Vert, JP",,,,"Abernethy, Jacob; Bach, Francis; Evgeniou, Theodoros; Vert, Jean-Philippe",,,A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a general approach for collaborative filtering (CF) using spectral regularization to learn linear operators mapping a set of users to a set of possibly desired objects. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects-a feature currently lacking in existing regularization-based CF approaches-using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach.",,,,,,"Vert, Jean-Philippe/0000-0001-9510-8441",,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,803,826,,,,,,,,,,,,,,,,WOS:000270824500011,0
J,"VanderWeele, TJ; Robins, JM",,,,"VanderWeele, Tyler J.; Robins, James M.",,,Properties of Monotonic Effects on Directed Acyclic Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Various relationships are shown hold between monotonic effects and weak monotonic effects and the monotonicity of certain conditional expectations. Counterexamples are provided to show that the results do not hold under less restrictive conditions. Monotonic effects are furthermore used to relate signed edges on a causal directed acyclic graph to qualitative effect modification. The theory is applied to an example concerning the direct effect of smoking on cardiovascular disease controlling for hypercholesterolemia. Monotonicity assumptions are used to construct a test for whether there is a variable that confounds the relationship between the mediator, hypercholesterolemia, and the outcome, cardiovascular disease.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,699,718,,,,,,,,,,,,,,,,WOS:000270824500006,0
J,"Su, XG; Tsai, CL; Wang, HS; Nickerson, DM; Li, BG",,,,"Su, Xiaogang; Tsai, Chih-Ling; Wang, Hansheng; Nickerson, David M.; Li, Bogong",,,Subgroup Analysis via Recursive Partitioning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Subgroup analysis is an integral part of comparative analysis where assessing the treatment effect on a response is of central interest. Its goal is to determine the heterogeneity of the treatment effect across subpopulations. In this paper, we adapt the idea of recursive partitioning and introduce an interaction tree (IT) procedure to conduct subgroup analysis. The IT procedure automatically facilitates a number of objectively defined subgroups, in some of which the treatment effect is found prominent while in others the treatment has a negligible or even negative effect. The standard CART (Breiman et al., 1984) methodology is inherited to construct the tree structure. Also, in order to extract factors that contribute to the heterogeneity of the treatment effect, variable importance measure is made available via random forests of the interaction trees. Both simulated experiments and analysis of census wage data are presented for illustration.",,,,,"Su, Xiaogang/B-7340-2009","Wang, Hansheng/0000-0003-2386-0209",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,141,158,,,,,,,,,,,,,,,,WOS:000270824200001,0
J,"Lebanon, G; Mao, Y",,,,"Lebanon, Guy; Mao, Yi",,,Non-Parametric Modeling of Partially Ranked Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Statistical models on full and partial rankings of n items are often of limited practical use for large n due to computational consideration. We explore the use of non-parametric models for partially ranked data and derive computationally efficient procedures for their use for large n. The derivations are largely possible through combinatorial and algebraic manipulations based on the lattice of partial rankings. A bias-variance analysis and an experimental study demonstrate the applicability of the proposed method.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2401,2429,,,,,,,,,,,,,,,,WOS:000262637300013,0
J,"Marchiori, E",,,,"Marchiori, Elena",,,Hit miss networks with applications to instance selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In supervised learning, a training set consisting of labeled instances is used by a learning algorithm for generating a model (classifier) that is subsequently employed for deciding the class label of new instances (for generalization). Characteristics of the training set, such as presence of noisy instances and size, influence the learning algorithm and affect generalization performance. This paper introduces a new network-based representation of a training set, called hit miss network (HMN), which provides a compact description of the nearest neighbor relation over pairs of instances from each pair of classes. We show that structural properties of HMN's correspond to properties of training points related to the one nearest neighbor (1-NN) decision rule, such as being border or central point. This motivates us to use HMN's for improving the performance of a 1-NN classifier by removing instances from the training set (instance selection). We introduce three new HMN-based algorithms for instance selection. HMN-C, which removes instances without affecting accuracy of 1-NN on the original training set, HMN-E, based on a more aggressive storage reduction, and HMN-EI, which applies iteratively HMN-E. Their performance is assessed on 22 data sets with different characteristics, such as input dimension, cardinality, class balance, number of classes, noise content, and presence of redundant variables. Results of experiments on these data sets show that accuracy of 1-NN classifier increases significantly when HMN-EI is applied. Comparison with state-of-the-art editing algorithms for instance selection on these data sets indicates best generalization performance of HMN-EI and no significant difference in storage requirements. In general, these results indicate that HMN's provide a powerful graph-based representation of a training set, which can be successfully applied for performing noise and redundance reduction in instance-based learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,997,1017,,,,,,,,,,,,,,,,WOS:000258646300002,0
J,"Maurer, A",,,,"Maurer, Andreas",,,Learning similarity with operator-valued large-margin classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A method is introduced to learn and represent similarity with linear operators in kernel induced Hilbert spaces. Transferring error bounds for vector valued large-margin classifiers to the setting of Hilbert-Schmidt operators leads to dimension free bounds on a risk functional for linear representations and motivates a regularized objective functional. Minimization of this objective is effected by a simple technique of stochastic gradient descent. The resulting representations are tested on transfer problems in image processing, involving plane and spatial geometric invariants, handwritten characters and face recognition.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1049,1082,,,,,,,,,,,,,,,,WOS:000258646300004,0
J,"Mease, D; Wyner, A",,,,"Mease, David; Wyner, Abraham",,,Evidence contrary to the statistical view of boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial flaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these flaws and their practical consequences.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,131,156,,,,,,,,,,,,,,,,WOS:000256641800001,0
J,"Castillo, E; Guijarro-Berdinas, B; Fontenla-Romero, O; Alonso-Betanzos, A",,,,"Castillo, Enrique; Guijarro-Berdinas, Bertha; Fontenla-Romero, Oscar; Alonso-Betanzos, Amparo",,,A very fast learning method for neural networks based on sensitivity analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces a learning method for two-layer feedforward neural networks based on sensitivity analysis, which uses a linear training algorithm for each of the two layers. First, random values are assigned to the outputs of the first layer; later, these initial values are updated based on sensitivity formulas, which use the weights in each of the layers; the process is repeated until convergence. Since these weights are learnt solving a linear system of equations, there is an important saving in computational time. The method also gives the local sensitivities of the least square errors with respect to input and output data, with no extra computational cost, because the necessary information becomes available without extra calculations. This method, called the Sensitivity-Based Linear Learning Method, can also be used to provide an initial set of weights, which significantly improves the behavior of other learning algorithms. The theoretical basis for the method is given and its performance is illustrated by its application to several examples in which it is compared with several learning algorithms and well known data sets. The results have shown a learning speed generally faster than other existing methods. In addition, it can be used as an initialization tool for other well known methods with significant improvements.",,,,,"Fontenla-Romero, Oscar/A-1142-2015; Alonso-Betanzos, Amparo/K-5057-2014; Alonso-Betanzos, Amparo/Z-6185-2019; Guijarro-Berdi√±as, Bertha/D-5446-2011; Castillo, Enrique F/A-7858-2008","Fontenla-Romero, Oscar/0000-0003-4203-8720; Alonso-Betanzos, Amparo/0000-0003-0950-0012; Alonso-Betanzos, Amparo/0000-0003-0950-0012; Guijarro-Berdi√±as, Bertha/0000-0001-8901-5441; Castillo, Enrique/0000-0002-8570-0844",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1159,1182,,,,,,,,,,,,,,,,WOS:000245388800001,0
J,"Munos, R",,,,"Munos, Remi",,,Policy gradient in continuous time,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy search is a method for approximately solving an optimal control problem by performing a parametric optimization search in a given class of parameterized policies. In order to process a local optimization technique, such as a gradient method, we wish to evaluate the sensitivity of the performance measure with respect to the policy parameters, the so-called policy gradient. This paper is concerned with the estimation of the policy gradient for continuous-time, deterministic state dynamics, in a reinforcement learning framework, that is, when the decision maker does not have a model of the state dynamics. We show that usual likelihood ratio methods used in discrete-time, fail to proceed the gradient because they are subject to variance explosion when the discretization time-step decreases to 0. We describe an alternative approach based on the approximation of the pathwise derivative, which leads to a policy gradient estimate that converges almost surely to the true gradient when the time-step tends to 0. The underlying idea starts with the derivation of an explicit representation of the policy gradient using pathwise derivation. This derivation makes use of the knowledge of the state dynamics. Then, in order to estimate the gradient from the observable data only, we use a stochastic policy to discretize the continuous deterministic system into a stochastic discrete process, which enables to replace the unknown coefficients by quantities that solely depend on known data. We prove the almost sure convergence of this estimate to the true policy gradient when the discretization time-step goes to zero. The method is illustrated on two target problems, in discrete and continuous control spaces.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,771,791,,,,,,,,,,,,,,,,WOS:000240173400003,0
J,"Crammer, K; Dekel, O; Keshet, J; Shalev-Shwartz, S; Singer, Y",,,,"Crammer, K; Dekel, O; Keshet, J; Shalev-Shwartz, S; Singer, Y",,,Online passive-aggressive algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view allows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothesis, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets.",,,,,,"Keshet, Joseph/0000-0003-2332-5783",,,,,,,,,,,,,1532-4435,,,,,MAR,2006,7,,,,,,551,585,,,,,,,,,,,,,,,,WOS:000237359000003,0
J,"Sigletos, G; Paliouras, G; Spyropoulos, CD; Hatzopoulos, M",,,,"Sigletos, G; Paliouras, G; Spyropoulos, CD; Hatzopoulos, M",,,Combining information extraction systems using voting and stacked generalization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta- level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the meta-level. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta- level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta- level, with respect to the varying degree of similarity in the output of the base-level systems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2005,6,,,,,,1751,1782,,,,,,,,,,,,,,,,WOS:000236330700001,0
J,"Kim, H; Howland, P; Park, H",,,,"Kim, H; Howland, P; Park, H",,,Dimension reduction in text classification with support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines (SVMs) have been recognized as one of the most successful classification methods for many applications including text classification. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,37,53,,,,,,,,,,,,,,,,WOS:000236328800002,0
J,"Mangasarian, OL; Shavlik, JW; Wild, EW",,,,"Mangasarian, OL; Shavlik, JW; Wild, EW",,,Knowledge-based kernel approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Prior knowledge, in the form of linear inequalities that need to be satisfied over multiple polyhedral sets, is incorporated into a function approximation generated by a linear combination of linear or nonlinear kernels. In addition, the approximation needs to satisfy conventional conditions such as having given exact or inexact function values at certain points. Determining such an approximation leads to a linear programming formulation. By using nonlinear kernels and mapping the prior polyhedral knowledge in the input space to one defined by the kernels, the prior knowledge translates into nonlinear inequalities in the original input space. Through a number of computational examples, including a real world breast cancer prognosis dataset, it is shown that prior knowledge can significantly improve function approximation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2004,5,,,,,,1127,1141,,,,,,,,,,,,,,,,WOS:000236328100003,0
J,"Cortes, C; Haffner, P; Mohri, M",,,,"Cortes, C; Haffner, P; Mohri, M",,,Rational kernels: Theory and algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many classification algorithms were originally designed for fixed-size vectors. Recent applications in text and speech processing and computational biology require however the analysis of variable-length sequences and more generally weighted automata. An approach widely used in statistical learning techniques such as Support Vector Machines (SVMs) is that of kernel methods, due to their computational efficiency in high-dimensional feature spaces. We introduce a general family of kernels based on weighted transducers or rational relations, rational kernels, that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata. We show that rational kernels can be computed efficiently using a general algorithm of composition of weighted transducers and a general single-source shortest-distance algorithm. Not all rational kernels are positive definite and symmetric (PDS), or equivalently verify the Mercer condition, a condition that guarantees the convergence of training for discriminant classification algorithms such as SVMs. We present several theoretical results related to PDS rational kernels. We show that under some general conditions these kernels are closed under sum, product, or Kleene-closure and give a general method for constructing a PDS rational kernel from an arbitrary transducer defined on some non-idempotent semirings. We give the proof of several characterization results that can be used to guide the design of PDS rational kernels. We also show that some commonly used string kernels or similarity measures such as the edit-distance, the convolution kernels of Haussler, and some string kernels used in the context of computational biology are specific instances of rational kernels. Our results include the proof that the edit-distance over a non-trivial alphabet is not negative definite, which, to the best of our knowledge, was never stated or proved before. Rational kernels can be combined with SVMs to form efficient and powerful techniques for a variety of classification tasks in text and speech processing, or computational biology. We describe examples of general families of PDS rational kernels that are useful in many of these applications and report the result of our experiments illustrating the use of rational kernels in several difficult large-vocabulary spoken-dialog classification tasks based on deployed spoken-dialog systems. Our results show that rational kernels are easy to design and implement and lead to substantial improvements of the classification accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,1035,1062,,,,,,,,,,,,,,,,WOS:000236328000007,0
J,"Clarke, B",,,,"Clarke, B",,,Comparing Bayes model averaging and stacking when model approximation error cannot be ignored,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We compare Bayes Model Averaging, BMA, to a non-Bayes form of model averaging called stacking. In stacking, the weights are no longer posterior probabilities of models; they are obtained by a technique based on cross-validation. When the correct data generating model (DGM) is on the list of models under consideration BMA is never worse than stacking and often is demonstrably better, provided that the noise level is of order commensurate with the coefficients and explanatory variables. Here, however, we focus on the case that the correct DGM is not on the model list and may not be well approximated by the elements on the model list. We give a sequence of computed examples by choosing model lists and DGM's to contrast the risk performance of stacking and BMA. In the first examples, the model lists are chosen to reflect geometric principles that should give good performance. In these cases, stacking typically outperforms BMA, sometimes by a wide margin. In the second set of examples we examine how stacking and BMA perform when the model list includes all subsets of a set of potential predictors. When we standardize the size of terms and coefficients in this setting, we find that BMA outperforms stacking when the deviant terms in the DGM 'point' in directions accommodated by the model list but that when the deviant term points outside the model list stacking seems to do better. Overall, our results suggest the stacking has better robustness properties than BMA in the most important settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,683,712,,10.1162/153244304773936090,0,,,,,,,,,,,,,WOS:000221345700011,0
J,"Sang, EFTK",,,,"Sang, EFTK",,,Memory-based shallow parsing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present memory-based learning approaches to shallow parsing and apply these to five tasks: base noun phrase identification, arbitrary base phrase recognition, clause detection, noun phrase parsing and full parsing. We use feature selection techniques and system combination methods for improving the performance of the memory-based learner. Our approach is evaluated on standard data sets and the results are compared with that of other systems. This reveals that our approach works well. for base phrase identification while its application towards recognizing embedded structures leaves some room for improvement.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,559,594,,10.1162/153244302320884542,0,,,,,,,,,,,,,WOS:000179542800002,0
J,"Mahony, RE; Williamson, RC",,,,"Mahony, RE; Williamson, RC",,,Prior knowledge and preferential structures in gradient descent learning algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,A family of gradient descent algorithms for learning linear functions in an online setting is considered. The family includes the classical LMS algorithm as well as new variants such as the Exponentiated Gradient (EG) algorithm due to Kivinen and Warmuth. The algorithms are based on prior distributions defined on the weight space. Techniques from differential geometry are used to develop the algorithms as gradient descent iterations with respect to the natural gradient in the Riemannian structure induced by the prior distribution. The proposed framework subsumes the notion of link-functions.,,,,,,"Mahony, Robert/0000-0002-7803-2868",,,,,,,,,,,,,1532-4435,,,,,SEP,2001,1,4,,,,,311,355,,10.1162/153244301753683735,0,,,,,,,,,,,,,WOS:000173337000003,0
J,"Alenlov, J; Doucet, A; Lindsten, F",,,,"Alenlov, Johan; Doucet, Arnaud; Lindsten, Fredrik",,,Pseudo-Marginal Hamiltonian Monte Carlo,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables, or pseudo-marginal Metropolis-Hastings (MH) schemes. The latter mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. However, in scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high-dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which combines the advantages of both HMC and pseudo-marginal schemes. Specifically, the PM-HMC method is controlled by a precision parameter N, controlling the approximation of the likelihood and, for any N, it samples the marginal posterior of the parameters. Additionally, as N tends to infinity, its sample trajectories and acceptance probability converge to those of an ideal, but intractable, HMC algorithm which would have access to the intractable likelihood and its gradient. We demonstrate through experiments that PM-HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687214000001,0
J,"Azmi, B; Kalise, D; Kunisch, K",,,,"Azmi, Behzad; Kalise, Dante; Kunisch, Karl",,,Optimal Feedback Law Recovery by Gradient-Augmented Sparse Polynomial Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A sparse regression approach for the computation of high-dimensional optimal feedback laws arising in deterministic nonlinear control is proposed. The approach exploits the control-theoretical link between Hamilton-Jacobi-Bellman PDEs characterizing the value function of the optimal control problems, and first-order optimality conditions via Pontryagin's Maximum Principle. The latter is used as a representation formula to recover the value function and its gradient at arbitrary points in the space-time domain through the solution of a two-point boundary value problem. After generating a dataset consisting of different state-value pairs, a hyperbolic cross polynomial model for the value function is fitted using a LASSO regression. An extended set of low and high-dimensional numerical tests in nonlinear optimal control reveal that enriching the dataset with gradient information reduces the number of training samples, and that the sparse polynomial regression consistently yields a feedback law of lower complexity.",,,,,"; Kalise, Dante/F-3498-2016","Azmi, behzad/0000-0002-6303-254X; Kalise, Dante/0000-0003-2327-1957",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500048,0
J,"Yang, XM; Gan, LR; Narisetty, NN; Liang, F",,,,"Yang, Xinming; Gan, Lingrui; Narisetty, Naveen N.; Liang, Feng",,,GemBag: Group Estimation of Multiple Bayesian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a novel hierarchical Bayesian model and an efficient estimation method for the problem of joint estimation of multiple graphical models, which have similar but different sparsity structures and signal strength. Our proposed hierarchical Bayesian model is well suited for sharing of sparsity structures, and our procedure, called as GemBag, is shown to enjoy optimal theoretical properties in terms of l(infinity) norm estimation accuracy and correct recovery of the graphical structure even when some of the signals are weak. Although optimization of the posterior distribution required for obtaining our proposed estimator is a non-convex optimization problem, we show that it turns out to be convex in a large constrained space facilitating the use of computationally efficient algorithms. Through extensive simulation studies and an application to a bike sharing data set, we demonstrate that the proposed GemBag procedure has strong empirical performance in comparison with alternative methods.",,,,,"Liang, Feng/GZK-4305-2022; Liang, Fenghua/HHM-3798-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,48,,,,,,,,,,,,,,,,WOS:000656354400001,0
J,"Arenz, O; Zhong, MJ; Neumann, G",,,,"Arenz, Oleg; Zhong, Mingjun; Neumann, Gerhard",,,Trust-Region Variational Inference with Gaussian Mixture Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many methods for machine learning rely on approximate inference from intractable probability distributions. Variational inference approximates such distributions by tractable models that can be subsequently used for approximate inference. Learning sufficiently accurate approximations requires a rich model family and careful exploration of the relevant modes of the target distribution. We propose a method for learning accurate GMM approximations of intractable probability distributions based on insights from policy search by using information-geometric trust regions for principled exploration. For efficient improvement of the GMM approximation, we derive a lower bound on the corresponding optimization objective enabling us to update the components independently. Our use of the lower bound ensures convergence to a stationary point of the original objective. The number of components is adapted online by adding new components in promising regions and by deleting components with negligible weight. We demonstrate on several domains that we can learn approximations of complex, multimodal distributions with a quality that is unmet by previous variational inference methods, and that the GMM approximation can be used for drawing samples that are on par with samples created by state-of-the-art MCMC samplers while requiring up to three orders of magnitude less computational resources.",,,,,,"Arenz, Oleg/0000-0002-9470-2833",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,163,,,,,,,,,,,,,,,WOS:000570151700001,0
J,"Arias-Castro, E; Javanmard, A; Pelletier, B",,,,"Arias-Castro, Ery; Javanmard, Adel; Pelletier, Bruno",,,"Perturbation Bounds for Procrustes, Classical Scaling, and Trilateration, with Applications to Manifold Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the common tasks in unsupervised learning is dimensionality reduction, where the goal is to find meaningful low-dimensional structures hidden in high-dimensional data. Sometimes referred to as manifold learning, this problem is closely related to the problem of localization, which aims at embedding a weighted graph into a low-dimensional Euclidean space. Several methods have been proposed for localization, and also manifold learning. Nonetheless, the robustness property of most of them is little understood. In this paper, we obtain perturbation bounds for classical scaling and trilateration, which are then applied to derive performance bounds for Isomap, Landmark Isomap, and Maximum Variance Unfolding. A new perturbation bound for procrustes analysis plays a key role.",,,,,"Javanmard, Adel/ABB-5000-2020",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300015,0
J,"Guha, N; Baladandayuthapani, V; Mallick, BK",,,,"Guha, Nilabja; Baladandayuthapani, Veera; Mallick, Bani K.",,,Quantile Graphical Models: a Bayesian Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graphical models are ubiquitous tools to describe the interdependence between variables measured simultaneously such as large-scale gene or protein expression data. Gaussian graphical models (GGMs) are well-established tools for probabilistic exploration of dependence structures using precision matrices and they are generated under a multivariate normal joint distribution. However, they suffer from several shortcomings since they are based on Gaussian distribution assumptions. In this article, we propose a Bayesian quantile based approach for sparse estimation of graphs. We demonstrate that the resulting graph estimation is robust to outliers and applicable under general distributional assumptions. Furthermore, we develop efficient variational Bayes approximations to scale the methods for large data sets. Our methods are applied to a novel cancer proteomics data dataset where-in multiple proteomic antibodies are simultaneously assessed on tumor samples using reverse-phase protein arrays (RPPA) technology.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000542194600002,0
J,"Jiang, T; Vavasis, S; Zhai, CW",,,,"Jiang, Tao; Vavasis, Stephen; Zhai, Chen Wen",,,Recovery of a Mixture of Gaussians by Sum-of-norms Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sum-of-norms clustering is a method for assigning n points in R-d to K clusters, 1 <= K <= n, using convex optimization. Recently, Panahi et al. (2017) proved that sum-of-norms clustering is guaranteed to recover a mixture of Gaussians under the restriction that the number of samples is not too large. The purpose of this note is to lift this restriction, that is, show that sum-of-norms clustering can recover a mixture of Gaussians even as the number of samples tends to infinity. Our proof relies on an interesting characterization of clusters computed by sum-of-norms clustering that was developed inside a proof of the agglomeration conjecture by Chiquet et al. (2017). Because we believe this theorem has independent interest, we restate and reprove the Chiquet et al. (2017) result herein.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,225,,,,,,,,,,,,,,,WOS:000605745100001,0
J,"Kulunchakov, A; Mairal, J",,,,"Kulunchakov, Andrei; Mairal, Julien",,,"Estimate Sequences for Stochastic Composite Optimization: Variance Reduction, Acceleration, and Robustness to Noise",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. More precisely, we interpret a large class of stochastic optimization methods as procedures that iteratively minimize a surrogate of the objective, which covers the stochastic gradient descent method and variants of the incremental approaches SAGA, SVRG, and MISO/Finito/SDCA. This point of view has several advantages: (i) we provide a simple generic proof of convergence for all of the aforementioned methods; (ii) we naturally obtain new algorithms with the same guarantees; (iii) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we propose a new accelerated stochastic gradient descent algorithm and a new accelerated SVRG algorithm that is robust to stochastic noise.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,155,,,,,,,,,,,,,,,WOS:000570115700001,0
J,"Little, A; Maggioni, M; Murphy, JM",,,,"Little, Anna; Maggioni, Mauro; Murphy, James M.",,,"Path-Based Spectral Clustering: Guarantees, Robustness to Outliers, and Fast Algorithms",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers. By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets, and prove guarantees on the labeling accuracy of the proposed algorithm. Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric. We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.",,,,,,"Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300006,0
J,"Mokhtari, A; Hassani, H; Karbasi, A",,,,"Mokhtari, Aryan; Hassani, Hamed; Karbasi, Amin",,,Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers stochastic optimization problems for a large class of objective functions, including convex and continuous submodular. Stochastic proximal gradient methods have been widely used to solve such problems; however, their applicability remains limited when the problem dimension is large and the projection onto a convex set is computationally costly. Instead, stochastic conditional gradient algorithms are proposed as an alternative solution which rely on (i) Approximating gradients via a simple averaging technique requiring a single stochastic gradient evaluation per iteration; (ii) Solving a linear program to compute the descent/ascent direction. The gradient averaging technique reduces the noise of gradient approximations as time progresses, and replacing projection step in proximal methods by a linear program lowers the computational complexity of each iteration. We show that under convexity and smoothness assumptions, our proposed stochastic conditional gradient method converges to the optimal objective function value at a sublinear rate of O(1/t(1/3)). Further, for a monotone and continuous DR-submodular function and subject to a general convex body constraint, we prove that our proposed method achieves a ((1-1/e)OPT-epsilon) guarantee (in expectation) with O(1/epsilon(3)) stochastic gradient computations. This guarantee matches the known hardness results and closes the gap between deterministic and stochastic continuous submodular maximization. Additionally, we achieve ((1/e)OPT-epsilon) guarantee after operating on O(1/epsilon(3)) stochastic gradients for the case that the objective function is continuous DR-submodular but non-monotone and the constraint set is a down-closed convex body. By using stochastic continuous optimization as an interface, we also provide the first (1-1/e) tight approximation guarantee for maximizing a monotone but stochastic submodular set function subject to a general matroid constraint and (1/e) approximation guarantee for the non-monotone case.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,49,105,,,,,,,,,,,,,,,WOS:000546626200001,0
J,"Pircalabelu, E; Claeskens, G",,,,"Pircalabelu, Eugen; Claeskens, Gerda",,,Community-Based Group Graphical Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new strategy for probabilistic graphical modeling is developed that draws parallels to community detection analysis. The method jointly estimates an undirected graph and homogenous communities of nodes. The structure of the communities is taken into account when estimating the graph and at the same time, the structure of the graph is accounted for when estimating communities of nodes. The procedure uses a joint group graphical lasso approach with community detection-based grouping, such that some groups of edges cooccur in the estimated graph. The grouping structure is unknown and is estimated based on community detection algorithms. Theoretical derivations regarding graph convergence and sparsistency, as well as accuracy of community recovery are included, while the method's empirical performance is illustrated in an fMRI context, as well as with simulated examples.",,,,,,"Claeskens, Gerda/0000-0002-3863-5197",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000016,0
J,"Plecko, D; Meinshausen, N",,,,"Plecko, Drago; Meinshausen, Nicolai",,,Fair Data Adaptation with Quantile Preservation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fairness of classification and regression has received much attention recently and various, partially non-compatible, criteria have been proposed. The fairness criteria can be enforced for a given classifier or, alternatively, the data can be adapted to ensure that every classifier trained on the data will adhere to desired fairness criteria. We present a practical data adaption method based on quantile preservation in causal structural equation models. The data adaptation is based on a presumed counterfactual model for the data. While the counterfactual model itself cannot be verified experimentally, we show that certain population notions of fairness are still guaranteed even if the counterfactual model is misspecified. The nature of the fulfilled observational non-causal fairness notion (such as demographic parity, separation or sufficiency) depends on the structure of the underlying causal model and the choice of resolving variables. We describe an implementation of the proposed data adaptation procedure based on Random Forests (Breiman, 2001) and demonstrate its practical use on simulated and real-world data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,242,,,,,,,,,,,,,,,WOS:000608913800001,0
J,"Song, H; Dai, R; Raskutti, G; Barber, RF",,,,"Song, Hyebin; Dai, Ran; Raskutti, Garvesh; Barber, Rina Foygel",,,Convex and Non-Convex Approaches for Statistical Inference with Class-Conditional Noisy Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of estimation and testing in logistic regression with class-conditional noise in the observed labels, which has an important implication in the Positive-Unlabeled (PU) learning setting. With the key observation that the label noise problem belongs to a special sub-class of generalized linear models (GLM), we discuss convex and non-convex approaches that address this problem. A non-convex approach based on the maximum likelihood estimation produces an estimator with several optimal properties, but a convex approach has an obvious advantage in optimization. We demonstrate that in the low-dimensional setting, both estimators are consistent and asymptotically normal, where the asymptotic variance of the non-convex estimator is smaller than the convex counterpart. We also quantify the efficiency gap which provides insight into when the two methods are comparable. In the high-dimensional setting, we show that both estimation procedures achieve '2-consistency at the minimax optimal root slog p/n rates under mild conditions. Finally, we propose an inference procedure using a de-biasing approach. We validate our theoretical findings through simulations and a real-data example.",,,,,"Song, Hyebin/AAR-1606-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,168,,,,,,,,,,,,,,,WOS:000570154600001,0
J,"Trillos, NG; Kaplan, Z; Samakhoana, T; Sanz-Alonso, D",,,,"Trillos, Nicolas Garcia; Kaplan, Zachary; Samakhoana, Thabo; Sanz-Alonso, Daniel",,,On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers a Bayesian approach to graph-based semi-supervised learning. We show that if the graph parameters are suitably scaled, the graph-posteriors converge to a continuum limit as the size of the unlabeled data set grows. This consistency result has profound algorithmic implications: we prove that when consistency holds, carefully designed Markov chain Monte Carlo algorithms have a uniform spectral gap, independent of the number of unlabeled inputs. Numerical experiments illustrate and complement the theory.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000003,0
J,"Valera, I; Pradier, MF; Lomeli, M; Ghahramani, Z",,,,"Valera, Isabel; Pradier, Melanie F.; Lomeli, Maria; Ghahramani, Zoubin",,,General Latent Feature Models for Heterogeneous Datasets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,100,,,,,,,,,,,,,,,WOS:000546144200001,0
J,"Wang, MY; Li, LX",,,,"Wang, Miaoyan; Li, Lexin",,,Learning from Binary Multiway Data: Probabilistic Tensor Decomposition and its Statistical Optimality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of decomposing a higher-order tensor with binary entries. Such data problems arise frequently in applications such as neuroimaging, recommendation system, topic modeling, and sensor network localization. We propose a multilinear Bernoulli model, develop a rank-constrained likelihood-based estimation method, and obtain the theoretical accuracy guarantees. In contrast to continuous-valued problems, the binary tensor problem exhibits an interesting phase transition phenomenon according to the signal-tonoise ratio. The error bound for the parameter tensor estimation is established, and we show that the obtained rate is minimax optimal under the considered model. Furthermore, we develop an alternating optimization algorithm with convergence guarantees. The efficacy of our approach is demonstrated through both simulations and analyses of multiple data sets on the tasks of tensor completion and clustering.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,154,,,,,,,,,,,,,,,WOS:000570115400001,0
J,"Ariafar, S; Coll-Font, J; Brooks, D; Dy, J",,,,"Ariafar, Setareh; Coll-Font, Jaume; Brooks, Dana; Dy, Jennifer",,,ADMMBO: Bayesian Optimization with Unknown Constraints using ADMM,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle 'decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions.",,,,,"Brooks, Dana/AAH-9976-2019",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,123,,,,,,,,,,31798351,,,,,WOS:000487068900007,0
J,"Burkhardt, S; Kramer, S",,,,"Burkhardt, Sophie; Kramer, Stefan",,,Decoupling Sparsity and Smoothness in the Dirichlet Variational Autoencoder Topic Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent work on variational autoencoders (VAEs) has enabled the development of generative topic models using neural networks. Topic models based on latent Dirichlet allocation (LDA) successfully use the Dirichlet distribution as a prior for the topic and word distributions to enforce sparseness. However, there is a trade-off between sparsity and smoothness in Dirichlet distributions. Sparsity is important for a low reconstruction error during training of the autoencoder, whereas smoothness enables generalization and leads to a better log-likelihood of the test data. Both of these properties are encoded in the Dirichlet parameter vector. By rewriting this parameter vector into a product of a sparse binary vector and a smoothness vector, we decouple the two properties, leading to a model that features both a competitive topic coherence and a high log-likelihood. Efficient training is enabled using rejection sampling variational inference for the reparameterization of the Dirichlet distribution. Our experiments show that our method is competitive with other recent VAE topic models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,131,,,,,,,,,,,,,,,WOS:000487068900015,0
J,"Dai, B; Wang, JH; Shen, XT; Qu, AN",,,,"Dai, Ben; Wang, Junhui; Shen, Xiaotong; Qu, Annie",,,Smooth neighborhood recommender systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recommender systems predict users' preferences over a large number of items by pooling similar information from other users and/or items in the presence of sparse observations. One major challenge is how to utilize user-item specific covariates and networks describing user-item interactions in a high-dimensional situation, for accurate personalized prediction. In this article, we propose a smooth neighborhood recommender in the framework of the latent factor models. A similarity kernel is utilized to borrow neighborhood information from continuous covariates over a user-item specific network, such as a user's social network, where the grouping information defined by discrete covariates is also integrated through the network. Consequently, user-item specific information is built into the recommender to battle the 'cold-start issue in the absence of observations in collaborative and content-based filtering. Moreover, we utilize a divide-and-conquer version of the alternating least squares algorithm to achieve scalable computation, and establish asymptotic results for the proposed method, demonstrating that it achieves superior prediction accuracy. Finally, we illustrate that the proposed method improves substantially over its competitors in simulated examples and real benchmark data-Last.fm music data.",,,,,"Dai, Ben/AAY-1165-2020; Qu, Annie/Q-9314-2019","Dai, Ben/0000-0002-1620-1021",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,1,,,,,,,,,,,,,,,WOS:000458666600001,0
J,"Pirs, G; Strumbelj, E",,,,"Pirs, Gregor; Strumbelj, Erik",,,Bayesian Combination of Probabilistic Classifiers using Multivariate Normal Mixtures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ensemble methods are a powerful tool, often outperforming individual prediction models. Existing Bayesian ensembles either do not model the correlations between sources, or they are only capable of combining non-probabilistic predictions. We propose a new model, which overcomes these disadvantages. Transforming the probabilistic predictions with the inverse additive logistic transformation allows us to model the correlations with multivariate normal mixtures. We derive an efficient Gibbs sampler for the proposed model and implement a regularization method to make it more robust. We compare our method to related work and the classical linear opinion pool. Empirical evaluation on several toy and real-world data sets, including a case study on air-pollution forecasting, shows that the method outperforms other methods, while being robust and easy to use.",,,,,,"Pirs, Gregor/0000-0003-4082-1252",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,51,,,,,,,,,,,,,,,WOS:000463322900001,0
J,"Tremblay, N; Barthelme, S; Amblard, PO",,,,"Tremblay, Nicolas; Barthelme, Simon; Amblard, Pierre-Olivier",,,Determinantal Point Processes for Coresets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When faced with a data set too large to be processed all at once, an obvious solution is to retain only part of it. In practice this takes a wide variety of different forms, and among them coresets are especially appealing. A coreset is a (small) weighted sample of the original data that comes with the following guarantee: a cost function can be evaluated on the smaller set instead of the larger one, with low relative error. For some classes of problems, and via a careful choice of sampling distribution (based on the so-called sensitivity metric), iid random sampling has turned to be one of the most successful methods for building coresets efficiently. However, independent samples are sometimes overly redundant, and one could hope that enforcing diversity would lead to better performance. The difficulty lies in proving coreset properties in non-iid samples. We show that the coreset property holds for samples formed with determinantal point processes (DPP). DPPs are interesting because they are a rare example of repulsive point processes with tractable theoretical properties, enabling us to prove general coreset theorems. We apply our results to both the k-means and the linear regression problems, and give extensive empirical evidence that the small additional computational cost of DPP sampling comes with superior performance over its iid counterpart. Of independent interest, we also provide analytical formulas for the sensitivity in the linear regression and 1-means cases.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,168,,,,,,,,,,,,,,,WOS:000506403100008,0
J,"Zhou, JTY; Tsang, IW; Pan, SJL; Tan, MK",,,,"Zhou, Joey Tianyi; Tsang, Ivor W.; Pan, Sinno Jialin; Tan, Mingkui",,,Multi-class Heterogeneous Domain Adaptation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A crucial issue in heterogeneous domain adaptation (HDA) is the ability to learn a feature mapping between different types of features across domains. Inspired by language translation, a word translated from one language corresponds to only a few words in another language, we present an efficient method named Sparse Heterogeneous Feature Representation (SHFR) in this paper for multi-class HDA to learn a sparse feature transformation between domains with multiple classes. Specifically, we formulate the problem of learning the feature transformation as a compressed sensing problem by building multiple binary classifiers in the target domain as various measurement sensors, which are decomposed from the target multi-class classification problem. We show that the estimation error of the learned transformation decreases with the increasing number of binary classifiers. In other words, for adaptation across heterogeneous domains to be successful, it is necessary to construct a sufficient number of incoherent binary classifiers from the original multi-class classification problem. To achieve this, we propose to apply the error correcting output correcting (ECOC) scheme to generate incoherent classifiers. To speed up the learning of the feature transformation across domains, we apply an efficient batch-mode algorithm to solve the resultant nonnegative sparse recovery problem. Theoretically, we present a generalization error bound of our proposed HDA method under a multi-class setting. Lastly, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate the superiority of our proposed method over existing state-of-the-art HDA methods in terms of prediction accuracy and training efficiency.",,,,,"PAN, Sinno Jialin/P-6696-2014","PAN, Sinno Jialin/0000-0001-6565-3836",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,57,,,,,,,,,,,,,,,WOS:000467877500001,0
J,"Au, TC",,,,"Au, Timothy C.",,,"Random Forests, Decision Trees, and Categorical Predictors: The Absent Levels Problem",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One advantage of decision tree based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques). However, in this paper, we show how this capability can lead to an inherent absent levels problem for decision tree based methods that has never been thoroughly discussed, and whose consequences have never been carefully explored. This problem occurs whenever there is an indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question's level was absent during training. Although these incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's random forests FORTRAN code and the randomForest R package (Liaw and Wiener, 2002) as motivating case studies, we examine how overlooking the absent levels problem can systematically bias a model. Furthermore, by using three real data examples, we illustrate how absent levels can dramatically alter a model's performance in practice, and we empirically demonstrate how some simple heuristics can be used to help mitigate the effects of the absent levels problem until a more robust theoretical solution is found.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,30,45,,,,,,,,,,,,,,,WOS:000448368400001,0
J,"Boyd, N; Hastie, T; Boyd, S; Recht, B; Jordan, MI",,,,"Boyd, Nicholas; Hastie, Trevor; Boyd, Stephen; Recht, Benjamin; Jordan, Michael, I",,,Saturating Splines and Feature Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite-dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range.",,,,,"Jordan, Michael I/C-5253-2013","Hastie, Trevor/0000-0002-0164-3142; Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,197,,,,,,,,,,31007630,,,,,WOS:000435454900001,0
J,"Dessein, A; Papadakis, N; Rouas, JL",,,,"Dessein, Arnaud; Papadakis, Nicolas; Rouas, Jean-Luc",,,Regularized Optimal Transport and the Rot Mover's Distance,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classification.",,,,,,"ROUAS, Jean-Luc/0000-0003-1933-0504",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,15,,,,,,,,,,,,,,,WOS:000443226400001,0
J,"Duan, LL; Johndrow, JE; Dunson, DB",,,,"Duan, Leo L.; Johndrow, James E.; Dunson, David B.",,,Scaling up Data Augmentation MCMC via Calibration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There has been considerable interest in making Bayesian inference more scalable. In big data settings, most of the focus has been on reducing the computing time per iteration rather than reducing the number of iterations needed in Markov chain Monte Carlo (MCMC). This article considers data augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend to become highly autocorrelated in large samples, due to a mis-calibration problem in which conditional posterior distributions given augmented data are too concentrated. This makes it necessary to collect very long MCMC paths to obtain acceptably low MC error. To combat this inefficiency, we propose a family of calibrated data augmentation algorithms, which appropriately adjust the variance of conditional posterior distributions. A Metropolis-Hastings step is used to eliminate bias in the stationary distribution of the resulting sampler. Compared to existing alternatives, this approach can dramatically reduce MC error by reducing autocorrelation and increasing the effective number of DA-MCMC samples per unit of computing time. The approach is simple and applicable to a broad variety of existing data augmentation algorithms. We focus on three popular generalized linear models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,64,,,,,,,,,,,,,,,WOS:000452053300001,0
J,"Kailkhura, B; Thiagarajan, JJ; Rastogi, C; Varshney, PK; Bremer, PT",,,,"Kailkhura, Bhavya; Thiagarajan, Jayaraman J.; Rastogi, Charvi; Varshney, Pramod K.; Bremer, Peer-Timo",,,"A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the quality metric of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework for generating high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in varying dimensions: a) image reconstruction and b) surrogate modeling for several benchmark optimization functions and a physics simulation code for inertial confinement fusion (ICF). Our results clearly evidence the superiority of the proposed space-filling designs over existing approaches, particularly in high dimensions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,34,,,,,,,,,,,,,,,WOS:000444404300001,0
J,"Konev, B; Lutz, C; Ozaki, A; Wolter, F",,,,"Konev, Boris; Lutz, Carsten; Ozaki, Ana; Wolter, Frank",,,Exact Learning of Light weight Description Logic Ontologies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning description logic (DL) ontologies in Angluin et al.'s framework of exact learning via queries. We admit membership queries (is a given subsumption entailed by the target ontology?) and equivalence queries (is a given ontology equivalent to the target ontology?). We present three main results: (1) ontologies formulated in (two relevant versions of) the description logic DL-Lite can be learned with polynomially many queries of polynomial size; (2) this is not the case for ontologies formulated in the description logic epsilon L, even when only acyclic ontologies are admitted; and (3) ontologies formulated in a fragment of epsilon L related to the web ontology language OWL 2 RL can be learned in polynomial time. We also show that neither membership nor equivalence queries alone are sufficient in cases (1) and (3).",,,,,"Wolter, Frank/A-3727-2014",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,201,,,,,,,,,,,,,,,WOS:000435627200001,0
J,"Kusano, G; Fukumizu, K; Hiraoka, Y",,,,"Kusano, Genki; Fukumizu, Kenji; Hiraoka, Yasuaki",,,Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complicated data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, distinguishing robust and noisy topological properties. This paper introduces a kernel method for persistence diagrams to develop a statistical framework in TDA. The proposed kernel is stable under perturbation of data, enables one to explicitly control the effect of persistence by a weight function, and allows an efficient and accurate approximate computation. The method is applied into practical data on granular systems, oxide glasses and proteins, showing advantages of our method compared to other relevant methods for persistence diagrams.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,189,,,,,,,,,,,,,,,WOS:000435450000001,0
J,"Lai, ZR; Yang, PY; Fang, LD; Wu, XT",,,,"Lai, Zhao-Rong; Yang, Pei-Yi; Fang, Liangda; Wu, Xiaotian",,,Short-term Sparse Portfolio Optimization Based on Alternating Direction Method of Multipliers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a short-term sparse portfolio optimization (SSPO) system based on alternating direction method of multipliers (ADMM). Although some existing strategies have also exploited sparsity, they either constrain the quantity of the portfolio change or aim at the long-term portfolio optimization. Very few of them are dedicated to constructing sparse portfolios for the short-term portfolio optimization, which will be complemented by the proposed SSPO. SSPO concentrates wealth on a small proportion of assets that have good increasing potential according to some empirical financial principles, so as to maximize the cumulative wealth for the whole investment. We also propose a solving algorithm based on ADMM to handle the l(1)-regularization term and the self-financing constraint simultaneously. As a significant improvement in the proposed ADMM, we have proven that its augmented Lagrangian has a saddle point, which is the foundation of the iterative formulae of ADMM but is seldom addressed by other sparsity strategies. Extensive experiments on 5 benchmark data sets from real-world stock markets show that SSPO outperforms other state-of-the-art systems in thorough evaluations, withstands reasonable transaction costs and runs fast. Thus it is suitable for real-world financial environments.",,,,,,"Wu, Xiaotian/0000-0002-1484-2247",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,63,,,,,,,,,,,,,,,WOS:000452053100001,0
J,"Lin, HZ; Mairal, J; Harchaoui, Z",,,,"Lin, Hongzhou; Mairal, Julien; Harchaoui, Zaid",,,Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,212,,,,,,,,,,,,,,,WOS:000435628600001,0
J,"Lu, JW; Kolar, M; Liu, H",,,,"Lu, Junwei; Kolar, Mladen; Liu, Han",,,Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000435627400001,0
J,"Simon-Gabriel, CJ; Scholkopf, B",,,,"Simon-Gabriel, Carl-Johann; Schoelkopf, Bernhard",,,"Kernel Distribution Embeddings: Universal Kernels, Characteristic Kernels and Kernel Metrics on Distributions",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel mean embeddings have become a popular tool in machine learning. They map probability measures to functions in a reproducing kernel Hilbert space. The distance between two mapped measures defines a semi-distance over the probability measures known as the maximum mean discrepancy (MMD). Its properties depend on the underlying kernel and have been linked to three fundamental concepts of the kernel literature: universal, characteristic and strictly positive definite kernels. The contributions of this paper are three-fold. First, by slightly extending the usual definitions of universal, characteristic and strictly positive definite kernels, we show that these three concepts are essentially equivalent. Second, we give the first complete characterization of those kernels whose associated MMD-distance metrizes the weak convergence of probability measures. Third, we show that kernel mean embeddings can be extended from probability measures to generalized measures called Schwartz-distributions and analyze a few properties of these distribution embeddings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,44,,,,,,,,,,,,,,,WOS:000448367000001,0
J,"Egorov, M; Sunberg, ZN; Balaban, E; Wheeler, TA; Gupta, JK; Kochenderfer, MJ",,,,"Egorov, Maxim; Sunberg, Zachary N.; Balaban, Edward; Wheeler, Tim A.; Gupta, Jayesh K.; Kochenderfer, Mykel J.",,,POMDPs.j1: A Framework for Sequential Decision Making under Uncertainty,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"POMDPs.j1 is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.j1 allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs. jl, the related packages, and documentation can be found at https://github.com/JuliaPOMDP/POMDPs.jl.",,,,,"Balaban, Edward/U-7068-2019",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,26,,,,,,,,,,,,,,,WOS:000399842100001,0
J,"Gates, AJ; Ahn, YY",,,,"Gates, Alexander J.; Ahn, Yong-Yeol",,,The Impact of Random Models on Clustering Similarity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering returns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified.",,,,,"Ahn, Yong-Yeol/C-6334-2011","Ahn, Yong-Yeol/0000-0002-4352-4301",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,87,,,,,,,,,,,,,,,WOS:000412069900001,0
J,"Lee, JD; Liu, Q; Sun, Y; Taylor, JE",,,,"Lee, Jason D.; Liu, Qiang; Sun, Yuekai; Taylor, Jonathan E.",,,Communication-efficient Sparse Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,5,,,,,,,,,,,,,,,WOS:000397018800001,0
J,"Adi, Y; Keshet, J",,,,"Adi, Yossi; Keshet, Joseph",,,StructED : Risk Minimization in Structured Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Structured tasks are distinctive: each task has its own measure of performance, such as the word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in information retrieval, or the intersection-over-union score in visual object segmentation. This paper presents STRUCTED, a software package for learning structured prediction models with training methods that aimed at optimizing the task measure of performance. The package was written in Java and released under the MIT license.",,,,,,"Keshet, Joseph/0000-0003-2332-5783",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,64,,,,,,,,,,,,,,,WOS:000391522400001,0
J,"Bedo, J; Ong, CS",,,,"Bedo, Justin; Ong, Cheng Soon",,,Multivariate pearman's rho for Aggregating Ranks Using Copula,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of rank aggregation: given a set of ranked lists, we want to form a consensus ranking. Furthermore, we consider the case of extreme lists: i.e., only the rank of the best or worst elements are known. We impute missing ranks by the average value and generalise Spearman's rho to extreme ranks. Our main contribution is the derivation of a non-parametric estimator for rank aggregation based on multivariate extensions of Spearman's rho, which measures correlation between a set of ranked lists. Multivariate Spearman's rho is defined using copulas, and we show that the geometric mean of normalised ranks maximises multivariate correlation. Motivated by this, we propose a weighted geometric mean approach for learning to rank which has a closed form least squares solution. When only the best (top-k) or worst (bottom-k) elements of a ranked list are known, we impute the missing ranks by the average value, allowing us to apply Spearman's rho. We discuss an optimistic and pessimistic imputation of missing values,which respectively maximise and minimise correlation, and showits effect on aggregating university rankings Finally, we demonstrate good performance on the rank aggregation benchmarks MQ2007 and MQ2008.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,201,,,,,,,,,,,,,,,WOS:000391829300001,0
J,"Goncalves, AR; Von Zuben, FJ; Banerjee, A",,,,"Goncalves, Andre R.; Von Zuben, Fernando J.; Banerjee, Arindam",,,Multi-task Sparse Structure Learning with Gaussian Copula Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultaneously. While sometimes the underlying task relationship structure is known, often the structure needs to be estimated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and classification problems, capable of learning the structure of tasks relationship. In particular, we consider a joint estimation problem of the tasks relationship structure and the individual task parameters, which is solved using alternating minimization. The task relationship revealed by structure learning is founded on recent advances in Gaussian graphical models endowed with sparse estimators of the precision (inverse covariance) matrix. An extension to include flexible Gaussian copula models that relaxes the Gaussian marginal assumption is also proposed. We illustrate the e ff ectiveness of the proposed model on a variety of synthetic and benchmark data sets for regression and classi fi cation. We also consider the problem of combining Earth System Model (ESM) outputs for better projections of future climate, with focus on projections of temperature by combining ESMs in South and North America, and show that the proposed model outperforms several existing methods for the problem.",,,,,"Goncalves, Andre R/H-8065-2015; Goncalves, Andre/AAW-2632-2021; Von Zuben, Fernando J./K-5979-2017; Goncalves, Andre/V-7132-2019","Goncalves, Andre/0000-0002-0320-280X; Von Zuben, Fernando J./0000-0002-4128-5415; Goncalves, Andre/0000-0002-0320-280X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,30,33,,,,,,,,,,,,,,,WOS:000391480500001,0
J,"Guhaniyogi, R; Dunson, DB",,,,"Guhaniyogi, Rajarshi; Dunson, David B.",,,Compressed Gaussian Process for Manifold Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonparametric regression for large numbers of features (p) is an increasingly important problem. If the sample size n is massive, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. This is not ideal when n is modest relative to p, and we propose an alternative approach relying on random compression of the feature vector combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very large p and moderately large n nonparametric regression, has strong theoretical justification, and is found to yield state of the art predictive performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,69,,,,,,,,,,,,,,,WOS:000391522900001,0
J,"Hazan, T; Schwing, AG; Urtasun, R",,,,"Hazan, Tamir; Schwing, Alexander G.; Urtasun, Raquel",,,Blending Learning and Inference in Conditional Random Fields,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional random fields maximize the log-likelihood of training labels given the training data, e.g., objects given images. In many cases the training labels are structures that consist of a set of variables and the computational complexity for estimating their likelihood is exponential in the number of the variables. Learning algorithms relax this computational burden using approximate inference that is nested as a sub-procedure. In this paper we describe the objective function for nested learning and inference in conditional random fields. The devised objective maximizes the log-beliefs - probability distributions over subsets of training variables that agree on their marginal probabilities. This objective is concave and consists of two types of variables that are related to the learning and inference tasks respectively. Importantly, we afterwards show how to blend the learning and inference procedure and effectively get to the identical optimum much faster. The proposed algorithm currently achieves the state-of-the-art in various computer vision applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,237,,,,,,,,,,,,,,,WOS:000391920100001,0
J,"Henao, R; Lu, JT; Lucas, JE; Ferranti, J; Carin, L",,,,"Henao, Ricardo; Lu, James T.; Lucas, Joseph E.; Ferranti, Jeffrey; Carin, Lawrence",,,Electronic Health Record Analysis via Deep Poisson Factor Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,186,,,,,,,,,,,,,,,WOS:000391825400001,0
J,"Mohri, M; Medina, AM",,,,"Mohri, Mehryar; Medina, Andres Munoz",,,Learning Algorithms for Second-Price Auctions with Reserve,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Second-price auctions with reserve play a critical role in the revenue of modern search engine and popular online sites since the revenue of these companies often directly depends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real-world data demonstrating their effectiveness.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,74,,,,,,,,,,,,,,,WOS:000391524400001,0
J,"Neykov, M; Liu, JS; Cai, T",,,,"Neykov, Matey; Liu, Jun S.; Cai, Tianxi",,,On the Characterization of a Class of Fisher-Consistent Loss Functions and its Application to Boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Accurate classification of categorical outcomes is essential in a wide range of applications. Due to computational issues with minimizing the empirical 0/1 loss, Fisher consistent losses have been proposed as viable proxies. However, even with smooth losses, direct minimization remains a daunting task. To approximate such a minimizer, various boosting algorithms have been suggested. For example, with exponential loss, the AdaBoost algorithm (Freund and Schapire, 1995) is widely used for two- class problems and has been extended to the multi-class setting (Zhu et al., 2009). Alternative loss functions, such as the logistic and the hinge losses, and their corresponding boosting algorithms have also been proposed (Zou et al., 2008; Wang, 2012). In this paper we demonstrate that a broad class of losses, including non-convex functions, achieve Fisher consistency, and in addition can be used for explicit estimation of the conditional class probabilities. Furthermore, we provide a generic boosting algorithm that is not loss-specific. Extensive simulation results suggest that the proposed boosting algorithms could outperform existing methods with properly chosen losses and bags of weak learners.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,70,,,,,,,,,,,,,,,WOS:000391523300001,0
J,"Reyes, O; Perez, E; Rodriguez-Hernandez, MD; Fardoun, HM; Ventura, S",,,,"Reyes, Oscar; Perez, Eduardo; del Carmen Rodriguez-Hernandez, Maria; Fardoun, Habib M.; Ventura, Sebastian",,,JCLAL: A Java Framework for Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license.",,,,,"Del+Carmen+Rodr√≠guez+Hern√°ndez, Mar√≠a/AFX-5058-2022; Fardoun, Habib M/B-6703-2013; Ventura, Sebastian/A-7753-2008","Fardoun, Habib M/0000-0002-3641-389X; Ventura, Sebastian/0000-0003-4216-6378; Perez, Eduardo/0000-0003-2343-2634",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,95,,,,,,,,,,,,,,,WOS:000391539500001,0
J,"Hermans, M; Soriano, MC; Dambre, J; Bienstman, P; Fischer, I",,,,"Hermans, Michiel; Soriano, Miguel C.; Dambre, Joni; Bienstman, Peter; Fischer, Ingo",,,Photonic Delay Systems as Machine Learning Implementations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.",,,,,"Dambre, Joni/C-2926-2013; Soriano, Miguel C./D-8480-2011; Fischer, Ingo/S-9129-2019; Fischer, Ingo/C-2843-2011","Dambre, Joni/0000-0002-9373-1210; Soriano, Miguel C./0000-0002-6140-8451; Fischer, Ingo/0000-0003-1881-9842",,,,,,,,,,,,,1532-4435,,,,,OCT,2015,16,,,,,,2081,2097,,,,,,,,,,,,,,,,WOS:000369887400001,0
J,"Moroshko, E; Vaits, N; Crammer, K",,,,"Moroshko, Edward; Vaits, Nina; Crammer, Koby",,,Second-Order Non-Stationary Online Learning for Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of a learner in standard online learning, is to have the cumulative loss not much larger compared with the best-performing function from some fixed class. Numerous algorithms were shown to have this gap arbitrarily close to zero, compared with the best function that is chosen off-line. Nevertheless, many real-world applications, such as adaptive filtering, are non-stationary in nature, and the best prediction function may drift over time. We introduce two novel algorithms for online regression, designed to work well in non-stationary environment. Our first algorithm performs adaptive resets to forget the history, while the second is last-step min-max optimal in context of a drift. We analyze both algorithms in the worst-case regret framework and show that they maintain an average loss close to that of the best slowly changing sequence of linear functions, as long as the cumulative drift is sublinear. In addition, in the stationary case, when no drift occurs, our algorithms suffer logarithmic regret, as for previous algorithms. Our bounds improve over existing ones, and simulations demonstrate the usefulness of these algorithms compared with other state-of-the-art approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1481,1517,,,,,,,,,,,,,,,,WOS:000369887100005,0
J,"Jiang, F; Zhang, SP; Wu, S; Gao, Y; Zhao, DB",,,,"Jiang, Feng; Zhang, Shengping; Wu, Shen; Gao, Yang; Zhao, Debin",,,Multi-layered Gesture Recognition with Kinect,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes a novel multi-layered gesture recognition method with Kinect. We explore the essential linguistic characters of gestures: the components concurrent character and the sequential organization character, in a multi-layered framework, which extracts features from both the segmented semantic units and the whole gesture sequence and then sequentially classifies the motion, location and shape components. In the first layer, an improved principle motion is applied to model the motion component. In the second layer, a particle-based descriptor and a weighted dynamic time warping are proposed for the location component classification. In the last layer, the spatial path warping is further proposed to classify the shape component represented by unclosed shape context. The proposed method can obtain relatively high performance for one-shot learning gesture recognition on the ChaLearn Gesture Dataset comprising more than 50, 000 gesture sequences recorded with Kinect.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2015,16,,,,,,227,254,,,,,,,,,,,,,,,,WOS:000369885800003,0
J,"Hamilton, W; Fard, MM; Pineau, J",,,,"Hamilton, William; Fard, Mahdi Milani; Pineau, Joelle",,,Efficient Learning and Planning with Compressed Predictive States,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predictive state representations (PSRs) offer an expressive framework for modelling partially observable systems. By compactly representing systems as functions of observable quantities, the PSR learning approach avoids using local-minima prone expectation-maximization and instead employs a globally optimal moment-based algorithm. Moreover, since PSRs do not require a predetermined latent state structure as an input, they offer an attractive framework for model-based reinforcement learning when agents must plan without a priori access to a system model. Unfortunately, the expressiveness of PSRs comes with significant computational cost, and this cost is a major factor inhibiting the use of PSRs in applications. In order to alleviate this shortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR learning approach combines recent advancements in dimensionality reduction, incremental matrix decomposition, and compressed sensing. We show how this approach provides a principled avenue for learning accurate approximations of PSRs, drastically reducing the computational costs associated with learning while also providing effective regularization. Going further, we propose a planning framework which exploits these learned models. And we show that this approach facilitates model-learning and planning in large complex partially observable domains, a task that is infeasible without the principled use of compression.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3395,3439,,,,,,,,,,,,,,,,WOS:000353126200003,0
J,"Srivastava, N; Salakhutdinov, R",,,,"Srivastava, Nitish; Salakhutdinov, Ruslan",,,Multimodal Learning with Deep Boltzmann Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,2949,2980,,,,,,,,,,,,,,,,WOS:000344638800004,0
J,"Hoffman, MD; Gelman, A",,,,"Hoffman, Matthew D.; Gelman, Andrew",,,The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size is an element of and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as efficiently as (and sometimes more efficiently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter is an element of on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require efficient turnkey samplers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1593,1623,,,,,,,,,,,,,,,,WOS:000338420000013,0
J,"Wierstra, D; Schaul, T; Glasmachers, T; Sun, Y; Peters, J; Schmidhuber, J",,,,"Wierstra, Daan; Schaul, Tom; Glasmachers, Tobias; Sun, Yi; Peters, Jan; Schmidhuber, Juergen",,,Natural Evolution Strategies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.",,,,,"Peters, Jan R/D-5068-2009; Schaul, Tom/C-4349-2011; Peters, Jan/P-6027-2019","Peters, Jan R/0000-0002-5266-8091; Schaul, Tom/0000-0002-2961-8782; Peters, Jan/0000-0002-5266-8091",,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,949,980,,,,,,,,,,,,,,,,WOS:000335458100004,0
J,"Bach, F",,,,"Bach, Francis",,,Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider supervised learning problems such as logistic regression and study the stochastic gradient method with averaging, in the usual stochastic approximation setting where observations are used only once. We show that after N iterations, with a constant step-size proportional to 1/R-2 root N where N is the number of observations and R is the maximum norm of the observations, the convergence rate is always of order O(1/root N), and improves to O (R-2/mu N) where mu is the lowest eigenvalue of the Hessian at the global optimum (when this eigenvalue is greater than R-2/root N). Since mu does not need to be known in advance, this shows that averaged stochastic gradient is adaptive to unknown local strong convexity of the objective function. Our proof relies on the generalized self-concordance properties of the logistic loss and thus extends to all generalized linear models with uniformly bounded features.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,595,627,,,,,,,,,,,,,,,,WOS:000335457700008,0
J,"Cuturi, M; Avis, D",,,,"Cuturi, Marco; Avis, Davis",,,Ground Metric Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimal transport distances have been used for more than a decade in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, optimal transport distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of optimal transport distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two convex polyhedral functions over a convex set of metric matrices. We follow the presentation of our algorithms with promising experimental results which show that this approach is useful both for retrieval and binary/multiclass classification tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,533,564,,,,,,,,,,,,,,,,WOS:000335457700006,0
J,"Jethava, V; Martinsson, A; Bhattacharyya, C; Dubhashi, D",,,,"Jethava, Vinay; Martinsson, Anders; Bhattacharyya, Chiranjib; Dubhashi, Devdatt",,,"Lovasz theta function, SVMs and Finding Dense Subgraphs",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we establish that the Lovasz theta function on a graph can be restated as a kernel learning problem. We introduce the notion of SVM-theta graphs, on which Lovasz theta function can be approximated well by a Support vector machine (SVM). We show that Erdos-Renyi random G(n, p) graphs are SVM-theta graphs for log(4)n/n <= p < 1. Even if we embed a large clique of size Theta(root np/1-p) in a G(n, p) graph the resultant graph still remains a SVM-theta graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the theta function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3495,3536,,,,,,,,,,,,,,,,WOS:000335457100001,0
J,"Talwalkar, A; Kumar, S; Mohri, M; Rowley, H",,,,"Talwalkar, Ameet; Kumar, Sanjiv; Mohri, Mehryar; Rowley, Henry",,,Large-scale SVD and Manifold Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper examines the efficacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystrom and Column sampling methods. We present a theoretical comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the large-scale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystrom approximation is superior to the Column sampling method for this task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classification with the labeled CMU-PIE data set.",,,,,"Rowley, Henry/R-8544-2019",,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3129,3152,,,,,,,,,,,,,,,,WOS:000328603600007,0
J,"Chaudhuri, K; Sarwate, AD; Sinha, K",,,,"Chaudhuri, Kamalika; Sarwate, Anand D.; Sinha, Kaushik",,,A Near-Optimal Algorithm for Differentially-Private Principal Components,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The principal components analysis (PCA) algorithm is a standard tool for identifying good low-dimensional approximations to high-dimensional data. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method.",,,,,,"Sarwate, Anand/0000-0001-6123-5282",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2905,2943,,,,,,,,,,,,,,,,WOS:000327007400014,0
J,"Dyer, EL; Sankaranarayanan, AC; Baraniuk, RG",,,,"Dyer, Eva L.; Sankaranarayanan, Aswin C.; Baraniuk, Richard G.",,,Greedy Feature Selection for Subspace Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation-the identification of points that live in the same subspace-and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)-recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with l(1)-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble.",,,,,"Dyer, Eva/L-4481-2017; Baraniuk, Richard/ABA-1743-2020","Dyer, Eva/0000-0002-6962-524X; Sankaranarayanan, Aswin/0000-0003-0906-4046",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2487,2517,,,,,,,,,,,,,,,,WOS:000327007400001,0
J,"Urry, MJ; Sollich, P",,,,"Urry, Matthew J.; Sollich, Peter",,,Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to significant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is significantly more accurate than previous approximations and should become exact in the limit of large random graphs.",,,,,"Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020","Sollich, Peter/0000-0003-0169-7893; ",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1801,1835,,,,,,,,,,,,,,,,WOS:000323367000005,0
J,"Zadeh, RB; Goel, A",,,,"Zadeh, Reza Bosagh; Goel, Ashish",,,Dimension Independent Similarity Computation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1605,1626,,,,,,,,,,,,,,,,WOS:000322506400006,0
J,"Cesa-Bianchi, N; Gentile, C; Vitale, F; Zappella, G",,,,"Cesa-Bianchi, Nicolo; Gentile, Claudio; Vitale, Fabio; Zappella, Giovanni",,,Random Spanning Trees and the Prediction of Weighted Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1251,1284,,,,,,,,,,,,,,,,WOS:000320709300002,0
J,"Scherrer, B",,,,"Scherrer, Bruno",,,Performance Bounds for lambda Policy Iteration and Application to the Game of Tetris,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the discrete-time infinite-horizon optimal control problem formalized by Markov decision processes (Puterman, 1994; Bertsekas and Tsitsiklis, 1996). We revisit the work of Bertsekas and Ioffe (1996), that introduced lambda policy iteration-a family of algorithms parametrized by a parameter lambda-that generalizes the standard algorithms value and policy iteration, and has some deep connections with the temporal-difference algorithms described by Sutton and Barto (1998). We deepen the original theory developed by the authors by providing convergence rate bounds which generalize standard bounds for value iteration described for instance by Puterman (1994). Then, the main contribution of this paper is to develop the theory of this algorithm when it is used in an approximate form. We extend and unify the separate analyzes developed by Munos for approximate value iteration (Munos, 2007) and approximate policy iteration (Munos, 2003), and provide performance bounds in the discounted and the undiscounted situations. Finally, we revisit the use of this algorithm in the training of a Tetris playing controller as originally done by Bertsekas and Ioffe (1996). Our empirical results are different from those of Bertsekas and Ioffe (which were originally qualified as paradoxical and intriguing). We track down the reason to be a minor implementation error of the algorithm, which suggests that, in practice, lambda policy iteration may be more stable than previously thought.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1181,1227,,,,,,,,,,,,,,,,WOS:000318590500014,0
J,"Hennig, P; Kiefel, M",,,,"Hennig, Philipp; Kiefel, Martin",,,Quasi-Newton Methods: A New Direction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,843,865,,,,,,,,,,,,,,,,WOS:000317461700005,0
J,"Valsalam, VK; Miikkulainen, R",,,,"Valsalam, Vinod K.; Miikkulainen, Risto",,,Using Symmetry and Evolutionary Search to Minimize Sorting Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparison-exchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The final solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difficult combinatorial problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,303,331,,,,,,,,,,,,,,,,WOS:000315981900001,0
J,"Kohlsdorf, DKH; Starner, TE",,,,"Kohlsdorf, Daniel Kyu Hwa; Starner, Thad E.",,,MAGIC Summoning: Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users' normal movements. Our tool MAGIC Summoning addresses this problem. Given a specific platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an Everyday Gesture Library or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classifier (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC's effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers.",,,,,"Kohlsdorf, Daniel/ABD-3339-2020",,,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,209,242,,,,,,,,,,,,,,,,WOS:000314530200007,0
J,"Gillis, N",,,,"Gillis, Nicolas",,,Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations finite. We illustrate the effectiveness of our technique on several image data sets.",,,,,,"Gillis, Nicolas/0000-0001-6423-6897",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3349,3386,,,,,,,,,,,,,,,,WOS:000313200200008,0
J,"Verstraeten, D; Schrauwen, B; Dieleman, S; Brakel, P; Buteneers, P; Pecevski, D",,,,"Verstraeten, David; Schrauwen, Benjamin; Dieleman, Sander; Brakel, Philemon; Buteneers, Pieter; Pecevski, Dejan",,,Oger: Modular Learning Architectures For Large-Scale Sequential Processing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Oger (OrGanic Environment for Reservoir computing) is a Python toolbox for building, training and evaluating modular learning architectures on large data sets. It builds on MDP for its modularity, and adds processing of sequential data sets, gradient descent training, several cross-validation schemes and parallel parameter optimization methods. Additionally, several learning algorithms are implemented, such as different reservoir implementations (both sigmoid and spiking), ridge regression, conditional restricted Boltzmann machine (CRBM) and others, including GPU accelerated versions. Oger is released under the GNU LGPL, and is available from http://organic.elis.ugent.be/oger.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2995,2998,,,,,,,,,,,,,,,,WOS:000313200000006,0
J,"Maillard, OA; Munos, R",,,,"Maillard, Odalric-Ambrym; Munos, Remi",,,Linear Regression With Random Projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate a method for regression that makes use of a randomly generated subspace G(P) subset of F (of finite dimension P) of a given large (possibly infinite) dimensional function space F, for example, L-2([0, 1](d);R). G(P) is defined as the span of P random features that are linear combinations of a basis functions of F weighted by random Gaussian i.i.d. coefficients. We show practical motivation for the use of this approach, detail the link that this random projections method share with RKHS and Gaussian objects theory and prove, both in deterministic and random design, approximation error bounds when searching for the best regression function in G(P) rather than in F, and derive excess risk bounds for a specific regression algorithm (least squares regression in G(P)). This paper stresses the motivation to study such methods, thus the analysis developed is kept simple for explanations purpose and leaves room for future developments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2735,2772,,,,,,,,,,,,,,,,WOS:000309580600009,0
J,"van Erven, T; Reid, MD; Williamson, RC",,,,"van Erven, Tim; Reid, Mark D.; Williamson, Robert C.",,,Mixability is Bayes Risk Curvature Relative to Log Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mixability of a loss characterizes fast rates in the online learning setting of prediction with expert advice. The determination of the mixability constant for binary losses is straightforward but opaque. In the binary case we make this transparent and simpler by characterising mixability in terms of the second derivative of the Bayes risk of proper losses. We then extend this result to multiclass proper losses where there are few existing results. We show that mixability is governed by the maximum eigenvalue of the Hessian of the Bayes risk, relative to the Hessian of the Bayes risk for log loss. We conclude by comparing our result to other work that bounds prediction performance in terms of the geometry of the Bayes risk. Although all calculations are for proper losses, we also show how to carry the results across to improper losses.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1639,1663,,,,,,,,,,,,,,,,WOS:000305456600010,0
J,"Shen, CH; Kim, JA; Wang, L; van den Hengel, A",,,,"Shen, Chunhua; Kim, Junae; Wang, Lei; van den Hengel, Anton",,,Positive Semidefinite Metric Learning Using Boosting-like Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The success of many machine learning and pattern recognition methods relies heavily upon the identification of an appropriate distance metric on the input data. It is often beneficial to learn such a metric from the input training data, instead of using a default one such as the Euclidean distance. In this work, we propose a boosting-based technique, termed BOOSTMETRIC, for learning a quadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance metric requires enforcing the constraint that the matrix parameter to the metric remains positive semidefinite. Semidefinite programming is often used to enforce this constraint, but does not scale well and is not easy to implement. BOOSTMETRIC is instead based on the observation that any positive semidefinite matrix can be decomposed into a linear combination of trace-one rank-one matrices. BOOSTMETRIC thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process. The resulting methods are easy to implement, efficient, and can accommodate various types of constraints. We extend traditional boosting algorithms in that its weak learner is a positive semidefinite matrix with trace and rank being one rather than a classifier or regressor. Experiments on various data sets demonstrate that the proposed algorithms compare favorably to those state-of-the-art methods in terms of classification accuracy and running time.",,,,,"Wang, Lei/D-9079-2013; Wang, Lei/AAL-9684-2020","Wang, Lei/0000-0002-0961-0441; van den Hengel, Anton/0000-0003-3027-8364",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1007,1036,,,,,,,,,,,,,,,,WOS:000303773100004,0
J,"Tahan, G; Rokach, L; Shahar, Y",,,,"Tahan, Gil; Rokach, Lior; Shahar, Yuval",,,Mal-ID: Automatic Malware Detection Using Common Segment Analysis and Meta-Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes several novel methods, based on machine learning, to detect malware in executable files without any need for preprocessing, such as unpacking or disassembling. The basic method (Mal-ID) is a new static (form-based) analysis methodology that uses common segment analysis in order to detect malware files. By using common segment analysis, Mal-ID is able to discard malware parts that originate from benign code. In addition, Mal-ID uses a new kind of feature, termed meta-feature, to better capture the properties of the analyzed segments. Rather than using the entire file, as is usually the case with machine learning based techniques, the new approach detects malware on the segment level. This study also introduces two Mal-ID extensions that improve the Mal-ID basic method in various aspects. We rigorously evaluated Mal-ID and its two extensions with more than ten performance measures, and compared them to the highly rated boosted decision tree method under identical settings. The evaluation demonstrated that Mal-ID and the two Mal-ID extensions outperformed the boosted decision tree method in almost all respects. In addition, the results indicated that by extracting meaningful features, it is sufficient to employ one simple detection rule for classifying executable files.",,,,,"Rokach, Lior/F-8247-2010","Shahar, Yuval/0000-0003-0328-2333",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,949,979,,,,,,,,,,,,,,,,WOS:000303773100002,0
J,"Raykar, VC; Yu, SP",,,,"Raykar, Vikas C.; Yu, Shipeng",,,Eliminating Spammers and Ranking Annotators for Crowdsourced Labeling Tasks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a data set labeled by multiple annotators in a short amount of time. Various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise. Since we do not have control over the quality of the annotators, very often the annotations can be dominated by spammers, defined as annotators who assign labels randomly without actually looking at the instance. Spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the final consensus labels. In this paper we propose an empirical Bayesian algorithm called SpEM that iteratively eliminates the spammers and estimates the consensus labels based only on the good annotators. The algorithm is motivated by defining a spammer score that can be used to rank the annotators. Experiments on simulated and real data show that the proposed approach is better than (or as good as) the earlier approaches in terms of the accuracy and uses a significantly smaller number of annotators.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,491,518,,,,,,,,,,,,,,,,WOS:000303046000009,0
J,"Tomioka, R; Suzuki, T; Sugiyama, M",,,,"Tomioka, Ryota; Suzuki, Taiji; Sugiyama, Masashi",,,Super-Linear Convergence of Dual Augmented Lagrangian Algorithm for Sparsity Regularized Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze the convergence behaviour of a recently proposed algorithm for regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is based on a new interpretation of DAL as a proximal minimization algorithm. We theoretically show under some conditions that DAL converges super-linearly in a non-asymptotic and global sense. Due to a special modelling of sparse estimation problems in the context of machine learning, the assumptions we make are milder and more natural than those made in conventional analysis of augmented Lagrangian algorithms. In addition, the new interpretation enables us to generalize DAL to wide varieties of sparse estimation problems. We experimentally confirm our analysis in a large scale l(1)-regularized logistic regression problem and extensively compare the efficiency of DAL algorithm to previously proposed algorithms on both synthetic and benchmark data sets.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1537,1586,,,,,,,,,,,,,,,,WOS:000292304000004,0
J,"Zhuang, JF; Tsang, IW; Hoi, SCH",,,,"Zhuang, Jinfeng; Tsang, Ivor W.; Hoi, Steven C. H.",,,A Family of Simple Non-Parametric Kernel Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Previous studies of Non-Parametric Kernel Learning (NPKL) usually formulate the learning task as a Semi-Definite Programming (SDP) problem that is often solved by some general purpose SDP solvers. However, for N data examples, the time complexity of NPKL using a standard interior-point SDP solver could be as high as O(N-6.5), which prohibits NPKL methods applicable to real applications, even for data sets of moderate size. In this paper, we present a family of efficient NPKL algorithms, termed SimpleNPKL, which can learn non-parametric kernels from a large set of pairwise constraints efficiently. In particular, we propose two efficient SimpleNPKL algorithms. One is SimpleNPKL algorithm with linear loss, which enjoys a closed-form solution that can be efficiently computed by the Lanczos sparse eigen decomposition technique. Another one is SimpleNPKL algorithm with other loss functions (including square hinge loss, hinge loss, square loss) that can be re-formulated as a saddle-point optimization problem, which can be further resolved by a fast iterative algorithm. In contrast to the previous NPKL approaches, our empirical results show that the proposed new technique, maintaining the same accuracy, is significantly more efficient and scalable. Finally, we also demonstrate that the proposed new technique is also applicable to speed up many kernel learning tasks, including colored maximum variance unfolding, minimum volume embedding, and structure preserving embedding.",,,,,"HOI, Steven C. H./A-3736-2011; Tsang, Ivor/E-8653-2011","Tsang, Ivor/0000-0003-2211-8176; Hoi, Steven/0000-0002-4584-3453; Tsang, Ivor/0000-0001-8095-4637",,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1313,1347,,,,,,,,,,,,,,,,WOS:000290096100005,0
J,"Melnykov, V; Maitra, R",,,,"Melnykov, Volodymyr; Maitra, Ranjan",,,CARP: Software for Fishing Out Good Clustering Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the CLUSTERING ALGORITHMS' REFEREE PACKAGE or CARP, an open source GNU GPL-licensed C package for evaluating clustering algorithms. Calibrating performance of such algorithms is important and CARP addresses this need by generating datasets of different clustering complexity and by assessing the performance of the concerned algorithm in terms of its ability to classify each dataset relative to the true grouping. This paper briefly describes the software and its capabilities.",,,,,,"Maitra, Ranjan/0000-0002-3515-8532",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,69,73,,,,,,,,,,,,,,,,WOS:000287938500003,0
J,"Ren, L; Du, L; Carin, L; Dunson, DB",,,,"Ren, Lu; Du, Lan; Carin, Lawrence; Dunson, David B.",,,Logistic Stick-Breaking Process,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A logistic stick-breaking process (LSBP) is proposed for non-parametric clustering of general spatially-or temporally-dependent data, imposing the belief that proximate data are more likely to be clustered together. The sticks in the LSBP are realized via multiple logistic regression functions, with shrinkage priors employed to favor contiguous and spatially localized segments. The LSBP is also extended for the simultaneous processing of multiple data sets, yielding a hierarchical logistic stick-breaking process (H-LSBP). The model parameters (atoms) within the H-LSBP are shared across the multiple learning tasks. Efficient variational Bayesian inference is derived, and comparisons are made to related techniques in the literature. Experimental analysis is performed for audio waveforms and images, and it is demonstrated that for segmentation applications the LSBP yields generally homogeneous segments with sharp boundaries.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,203,239,,,,,,,,,,,25258593,,,,,WOS:000287938500007,0
J,"Cawley, GC; Talbot, NLC",,,,"Cawley, Gavin C.; Talbot, Nicola L. C.",,,On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.",,,,,,"Cawley, Gavin/0000-0002-4118-9095",,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,2079,2107,,,,,,,,,,,,,,,,WOS:000282523000006,0
J,"Gorissen, D; Couckuyt, I; Demeester, P; Dhaene, T; Crombecq, K",,,,"Gorissen, Dirk; Couckuyt, Ivo; Demeester, Piet; Dhaene, Tom; Crombecq, Karel",,,A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An exceedingly large number of scientific and engineering fields are confronted with the need for computer simulations to study complex, real world phenomena or solve challenging design problems. However, due to the computational cost of these high fidelity simulations, the use of neural networks, kernel methods, and other surrogate modeling techniques have become indispensable. Surrogate models are compact and cheap to evaluate, and have proven very useful for tasks such as optimization, design space exploration, prototyping, and sensitivity analysis. Consequently, in many fields there is great interest in tools and techniques that facilitate the construction of such regression models, while minimizing the computational cost and maximizing model accuracy. This paper presents a mature, flexible, and adaptive machine learning toolkit for regression modeling and active learning to tackle these issues. The toolkit brings together algorithms for data fitting, model selection, sample selection (active learning), hyperparameter optimization, and distributed computing in order to empower a domain expert to efficiently generate an accurate model for the problem or data at hand.",,,,,"Demeester, Piet/N-6619-2013; Couckuyt, Ivo/V-6435-2019; Dhaene, Tom/A-4541-2009","Demeester, Piet/0000-0003-2810-3899; Couckuyt, Ivo/0000-0002-9524-4205; Dhaene, Tom/0000-0003-2899-4636",,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,2051,2055,,,,,,,,,,,,,,,,WOS:000282523000004,0
J,"Mohri, M; Rostamizadeh, A",,,,"Mohri, Mehryar; Rostamizadeh, Afshin",,,Stability Bounds for Stationary phi-mixing and beta-mixing Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence. This paper studies the scenario where the observations are drawn from a stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in the study of non-i.i.d. processes that implies a dependence between observations weakening over time. We prove novel and distinct stability-based generalization bounds for stationary phi-mixing and beta-mixing sequences. These bounds strictly generalize the bounds given in the i.i.d. case and apply to all stable learning algorithms, thereby extending the use of stability-bounds to non-i.i.d. scenarios. We also illustrate the application of our phi-mixing generalization bounds to general classes of learning algorithms, including Support Vector Regression, Kernel Ridge Regression, and Support Vector Machines, and many other kernel regularization-based and relative entropy-based regularization algorithms. These novel bounds can thus be viewed as the first theoretical basis for the use of these algorithms in non-i.i.d. scenarios.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,789,814,,,,,,,,,,,,,,,,WOS:000277186500014,0
J,"Aliferis, CF; Statnikov, A; Tsamardinos, I; Mani, S; Koutsoukos, XD",,,,"Aliferis, Constantin F.; Statnikov, Alexander; Tsamardinos, Ioannis; Mani, Subramani; Koutsoukos, Xenofon D.",,,Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance. Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distributions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,171,234,,,,,,,,,,,,,,,,WOS:000277186400007,0
J,"Rosset, S",,,,"Rosset, Saharon",,,Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show how to follow the path of cross validated solutions to families of regularized optimization problems, defined by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artificial data. This algorithm allows us to efficiently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2473,2505,,,,,,,,,,,,,,,,WOS:000272346600003,0
J,"Li, H; Liao, XJ; Carin, L",,,,"Li, Hui; Liao, Xuejun; Carin, Lawrence",,,Multi-task Reinforcement Learning in Partially Observable Stochastic Environments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of multi-task reinforcement learning (MTRL) in multiple partially observable stochastic environments. We introduce the regionalized policy representation (RPR) to characterize the agent's behavior in each environment. The RPR is a parametric model of the conditional distribution over current actions given the history of past actions and observations; the agent's choice of actions is directly based on this conditional distribution, without an intervening model to characterize the environment itself. We propose off-policy batch algorithms to learn the parameters of the RPRs, using episodic data collected when following a behavior policy, and show their linkage to policy iteration. We employ the Dirichlet process as a nonparametric prior over the RPRs across multiple environments. The intrinsic clustering property of the Dirichlet process imposes sharing of episodes among similar environments, which effectively reduces the number of episodes required for learning a good policy in each environment, when data sharing is appropriate. The number of distinct RPRs and the associated clusters (the sharing patterns) are automatically discovered by exploiting the episodic data as well as the nonparametric nature of the Dirichlet process. We demonstrate the effectiveness of the proposed RPR as well as the RPR-based MTRL framework on various problems, including grid-world navigation and multi-aspect target classification. The experimental results show that the RPR is a competitive reinforcement learning algorithm in partially observable domains, and the MTRL consistently achieves better performance than single task reinforcement learning.",,,,,,"Carin, Lawrence/0000-0001-6277-7948",,,,,,,,,,,,,1532-4435,,,,,MAY,2009,10,,,,,,1131,1186,,,,,,,,,,,,,,,,WOS:000270824800004,0
J,"Chen, YH; Garcia, EK; Gupta, MR; Rahimi, A; Cazzanti, L",,,,"Chen, Yihua; Garcia, Eric K.; Gupta, Maya R.; Rahimi, Ali; Cazzanti, Luca",,,Similarity-based Classification: Concepts and Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,747,776,,,,,,,,,,,,,,,,WOS:000270824500009,0
J,"Yoon, S; Fern, A; Givan, R",,,,"Yoon, Sungwook; Fern, Alan; Givan, Robert",,,Learning control knowledge for forward search planning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A number of today's state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to find domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-specific knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to define features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge-reactive policies (decision list rules and measures of progress) and linear heuristics-and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art non-learning planners across a wide range of planning competition domains.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,683,718,,,,,,,,,,,,,,,,WOS:000256642100006,0
J,"Lin, HT; Li, L",,,,"Lin, Hsuan-Tien; Li, Ling",,,Support vector machinery for infinite ensemble learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a finite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classifier with a larger or even an infinite number of hypotheses. In addition, constructing an infinite ensemble itself is a challenging task. In this paper, we formulate an infinite ensemble learning framework based on the support vector machine (SVM). The framework can output an infinite and nonsparse ensemble through embedding infinitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies infinitely many decision stumps, and the perceptron kernel embodies infinitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies infinitely many decision trees, and can thus be explained through infinite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the benefit of faster parameter selection. These properties make the novel kernels favorable choices in practice.",,,,,"Lin, Hsuan-Tien/AAE-4359-2020","Lin, Hsuan-Tien/0000-0003-2968-0671",,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,285,312,,,,,,,,,,,,,,,,WOS:000256641800011,0
J,"Camps-Valls, G; Gutierrez, J; Gomez-Perez, G; Malo, J",,,,"Camps-Valls, Gustavo; Gutierrez, Juan; Gomez-Perez, Gabriel; Malo, Jesus",,,On the suitable domain for SVM training in image coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conventional SVM-based image coding methods are founded on independently restricting the distortion in every image coefficient at some particular image representation. Geometrically, this implies allowing arbitrary signal distortions in an n-dimensional rectangle defined by the e-insensitivity zone in each dimension of the selected image representation domain. Unfortunately, not every image representation domain is well-suited for such a simple, scalar-wise, approach because statistical and/or perceptual interactions between the coefficients may exist. These interactions imply that scalar approaches may induce distortions that do not follow the image statistics and/or are perceptually annoying. Taking into account these relations would imply using non-rectangular epsilon-insensitivity regions (allowing coupled distortions in different coefficients), which is beyond the conventional SVM formulation. In this paper, we report a condition on the suitable domain for developing efficient SVM image coding schemes. We analytically demonstrate that no linear domain fulfills this condition because of the statistical and perceptual inter-coefficient relations that exist in these domains. This theoretical result is experimentally confirmed by comparing SVM learning in previously reported linear domains and in a recently proposed non-linear perceptual domain that simultaneously reduces the statistical and perceptual relations (so it is closer to fulfilling the proposed condition). These results highlight the relevance of an appropriate choice of the image representation before SVM learning.",,,,,", Gustavo/ABC-1706-2022; Camps-Valls, Gustavo/A-2532-2011; Malo, Jesus/K-9235-2017; Gutierr√©z-Aguado, Juan/L-8202-2014","Camps-Valls, Gustavo/0000-0003-1683-2138; Malo, Jesus/0000-0002-5684-8591; Gutierr√©z-Aguado, Juan/0000-0001-5527-8091",,,,,,,,,,,,,1532-4435,,,,,JAN,2008,9,,,,,,49,66,,,,,,,,,,,,,,,,WOS:000256641400003,0
J,"Bartlett, PL; Traskin, M",,,,"Bartlett, Peter L.; Traskin, Mikhail",,,AdaBoost is consistent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The risk, or probability of error, of the classifier produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n(1-epsilon) iterations-for sample size n and epsilon epsilon (0, 1) - the sequence of risks of the classifiers it produces approaches the Bayes risk.",,,,,,"Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2347,2368,,,,,,,,,,,,,,,,WOS:000252744800005,0
J,"Dyrholm, M; Christoforou, C; Parra, LC",,,,"Dyrholm, Mads; Christoforou, Christoforos; Parra, Lucas C.",,,Bilinear discriminant component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1097,1111,,,,,,,,,,,,,,,,WOS:000248351700007,0
J,"Chakrabartty, S; Cauwenberghs, G",,,,"Chakrabartty, Shantanu; Cauwenberghs, Gert",,,Gini support vector machine: Quadratic entropy based robust multi- class probability regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many classification tasks require estimation of output class probabilities for use as confidence scores or for inference integrated with other models. Probability estimates derived from large margin classifiers such as support vector machines ( SVMs) are often unreliable. We extend SVM large margin classification to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic ( Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise filtering property of the reverse water-filling procedure to arrive at normalized classification margins. The GiniSVM normalized classification margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression ( KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efficiently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker verification and face detection data, show improved classification performance and increased tolerance to imprecision over soft-margin SVM and KLR.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,813,839,,,,,,,,,,,,,,,,WOS:000247002800005,0
J,"Yanover, C; Meltzer, T; Weiss, Y",,,,"Yanover, Chen; Meltzer, Talya; Weiss, Yair",,,Linear programming relaxations and belief propagation - An empirical study,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of finding the most probable (MAP) configuration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for finding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful general-purpose LP solvers (CPLEX) on relaxations of real-world graphical models from the fields of computer vision and computational biology. We find that TRBP almost always finds the solution significantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can find the MAP configurations in a matter of minutes for a large range of real world problems.",,,,,"Yanover, Chen/A-3754-2012","Yanover, Chen/0000-0003-3663-4286",,,,,,,,,,,,,1532-4435,,,,,SEP,2006,7,,,,,,1887,1907,,,,,,,,,,,,,,,,WOS:000245389400004,0
J,"Begleiter, R; El-Yaniv, R",,,,"Begleiter, R; El-Yaniv, R",,,Superior guarantees for sequential prediction and lossless compression via alphabet decomposition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present worst case bounds for the learning rate of a known prediction method that is based on hierarchical applications of binary context tree weighting (CTW) predictors. A heuristic application of this approach that relies on Huffman's alphabet decomposition is known to achieve state-of-the-art performance in prediction and lossless compression benchmarks. We show that our new bound for this heuristic is tighter than the best known performance guarantees for prediction and lossless compression algorithms in various settings. This result substantiates the efficiency of this hierarchical method and provides a compelling explanation for its practical success. In addition, we present the results of a few experiments that examine other possibilities for improving the multi-alphabet prediction performance of CTW-based algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,379,411,,,,,,,,,,,,,,,,WOS:000236331700007,0
J,"Wingate, D; Seppi, KD",,,,"Wingate, D; Seppi, KD",,,Prioritization methods for accelerating MDP solvers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The performance of value and policy iteration can be dramatically improved by eliminating redundant or useless backups, and by backing up states in the right order. We study several methods designed to accelerate these iterative solvers, including prioritization, partitioning, and variable reordering. We generate a family of algorithms by combining several of the methods discussed, and present extensive empirical evidence demonstrating that performance can improve by several orders of magnitude for many problems, while preserving accuracy and convergence guarantees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,851,881,,,,,,,,,,,,,,,,WOS:000236329700005,0
J,"Valentini, G; Dietterich, TG",,,,"Valentini, G; Dietterich, TG",,,Bias-variance analysis of support vector machines for the development of SVM-based ensemble methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bias-variance analysis provides a tool to study learning algorithms and can be used to properly design ensemble methods well tuned to the properties of a specific base learner. Indeed the effectiveness of ensemble methods critically depends on accuracy, diversity and learning characteristics of base learners. We present an extended experimental analysis of bias-variance decomposition of the error in Support Vector Machines (SVMs), considering Gaussian, polynomial and dot product kernels. A characterization of the error decomposition is provided, by means of the analysis of the relationships between bias, variance, kernel type and its parameters, offering insights into the way SVMs learn. The results show that the expected trade-off between bias and variance is sometimes observed, but more complex relationships can be detected, especially in Gaussian and polynomial kernels. We show that the bias-variance decomposition offers a rationale to develop ensemble methods using SVMs as base learners, and we outline two directions for developing SVM ensembles, exploiting the SVM bias characteristics and the bias-variance dependence on the kernel parameters.",,,,,"Valentini, Giorgio/H-2134-2012","Valentini, Giorgio/0000-0002-5694-3919",,,,,,,,,,,,,1532-4435,,,,,JUL,2004,5,,,,,,725,775,,,,,,,,,,,,,,,,WOS:000236327800001,0
J,"Ginter, F; Boberg, J; Jarvinen, J; Salakoski, T",,,,"Ginter, F; Boberg, J; Jarvinen, J; Salakoski, T",,,New techniques for disambiguation in natural language and their application to biological text,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problems of disambiguation in natural language, focusing on the problem of gene vs. protein name disambiguation in biological text and also considering the problem of context-sensitive spelling error correction. We introduce a new family of classifiers based on ordering and weighting the feature vectors obtained from word counts and word co-occurrence in the text, and inspect several concrete classifiers from this family. We obtain the most accurate prediction when weighting by positions of the words in the context. On the gene/protein name disambiguation problem, this classifier outperforms both the Naive Bayes and SNoW baseline classifiers. We also study the effect of the smoothing techniques with the Naive Bayes classifier, the collocation features, and the context length on the classification accuracy and show that correct setting of the context length is important and also problem-dependent.",,,,,"J√§rvinen, Jouni/E-4885-2014","J√§rvinen, Jouni/0000-0002-8729-8633",,,,,,,,,,,,,1532-4435,,,,,JUN,2004,5,,,,,,605,621,,,,,,,,,,,,,,,,WOS:000236327700002,0
J,"Servedio, RA",,,,"Servedio, RA",,,Smooth boosting and learning with malicious noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a new boosting algorithm which generates only smooth distributions which do not assign too much weight to any single example. We show that this new boosting algorithm can be used to construct efficient PAC learning algorithms which tolerate relatively high rates of malicious noise. In particular, we use the new smooth boosting algorithm to construct malicious noise tolerant versions of the PAC-model p-norm linear threshold learning algorithms described by Servedio (2002). The bounds on sample complexity and malicious noise tolerance of these new PAC algorithms closely correspond to known bounds for the online p-norm algorithms of Grove, Littlestone and Schuurmans (1997) and Gentile and Littlestone (1999). As special cases of our new algorithms we obtain linear threshold learning algorithms which match the sample complexity and malicious noise tolerance of the online Perceptron and Winnow algorithms. Our analysis reveals an interesting connection between boosting and noise tolerance in the PAC setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,633,648,,10.1162/153244304773936072,0,,,,,,,,,,,,,WOS:000221345700009,0
J,"Mendelson, S; Philips, P",,,,"Mendelson, S; Philips, P",,,On the importance of small coordinate projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It has been recently shown that sharp generalization bounds can be obtained when the function class from which the algorithm chooses its hypotheses is small in the sense that the Rademacher averages of this function class are small. We show that a new more general principle guarantees good generalization bounds. The new principle requires that random coordinate projections of the function class evaluated on random samples are small with high probability and that the random class of functions allows symmetrization. As an example, we prove that this geometric property of the function class is exactly the reason why the two lately proposed frameworks, the luckiness ( Shawe- Taylor et al., 1998) and the algorithmic luckiness (Herbrich and Williamson, 2002), can be used to establish generalization bounds.",,,,,,"Mendelson, Shahar/0000-0002-5673-7576",,,,,,,,,,,,,1532-4435,,,,,MAR,2004,5,,,,,,219,238,,,,,,,,,,,,,,,,WOS:000236327200001,0
J,"Fung, GM; Mangasarian, OL; Smola, AJ",,,,"Fung, GM; Mangasarian, OL; Smola, AJ",,,Minimal kernel classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A finite concave minimization algorithm is proposed for constructing kernel classifiers that use a minimal number of data points both in generating and characterizing a classifier. The algorithm is theoretically justified on the basis of linear programming perturbation theory and a leave-one-out error bound as well as effective computational results on seven real world datasets. A nonlinear rectangular kernel is generated by systematically utilizing as few of the data as possible both in training and in characterizing a nonlinear separating surface. This can result in substantial reduction in kernel data-dependence (over 94% in six of the seven public datasets tested on) and with test set correctness equal to that obtained by using a conventional support vector machine classifier that depends on many more data points. This reduction in data dependence results in a much faster classifier that requires less storage. To eliminate data points, the proposed approach makes use of a novel loss function, the pound function ((.)-)(#), which is a linear combination of the 1-norm and the step function that measures both the magnitude and the presence of any error.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,303,321,,10.1162/153244303765208403,0,,,,,,,,,,,,,WOS:000182488500005,0
J,"Zhang, T",,,,"Zhang, T",,,Covering number bounds of certain regularized linear function classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In many of these theoretical studies, the concept of covering numbers played an important role. It is thus useful to study covering numbers for linear function classes. In this paper, we investigate two closely related methods to derive upper bounds on these covering numbers. The first method, already employed in some earlier studies, relies on the so-called Maurey's lemma; the second method uses techniques from the mistake bound framework in online learning. We compare results from these two methods, as well as their consequences in some learning formulations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,3,,,,,527,550,,10.1162/153244302760200713,0,,,,,,,,,,,,,WOS:000178101500008,0
J,"Genton, MG",,,,"Genton, MG",,,Classes of kernels for machine learning: A statistics perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, COLORADO",,,,,"In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,299,312,,10.1162/15324430260185646,0,,,,,,,,,,,,,WOS:000176055300011,0
J,"Heckerman, D; Chickering, DM; Meek, C; Rounthwaite, R; Kadie, C",,,,"Heckerman, D; Chickering, DM; Meek, C; Rounthwaite, R; Kadie, C",,,"Dependency networks for inference, collaborative filtering, and data visualization",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a graphical model for probabilistic relationships-an alternative to the Bayesian network-called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2000,1,1,,,,,49,75,,10.1162/153244301753344614,0,,,,,,,,,,,,,WOS:000173336700002,0
J,"Ding, ZY; Chen, S; Li, Q; Wright, SJ",,,,"Ding, Zhiyan; Chen, Shi; Li, Qin; Wright, Stephen J.",,,Overparameterization of Deep ResNet: Zero Loss and Mean-field Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finding parameters in a deep neural network (NN) that fit training data is a nonconvex optimization problem, but a basic first-order optimization method (gradient descent) finds a global optimizer with perfect fit (zero-loss) in many practical situations. We examine this phenomenon for the case of Residual Neural Networks (ResNet) with smooth activation functions in a limiting regime in which both the number of layers (depth) and the number of weights in each layer (width) go to infinity. First, we use a mean-field-limit argument to prove that the gradient descent for parameter training becomes a gradient flow for a probability distribution that is characterized by a partial differential equation (PDE) in the large-NN limit. Next, we show that under certain assumptions, the solution to the PDE converges in the training time to a zero-loss solution. Together, these results suggest that the training of the ResNet gives a near-zero loss if the ResNet is large enough. We give estimates of the depth and width needed to reduce the loss below a given threshold, with high probability.",,,,,,"Li, Qin/0000-0001-9210-8948; DING, ZHIYAN/0000-0001-8863-403X",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,65,,,,,,,,,,,,,,,,WOS:000752963000001,0
J,"Ahmed, I; Hu, XB; Acharya, MP; Ding, Y",,,,"Ahmed, Imtiaz; Hu, Xia Ben; Acharya, Mithun P.; Ding, Yu",,,Neighborhood Structure Assisted Non-negative Matrix Factorization and Its Application in Unsupervised Point-wise Anomaly Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dimensionality reduction is considered as an important step for ensuring competitive performance in unsupervised learning such as anomaly detection. Non-negative matrix factorization (NMF) is a widely used method to accomplish this goal. But NMF do not have the provision to include the neighborhood structure information and, as a result, may fail to provide satisfactory performance in presence of nonlinear manifold structure. To address this shortcoming, we propose to consider the neighborhood structural similarity information within the NMF framework and do so by modeling the data through a minimum spanning tree. We label the resulting method as the neighborhood structure-assisted NMF. We further develop both offline and online algorithms for implementing the proposed method. Empirical comparisons using twenty benchmark data sets as well as an industrial data set extracted from a hydropower plant demonstrate the superiority of the neighborhood structure-assisted NMF. Looking closer into the formulation and properties of the proposed NMF method and comparing it with several NMF variants reveal that inclusion of the MST-based neighborhood structure plays a key role in attaining the enhanced performance in anomaly detection.",,,,,"hu, xia hong/GQP-8544-2022","Ahmed, Imtiaz/0000-0003-1577-7384",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500034,0
J,"Axiotis, K; Sviridenko, M",,,,"Axiotis, Kyriakos; Sviridenko, Maxim",,,Sparse Convex Optimization via Adaptively Regularized Hard Thresholding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of Sparse Convex Optimization is to optimize a convex function f under a sparsity constraint s <= s* gamma, where s* is the target number of non-zero entries in a feasible solution (sparsity) and gamma >= 1 is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number kappa. The best known algorithms guarantee to find an approximate solution of value f(x*) + epsilon with the sparsity bound of gamma = O (kappa min {log f (x(0))- f (x*)/epsilon, kappa}), where x* is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to gamma = O(K), which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general f, under the condition s > s* kappa(2)/4, which yields compressed sensing bounds under the Restricted Isometry Property (RIP). When compared to other compressed sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function f that meets the RIP condition.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,47,122,,,,,,,,,,,,,,,WOS:000663172700001,0
J,"Noroozi, M; Pensky, M; Rimal, R",,,,"Noroozi, Majid; Pensky, Marianna; Rimal, Ramchandra",,,Sparse Popularity Adjusted Stochastic Block Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the present paper we study a sparse stochastic network enabled with a block structure. The popular Stochastic Block Model (SBM) and the Degree Corrected Block Model (DCBM) address sparsity by placing an upper bound on the maximum probability of connections between any pair of nodes. As a result, sparsity describes only the behavior of network as a whole, without distinguishing between the block-dependent sparsity patterns. To the best of our knowledge, the recently introduced Popularity Adjusted Block Model (PABM) is the only block model that allows to introduce a structural sparsity where some probabilities of connections are identically equal to zero while the rest of them remain above a certain threshold. The latter presents a more nuanced view of the network.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706887200001,0
J,"Perry, R; Mischler, G; Guo, R; Lee, T; Chang, A; Koul, A; Franz, C; Richard, H; Carmichael, I; Ablin, P; Gramfort, A; Vogelstein, JT",,,,"Perry, Ronan; Mischler, Gavin; Guo, Richard; Lee, Theodore; Chang, Alexander; Koul, Arman; Franz, Cameron; Richard, Hugo; Carmichael, Iain; Ablin, Pierre; Gramfort, Alexandre; Vogelstein, Joshua T.",,,mvlearn: Multiview Machine Learning in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have grown in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license. The documentation, detailed examples, and all releases are available at https://mvlearn.github.io/.",,,,,,"Mischler, Gavin/0000-0003-4776-3518",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,109,,,,,,,,,,,,,,,WOS:000663164600001,0
J,"Zhang, YK; Chen, YC",,,,"Zhang, Yikun; Chen, Yen-Chi",,,"Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Directional data consist of observations distributed on a (hyper)sphere, and appear in many applied fields, such as astronomy, ecology, and environmental science. This paper studies both statistical and computational problems of kernel smoothing for directional data. We generalize the classical mean shift algorithm to directional data, which allows us to identify local modes of the directional kernel density estimator (KDE). The statistical convergence rates of the directional KDE and its derivatives are derived, and the problem of mode estimation is examined. We also prove the ascending property of the directional mean shift algorithm and investigate a general problem of gradient ascent on the unit hypersphere. To demonstrate the applicability of the algorithm, we evaluate it as a mode clustering method on both simulated and real-world data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687172500001,0
J,"Zheng, LL; Raskutti, G; Willett, R; Mark, B",,,,"Zheng, Lili; Raskutti, Garvesh; Willett, Rebecca; Mark, Benjamin",,,"Context-dependent Networks in Multivariate Time Series: Models, Methods, and Risk Bounds in High Dimensions",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High-dimensional autoregressive generalized linear models arise naturally for capturing how current events trigger or inhibit future events, such as activity by one member of a social network can affect the future activities of his or her neighbors. While past work has focused on estimating the underlying network structure based solely on the times at which events occur on each node of the network, this paper examines the more nuanced problem of esti-mating context-dependent networks that reflect how features associated with an event (such as the content of a social media post) modulate the strength of influences among nodes. Specifically, we leverage ideas from compositional time series and regularization methods in machine learning to conduct context-dependent network estimation for high-dimensional autoregressive time series of annotated event data. Two models and corresponding esti-mators are considered in detail: an autoregressive multinomial model suited to categorical features and a logistic-normal model suited to features with mixed membership in different categories. Importantly, the logistic-normal model leads to a convex negative log-likelihood objective and captures dependence across categories. We provide theoretical guarantees for both estimators that are supported by simulations. We further validate our methods and demonstrate the advantages and disadvantages of both approaches through two real data examples and a synthetic data-generating model. Finally, a mixture approach enjoying both approaches' merits is proposed and illustrated on synthetic and real data examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706453100001,0
J,"Devijver, E; Perthame, E",,,,"Devijver, Emilie; Perthame, Emeline",,,Prediction regions through Inverse Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predicting a new response from a covariate is a challenging task in regression, which raises new question since the era of high-dimensional data. In this paper, we are interested in the inverse regression method from a theoretical viewpoint. Theoretical results for the well-known Gaussian linear model are well-known, but the curse of dimensionality has increased the interest of practitioners and theoreticians into generalization of those results for various estimators, calibrated for the high-dimension context. We propose to focus on inverse regression. It is known to be a reliable and efficient approach when the number of features exceeds the number of observations. Indeed, under some conditions, dealing with the inverse regression problem associated to a forward regression problem drastically reduces the number of parameters to estimate, makes the problem tractable and allows to consider more general distributions, as elliptical distributions. When both the responses and the covariates are multivariate, estimators constructed by the inverse regression are studied in this paper, the main result being explicit asymptotic prediction regions for the response. The performances of the proposed estimators and prediction regions are also analyzed through a simulation study and compared with usual estimators.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,113,,,,,,,,,,,,,,,WOS:000546630200001,0
J,"Feng, YL; Fan, J; Suykens, JAK",,,,"Feng, Yunlong; Fan, Jun; Suykens, Johan A. K.",,,A Statistical Learning Approach to Modal Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the nonparametric modal regression problem systematically from a statistical learning viewpoint. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that the nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its Bayes rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols such as mean regression, median regression, and quantile regression. On the practical side, the implementation issues of modal regression including the computational algorithm and the selection of the tuning parameters are discussed. Numerical validations on modal regression are also conducted to verify our findings.",,,,,"Fan, Jun/O-4742-2017; Suykens, Johan/C-9781-2014","Fan, Jun/0000-0001-8451-3484; Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300002,0
J,"Gorgen, C; Leonelli, M",,,,"Goergen, Christiane; Leonelli, Manuele",,,Model-Preserving Sensitivity Analysis for Families of Gaussian Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The accuracy of probability distributions inferred using machine-learning algorithms heavily depends on data availability and quality. In practical applications it is therefore fundamental to investigate the robustness of a statistical model to misspecification of some of its underlying probabilities. In the context of graphical models, investigations of robustness fall under the notion of sensitivity analyses. These analyses consist in varying some of the model's probabilities or parameters and then assessing how far apart the original and the varied distributions are. However, for Gaussian graphical models, such variations usually make the original graph an incoherent representation of the model's conditional independence structure. Here we develop an approach to sensitivity analysis which guarantees the original graph remains valid after any probability variation and we quantify the effect of such variations using different measures. To achieve this we take advantage of algebraic techniques to both concisely represent conditional independence and to provide a straightforward way of checking the validity of such relationships. Our methods are demonstrated to be robust and comparable to standard ones, which can break the conditional independence structure of the model, using an artificial example and a medical real-world application.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,84,,,,,,,,,,,,,,,WOS:000545026000001,0
J,"Guo, X; Hu, T; Wu, Q",,,,"Guo, Xin; Hu, Ting; Wu, Qiang",,,Distributed Minimum Error Entropy Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Minimum Error Entropy (MEE) principle is an important approach in Information Theoretical Learning (ITL). It is widely applied and studied in various fields for its robustness to noise. In this paper, we study a reproducing kernel-based distributed MEE algorithm, DMEE, which is designed to work with both fully supervised data and semi-supervised data. The divide-and-conquer approach is employed, so there is no inter-node communication overhead. Similar as other distributed algorithms, DMEE significantly reduces the computational complexity and memory requirement on single computing nodes. With fully supervised data, our proved learning rates equal the minimax optimal learning rates of the classical pointwise kernel-based regressions. Under the semi-supervised learning scenarios, we show that DMEE exploits unlabeled data effectively, in the sense that first, under the settings with weak regularity assumptions, additional unlabeled data significantly improves the learning rates of DMEE. Second, with sufficient unlabeled data, labeled data can be distributed to many more computing nodes, that each node takes only O(1) labels, without spoiling the learning rates in terms of the number of labels. This conclusion overcomes the saturation phenomenon in unlabeled data size. It parallels a recent results for regularized least squares (Lin and Zhou, 2018), and suggests that an inflation of unlabeled data is a solution to the MEE learning problems with decentralized data source for the concerns of privacy protection. Our work refers to pairwise learning and non-convex loss. The theoretical analysis is achieved by distributed U-statistics and error decomposition techniques in integral operators.",,,,,"Guo, Xin/AFU-9462-2022","Guo, Xin/0000-0002-7465-9356",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,126,,,,,,,,,,,,,,,WOS:000556294500001,0
J,"Li, XF; Whinston, AB",,,,"Li, Xiaofan; Whinston, Andrew B.",,,A model of fake data in data-driven analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Data-driven analysis has been increasingly used in various decision making processes. With more sources, including reviews, news, and pictures, can now be used for data analysis, the authenticity of data sources is in doubt. While previous literature attempted to detect fake data piece by piece, in the current work, we try to capture the fake data sender's strategic behavior to detect the fake data source. Specifically, we model the tension between a data receiver who makes data-driven decisions and a fake data sender who benefits from misleading the receiver. We propose a potentially infinite horizon continuous time game-theoretic model with asymmetric information to capture the fact that the receiver does not initially know the existence of fake data and learns about it during the course of the game. We use point processes to model the data traffic, where each piece of data can occur at any discrete moment in a continuous time flow. We fully solve the model and employ numerical examples to illustrate the players' strategies and payoffs for insights. Specifically, our results show that maintaining some suspicion about the data sources and understanding that the sender can be strategic are very helpful to the data receiver. In addition, based on our model, we propose a methodology of detecting fake data that is complementary to the previous studies on this topic, which suggested various approaches on analyzing the data piece by piece. We show that after analyzing each piece of data, understanding a source by looking at the its whole history of pushing data can be helpful.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300003,0
J,"Mentch, L; Zhou, SY",,,,"Mentch, Lucas; Zhou, Siyu",,,Randomization as Regularization: A Degrees of Freedom Explanation for Random Forest Success,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Random forests remain among the most popular off-the-shelf supervised machine learning tools with a well-established track record of predictive accuracy in both regression and classification settings. Despite their empirical success as well as a bevy of recent work investigating their statistical properties, a full and satisfying explanation for their success has yet to be put forth. Here we aim to take a step forward in this direction by demonstrating that the additional randomness injected into individual trees serves as a form of implicit regularization, making random forests an ideal model in low signal-to-noise ratio (SNR) settings. Specifically, from a model-complexity perspective, we show that the mtry parameter in random forests serves much the same purpose as the shrinkage penalty in explicitly regularized regression procedures like lasso and ridge regression. To highlight this point, we design a randomized linear-model-based forward selection procedure intended as an analogue to tree-based random forests and demonstrate its surprisingly strong empirical performance. Numerous demonstrations on both real and synthetic data are provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,171,,,,,,,,,,,,,,,WOS:000570157200001,0
J,"Molybog, I; Madani, R; Lavaei, J",,,,"Molybog, Igor; Madani, Ramtin; Lavaei, Javad",,,Conic Optimization for Quadratic Regression Under Sparse Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with the quadratic regression problem, where the goal is to find the unknown state (numerical parameters) of a system modeled by a set of equations that are quadratic in the state. We focus on the setting when a subset of equations of fixed cardinality is subject to errors of arbitrary magnitudes (potentially adversarial). We develop two methods to address this problem, which are both based on conic optimization and are able to accept any available prior knowledge on the solution as an input. We derive sufficient conditions for guaranteeing the correct recovery of the unknown state for each method and show that one method provides a better accuracy while the other one scales better to large-scale systems. The obtained conditions consist in bounds on the number of bad measurements each method can tolerate without producing a nonzero estimation error. In the case when no prior knowledge is available, we develop an iterative-based conic optimization technique. It is proved that the proposed methods allow up to half of the total number of measurements to be grossly erroneous.The efficacy of the developed methods is demonstrated in different case studies, including data analytics for a European power grid.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,195,,,,,,,,,,,,,,,WOS:000590002100001,0
J,"Rejchel, W; Bogdan, M",,,,"Rejchel, Wojciech; Bogdan, Malgorzata",,,Rank-based Lasso - efficient methods for high-dimensional robust model selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of identifying significant predictors in large data bases, where the response variable depends on the linear combination of explanatory variables through an unknown monotonic link function, corrupted with the noise from the unknown distribution. We utilize the natural, robust and efficient approach, which relies on replacing values of the response variables by their ranks and then identifying significant predictors by using well known Lasso. We provide new consistency results for the proposed procedure (called,,RankLasso) and extend the scope of its applications by proposing its thresholded and adaptive versions. Our theoretical results show that these modifications can identify the set of relevant predictors under a wide range of data generating scenarios. Theoretical results are supported by the simulation study and the real data analysis, which show that our methods can properly identify relevant predictors, even when the error terms come from the Cauchy distribution and the link function is nonlinear. They also demonstrate the superiority of the modified versions of RankLasso over its regular version in the case when predictors are substantially correlated. The numerical study shows also that RankLasso performs substantially better in model selection than LADLasso, which is a well established methodology for robust model selection.",,,,,,"Rejchel, Wojciech/0000-0003-1148-1439; Bogdan, Malgorzata/0000-0002-0657-4342",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,244,,,,,,,,,,,,,,,WOS:000608916100001,0
J,"Shilton, A; Rajasegarar, S; Palaniswami, M",,,,"Shilton, Alistair; Rajasegarar, Sutharshan; Palaniswami, Marimuthu",,,Multiclass Anomaly Detector: the CS plus plus Support Vector Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new support vector machine (SVM) variant, called CS++-SVM, is presented combining multiclass classification and anomaly detection in a single-step process to create a trained machine that can simultaneously classify test data belonging to classes represented in the training set and label as anomalous test data belonging to classes not represented in the training set. A theoretical analysis of the properties of the new method, showing how it combines properties inherited both from the conic-segmentation SVM (CS-SVM) and the 1-class SVM (to which the method described reduces to in the case of unlabelled training data), is given. Finally, experimental results are presented to demonstrate the effectiveness of the algorithm for both simulated and real-world data.",,,,,"Palaniswami, Marimuthu/AAE-2179-2022; Rajasegarar, Sutharshan/AAW-4727-2021","Rajasegarar, Sutharshan/0000-0002-6559-6736",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,213,,,,,,,,,,,,,,,WOS:000590024400001,0
J,"Tomita, TM; Browne, J; Shen, CC; Chung, J; Patsolic, JL; Falk, B; Priebe, CE; Yim, J; Burns, R; Maggioni, M; Vogelstein, JT",,,,"Tomita, Tyler M.; Browne, James; Shen, Cencheng; Chung, Jaewon; Patsolic, Jesse L.; Falk, Benjamin; Priebe, Carey E.; Yim, Jason; Burns, Randal; Maggioni, Mauro; Vogelstein, Joshua T.",,,Sparse Projection Oblique Randomer Forests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called Sparse Projection Oblique Randomer Forests (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with > 100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. Very sparse random projections can be incorporated into gradient boosted trees to obtain potentially similar gains.",,,,,,"Maggioni, Mauro/0000-0003-3258-9297; Burns, Randal/0000-0002-2924-1997",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,39,104,,,,,,,,,,,,,,,WOS:000546625600001,0
J,"Younes, L",,,,"Younes, Laurent",,,Diffeomorphic Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce in this paper a learning paradigm in which training data is transformed by a diffeomorphic transformation before prediction. The learning algorithm minimizes a cost function evaluating the prediction error on the training set penalized by the distance between the diffeomorphism and the identity. The approach borrows ideas from shape analysis where diffeomorphisms are estimated for shape and image alignment, and brings them in a previously unexplored setting, estimating, in particular diffeomorphisms in much larger dimensions. After introducing the concept and describing a learning algorithm, we present diverse applications, mostly with synthetic examples, demonstrating the potential of the approach, as well as some insight on how it can be improved.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,220,,,,,,,,,,,,,,,WOS:000605740100001,0
J,"Zhang, YK; Lian, H; Yu, Y",,,,"Zhang, Yuankun; Lian, Heng; Yu, Yan",,,Ultra-High Dimensional Single-Index Quantile Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a flexible semiparametric single-index quantile regression model where the number of covariates may be ultra-high dimensional, and the number of the relevant covariates is potentially diverging. The approach is particularly appealing to uncover the complex heterogeneity in high-dimensional data, incorporate nonlinearity and potential interaction, avoid the curse of dimensionality, and allow different variables to be included at different quantile levels. We estimate the unknown function via polynomial splines nonparametrically and adopt a nonconvex penalty function to identify the sparse variable set. We further extend it to partially linear single-index quantile model where both the single-index components in the nonparametric term and the partially linear components can be in ultra-high dimension. However, a number of major challenges arise in developing both theory and computation: (a) The model is highly nonlinear in single-index coefficients because the high-dimensional single-index covariates are embedded inside the unknown flexible function. (b) The data are ultra-high dimensional where the dimension of the single-index covariates (p(n)) is diverging or even in the exponential order of sample size n. (c) The objective function is non-smooth for quantile regression. (d) Nonconvex variable selection such as SCAD is adopted for regularization. (e) The extended partially linear single-index quantile models may include both ultra-high dimensional (p(n)) single-index covariates and ultra-high dimensional (q(n)) partially linear covariates. We develop a novel approach using empirical process techniques in establishing the theoretical properties of the nonconvex penalized estimators for partially linear single-index quantile models and show those estimators indeed possess the oracle property in ultra-high dimensional setting. We propose an efficient algorithm to circumvent the computational challenges. The results of Monte Carlo simulations and an application to gene expression data demonstrate the effectiveness of the proposed models and estimation method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,224,,,,,,,,,,,,,,,WOS:000605744600001,0
J,"Alber, M; Lapuschkin, S; Seegerer, P; Hagele, M; Schutt, KT; Montavon, G; Samek, W; Muller, KR; Dahne, S; Kindermans, PJ",,,,"Alber, Maximilian; Lapuschkin, Sebastian; Seegerer, Philipp; Haegele, Miriam; Schuett, Kristof T.; Montavon, Gregoire; Samek, Wojciech; Mueller, Klaus-Robert; Daehne, Sven; Kindermans, Pieter-Jan",,,iNNvestigate Neural Networks!,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short-coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-thebox implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.",,,,,"Samek, Wojciech/AAZ-2165-2021; Sch√ºtt, Kristof T/Q-2604-2017; Samek, Wojciech/AAZ-2156-2021; Seegerer, Philipp/AAE-3706-2020; Mueller, Klaus-Robert/Y-3547-2019","Samek, Wojciech/0000-0002-6283-3265; Sch√ºtt, Kristof T/0000-0001-8342-0964; Samek, Wojciech/0000-0002-6283-3265; Seegerer, Philipp/0000-0002-4707-7991; Mueller, Klaus-Robert/0000-0002-3861-7685; Lapuschkin, Sebastian/0000-0002-0762-7258",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,93,,,,,,,,,,,,,,,WOS:000470908100001,0
J,"Altschuler, J; Brunel, VE; Malek, A",,,,"Altschuler, Jason; Brunel, Victor-Emmanuel; Malek, Alan",,,Best Arm Identification for Contaminated Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies active learning in the context of robust statistics. Specifically, we propose a variant of the Best Arm Identification problem for contaminated bandits, where each arm pull has probability epsilon of generating a sample from an arbitrary contamination distribution instead of the true underlying distribution. The goal is to identify the best (or approximately best) true distribution with high probability, with a secondary goal of providing guarantees on the quality of this distribution. The primary challenge of the contaminated bandit setting is that the true distributions are only partially identifiable, even with infinite samples. To address this, we develop tight, non-asymptotic sample complexity bounds for high-probability estimation of the first two robust moments (median and median absolute deviation) from contaminated samples. These concentration inequalities are the main technical contributions of the paper and may be of independent interest. Using these results, we adapt several classical Best Arm Identification algorithms to the contaminated bandit setting and derive sample complexity upper bounds for our problem. Finally, we provide matching information-theoretic lower bounds on the sample complexity (up to a small logarithmic factor).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,91,,,,,,,,,,,,,,,WOS:000470907900001,0
J,"Lin, SB; Lei, YW; Zhou, DX",,,,"Lin, Shao-Bo; Lei, Yunwen; Zhou, Ding-Xuan",,,Boosted Kernel Ridge Regression: Optimal Learning Rates and Early Stopping,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce a learning algorithm, boosted kernel ridge regression (BKRR), that combines L-2-Boosting with the kernel ridge regression (KRR). We analyze the learning performance of this algorithm in the framework of learning theory. We show that BKRR provides a new bias-variance trade-off via tuning the number of boosting iterations, which is different from KRR via adjusting the regularization parameter. A (semi-)exponential bias-variance trade-off is derived for BKRR, exhibiting a stable relationship between the generalization error and the number of iterations. Furthermore, an adaptive stopping rule is proposed, with which BKRR achieves the optimal learning rate without saturation.",,,,,"Lei, Yunwen/V-2782-2018; Zhou, Ding-Xuan/B-3160-2013","Zhou, Ding-Xuan/0000-0003-0224-9216; Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,46,,,,,,,,,,,,,,,WOS:000463320900001,0
J,"Nguyen, TV; Wong, RKW; Hegde, C",,,,"Nguyen, Thanh V.; Wong, Raymond K. W.; Hegde, Chinmay",,,Provably Accurate Double-Sparse Coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse coding is a crucial subroutine in algorithms for various signal processing, deep learning, and other machine learning applications. The central goal is to learn an overcomplete dictionary that can sparsely represent a given input dataset. However, a key challenge is that storage, transmission, and processing of the learned dictionary can be untenably high if the data dimension is high. In this paper, we consider the double-sparsity model introduced by Rubinstein et al. (2010b) where the dictionary itself is the product of a fixed, known basis and a data-adaptive sparse component. First, we introduce a simple algorithm for double-sparse coding that can be amenable to efficient implementation via neural architectures. Second, we theoretically analyze its performance and demonstrate asymptotic sample complexity and running time benefits over existing (provable) approaches for sparse coding. To our knowledge, our work introduces the first computationally efficient algorithm for double-sparse coding that enjoys rigorous statistical guarantees. Finally, we corroborate our theory with several numerical experiments on simulated data, suggesting that our method may be useful for problem sizes encountered in practice.",,,,,,"Wong, Raymond K. W./0000-0001-9342-3755",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,141,,,,,,,,,,,,,,,WOS:000491132200005,0
J,"Rai, A; Antonova, R; Meier, F; Atkeson, CG",,,,"Rai, Akshara; Antonova, Rika; Meier, Franziska; Atkeson, Christopher G.",,,Using Simulation to Improve Sample-Efficiency of Bayesian Optimization for Bipedal Robots,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning for control can acquire controllers for novel robotic tasks, paving the path for autonomous agents. Such controllers can be expert-designed policies, which typically require tuning of parameters for each task scenario. In this context, Bayesian optimization (BO) has emerged as a promising approach for automatically tuning controllers. However, sample-efficiency can still be an issue for high-dimensional policies on hardware. Here, we develop an approach that utilizes simulation to learn structured feature transforms that map the original parameter space into a domain-informed space. During BO, similarity between controllers is now calculated in this transformed space. Experiments on the ATRIAS robot hardware and simulation show that our approach succeeds at sample-efficiently learning controllers for multiple robots. Another question arises: What if the simulation significantly differs from hardware? To answer this, we create increasingly approximate simulators and study the effect of increasing simulation-hardware mismatch on the performance of Bayesian optimization. We also compare our approach to other approaches from literature, and find it to be more reliable, especially in cases of high mismatch. Our experiments show that our approach succeeds across different controller types, bipedal robot models and simulator fidelity levels, making it applicable to a wide range of bipedal locomotion problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,49,,,,,,,,,,,,,,,WOS:000463322000001,0
J,"Shi, L; Huang, XL; Feng, YL; Suykens, JAK",,,,"Shi, Lei; Huang, Xiaolin; Feng, Yunlong; Suykens, Johan A. K.",,,Kernel Regression with Coefficient-based l(q)-regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the l(q)-regularized kernel regression with 0 < q <= 1. In form, the algorithm minimizes a least-square loss functional adding a coefficient-based l(q) penalty term over a linear span of features generated by a kernel function. We study the asymptotic behavior of the algorithm under the framework of learning theory. The contribution of this paper is two-fold. First, we derive a tight bound on the l(2) - empirical covering numbers of the related function space involved in the error analysis. Based on this result, we obtain the convergence rates for the f regularized kernel regression which is the best so far. Second, for the case 0 < q < 1, we show that the regularization parameter plays a role as a trade-off between sparsity and convergence rates. Under some mild conditions, the fraction of non-zero coefficients in a local minimizer of the algorithm will tend to 0 at a polynomial decay rate when the sample size m becomes large. As the concerned algorithm is non-convex, we also discuss how to generate a minimizing sequence iteratively, which can help us to search a local minimizer around any initial point.",,,,,"Suykens, Johan/C-9781-2014","Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,161,,,,,,,,,,,,,,,WOS:000506403100001,0
J,"Srinivasan, A; Vig, L; Bain, M",,,,"Srinivasan, Ashwin; Vig, Lovekesh; Bain, Michael",,,Logical Explanations for Deep Relational Machines Using Relevance Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Our interest in this paper is in the construction of symbolic explanations for predictions made by a deep neural network. We will focus attention on deep relational machines (DRMs: a term introduced in Lodhi (2013)). A DRM is a deep network in which the input layer consists of Boolean-valued functions (features) that are defined in terms of relations provided as domain, or background, knowledge. Our DRMs differ from those in Lodhi (2013), which uses an Inductive Logic Programming (ILP) engine to first select features (we use random selections from a space of features that satisfies some approximate constraints on logical relevance and non-redundancy). But why do the DRMs predict what they do? One way of answering this was provided in recent work Ribeiro et al. (2016), by constructing readable proxies for a black-box predictor. The proxies are intended only to model the predictions of the black-box in local regions of the instance-space. But readability alone may not be enough: to be understandable, the local models must use relevant concepts in an meaningful manner. We investigate the use of a Bayes-like approach to identify logical proxies for local predictions of a DRM. As a preliminary step, we show that DRM's with our randomised propositionalization method achieve predictive performance that is comparable to the best reports in the ILP literature. Our principal results on logical explanations show: (a) Models in first-order logic can approximate the DRM's prediction closely in a small local region; and (b) Expert-provided relevance information can play the role of a prior to distinguish between logical explanations that perform equivalently on prediction alone.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,130,,,,,,,,,,,,,,,WOS:000487068900014,0
J,"Wang, WW; Yu, P; Lin, L; Tong, TJ",,,,"Wang, WenWu; Yu, Ping; Lin, Lu; Tong, Tiejun",,,Robust Estimation of Derivatives Using Locally Weighted Least Absolute Deviation Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In nonparametric regression, the derivative estimation has attracted much attention in recent years due to its wide applications. In this paper, we propose a new method for the derivative estimation using the locally weighted least absolute deviation regression. Different from the local polynomial regression, the proposed method does not require a finite variance for the error term and so is robust to the presence of heavy-tailed errors. Meanwhile, it does not require a zero median or a positive density at zero for the error term in comparison with the local median regression. We further show that the proposed estimator with random difference is asymptotically equivalent to the (infinitely) composite quantile regression estimator. In other words, running one regression is equivalent to combining infinitely many quantile regressions. In addition, the proposed method is also extended to estimate the derivatives at the boundaries and to estimate higher-order derivatives. For the equidistant design, we derive theoretical results for the proposed estimators, including the asymptotic bias and variance, consistency, and asymptotic normality. Finally, we conduct simulation studies to demonstrate that the proposed method has better performance than the existing methods in the presence of outliers and heavy-tailed errors, and analyze the Chinese house price data for the past ten years to illustrate the usefulness of the proposed method.",,,,,"Tong, Tiejun/F-3880-2010","Tong, Tiejun/0000-0003-0947-3990",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,60,,,,,,,,,,,,,,,WOS:000467878700001,0
J,"Weissbrod, O; Kaufman, S; Golan, D; Rosset, S",,,,"Weissbrod, Omer; Kaufman, Shachar; Golan, David; Rosset, Saharon",,,Maximum Likelihood for Gaussian Process Classification and Generalized Linear Mixed Models under Case-Control Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modern data sets in various domains often include units that were sampled non-randomly from the population and have a latent correlation structure. Here we investigate a common form of this setting, where every unit is associated with a latent variable, all latent variables are correlated, and the probability of sampling a unit depends on its response. Such settings often arise in case-control studies, where the sampled units are correlated due to spatial proximity, family relations, or other sources of relatedness. Maximum likelihood estimation in such settings is challenging from both a computational and statistical perspective, necessitating approximations that take the sampling scheme into account. We propose a family of approximate likelihood approaches which combine composite likelihood and expectation propagation. We demonstrate the efficacy of our solutions via extensive simulations. We utilize them to investigate the genetic architecture of several complex disorders collected in case-control genetic association studies, where hundreds of thousands of genetic variants are measured for every individual, and the underlying disease liabilities of individuals are correlated due to genetic similarity. Our work is the first to provide a tractable likelihood-based solution for case-control data with complex dependency structures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,108,,,,,,,,,,,,,,,WOS:000476623600001,0
J,"Wu, AQ; Koyejo, O; Pillow, J",,,,"Wu, Anqi; Koyejo, Oluwasanmi; Pillow, Jonathan",,,Dependent relevance determination for smooth and structured sparse regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many problem settings, parameter vectors are not merely sparse but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as region sparsity. Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging.",,,,,,"wu, anqi/0000-0002-7866-9455",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,89,,,,,,,,,,,,,,,WOS:000470907500001,0
J,"Donner, C; Opper, M",,,,"Donner, Christian; Opper, Manfred",,,Efficient Bayesian Inference of Sigmoidal Gaussian Cox Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present an approximate Bayesian inference approach for estimating the intensity of a inhomogeneous Poisson process, where the intensity function is modelled using a Gaussian process (GP) prior via a sigmoid link function. Augmenting the model using a latent marked Poisson process and Polya-Gamma random variables we obtain a representation of the likelihood which is conjugate to the GP prior. We estimate the posterior using a variational free-form mean field optimisation together with the framework of sparse GPs. Furthermore, as alternative approximation we suggest a sparse Laplace's method for the posterior, for which an efficient expectation-maximisation algorithm is derived to find the posterior's mode. Both algorithms compare well against exact inference obtained by a Markov Chain Monte Carlo sampler and standard variational Gauss approach solving the same model, while being one order of magnitude faster. Furthermore, the performance and speed of our method is competitive with that of another recently proposed Poisson process model based on a quadratic link function, while not being limited to GPs with squared exponential kernels and rectangular domains.",,,,,,"Donner, Christian/0000-0002-4499-2895",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,67,,,,,,,,,,,,,,,WOS:000452054500001,0
J,"Liang, SH; Lu, WB; Song, R; Wang, L",,,,"Liang, Shuhan; Lu, Wenbin; Song, Rui; Wang, Lan",,,Sparse Concordance-assisted Learning for Optimal Treatment Decision,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance -assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the L-2 error bound of the estimated coefficients under ultra -high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,30416396,,,,,WOS:000435627300001,0
J,"Smith, V; Forte, S; Ma, CX; Takac, M; Jordan, MI; Jaggi, M",,,,"Smith, Virginia; Forte, Simone; Ma, Chenxin; Takac, Martin; Jordan, Michael I.; Jaggi, Martin",,,CoCoA: A General Framework for Communication-Efficient Distributed Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.",,,,,"Takac, Martin/AAA-8564-2022; Jordan, Michael I/C-5253-2013","Takac, Martin/0000-0001-7455-2025; Jordan, Michael/0000-0001-8935-817X; Jaggi, Martin/0000-0003-1579-5558",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000440886000001,0
J,"Tiomoko Ali, H; Couillet, R",,,,"Tiomoko Ali, Hafiz; Couillet, Romain",,,Improved spectral community detection in large heterogeneous networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we propose and study the performance of spectral community detection for a family of alpha-normalized adjacency matrices A, of the type D(-alpha)AD(-alpha) with D the degree matrix, in heterogeneous dense graph models. We show that the previously used normalization methods based on A or D(-1)AD(-1) are in general suboptimal in terms of correct recovery rates and, relying on advanced random matrix methods, we prove instead the existence of an optimal value alpha(opt) of the parameter alpha in our generic model; we further provide an online estimation of alpha(opt) only based on the node degrees in the graph. Numerical simulations show that the proposed method outperforms state-of-the-art spectral approaches on moderately dense to dense heterogeneous graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,225,,,,,,,,,,,,,,,WOS:000438191100001,0
J,"Bigot, J; Deledalle, C; Feral, D",,,,"Bigot, Jeremie; Deledalle, Charles; Feral, Delphine",,,Generalized SURE for optimal shrinkage of singular values in low-rank matrix denoising,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating a low-rank signal matrix from noisy measurements under the assumption that the distribution of the data matrix belongs to an exponential family. In this setting, we derive generalized Stein's unbiased risk estimation (SURE) formulas that hold for any spectral estimators which shrink or threshold the singular values of the data matrix. This leads to new data-driven spectral estimators, whose optimality is discussed using tools from random matrix theory and through numerical experiments. Under the spiked population model and in the asymptotic setting where the dimensions of the data matrix are let going to infinity, some theoretical properties of our approach are compared to recent results on asymptotically optimal shrinking rules for Gaussian noise. It also leads to new procedures for singular values shrinkage infinite-dimensional matrix denoising for Gamma-distributed and Poisson-distributed measurements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000424547500001,0
J,"Dupuy, C; Bach, F",,,,"Dupuy, Christophe; Bach, Francis",,,Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study parameter inference in large-scale latent variable models. We first propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirichlet allocation, we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,126,,,,,,,,,,,,,,,WOS:000424542400001,0
J,"Huang, RT; Lattimore, T; Gyorgy, A; Szepesvari, C",,,,"Huang, Ruitong; Lattimore, Tor; Gyorgy, Andras; Szepesvari, Csaba",,,Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Follow the leader (FTL) is a simple online learning algorithm that is known to perform well when the loss functions are convex and positively curved. In this paper we ask whether there are other settings when FTL achieves low regret. In particular, we study the fundamental problem of linear prediction over a convex, compact domain with non-empty interior. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys logarithmic regret, while for polytope domains and stochastic data it enjoys finite expected regret. The former result is also extended to strongly convex domains by establishing an equivalence between the strong convexity of sets and the minimum curvature of their boundary, which may be of independent interest. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the smaller regret of FTL when the data is 'easy'. Finally, we show that such guarantees are achievable directly (e.g., by the follow the regularized leader algorithm or by a shrinkage-based variant of FTL) when the constraint set is an ellipsoid.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,145,,,,,,,,,,,,,,,WOS:000424549700001,0
J,"Lin, SB; Guo, X; Zhou, DX",,,,"Lin, Shao-Bo; Guo, Xin; Zhou, Ding-Xuan",,,Distributed Learning with Regularized Least Squares,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the L-2-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature.",,,,,"Guo, Xin/M-6860-2017","Guo, Xin/0000-0002-7465-9356",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,92,,,,,,,,,,,,,,,WOS:000412483700001,0
J,"Nevo, D; Ritov, Y",,,,"Nevo, Daniel; Ritov, Ya'acov",,,Identifying a Minimal Class of Models for High-dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Model selection consistency in the high-dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000399841900001,0
J,"Popovici, E",,,,"Popovici, Elena",,,Bridging Supervised Learning and Test-Based Co-optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of cooptimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,38,,,,,,,,,,,,,,,WOS:000400520600001,0
J,"Sun, WW; Li, LX",,,,"Sun, Will Wei; Li, Lexin",,,STORE: Sparse Tensor Response Regression and Neuroimaging Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by applications in neuroimaging analysis, we propose a new regression model, Sparse TensOr REsponse regression (STORE), with a tensor response and a vector predictor. STORE embeds two key sparse structures: element-wise sparsity and low-rankness. It can handle both a non-symmetric and a symmetric tensor response, and thus is applicable to both structural and functional neuroimaging data. We formulate the parameter estimation as a non-convex optimization problem, and develop an efficient alternating updating algorithm. We establish a non-asymptotic estimation error bound for the actual estimator obtained from the proposed algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. When the distribution of the error tensor is Gaussian, we further obtain a fast estimation error rate which allows the tensor dimension to grow exponentially with the sample size. We illustrate the efficacy of our model through intensive simulations and an analysis of the Autism spectrum disorder neuroimaging data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,135,,,,,,,,,,,,,,,WOS:000424546900001,0
J,"Le, T; Nguyen, TD; Nguyen, V; Phung, D",,,,Trung Le; Tu Dinh Nguyen; Vu Nguyen; Dinh Phung,,,Approximation Vector Machines for Large-scale Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the most challenging problems in kernel online learning is to bound the model size and to promote model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity - a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage sparsity and safeguard its risk in compromising the performance. In an online setting context, when an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis for the common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classification task) and l(1), l(2), and epsilon-insensitive (i.e., for the regression task) to characterize the gap between the approximation and optimal solutions. This gap crucially depends on two key factors including the frequency of approximation (i.e., how frequent the approximation operation takes place) and the predefined threshold. We conducted extensive experiments for classification and regression tasks in batch and online modes using several benchmark datasets. The quantitative results show that our proposed AVM obtained comparable predictive performances with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size.",,,,,"Nguyen, Vu/HGV-1806-2022; Nguyen, Vu/AAQ-5062-2020","Nguyen, Vu/0000-0002-0294-4561",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,111,,,,,,,,,,,,,,,WOS:000424539100001,0
J,"Wang, T; Rudin, C; Doshi-Velez, F; Liu, YM; Klampfl, E; MacNeille, P",,,,"Wang, Tong; Rudin, Cynthia; Doshi-Velez, Finale; Liu, Yimin; Klampfl, Erica; MacNeille, Perry",,,A Bayesian Framework for Learning Rule Sets for Interpretable Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If X satisfies (condition A AND condition B) OR (condition C) OR . . . , then Y = 1. Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets - BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,37,,,,,,,,,,,,,,,,WOS:000412058800001,0
J,"Wilson, JD; Palowitch, J; Bhamidi, S; Nobel, AB",,,,"Wilson, James D.; Palowitch, John; Bhamidi, Shankar; Nobel, Andrew B.",,,Community Extraction in Multilayer Networks with Heterogeneous Community Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multilayer networks are a useful way to capture and model multiple, binary or weighted relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set through comparison with a fixed degree random graph model. Multilayer Extraction directly handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure can capture overlapping communities, as well as background vertex-layer pairs that do not belong to any community. We establish consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction on three applications and a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,149,,,,,,,,,,31871433,,,,,WOS:000424550300001,0
J,"Wirth, C; Akrour, R; Neumann, G; Furnkranz, J",,,,"Wirth, Christian; Akrour, Riad; Neumann, Gerhard; Fuernkranz, Johannes",,,A Survey of Preference-Based Reinforcement Learning Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.",,,,,"F√ºrnkranz, Johannes/AAH-2585-2019",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,136,,,,,,,,,,,,,,,WOS:000424547200001,0
J,"Adamczak, R",,,,"Adamczak, Radoslaw",,,"A Note on the Sample Complexity of the Er-SpUD Algorithm by Spielman, Wang and Wright for Ex act Recovery of Sparsely Used Dictionaries",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of recovering an invertible n x n matrix A and a sparse n x p random matrix X based on the observation of Y = AX (up to a scaling and permutation of columns of A and rows of X). Using only elementary tools from the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers A and X exactly, provided that p >= Cn log n, which is optimal up to the constant C.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,18,177,,,,,,,,,,,,,,,WOS:000391681000001,0
J,"De Castro, Y; Gassiat, E; Lacour, C",,,,"De Castro, Yohann; Gassiat, Elisabeth; Lacour, Claire",,,Minimax Adaptive Estimation of Nonparametric Hidden Markov Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider stationary hidden Markov models with finite state space and nonparametric modeling of the emission distributions. It has remained unknown until very recently that such models are identifiable. In this paper, we propose a new penalized least-squares estimator for the emission distributions which is statistically optimal and practically tractable. We prove a non asymptotic oracle inequality for our nonparametric estimator of the emission distributions. A consequence is that this new estimator is rate minimax adaptive up to a logarithmic term. Our methodology is based on projections of the emission distributions onto nested subspaces of increasing complexity. The popular spectral estimators are unable to achieve the optimal rate but may be used as initial points in our procedure. Simulations are given that show the improvement obtained when applying the least-squares minimization consecutively to the spectral estimation.",,,,,,"De Castro, Yohann/0000-0002-9008-7474; Lacour, Claire/0000-0002-1321-648X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,111,,,,,,,,,,,,,,,WOS:000391548100001,0
J,"Diamond, S; Boyd, S",,,,"Diamond, Steven; Boyd, Stephen",,,CVXPY: A Python-Embedded Modeling Language for Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at cvxpy.org under the GPL license, along with documentation and examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,83,,,,,,,,,,27375369,,,,,WOS:000391526800001,0
J,"Kong, YF; Zheng, ZM; Lv, JC",,,,"Kong, Yinfei; Zheng, Zemin; Lv, Jinchi",,,The Constrained Dantzig Selector with Enhanced Consistency,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Dantzig selector has received popularity for many applications such as compressed sensing and sparse modeling, thanks to its computational efficiency as a linear programming problem and its nice sampling properties. Existing results show that it can recover sparse signals mimicking the accuracy of the ideal procedure, up to a logarithmic factor of the dimensionality. Such a factor has been shown to hold for many regularization methods. An important question is whether this factor can be reduced to a logarithmic factor of the sample size in ultra-high dimensions under mild regularity conditions. To provide an affirmative answer, in this paper we suggest the constrained Dantzig selector, which has more flexible constraints and parameter space. We prove that the suggested method can achieve convergence rates within a logarithmic factor of the sample size of the oracle rates and improved sparsity, under a fairly weak assumption on the signal strength. Such improvement is significant in ultra-high dimensions. This method can be implemented efficiently through sequential linear programming. Numerical studies confirm that the sample size needed for a certain level of accuracy in these problems can be much reduced.",,,,,"Lv, Jinchi/D-2295-2012",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,22,123,,,,,,,,,,,,,,,WOS:000391654700001,0
J,"Peng, B; Wang, L; Wu, YC",,,,"Peng, Bo; Wang, Lan; Wu, Yichao",,,An Error Bound for L-1-norm Support Vector Machine Coefficients in Ultra-high Dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Comparing with the standard L-2-norm support vector machine (SVM), the L-1-norm SVM enjoys the nice property of simultaneously preforming classification and feature selection. In this paper, we investigate the statistical performance of L-1-norm SVM in ultra-high dimension, where the number of features p grows at an exponential rate of the sample size n. Different from existing theory for SVM which has been mainly focused on the generalization error rates and empirical risk, we study the asymptotic behavior of the coefficients of L-1-norm SVM. Our analysis reveals that the estimated L-1-norm SVM coefficients achieve near oracle rate, that is, with high probability, the L-2 error bound of the estimated L-1-norm SVM coefficients is of order O-p(root qlog p/n), where q is the number of features with nonzero coefficients. Furthermore, we show that if the L-1-norm SVM is used as an initial value for a recently proposed algorithm for solving non-convex penalized SVM (Zhang et al., 2016b), then in two iterative steps it is guaranteed to produce an estimator that possesses the oracle property in ultra-high dimension, which in particular implies that with probability approaching one the zero coefficients are estimated as exactly zero. Simulation studies demonstrate the fine performance of L-1-norm SVM as a sparse classifier and its effectiveness to be utilized to solve non-convex penalized SVM problems in high dimension.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,236,,,,,,,,,,,,,,,WOS:000391919300001,0
J,"Pilanci, M; Wainwright, MJ",,,,"Pilanci, Mert; Wainwright, Martin J.",,,Iterative Hessian Sketch:Fast and Accurate Solution Approximation for Constrained Least-Squares,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including rho 1-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,53,,,,,,,,,,,,,,,WOS:000391487900001,0
J,"Tai, C; E, WN",,,,"Tai, Cheng; E, Weinan",,,Multiscale Adaptive Representation of Signals: I. The Basic Framework,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,140,,,,,,,,,,,,,,,WOS:000391660000001,0
J,"Cunningham, JP; Ghahramani, Z",,,,"Cunningham, John P.; Ghahramani, Zoubin",,,"Linear Dimensionality Reduction: Survey, Insights, and Generalizations",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2859,2900,,,,,,,,,,,,,,,,WOS:000369888000018,0
J,"Han, F; Lu, HR; Liu, H",,,,"Han, Fang; Lu, Huanran; Liu, Han",,,A Direct Estimation of High Dimensional Stationary Vector Autoregressions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The vector autoregressive (VAR) model is a powerful tool in learning complex time series and has been exploited in many fields. The VAR model poses some unique challenges to researchers: On one hand, the dimensionality, introduced by incorporating multiple numbers of time series and adding the order of the vector autoregression, is usually much higher than the time series length; On the other hand, the temporal dependence structure naturally present in the VAR model gives rise to extra difficulties in data analysis. The regular way in cracking the VAR model is via least squares and usually involves adding different penalty terms (e.g., ridge or lasso penalty) in handling high dimensionality. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, formulating the estimating problem to a linear program. There is instant advantage of the proposed approach over the lasso type estimators: The estimation equation can be decomposed to multiple sub-equations and accordingly can be solved efficiently using parallel computing. Besides that, we also bring new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel, 2011 and Kock and Callot, 2015) are based on stringent assumptions that are not transparent. Our results, on the other hand, show that the spectral norms of the transition matrices play an important role in estimation accuracy and build estimation and prediction consistency accordingly. Moreover, we provide some experiments on both synthetic and real-world equity data. We show that there are empirical advantages of our method over the lasso-type estimators in parameter estimation and forecasting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3115,3150,,,,,,,,,,,,,,,,WOS:000369888000026,0
J,"Plumb, G; Pachauri, D; Kondor, R; Singh, V",,,,"Plumb, Gregory; Pachauri, Deepti; Kondor, Risi; Singh, Vikas",,,SnFFT: A Julia Toolkit for Fourier Analysis of Functions over Permutations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"SnFFT is an easy to use software library written in the Julia language to facilitate Fourier analysis on the symmetric group (set of permutations) of degree n, denoted S-n and make it more easily deployable within statistical machine learning algorithms. Our implementation internally creates the irreducible matrix representations of S-n, and efficiently computes fast Fourier transforms (FFTs) and inverse fast Fourier transforms (iFFTs). Advanced users can achieve scalability and promising practical performance by exploiting various other forms of sparsity. Further, the library also supports the partial inverse Fourier transforms which utilizes the smoothness properties of functions by maintaining only the first few Fourier coefficients. Out of the box, SnFFT currently offers two non-trivial operations for functions defined on S-n, namely convolution and correlation. While the potential applicability of SnFFT is fairly broad, as an example, we show how it can be used for clustering ranked data, where each ranking is modeled as a distribution on S-n.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3469,3473,,,,,,,,,,,,,,,,WOS:000369888000036,0
J,"Gammerman, A; Vovk, V",,,,"Gammerman, Alex; Vovk, Vladimir",,,Alexey Chervonenkis's Bibliography: Introductory Comments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,2051,2065,,,,,,,,,,,,,,,,WOS:000369887300013,0
J,"da Silva, CP; Dias, DM; Bentes, C; Pacheco, MAC; Cupertino, LF",,,,"da Silva, Cleomar Pereira; Dias, Douglas Mota; Bentes, Cristiana; Cavalcanti Pacheco, Marco Aurelio; Cupertino, Leandro Fontoura",,,Evolving GPU Machine Code,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Parallel Graphics Processing Unit (GPU) implementations of GP have appeared in the literature using three main methodologies: (i) compilation, which generates the individuals in GPU code and requires compilation; (ii) pseudo-assembly, which generates the individuals in an intermediary assembly code and also requires compilation; and (iii) interpretation, which interprets the codes. This paper proposes a new methodology that uses the concepts of quantum computing and directly handles the GPU machine code instructions. Our methodology utilizes a probabilistic representation of an individual to improve the global search capability. In addition, the evolution in machine code eliminates both the overhead of compiling the code and the cost of parsing the program during evaluation. We obtained up to 2.74 trillion GP operations per second for the 20-bit Boolean Multiplexer benchmark. We also compared our approach with the other three GPU-based acceleration methodologies implemented for quantum-inspired linear GP. Significant gains in performance were obtained.",,,,,"Dias, Douglas M/I-4602-2012; Pacheco, Marco Aur√©lio/Q-3592-2016","Dias, Douglas M/0000-0002-1783-6352; ",,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,673,712,,,,,,,,,,,,,,,,WOS:000369886300002,0
J,"Gillian, N; Paradiso, JA",,,,"Gillian, Nicholas; Paradiso, Joseph A.",,,The Gesture Recognition Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Gesture Recognition Toolkit is a cross-platform open-source C++ library designed to make real-time machine learning and gesture recognition more accessible for non-specialists. Emphasis is placed on ease of use, with a consistent, minimalist design that promotes accessibility while supporting flexibility and customization for advanced users. The toolkit features a broad range of classification and regression algorithms and has extensive support for building real-time systems. This includes algorithms for signal processing, feature extraction and automatic gesture spotting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3483,3487,,,,,,,,,,,,,,,,WOS:000344638800021,0
J,"Lin, XD; Pham, M; Ruszczynski, A",,,,"Lin, Xiaodong; Pham, Minh; Ruszczynski, Andrzej",,,Alternating Linearization for Structured Regularization Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We adapt the alternating linearization method for proximal decomposition to structured regularization problems, in particular, to the generalized lasso problems. The method is related to two well-known operator splitting methods, the Douglas Rachford and the Peaceman Rachford method, but it has descent properties with respect to the objective function. This is achieved by employing a special update test, which decides whether it is beneficial to make a Peaceman Rachford step, any of the two possible Douglas Rachford steps, or none. The convergence mechanism of the method is related to that of bundle methods of nonsmooth optimization. We also discuss implementation for very large problems, with the use of specialized algorithms and sparse data structures. Finally, we present numerical results for several synthetic and real-world examples, including a three-dimensional fused lasso problem, which illustrate the scalability, efficacy, and accuracy of the method.",,,,,"Ruszczynski, Andrzej/Q-3469-2019",,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3447,3481,,,,,,,,,,,,,,,,WOS:000344638800020,0
J,"Loh, PL; Buhlmann, P",,,,"Loh, Po-Ling; Buehlmann, Peter",,,High-Dimensional Learning of Linear Causal Networks via Inverse Covariance Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared p2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for highdimensional consistency of our two-part algorithm, defined in terms of a gap between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3065,3105,,,,,,,,,,,,,,,,WOS:000344638800008,0
J,"Sheikh, AS; Shelton, JA; Lucke, J",,,,"Sheikh, Abdul-Saboor; Shelton, Jacquelyn A.; Luecke, Joerg",,,A Truncated EM Approach for Spike-and-Slab Sparse Coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study inference and learning based on a sparse coding model with 'spike-and-slab' prior. As in standard sparse coding, the model used assumes independent latent sources that linearly combine to generate data points. However, instead of using a standard sparse prior such as a Laplace distribution, we study the application of a more flexible 'spike-and-slab' distribution which models the absence or presence of a source's contribution independently of its strength if it contributes. We investigate two approaches to optimize the parameters of spike-and-slab sparse coding: a novel truncated EM approach and, for comparison, an approach based on standard factored variational distributions. The truncated approach can be regarded as a variational approach with truncated posteriors as variational distributions. In applications to source separation we find that both approaches improve the state-of-the-art in a number of standard benchmarks, which argues for the use of 'spike-and-slab' priors for the corresponding data domains. Furthermore, we find that the truncated EM approach improves on the standard factored approach in source separation tasks-which hints to biases introduced by assuming posterior independence in the factored variational approach. Likewise, on a standard benchmark for image denoising, we find that the truncated EM approach improves on the factored variational approach. While the performance of the factored approach saturates with increasing numbers of hidden dimensions, the performance of the truncated approach improves the state-of-the-art for higher noise levels.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2014,15,,,,,,2653,2687,,,,,,,,,,,,,,,,WOS:000344638600002,0
J,"Tziortziotis, N; Dimitrakakis, C; Blekas, K",,,,"Tziortziotis, Nikolaos; Dimitrakakis, Christos; Blekas, Konstantinos",,,Cover Tree Bayesian Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes an online tree-based I3ayesian approach for reinforcement learning. For inference, WC employ a generalised context tree model. This defines a distribution On multivariate Gttussian piecewise-linear models, vhich can be updated in closed form. The tree structure itself k constructed using the cover tree inethocl, v1iidi remains efficient in high dimensional spaces. We cOITIbine the model with Thompson sampling and approximate dynamic programming to obtain effective exploration policies in unkritAVri environments. The flexibility and computational simplicity of the model render it suitable for many reinforcement learning problems in continuous state spaces. We demonstrate this in an experimental comparison with a Gaussian process model, a linear model and simple least squares policy iteration.",,,,,"Dimitrakakis, Christos/F-6404-2011","Dimitrakakis, Christos/0000-0002-5367-5189",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2313,2335,,,,,,,,,,,,,,,,WOS:000344638300013,0
J,"von Luxburg, U; Radl, A; Hein, M",,,,"von Luxburg, Ulrike; Radl, Agnes; Hein, Matthias",,,Hitting and Commute Times in Large Random Neighborhood Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In machine learning, a popular tool to analyze the structure of graphs is the hitting time and the commute distance (resistance distance). For two vertices u and v, the hitting time H-uv is the expected time it takes a random walk to travel from u to v. The commute distance is its symmetrized version C-uv = H-uv + H-vu. In our paper we study the behavior of hitting times and commute distances when the number n of vertices in the graph tends to infinity. We focus on random geometric graphs (epsilon-graphs, kNN graphs and Gaussian similarity graphs), but our results also extend to graphs with a given expected degree distribution or Erd6s-Renyi graphs with planted partitions. We prove that in these graph families, the suitably rescaled hitting time H uv converges to 1/d(v) and the rescaled commute time to 1/d(u) + 1/d(v) where du and dv denote the degrees of vertices u and v. In these cases, hitting and commute times do not provide information about the structure of the graph, and their use is discouraged in many machine learning applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1751,1798,,,,,,,,,,,,,,,,WOS:000344638100005,0
J,"Dann, C; Neumann, G; Peters, J",,,,"Dann, Christoph; Neumann, Gerhard; Peters, Jan",,,Policy Evaluation with Temporal Differences: A Survey and Comparison,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy evaluation is an essential step in most reinforcement learning approaches. It yields a value function, the quality assessment of states for a given policy, which can be used in a policy improvement step. Since the late 1980s, this research area has been dominated by temporal-difference (TD) methods due to their data-efficiency. However, core issues such as stability guarantees in the off-policy scenario, improved sample efficiency and probabilistic treatment of the uncertainty in the estimates have only been tackled recently, which has led to a large number of new approaches. This paper aims at making these new developments accessible in a concise overview, with foci on underlying cost functions, the off-policy scenario as well as on regularization in high dimensional feature, spaces. By presenting the first extensive, systematic comparative evaluations comparing TD, LSID, LSPE, FPKF, the residual-gradient algorithm, Bellman residual minimization, GTD, GTD2 and TDC, we shed light on the strengths and weaknesses of the methods. Moreover, we present alternative versions of LSTD and LSPE with drastically improved off-policy performance.",,,,,"Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091; Neumann, Gerhard/0000-0002-5483-4225",,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,809,883,,,,,,,,,,,,,,,,WOS:000335458100001,0
J,"Zhu, J; Chen, N; Perkins, H; Zhang, B",,,,"Zhu, Jun; Chen, Ning; Perkins, Hugh; Zhang, Bo",,,Gibbs Max-margin Topic Models with Data Augmentation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restrictive assumptions and no need to solve SVM subproblems. Furthermore, each step of the augment-and-collapse Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results on several medium-sized and large-scale data sets demonstrate significant improvements on time efficiency. The classification performance is also improved over competitors on binary, multi-class and multi-label classification tasks.",,,,,"Perkins, Hugh/AAX-5116-2020",,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1073,1110,,,,,,,,,,,,,,,,WOS:000335458100008,0
J,"Shankar, KH; Howard, MW",,,,"Shankar, Karthik H.; Howard, Marc W.",,,Optimally Fuzzy Temporal Memory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register-a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system.",,,,,"Howard, Marc/E-2518-2012",,,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3785,3812,,,,,,,,,,,,,,,,WOS:000335457100010,0
J,"Liu, Q; Ihler, A",,,,"Liu, Qiang; Ihler, Alexander",,,Variational Algorithms for Marginal MAP,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of mixed-product message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel argmax-product message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods.",,,,,,"Ihler, Alexander/0000-0002-4331-1015",,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3165,3200,,,,,,,,,,,,,,,,WOS:000328603600010,0
J,"Fanello, SR; Gori, I; Metta, G; Odone, F",,,,"Fanello, Sean Ryan; Gori, Ilaria; Metta, Giorgio; Odone, Francesca",,,Keep It Simple And Sparse: Real-Time Action Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efficient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective real-time system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, All Gestures You Can, to be played against a humanoid robot.",,,,,"Fanello, Sean/AAS-6647-2021","Fanello, Sean/0000-0001-9726-4501",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2617,2640,,,,,,,,,,,,,,,,WOS:000327007400005,0
J,"Mahdi, R; Mezey, J",,,,"Mahdi, Rami; Mezey, Jason",,,Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conflicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conflicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1563,1603,,,,,,,,,,,,,,,,WOS:000322506400005,0
J,"Niyogi, P",,,,"Niyogi, Partha",,,Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on finite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1229,1250,,,,,,,,,,,,,,,,WOS:000320709300001,0
J,"Klami, A; Virtanen, S; Kaski, S",,,,"Klami, Arto; Virtanen, Seppo; Kaski, Samuel",,,Bayesian Canonical Correlation Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combination of large dimensionalities and small sample sizes. The existing methods have not been particularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,965,1003,,,,,,,,,,,,,,,,WOS:000318590500008,0
J,"Curtin, RR; Cline, JR; Slagle, NP; March, WB; Ram, P; Mehta, NA; Gray, AG",,,,"Curtin, Ryan R.; Cline, James R.; Slagle, N. P.; March, William B.; Ram, Parikshit; Mehta, Nishant A.; Gray, Alexander G.",,,MLPACK: A Scalable C++ Machine Learning Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.",,,,,,"Mehta, Nishant/0000-0002-9639-0124",,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,801,805,,,,,,,,,,,,,,,,WOS:000317461700003,0
J,"Hable, R",,,,"Hable, Robert",,,Universal Consistency of Localized Versions of Regularized Kernel Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,153,186,,,,,,,,,,,,,,,,WOS:000314530200005,0
J,"Zhu, J; Ahmed, A; Xing, EP",,,,"Zhu, Jun; Ahmed, Amr; Xing, Eric P.",,,MedLDA: Maximum Margin Supervised Topic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, especially for classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2237,2278,,,,,,,,,,,,,,,,WOS:000308795200001,0
J,"Negahban, S; Wainwright, MJ",,,,"Negahban, Sahand; Wainwright, Martin J.",,,Restricted Strong Convexity and Weighted Matrix Completion: Optimal Bounds with Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the matrix completion problem under a form of row/column weighted entrywise sampling, including the case of uniform entrywise sampling as a special case. We analyze the associated random observation operator, and prove that with high probability, it satisfies a form of restricted strong convexity with respect to weighted Frobenius norm. Using this property, we obtain as corollaries a number of error bounds on matrix completion in the weighted Frobenius norm under noisy sampling and for both exact and near low-rank matrices. Our results are based on measures of the spikiness and low-rankness of matrices that are less restrictive than the incoherence conditions imposed in previous work. Our technique involves an M-estimator that includes controls on both the rank and spikiness of the solution, and we establish non-asymptotic error bounds in weighted Frobenius norm for recovering matrices lying with l(q)-balls of bounded spikiness. Using information-theoretic methods, we show that no algorithm can achieve better estimates (up to a logarithmic factor) over these same sets, showing that our conditions on matrices and associated rates are essentially optimal.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1665,1697,,,,,,,,,,,,,,,,WOS:000305456600011,0
J,"Vainsencher, D; Mannor, S; Bruckstein, AM",,,,"Vainsencher, Daniel; Mannor, Shie; Bruckstein, Alfred M.",,,The Sample Complexity of Dictionary Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary. Algorithms for various signal processing applications, including classification, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection, as measured by the expected L-2 error in representation when the dictionary is used. For the case of l(1) regularized coefficient selection we provide a generalization bound of the order of O(root npln(m lambda)/m), where n is the dimension, p is the number of elements in the dictionary, l is a bound on the l(1) norm of the coefficient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most k dictionary elements, we provide a bound of the order O(root npln(mk)/m) under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as 1/m, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements.",,,,,,"Bruckstein, Alfred/0000-0001-5669-0037; Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3259,3281,,,,,,,,,,,,,,,,WOS:000298103700007,0
J,"Tamada, Y; Imoto, S; Miyano, S",,,,"Tamada, Yoshinori; Imoto, Seiya; Miyano, Satoru",,,Parallel Algorithm for Learning Optimal Bayesian Network Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a parallel algorithm for the score-based optimal structure search of Bayesian networks. This algorithm is based on a dynamic programming (DP) algorithm having O(n . 2(n)) time and space complexity, which is known to be the fastest algorithm for the optimal structure search of networks with n nodes. The bottleneck of the problem is the memory requirement, and therefore, the algorithm is currently applicable for up to a few tens of nodes. While the recently proposed algorithm overcomes this limitation by a space-time trade-off, our proposed algorithm realizes direct parallelization of the original DP algorithm with O(n(sigma)) time and space overhead calculations, where sigma > 0 controls the communication-space trade-off. The overall time and space complexity is O(n(sigma+1)2(n)). This algorithm splits the search space so that the required communication between independent calculations is minimal. Because of this advantage, our algorithm can run on distributed memory supercomputers. Through computational experiments, we confirmed that our algorithm can run in parallel using up to 256 processors with a parallelization efficiency of 0.74, compared to the original DP algorithm with a single processor. We also demonstrate optimal structure search for a 32-node network without any constraints, which is the largest network search presented in literature.",,,,,"Tamada, Yoshinori/AAY-4542-2020","Tamada, Yoshinori/0000-0003-3483-9712",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2437,2459,,,,,,,,,,,,,,,,WOS:000293757900013,0
J,"Reid, MD; Williamson, RC",,,,"Reid, Mark D.; Williamson, Robert C.",,,"Information, Divergence and Risk for Binary Experiments",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We unify f-divergences, Bregman divergences, surrogate regret bounds, proper scoring rules, cost curves, ROC-curves and statistical information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their representation primitives which all are related to cost-sensitive binary classification. As well as developing relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate regret bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint also illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates maximum mean discrepancy to Fisher linear discriminants.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,731,817,,,,,,,,,,,,,,,,WOS:000289635000003,0
J,"McFee, B; Lanckriet, G",,,,"McFee, Brian; Lanckriet, Gert",,,Learning Multi-modal Similarity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications involving multi-media data, the definition of similarity between items is integral to several key tasks, including nearest-neighbor retrieval, classification, and recommendation. Data in such regimes typically exhibits multiple modalities, such as acoustic and visual content of video. Integrating such heterogeneous data to form a holistic similarity space is therefore a key challenge to be overcome in many real-world applications. We present a novel multiple kernel learning technique for integrating heterogeneous data into a single, unified similarity space. Our algorithm learns an optimal ensemble of kernel transformations which conform to measurements of human perceptual similarity, as expressed by relative comparisons. To cope with the ubiquitous problems of subjectivity and inconsistency in multimedia similarity, we develop graph-based techniques to filter similarity measurements, resulting in a simplified and robust training procedure.",,,,,,"McFee, Brian/0000-0001-6261-9747",,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,491,523,,,,,,,,,,,,,,,,WOS:000288896800005,0
J,"Baehrens, D; Schroeter, T; Harmeling, S; Kawanabe, M; Hansen, K; Muller, KR",,,,"Baehrens, David; Schroeter, Timon; Harmeling, Stefan; Kawanabe, Motoaki; Hansen, Katja; Mueller, Klaus-Robert",,,How to Explain Individual Classification Decisions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,JUN,2010,11,,,,,,1803,1831,,,,,,,,,,,,,,,,WOS:000282522400002,0
J,"Raykar, VC; Yu, SP; Zhao, LH; Valadez, GH; Florin, C; Bogoni, L; Moy, L",,,,"Raykar, Vikas C.; Yu, Shipeng; Zhao, Linda H.; Valadez, Gerardo Hermosillo; Florin, Charles; Bogoni, Luca; Moy, Linda",,,Learning From Crowds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For many supervised learning tasks it may be infeasible (or very expensive) to obtain objective and reliable labels. Instead, we can collect subjective (possibly noisy) labels from multiple experts or annotators. In practice, there is a substantial amount of disagreement among the annotators, and hence it is of great practical interest to address conventional supervised learning problems in this scenario. In this paper we describe a probabilistic approach for supervised learning when we have multiple annotators providing (possibly noisy) labels but no absolute gold standard. The proposed algorithm evaluates the different experts and also gives an estimate of the actual hidden labels. Experimental results indicate that the proposed method is superior to the commonly used majority voting baseline.",,,,,"zhao, Linda/HGU-0391-2022; Moy, Linda/U-8018-2019","Moy, Linda/0000-0001-9564-9360",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1297,1322,,,,,,,,,,,,,,,,WOS:000282521500004,0
J,"Mann, GS; McCallum, A",,,,"Mann, Gideon S.; McCallum, Andrew",,,Generalized Expectation Criteria for Semi-Supervised Learning with Weakly Labeled Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we present an overview of generalized expectation criteria (GE), a simple, robust, scalable method for semi-supervised training using weakly-labeled data. GE fits model parameters by favoring models that match certain expectation constraints, such as marginal label distributions, on the unlabeled data. This paper shows how to apply generalized expectation criteria to two classes of parametric models: maximum entropy models and conditional random fields. Experimental results demonstrate accuracy improvements over supervised training and a number of other state-of-the-art semi-supervised learning methods for these models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,955,984,,,,,,,,,,,,,,,,WOS:000277186500020,0
J,"Perry, PO; Owen, AB",,,,"Perry, Patrick O.; Owen, Art B.",,,A Rotation Test to Verify Latent Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In multivariate regression models we have the opportunity to look for hidden structure unrelated to the observed predictors. However, when one fits a model involving such latent variables it is important to be able to tell if the structure is real, or just an artifact of correlation in the regression errors. We develop a new statistical test based on random rotations for verifying the existence of latent variables. The rotations are carefully constructed to rotate orthogonally to the column space of the regression model. We find that only non-Gaussian latent variables are detectable, a finding that parallels a well known phenomenon in independent components analysis. We base our test on a measure of non-Gaussianity in the histogram of the principal eigenvector components instead of on the eigenvalue. The method finds and verifies some latent dichotomies in the microarray data from the AGEMAP consortium.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,603,624,,,,,,,,,,,,,,,,WOS:000277186500006,0
J,"Duchi, J; Singer, Y",,,,"Duchi, John; Singer, Yoram",,,Efficient Online and Batch Learning Using Forward Backward Splitting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe, analyze, and experiment with a framework for empirical loss minimization with regularization. Our algorithmic framework alternates between two phases. On each iteration we first perform an unconstrained gradient descent step. We then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase. This view yields a simple yet effective algorithm that can be used for batch penalized risk minimization and online learning. Furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as l(1). We derive concrete and very simple algorithms for minimization of loss functions with l(1), l(2), l(2)(2), and l(infinity) regularization. We also show how to construct efficient algorithms for mixed-norm l(1)/l(q) regularization. We further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity. We demonstrate the potential of the proposed framework in a series of experiments with synthetic and natural data sets.",,,,,,"Duchi, John/0000-0003-0045-7185",,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2899,2934,,,,,,,,,,,,,,,,WOS:000273877300006,0
J,"Szlam, AD; Maggioni, M; Coifman, RR",,,,"Szlam, Arthur D.; Maggioni, Mauro; Coifman, Ronald R.",,,Regularization on Graphs with Function-adapted Diffusion Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Harmonic analysis and diffusion on discrete data has been shown to lead to state-of-the-art algorithms for machine learning tasks, especially in the context of semi-supervised and transductive learning. The success of these algorithms rests on the assumption that the function(s) to be studied (learned, interpolated, etc.) are smooth with respect to the geometry of the data. In this paper we present a method for modifying the given geometry so the function(s) to be studied are smoother with respect to the modified geometry, and thus more amenable to treatment using harmonic analysis methods. Among the many possible applications, we consider the problems of image denoising and transductive classification. In both settings, our approach improves on standard diffusion based methods.",,,,,,"Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1711,1739,,,,,,,,,,,,,,,,WOS:000262636800003,0
J,"Mease, D; Wyner, A",,,,"Mease, David; Wyner, Abraham",,,Evidence contrary to the statistical view of boosting: A rejoinder to responses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,195,201,,,,,,,,,,,,,,,,WOS:000256641800008,0
J,"Srivastava, S; Gupta, MR; Frigyik, BA",,,,"Srivastava, Santosh; Gupta, Maya R.; Frigyik, Bela A.",,,Bayesian quadratic discriminant analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Quadratic discriminant analysis is a common tool for classification, but estimation of the Gaussian parameters can be ill-posed. This paper contains theoretical and algorithmic contributions to Bayesian estimation for quadratic discriminant analysis. A distribution-based Bayesian classifier is derived using information geometry. Using a calculus of variations approach to define a functional Bregman divergence for distributions, it is shown that the Bayesian distribution-based classifier that minimizes the expected Bregman divergence of each class conditional distribution also minimizes the expected misclassification cost. A series approximation is used to relate regularized discriminant analysis to Bayesian discriminant analysis. A new Bayesian quadratic discriminant analysis classifier is proposed where the prior is defined using a coarse estimate of the covariance based on the training data; this classifier is termed BDA7. Results on benchmark data sets and simulations show that BDA7 performance is competitive with, and in some cases significantly better than, regularized quadratic discriminant analysis and the cross-validated Bayesian quadratic discriminant analysis classifier Quadratic Bayes.",,,,,"Frigyik, Andrew B/H-9400-2014","Frigyik, Andrew B/0000-0002-4220-4680",,,,,,,,,,,,,1532-4435,,,,,JUN,2007,8,,,,,,1277,1305,,,,,,,,,,,,,,,,WOS:000248351800003,0
J,"Landwehr, N; Kersting, K; De Raedt, L",,,,"Landwehr, Niels; Kersting, Kristian; De Raedt, Luc",,,Integrating naive Bayes and FOIL,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A novel relational learning approach that tightly integrates the naive Bayes learning scheme with the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed naive Bayes only for post-processing the rule sets, the presented approach employs the naive Bayes criterion to guide its search directly. The proposed technique is implemented in the NFOIL and TFOIL systems, which employ standard naive Bayes and tree augmented naive Bayes models respectively. We show that these integrated approaches to probabilistic model and rule learning outperform post-processing approaches. They also yield significantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems.",,,,,"De Raedt, Luc/AAX-1544-2021","De Raedt, Luc/0000-0002-6860-6303",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,481,507,,,,,,,,,,,,,,,,WOS:000247002700004,0
J,"Mukherjee, S; Wu, Q",,,,"Mukherjee, Sayan; Wu, Qiang",,,Estimation of gradients and coordinate covariation in classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient variables and estimate how they covary. An efficient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classification function and its gradient to the true classification function and true gradient.,,,,,"Wu, Qiang/B-1620-2008","Wu, Qiang/0000-0002-4698-6966; Mukherjee, Sayan/0000-0002-6715-3920",,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2481,2514,,,,,,,,,,,,,,,,WOS:000245390700008,0
J,"Niculescu, RS; Mitchell, TM; Rao, RB",,,,"Niculescu, Radu Stefan; Mitchell, Tom M.; Rao, R. Bharat",,,Bayesian network learning with parameter constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The task of learning models for many real-world problems requires incorporating domain knowledge into learning algorithms, to enable accurate learning from a realistic volume of training data. This paper considers a variety of types of domain knowledge for constraining parameter estimates when learning Bayesian networks. In particular, we consider domain knowledge that constrains the values or relationships among subsets of parameters in a Bayesian network with known structure. We incorporate a wide variety of parameter constraints into learning procedures for Bayesian networks, by formulating this task as a constrained optimization problem. The assumptions made in module networks, dynamic Bayes nets and context specific independence models can be viewed as particular cases of such parameter constraints. We present closed form solutions or fast iterative algorithms for estimating parameters subject to several specific classes of parameter constraints, including equalities and inequalities among parameters, constraints on individual parameters, and constraints on sums and ratios of parameters, for discrete and continuous variables. Our methods cover learning from both frequentist and Bayesian points of view, from both complete and incomplete data. We present formal guarantees for our estimators, as well as methods for automatically learning useful parameter constraints from data. To validate our approach, we apply it to the domain of fMRI brain image analysis. Here we demonstrate the ability of our system to first learn useful relationships among parameters, and then to use them to constrain the training of the Bayesian network, resulting in improved cross-validated accuracy of the learned model. Experiments on synthetic data are also presented.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1357,1383,,,,,,,,,,,,,,,,WOS:000245388800009,0
J,"Gardner, AB; Krieger, AM; Vachtsevanos, G; Litt, B",,,,"Gardner, Andrew B.; Krieger, Abba M.; Vachtsevanos, George; Litt, Brian",,,One-class novelty detection for seizure analysis from intracranial EEG,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes an application of one-class support vector machine (SVM) novelty detection for detecting seizures in humans. Our technique maps intracranial electroencephalogram (EEG) time series into corresponding novelty sequences by classifying short-time, energy-based statistics computed from one-second windows of data. We train a classifier on epochs of interictal (normal) EEG. During ictal (seizure) epochs of EEG, seizure activity induces distributional changes in feature space that increase the empirical outlier fraction. A hypothesis test determines when the parameter change differs significantly from its nominal value, signaling a seizure detection event. Outputs are gated in a one-shot manner using persistence to reduce the false alarm rate of the system. The detector was validated using leave-one-out cross-validation (LOO-CV) on a sample of 41 interictal and 29 ictal epochs, and achieved 97.1% sensitivity, a mean detection latency of -7.58 seconds, and an asymptotic false positive rate (FPR) of 1.56 false positives per hour (Fp/hr). These results are better than those obtained from a novelty detection technique based on Mahalanobis distance outlier detection, and comparable to the performance of a supervised learning technique used in experimental implantable devices (Echauz et al., 2001). The novelty detection paradigm overcomes three significant limitations of competing methods: the need to collect seizure data, precisely mark seizure onset and offset times, and perform patient-specific parameter tuning for detector training.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1025,1044,,,,,,,,,,,,,,,,WOS:000245388400006,0
J,"Hush, D; Kelly, P; Scovel, C; Steinwart, I",,,,"Hush, Don; Kelly, Patrick; Scovel, Clint; Steinwart, Ingo",,,QP algorithms with guaranteed accuracy and run time for support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe polynomial-time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two-stage process where the first stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2 root 2K(n) + 8 root lambda)(-2) lambda epsilon(2)(p) to an approximate primal solution with accuracy epsilon(p) where n is the number of data samples, K-n is the maximum kernel value over the data and lambda > 0 is the SVM regularization parameter. For the first stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for tau-rate certifying decomposition algorithms we establish the optimality of tau = 1/(n - 1). In addition we extend the recent tau = 1/(n - 1) algorithm of Simon ( 2004) to form two new composite algorithms that also achieve the t = 1/( n- 1) iteration bound of List and Simon ( 2005), but yield faster run times in practice. We also exploit the t - rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n(2)) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run time of O(n(2)(c(k) + 1)) where c(k) is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy epsilon(p) and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms.",,,,,,"Steinwart, Ingo/0000-0002-4436-7109",,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,733,769,,,,,,,,,,,,,,,,WOS:000240173400002,0
J,"Demsar, J",,,,"Demsar, J",,,Statistical comparisons of classifiers over multiple data sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD ( critical difference) diagrams.",,,,,"Dem≈°ar, Janez/AAS-2762-2020",,,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,1,30,,,,,,,,,,,,,,,,WOS:000236331400001,0
J,"Zoeter, O; Heskes, T",,,,"Zoeter, O; Heskes, T",,,Change point problems in linear dynamical systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the problem of learning two regimes (we have a normal and a prefault regime in mind) based on a train set of non-Markovian observation sequences. Key to the model is that we assume that once the system switches from the normal to the prefault regime it cannot restore and will eventually result in a fault. We refer to the particular setting as semi-supervised since we assume the only information given to the learner is whether a particular sequence ended with a stop (implying that the sequence was generated by the normal regime) or with a fault (implying that there was a switch from the normal to the fault regime). In the latter case the particular time point at which a switch occurred is not known. The underlying model used is a switching linear dynamical system (SLDS). The constraints in the regime transition probabilities result in an exact inference procedure that scales quadratically with the length of a sequence. Maximum aposteriori (MAP) parameter estimates can be found using an expectation maximization (EM) algorithm with this inference algorithm in the E-step. For long sequences this will not be practically feasible and an approximate inference and an approximate EM procedure is called for. We describe a flexible class of approximations corresponding to different choices of clusters in a Kikuchi free energy with weak consistency constraints.,,,,,"Heskes, Tom/A-1443-2010","Heskes, Tom/0000-0002-3398-5235",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,1999,2026,,,,,,,,,,,,,,,,WOS:000236331100005,0
J,"Kuss, M; Rasmussen, CE",,,,"Kuss, M; Rasmussen, CE",,,Assessing approximate inference for binary Gaussian process classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace's method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace's method.",,,,,,"Rasmussen, Carl Edward/0000-0001-8899-7850",,,,,,,,,,,,,1532-4435,,,,,OCT,2005,6,,,,,,1679,1704,,,,,,,,,,,,,,,,WOS:000236330500002,0
J,"Daume, H; Marcu, D",,,,"Daume, H; Marcu, D",,,A Bayesian model for supervised clustering with the dirichlet process prior,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a Bayesian framework for tackling the supervised clustering problem, the generic problem encountered in tasks such as reference matching, coreference resolution, identity uncertainty and record linkage. Our clustering model is based on the Dirichlet process prior, which enables us to define distributions over the countably infinite sets that naturally arise in this problem. We add supervision to our model by positing the existence of a set of unobserved random variables (we call these reference types) that are generic across all clusters. Inference in our framework, which requires integrating over infinitely many parameters, is solved using Markov chain Monte Carlo techniques. We present algorithms for both conjugate and non-conjugate priors. We present a simple-but general-parameterization of our model based on a Gaussian assumption. We evaluate this model on one artificial task and three real-world tasks, comparing it against both unsupervised and state-of-the-art supervised algorithms. Our results show that our model is able to outperform other models across a variety of tasks and performance metrics.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1551,1577,,,,,,,,,,,,,,,,WOS:000236330100011,0
J,"De Vito, E; Rosasco, L; Caponnetto, A; De Giovannini, U; Odone, F",,,,"De Vito, E; Rosasco, L; Caponnetto, A; De Giovannini, U; Odone, F",,,Learning from examples as an inverse problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (finite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows.",,,,,"De Giovannini, Umberto/A-4635-2010; De Vito, Ernesto/K-6354-2015; De Vito, Ernesto/AAX-5125-2021","De Giovannini, Umberto/0000-0002-4899-1304; De Vito, Ernesto/0000-0002-4320-3292; De Vito, Ernesto/0000-0002-4320-3292; CAPONNETTO, Andrea/0000-0002-6311-0667; rosasco, lorenzo/0000-0003-3098-383X",,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,883,904,,,,,,,,,,,,,,,,WOS:000236329700006,0
J,"Bach, FR; Jordan, MI",,,,"Bach, FR; Jordan, MI",,,Beyond independent components: Trees and clusters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, we look for a transforrn that makes the data components well fit by a tree-structured graphical model. This tree-dependent component analysis (TCA) provides a tractable and flexible approach to weakening the assumption of independence in ICA. In particular, TCA allows the underlying graph to have multiple connected components, and thus the method is able to find clusters of components such that components are dependent within a cluster and independent between clusters. Finally, we make use of a notion of graphical models for time series due to Brillinger (1996) to extend these ideas to the temporal setting. In particular, we are able to fit models that incorporate tree-structured dependencies among multiple time series.",,,,,"Jordan, Michael I/C-5253-2013",,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1205,1233,,,,,,,,,,,,,,,,WOS:000224808300003,0
J,"Ziehe, A; Laskov, P; Nolte, G; Muller, KR",,,,"Ziehe, A; Laskov, P; Nolte, G; Muller, KR",,,A fast algorithm for joint diagonalization with non-orthogonal transformations and its application to blind source separation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new efficient algorithm is presented for joint diagonalization of several matrices. The algorithm is based on the Frobenius-norm formulation of the joint diagonalization problem, and addresses diagonalization with a general, non-orthogonal transformation. The iterative scheme of the algorithm is based on a multiplicative update which ensures the invertibility of the diagonalizer. The algorithm's efficiency stems from the special approximation of the cost function resulting in a sparse, block-diagonal Hessian to be used in the computation of the quasi-Newton update step. Extensive numerical simulations illustrate the performance of the algorithm and provide a comparison to other leading diagonalization methods. The results of such comparison demonstrate that the proposed algorithm is a viable alternative to existing state-of-the-art joint diagonalization algorithms. The practical use of our algorithm is shown for blind source separation problems.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,JUL,2004,5,,,,,,777,800,,,,,,,,,,,,,,,,WOS:000236327800002,0
J,"Perlich, C; Provost, F; Simonoff, JS",,,,"Perlich, C; Provost, F; Simonoff, JS",,,Tree induction vs. logistic regression: A learning-curve analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tree induction and logistic regression are two standard, off-the-shelf methods for building models for classification. We present a large-scale experimental comparison of logistic regression and tree induction, assessing classification accuracy and the quality of rankings based on class-membership probabilities. We use a learning-curve analysis to examine the relationship of these measures to the size of the training set. The results of the study show several things. (1) Contrary to some prior observations, logistic regression does not generally outperform tree induction. (2) More specifically, and not surprisingly, logistic regression is better for smaller training sets and tree induction for larger data sets. Importantly, this often holds for training sets drawn from the same domain (that is, the learning curves cross), so conclusions about induction-algorithm superiority on a given domain must be based on an analysis of the learning curves. (3) Contrary to conventional wisdom, tree induction is effective at producing probability-based rankings, although apparently comparatively less so for a given training-set size than at making classifications. Finally, (4) the domains on which tree induction and logistic regression are ultimately preferable can be characterized surprisingly well by a simple measure of the separability of signal from noise.",,,,,"Simonoff, Jeffrey S/A-6765-2008",,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2004,4,2,,,,,211,255,,10.1162/153244304322972694,0,,,,,,,,,,,,,WOS:000221043600004,0
J,"Chan, K; Lee, TW; Sejnowski, TJ",,,,"Chan, K; Lee, TW; Sejnowski, TJ",,,Variational learning of clusters of undercomplete nonsymmetric independent components,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We apply a variational method to automatically determine the number of mixtures of independent components in high-dimensional datasets, in which the sources may be nonsymmetrically distributed. The data are modeled by clusters where each cluster is described as a linear mixture of independent factors. The variational Bayesian method yields an accurate density model for the observed data without overfitting problems. This allows the dimensionality of the data to be identified for each cluster. The new method was successfully applied to a difficult real-world medical dataset for diagnosing glaucoma.",,,,,"Sejnowski, Terrence/AAV-5558-2021",,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,99,114,,10.1162/153244303768966120,0,,,,,,,,21479123,,,,,WOS:000181462700005,0
J,"Meek, C; Thiesson, B; Heckerman, D",,,,"Meek, C; Thiesson, B; Heckerman, D",,,The learning-curve sampling method applied to model-based clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We examine the learning-curve sampling method, an approach for applying machine-learning algorithms to large data sets. The approach is based on the observation that the computational cost of learning a model increases as a function of the sample size of the training data, whereas the accuracy of a model has diminishing improvements as a function of sample size. Thus, the learning-curve sampling method monitors the increasing costs and performance as larger and larger amounts of data are used for training, and terminates learning when future costs outweigh future benefits. In this paper, we formalize the learning-curve sampling method and its associated cost-benefit tradeoff in terms of decision theory. In addition, we describe the application of the learning-curve sampling method to the task of model-based clustering via the expectation-maximization (EM) algorithm. In experiments on three real data sets, we show that the learning-curve sampling method produces models that are nearly as accurate as those trained on complete data sets, but with dramatically reduced learning times. Finally, we describe an extension of the basic learning-curve approach for model-based clustering that results in an additional speedup. This extension is based on the observation that the shape of the learning curve for a given model and data set is roughly independent of the number of EM iterations used during training. Thus, we run EM for only a few iterations to decide how many cases to use for training, and then run EM to full convergence once the number of cases is selected.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,397,418,,10.1162/153244302760200678,0,,,,,,,,,,,,,WOS:000178101500004,0
J,"Gentile, C",,,,"Gentile, C",,,A new approximate maximal margin classification algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p greater than or equal to 2 for a set of linearly separable data. Our algorithm, called ALMA(p) (Approximate Large Margin algorithm w.r.t. norm p), takes O ((p-1)/alpha2gamma2) corrections to separate the data with p-norm margin larger than (1 - a) -y, where -y is the (normalized) p-norm margin of the data. ALMA(p) avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm. We performed extensive experiments on both real-world and artificial datasets. We compared ALMA(2) (i.e., ALMA(p) with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA. The accuracy levels achieved by ALMA2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's. On the other hand, ALMA(2) is quite faster and easier to implement than standard SVM training algorithms. When learning sparse target vectors, ALMA(p) with p > 2 largely outperforms Perceptron-like algorithms, such as ALMA(2).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,213,242,,10.1162/15324430260185600,0,,,,,,,,,,,,,WOS:000176055300007,0
J,"Ausset, G; Clemencon, S; Portier, F",,,,"Ausset, Guillaume; Clemencon, Stephan; Portier, Francois",,,Empirical Risk Minimization under Random Censorship,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the classic supervised learning problem where a continuous non-negative random label Y (e.g. a random duration) is to be predicted based upon observing a random vector X valued in R-d with d >= 1 by means of a regression rule with minimum least square error. In various applications, ranging from industrial quality control to public health through credit risk analysis for instance, training observations can be right censored, meaning that, rather than on independent copies of (X, Y), statistical learning relies on a collection of n >= 1 independent realizations of the triplet (X, min{Y, C}, delta), where C is a nonnegative random variable with unknown distribution, modelling censoring and delta = I{Y <= C} indicates whether the duration is right censored or not. As ignoring censoring in the risk computation may clearly lead to a severe underestimation of the target duration and jeopardize prediction, we consider a plug-in estimate of the true risk based on a Kaplan-Meier estimator of the conditional survival function of the censoring C given X, referred to as Reran risk, in order to perform empirical risk minimization. It is established, under mild conditions, that the learning rate of minimizers of this biased/weighted empirical risk functional is of order O-p(root log(n)/n) when ignoring model bias issues inherent to plug-in estimation, as can be attained in absence of censoring. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,59,,,,,,,,,,,,,,,,WOS:000752307000001,0
J,"Zhang, GJ; Poupart, P; Yu, YL",,,,"Zhang, Guojun; Poupart, Pascal; Yu, Yaoliang",,,Optimality and Stability in Non-Convex Smooth Games,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Convergence to a saddle point for convex-concave functions has been studied for decades, while recent years has seen a surge of interest in non-convex (zero-sum) smooth games, motivated by their recent wide applications. It remains an intriguing research challenge how local optimal points are defined and which algorithm can converge to such points. An interesting concept is known as the local minimax point (Jin et al., 2020), which strongly correlates with the widely-known gradient descent ascent algorithm. This paper aims to provide a comprehensive analysis of local minimax points, such as their relation with other solution concepts and their optimality conditions. We find that local saddle points can be regarded as a special type of local minimax points, called uniformly local minimax points, under mild continuity assumptions. In (non-convex) quadratic games, we show that local minimax points are (in some sense) equivalent to global minimax points. Finally, we study the stability of gradient algorithms near local minimax points. Although gradient algorithms can converge to local/global minimax points in the non-degenerate case, they would often fail in general cases. This implies the necessity of either novel algorithms or concepts beyond saddle points and minimax points in non-convex smooth games.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,71,,,,,,,,,,,,,,,,WOS:000752349800001,0
J,"Baniecki, H; Kretowicz, W; Piatyszek, P; Wisniewski, J; Biecek, P",,,,"Baniecki, Hubert; Kretowicz, Wojciech; Piatyszek, Piotr; Wisniewski, Jakub; Biecek, Przemyslaw",,,dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In modern machine learning, we observe the phenomenon of opaqueness debt, which manifests itself by an increased risk of discrimination, lack of reproducibility, and deflated performance due to data drift. An increasing amount of available data and computing power results in the growing complexity of black-box predictive models. To manage these issues, good MLOps practice asks for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity for deeper model transparency comes from both scientific and social domains and is also caused by emerging laws and regulations on artificial intelligence. To facilitate the responsible development of machine learning models, we introduce dalex, a Python package which implements a model-agnostic interface for interactive explainability and fairness. It adopts the design crafted through the development of various tools for explainable machine learning; thus, it aims at the unification of existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai.",,,,,,"Baniecki, Hubert/0000-0001-6661-5364",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,214,,,,,,,,,,,,,,,WOS:000706445400001,0
J,"Chatterji, NS; Long, PM",,,,"Chatterji, Niladri S.; Long, Philip M.",,,Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We prove bounds on the population risk of the maximum margin algorithm for two-class linear classification. For linearly separable training data, the maximum margin algorithm has been shown in previous work to be equivalent to a limit of training with logistic loss using gradient descent, as the training error is driven to zero. We analyze this algorithm applied to random data including misclassification noise. Our assumptions on the clean data include the case in which the class-conditional distributions are standard normal distributions. The misclassification noise may be chosen by an adversary, subject to a limit on the fraction of corrupted labels. Our bounds show that, with sufficient over-parameterization, the maximum margin algorithm trained on noisy data can achieve nearly optimal population risk.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,129,,,,,,,,,,,,,,,WOS:000663182200001,0
J,"Curtin, RR; Edel, M; Prabhu, RG; Basak, S; Lou, ZH; Sanderson, C",,,,"Curtin, Ryan R.; Edel, Marcus; Prabhu, Rahul Ganesh; Basak, Suryoday; Lou, Zhihao; Sanderson, Conrad",,,The ensmallen library for flexible numerical optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We overview the ensmallen numerical optimization library, which provides a flexible C++ framework for mathematical optimization of user-supplied objective functions. Many types of objective functions are supported, including general, differentiable, separable, con-strained, and categorical. A diverse set of pre-built optimizers is provided, including Quasi-Newton optimizers and many variants of Stochastic Gradient Descent. The underlying framework facilitates the implementation of new optimizers. Optimization of an objective function typically requires supplying only one or two C++ functions. Custom behavior can be easily specified via callback functions. Empirical comparisons show that ensmallen outperforms other frameworks while providing more functionality. The library is available at https://ensmallen.org and is distributed under the permissive BSD license.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687193300001,0
J,"Dudeja, R; Hsu, D",,,,"Dudeja, Rishabh; Hsu, Daniel",,,Statistical Query Lower Bounds for Tensor PCA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the Tensor PCA problem introduced by Richard and Montanari (2014), one is given a dataset consisting of n samples T-1:n of i.i.d. Gaussian tensors of order k with the promise that ET1 is a rank-1 tensor and parallel to ET1 parallel to = 1. The goal is to estimate ET1. This problem exhibits a large conjectured hard phase when k > 2: When d less than or similar to n << d(k/2) it is information theoretically possible to estimate ET1, but no polynomial time estimator is known. We provide a sharp analysis of the optimal sample complexity in the Statistical Query (SQ) model and show that SQ algorithms with polynomial query complexity not only fail to solve Tensor PCA in the conjectured hard phase, but also have a strictly sub-optimal sample complexity compared to some polynomial time estimators such as the Richard-Montanari spectral estimator. Our analysis reveals that the optimal sample complexity in the SQ model depends on whether ET1 is symmetric or not. For symmetric, even order tensors, we also isolate a sample size regime in which it is possible to test if ET1 = 0 or ET1 not equal 0 with polynomially many queries but not estimate ET1. Our proofs rely on the Fourier analytic approach of Feldman et al. (2018) to prove sharp SQ lower bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656402800001,0
J,"Galanti, T; Benaim, S; Wolf, L",,,,"Galanti, Tomer; Benaim, Sagie; Wolf, Lior",,,Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The recent empirical success of unsupervised cross-domain mapping algorithms, in mapping between two domains that share common characteristics, is not well-supported by theoretical justifications. This lacuna is especially troubling, given the clear ambiguity in such mappings. We work with adversarial training methods based on integral probability metrics (IPMs) and derive a novel risk bound, which upper bounds the risk between the learned mapping h and the target mapping y, by a sum of three terms: (i) the risk between h and the most distant alternative mapping that was learned by the same cross-domain mapping algorithm, (ii) the minimal discrepancy between the target domain and the domain obtained by applying a hypothesis h* on the samples of the source domain, where h* is a hypothesis selectable by the same algorithm, and (iii) an approximation error term that decreases as the capacity of the class of discriminators increases and is empirically shown to be small. The bound is directly related to Occam's razor and encourages the selection of the minimal architecture that supports a small mapping discrepancy. The bound leads to multiple algorithmic consequences, including a method for hyperparameter selection and early stopping in cross-domain mapping.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,90,,,,,,,,,,,,,,,WOS:000663145600001,0
J,"Gupta, S; Kamble, V",,,,"Gupta, Swati; Kamble, Vijay",,,Individual Fairness in Hindsight,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The pervasive prevalence of algorithmic decision-making in societal domains necessitates that these algorithms satisfy reasonable notions of fairness. One compelling notion is that of individual fairness (IF), which advocates that similar individuals should be treated similarly (Dwork et al. 2012). In this paper, we extend the notion of IF to online contextual decisionmaking in settings where there exists a common notion of conduciveness of decisions as perceived by the affected individuals. We introduce two definitions: (i) fairness-acrosstime (FT) and (ii) fairness-in-hindsight (FH). FT requires the treatment of individuals to be individually fair relative to the past as well as future, while FH only requires individual fairness of a decision at the time of the decision. We show that these two definitions can have drastically different implications when the principal needs to learn the utility model. Linear regret relative to optimal individually fair decisions is generally unavoidable under FT. On the other hand, we design a new algorithm: Cautious Fair Exploration (CaFE), which satisfies FH and achieves order-optimal sublinear regret guarantees for a broad range of settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700310500001,0
J,"Kose, U; Ruszczynski, A",,,,"Kose, Umit; Ruszczynski, Andrzej",,,Risk-Averse Learning by Temporal Difference Methods with Markov Risk Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We propose a novel reinforcement learning methodology where the system performance is evaluated by a Markov coherent dynamic risk measure with the use of linear value function approximations. We construct projected risk-averse dynamic programming equations and study their properties. We propose new risk-averse counterparts of the basic and multi-step methods of temporal differences and we prove their convergence with probability one. We also perform an empirical study on a complex transportation problem.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500038,0
J,"Maillard, G; Arlot, S; Lerasle, M",,,,"Maillard, Guillaume; Arlot, Sylvain; Lerasle, Matthieu",,,Aggregated Hold-Out,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Aggregated hold-out (agghoo) is a method which averages learning rules selected by hold-out (that is, cross-validation with a single split). We provide the first theoretical guarantees on agghoo, ensuring that it can be used safely: Agghoo performs at worst like the hold-out when the risk is convex. The same holds true in classification with the 0-1 risk, with an additional constant factor. For the hold-out, oracle inequalities are known for bounded losses, as in binary classification. We show that similar results can be proved, under appropriate assumptions, for other risk-minimization problems. In particular, we obtain an oracle inequality for regularized kernel regression with a Lipschitz loss, without requiring that the Y variable or the regressors be bounded. Numerical experiments show that aggregation brings a significant improvement over the hold-out and that agghoo is competitive with cross-validation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500020,0
J,"Merrill, E; Fern, A; Fern, XL; Dolatnia, N",,,,"Merrill, Erich; Fern, Alan; Fern, Xiaoli; Dolatnia, Nima",,,An Empirical Study of Bayesian Optimization: Acquisition Versus Partition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian optimization (BO) is a popular framework for black-box optimization. Two classes of BO approaches have shown promising empirical performance while providing strong theoretical guarantees. The first class optimizes an acquisition function to select points, which is typically computationally expensive and can only be done approximately. The second class of algorithms use systematic space partitioning, which is much cheaper computationally but the selection is typically less informed. This points to a potential trade-off between the computational complexity and empirical performance of these algorithms. The current literature, however, only provides a sparse sampling of empirical comparison points, giving little insight into this trade-off. The primary contribution of this work is to conduct a comprehensive, repeatable evaluation within a common software framework, which we provide as an open-source package. Our results give strong evidence about the relative performance of these methods and reveal a consistent top performer, even when accounting for overall computation time.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500004,0
J,"Rakhsha, A; Radanovic, G; Devidze, R; Zhu, XJ; Singla, A",,,,"Rakhsha, Amin; Radanovic, Goran; Devidze, Rati; Zhu, Xiaojin; Singla, Adish",,,Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.",,,,,"Singla, Adish/ABG-8960-2021",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706876200001,0
J,"Sicks, R; Korn, R; Schwaar, S",,,,"Sicks, Robert; Korn, Ralf; Schwaar, Stefanie",,,A Generalised Linear Model Framework for beta-Variational Autoencoders based on Exponential Dispersion Families,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Although variational autoencoders (VAE) are successfully used to obtain meaningful low dimensional representations for high-dimensional data, the characterization of critical points of the loss function for general observation models is not fully understood. We introduce a theoretical framework that is based on a connection between /3-VAE and generalized linear models (GLM). The equality between the activation function of a /3-VAE and the inverse of the link function of a GLM enables us to provide a systematic generalization of the loss analysis for /3-VAE based on the assumption that the observation model distribution belongs to an exponential dispersion family (EDF). As a result, we can initialize /3-VAE nets by maximum likelihood estimates (MLE) that enhance the training performance on both synthetic and real world data sets. As a further consequence, we analytically describe the auto-pruning property inherent in the /3-VAE objective and reason for posterior collapse.",,,,,"Schwaar, Stefanie/ABD-4601-2021","Schwaar, Stefanie/0000-0001-8565-9231",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706877000001,0
J,"Smith, MT; Alvarez, MA; Lawrence, ND",,,,"Smith, Michael Thomas; Alvarez, Mauricio A.; Lawrence, Neil D.",,,Differentially Private Regression and Classification with Sparse Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A continuing challenge for machine learning is providing methods to perform computation on data while ensuring the data remains private. In this paper we build on the provable privacy guarantees of differential privacy which has been combined with Gaussian processes through the previously published cloaking method, an approach that tackles the problem of providing privacy for the outputs of a training set. In this paper we solve several shortcomings of this method, starting with the problem of predictions in regions with low data density. We experiment with the use of inducing points to provide a sparse approximation and show that these can provide robust differential privacy in outlier areas and at higher dimensions. We then look at classification, and modify the Laplace approximation approach to provide differentially private predictions. We then combine this with the sparse approximation and demonstrate the capability to perform classification in high dimensions. We finally explore the issue of hyperparameter selection and develop a method for their private selection. This paper and associated libraries provide a robust toolkit for combining differential privacy and Gaussian processes in a practical manner.",,,,,,"Alvarez, Mauricio A./0000-0002-8980-4472",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706880600001,0
J,"Cevid, D; Buhlmann, P; Meinshausen, N",,,,"Cevid, Domagoj; Buhlmann, Peter; Meinshausen, Nicolai",,,Spectral Deconfounding via Perturbed Sparse Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard high-dimensional regression methods assume that the underlying coefficient vector is sparse. This might not be true in some cases, in particular in presence of hidden, confounding variables. Such hidden confounding can be represented as a high-dimensional linear model where the sparse coefficient vector is perturbed. For this model, we develop and investigate a class of methods that are based on running the Lasso on preprocessed data. The preprocessing step consists of applying certain spectral transformations that change the singular values of the design matrix. We show that, under some assumptions, one can achieve the usual Lasso l(1)-error rate for estimating the underlying sparse coefficient vector, despite the presence of confounding. Our theory also covers the Lava estimator (Chernozhukov et al., 2017) for a special model class. The performance of the methodology is illustrated on simulated data and a genomic dataset.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,232,,,,,,,,,,,,,,,WOS:000605750000001,0
J,"Keshavarz, H; Michailidis, G; Atchade, Y",,,,"Keshavarz, Hossein; Michailidis, George; Atchade, Yves",,,Sequential change-point detection in high-dimensional Gaussian graphical models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High dimensional piecewise stationary graphical models represent a versatile class for modelling time varying networks arising in diverse application areas, including biology, economics, and social sciences. There has been recent work in offline detection and estimation of regime changes in the topology of sparse graphical models. However, the online setting remains largely unexplored, despite its high relevance to applications in sensor networks and other engineering monitoring systems, as well as financial markets. To that end, this work introduces a novel scalable online algorithm for detecting an unknown number of abrupt changes in the inverse covariance matrix of sparse Gaussian graphical models with small delay. The proposed algorithm is based upon monitoring the conditional log-likelihood of all nodes in the network and can be extended to a large class of continuous and discrete graphical models. We also investigate asymptotic properties of our procedure under certain mild regularity conditions on the graph size, sparsity level, number of samples, and pre-and post-changes in the topology of the network. Numerical works on both synthetic and real data illustrate the good performance of the proposed methodology both in terms of computational and statistical efficiency across numerous experimental settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000542194600005,0
J,"Kool, W; van Hoof, H; Welling, M",,,,"Kool, Wouter; van Hoof, Herke; Welling, Max",,,Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop ancestral Gumbel-Top-k sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top k of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size k. Our algorithm allows to set the number of parallel processors m, to trade off the number of iterations versus the total cost (iterations times m) of running the algorithm. For m = 1 the algorithm has minimum total cost, whereas for m = k the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. 1 We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of Gumbel-Top-k sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy.",,,,,"van Hoof, Herke/N-7775-2017","van Hoof, Herke/0000-0002-1583-3692",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000022,0
J,"Ma, Y; Olshevsky, A; Saligrama, V; Szepesvari, C",,,,"Ma, Yao; Olshevsky, Alex; Saligrama, Venkatesh; Szepesvari, Csaba",,,Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider worker skill estimation for the single-coin Dawid-Skene crowdsourcing model. In practice, skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills are identifiable if and only if the sampling matrix (observed components) does not have a bipartite connected component. We then propose a projected gradient descent scheme and show that skill estimates converge to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP-hard in general. Next, we derive sample complexity bounds in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,133,,,,,,,,,,,,,,,WOS:000556298800001,0
J,"Metelli, AM; Papini, M; Montali, N; Restelli, M",,,,"Metelli, Alberto Maria; Papini, Matteo; Montali, Nico; Restelli, Marcello",,,Importance Sampling Techniques for Policy Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods.",,,,,"Metelli, Alberto Maria/AAY-5206-2020","Metelli, Alberto Maria/0000-0002-3424-5212; Restelli, Marcello/0000-0002-6322-1076",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,141,,,,,,,,,,,,,,,WOS:000558804100001,0
J,"Miolane, N; Guigui, N; Le Brigant, A; Mathe, J; Hou, B; Thanwerdas, Y; Heyder, S; Peltre, O; Koep, N; Zaatiti, H; Hajri, H; Cabanes, Y; Gerald, T; Chauchat, P; Shewmake, C; Brooks, D; Kainz, B; Donnat, C; Holmes, S; Pennec, X",,,,"Miolane, Nina; Guigui, Nicolas; Le Brigant, Alice; Mathe, Johan; Hou, Benjamin; Thanwerdas, Yann; Heyder, Stefan; Peltre, Olivier; Koep, Niklas; Zaatiti, Hadi; Hajri, Hatem; Cabanes, Yann; Gerald, Thomas; Chauchat, Paul; Shewmake, Christian; Brooks, Daniel; Kainz, Bernhard; Donnat, Claire; Holmes, Susan; Pennec, Xavier",,,Geomstats: A Python Package for Riemannian Geometry in Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce Geomstats, an open-source Python package for computations and statistics on nonlinear manifolds such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Manifolds come equipped with families of Riemannian metrics with associated exponential and logarithmic maps, geodesics, and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering, and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends-namely NumPy, PyTorch, and TensorFlow. This paper presents the package, compares it with related libraries, and provides relevant code examples. We show that Geomstats provides reliable building blocks to both foster research in differential geometry and statistics and democratize the use of Riemannian geometry in machine learning applications.",,,,,"Kainz, Bernhard/H-3416-2016; Pennec, Xavier/L-2537-2013","Kainz, Bernhard/0000-0002-7813-5023; Pennec, Xavier/0000-0002-6617-7664; Miolane, Nina/0000-0002-1200-9024",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,223,,,,,,,,,,,,,,,WOS:000605743600001,0
J,"Nicholson, WB; Wilms, I; Bien, J; Matteson, DS",,,,"Nicholson, William B.; Wilms, Ines; Bien, Jacob; Matteson, David S.",,,High Dimensional Forecasting via Interpretable Vector Autoregression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Vector autoregression (VAR) is a fundamental tool for modeling multivariate time series. However, as the number of component series is increased, the VAR model becomes overparameterized. Several authors have addressed this issue by incorporating regularized approaches, such as the lasso in VAR estimation. Traditional approaches address overparameterization by selecting a low lag order, based on the assumption of short range dependence, assuming that a universal lag order applies to all components. Such an approach constrains the relationship between the components and impedes forecast performance. The lasso-based approaches perform much better in high-dimensional situations but do not incorporate the notion of lag order selection. We propose a new class of hierarchical lag structures (HLag) that embed the notion of lag selection into a convex regularizer. The key modeling tool is a group lasso with nested groups which guarantees that the sparsity pattern of lag coefficients honors the VAR's ordered structure. The proposed HLag framework offers three basic structures, which allow for varying levels of flexibility, with many possible generalizations. A simulation study demonstrates improved performance in forecasting and lag order selection over previous approaches, and macroeconomic, financial, and energy applications further highlight forecasting improvements as well as HLag's convenient, interpretable output.",,,,,,"Matteson, David/0000-0002-2674-0387; Wilms, Ines/0000-0003-3269-4601",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,166,,,,,,,,,,,,,,,WOS:000570153300001,0
J,"Papyan, V",,,,"Papyan, Vardan",,,Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Numerous researchers recently applied empirical spectral analysis to the study of modern deep learning classifiers. We identify and discuss an important formal class/cross-class structure and show how it lies at the origin of the many visually striking features observed in deep neural network spectra, some of which were reported in recent articles, others are unveiled here for the first time. These include spectral outliers, spikes, and small but distinct continuous distributions, bumps, often seen beyond the edge of a main bulk.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,252,,,,,,,,,,,,,,,WOS:000608920800001,0
J,"Wu, C; Xu, GJ; Shen, XT; Pan, W",,,,"Wu, Chong; Xu, Gongjun; Shen, Xiaotong; Pan, Wei",,,A Regularization-Based Adaptive Test for High-Dimensional Generalized Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In spite of its urgent importance in the era of big data, testing high-dimensional parameters in generalized linear models (GLMs) in the presence of high-dimensional nuisance parameters has been largely under-studied, especially with regard to constructing powerful tests for general (and unknown) alternatives. Most existing tests are powerful only against certain alternatives and may yield incorrect Type I error rates under high-dimensional nuisance parameter situations. In this paper, we propose the adaptive interaction sum of powered score (aiSPU) test in the framework of penalized regression with a non-convex penalty, called truncated Lasso penalty (TLP), which can maintain correct Type I error rates while yielding high statistical power across a wide range of alternatives. To calculate its p-values analytically, we derive its asymptotic null distribution. Via simulations, its superior finite-sample performance is demonstrated over several representative existing methods. In addition, we apply it and other representative tests to an Alzheimer's Disease Neuroimaging Initiative (ADNI) data set, detecting possible gene-gender interactions for Alzheimer's disease. We also put R package aispu implementing the proposed test on GitHub.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,128,,,,,,,,,,32802002,,,,,WOS:000556295500001,0
J,"Barker, E; Ras, C",,,,"Barker, Edward; Ras, Charl",,,Unsupervised Basis Function Adaptation for Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture online and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.",,,,,,"Ras, Charl/0000-0001-9986-2767",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,128,,,,,,,,,,,,,,,WOS:000487068900012,0
J,"Bingham, E; Chen, JP; Jankowiak, M; Obermeyer, F; Pradhan, N; Karaletsos, T; Singh, R; Szerlip, P; Horsfall, P; Goodman, ND",,,,"Bingham, Eli; Chen, Jonathan P.; Jankowiak, Martin; Obermeyer, Fritz; Pradhan, Neeraj; Karaletsos, Theofanis; Singh, Rohit; Szerlip, Paul; Horsfall, Paul; Goodman, Noah D.",,,Pyro: Deep Universal Probabilistic Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,28,,,,,,,,,,,,,,,WOS:000458670100001,0
J,"Castro, DC; Tan, J; Kainz, B; Konukoglu, E; Glocker, B",,,,"Castro, Daniel C.; Tan, Jeremy; Kainz, Bernhard; Konukoglu, Ender; Glocker, Ben",,,Morpho-MNIST: Quantitative Assessment and Diagnostics for Representation Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: to what extent has my model learned to represent specific factors of variation in the data? We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https : //github. com/dccastro/Morpho-MNIST.",,,,,"Kainz, Bernhard/H-3416-2016","Kainz, Bernhard/0000-0002-7813-5023; Tan, Jeremy/0000-0002-9769-068X; Coelho de Castro, Daniel/0000-0002-6829-7045",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,178,,,,,,,,,,,,,,,WOS:000506403100018,0
J,"Cortes, C; Mohri, M; Medina, AM",,,,"Cortes, Corinna; Mohri, Mehryar; Medina, Andres Munoz",,,Adaptation Based on Generalized Discrepancy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new algorithm for domain adaptation improving upon a discrepancy minimization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previously proposed solutions for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, the reweighting depends on the hypothesis sought. The algorithm is derived from a less conservative notion of discrepancy than the DM algorithm called generalized discrepancy. We present a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also give a detailed theoretical analysis of its learning guarantees which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon discrepancy minimization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,1,,,,,,,,,,,,,,,WOS:000457933600001,0
J,"Hong, B; Zhang, WZ; Liu, W; Ye, JP; Caiy, D; He, XF; Wang, J",,,,"Hong, Bin; Zhang, Weizhong; Liu, Wei; Ye, Jieping; Cai, Deng; He, Xiaofei; Wang, Jie",,,Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse support vector machine (SVM) is a popular classi fi cation technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and ultra-high dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the inactive features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identi fi ed inactive samples and features from the training phase, leading to substantial savings in the computational cost without sacri fi cing the accuracy. Moreover, we show that our method can be extended to multi-class sparse support vector machines. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVMs and multi-class sparse SVMs. Experiments on both synthetic and real data sets demonstrate that our approach signi fi cantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.",,,,,,"Liu, Wei/0000-0002-3865-8145",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,121,,,,,,,,,,,,,,,WOS:000487068900005,0
J,"Gamiz, ML; Martinez-Miranda, MD; Nielsen, JP",,,,"Luz Gamiz, Maria; Dolores Martinez-Miranda, Maria; Perch Nielsen, Jens",,,Multiplicative local linear hazard estimation and best one-sided cross-validation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops detailed mathematical statistical theory of a new class of cross-validation techniques of local linear kernel hazards and their multiplicative bias corrections. The new class of cross-validation combines principles of local information and recent advances in indirect cross-validation. A few applications of cross-validating multiplicative kernel hazard estimation do exist in the literature. However, detailed mathematical statistical theory and small sample performance are introduced via this paper and further upgraded to our new class of best one-sided cross-validation. Best one-sided cross-validation turns out to have excellent performance in its practical illustrations, in its small sample performance and in its mathematical statistical theoretical performance.",,,,,"Miranda, Maria Dolores Martinez/F-5051-2016; Gamiz, M.L./AAA-7826-2019","Gamiz, M.L./0000-0002-3393-7022; Nielsen, Jens Perch/0000-0002-2798-0817",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,18,,,,,,,,,,,,,,,WOS:000458667000001,0
J,"Page, S; Grunewalder, S",,,,"Page, Stephen; Grunewalder, Steffen",,,Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their Interpolation Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study kernel least-squares estimation under a norm constraint. This form of regularisation is known as Ivanov regularisation and it provides better control of the norm of the estimator than the well-established Tikhonov regularisation. Ivanov regularisation can be studied under minimal assumptions. In particular, we assume only that the RKHS is separable with a bounded and measurable kernel. We provide rates of convergence for the expected squared L-2 error of our estimator under the weak assumption that the variance of the response variables is bounded and the unknown regression function lies in an interpolation space between L-2 and the RKHS. We then obtain faster rates of convergence when the regression function is bounded by clipping the estimator. In fact, we attain the optimal rate of convergence. Furthermore, we provide a high-probability bound under the stronger assumption that the response variables have subgaussian errors and that the regression function lies in an interpolation space between L-infinity and the RKHS. Finally, we derive adaptive results for the settings in which the regression function is bounded.",,,,,,"Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,120,,,,,,,,,,,,,,,WOS:000487068900004,0
J,"Razaee, ZS; Amini, AA; Li, JYJ",,,,"Razaee, Zahra S.; Amini, Arash A.; Li, Jingyi Jessica",,,Matched Bipartite Block Model with Covariates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models.",,,,,"Li, Jingyi Jessica/I-2779-2019","Li, Jingyi Jessica/0000-0002-9288-5648",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,34,,,,,,,,,,,,,,,WOS:000463317300001,0
J,"Wang, SS; Gittens, A; Mahoney, MW",,,,"Wang, Shusen; Gittens, Alex; Mahoney, Michael W.",,,Scalable Kernel K-Means Clustering with Nystrom Approximation: Relative-Error Bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel k-means clustering can correctly identify and extract a far more varied collection of cluster structures than the linear k-means clustering algorithm. However, kernel k-means clustering is computationally expensive when the non-linear feature map is high-dimensional and there are many input points. Kernel approximation, e.g., the Nystrom method, has been applied in previous works to approximately solve kernel learning problems when both of the above conditions are present. This work analyzes the application of this paradigm to kernel k-means clustering, and shows that applying the linear k-means clustering algorithm to k/c(1 + o(1)) features constructed using a so-called rank-restricted Nystrom approximation results in cluster assignments that satisfy a 1+ epsilon approximation ratio in terms of the kernel k-means cost function, relative to the guarantee provided by the same algorithm without the use of the Nystrom method. As part of the analysis, this work establishes a novel 1+ epsilon relative-error trace norm guarantee for low-rank approximation using the rank-restricted Nystrom approximation. Empirical evaluations on the 8 : 1 million instance MNIST8M dataset demonstrate the scalability and usefulness of kernel k-means clustering with Nystrom approximation. This work argues that spectral clustering using Nystrom approximation a popular and computationally efficient, but theoretically unsound approach to non-linear clustering-should be replaced with the efficient and theoretically sound combination of kernel k-means clustering with Nystrom approximation. The superior performance of the latter approach is empirically verified.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,12,,,,,,,,,,,,,,,WOS:000458665700001,0
J,"Du, J; Ma, SD; Wu, YC; Kar, S; Moura, JMF",,,,"Du, Jian; Ma, Shaodan; Wu, Yik-Chung; Kar, Soummya; Moura, Jose M. F.",,,Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at an exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios.",,,,,"Wu, Yik Chung/C-1869-2009; Moura, Jose M F/G-2189-2010","Moura, Jose M F/0000-0002-9822-8294",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,172,,,,,,,,,,,,,,,WOS:000435442200001,0
J,"Hang, HY; Steinwart, I; Feng, YL; Suykens, JAK",,,,"Hang, Hanyuan; Steinwart, Ingo; Feng, Yunlong; Suykens, Johan A. K.",,,Kernel Density Estimation for Dynamical Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the density estimation problem with observations generated by certain dynamical systems that admit a unique underlying invariant Lebesgue density. Observations drawn from dynamical systems are not independent and moreover, usual mixing concepts may not be appropriate for measuring the dependence among these observations. By employing the C-mixing concept to measure the dependence, we conduct statistical analysis on the consistency and convergence of the kernel density estimator. Our main results are as follows: First, we show that with properly chosen bandwidth, the kernel density estimator is universally consistent under L-1-norm; Second, we establish convergence rates for the estimator with respect to several classes of dynamical systems under L-1-norm. In the analysis, the density function f is only assumed to be Holder continuous or pointwise Holder controllable which is a weak assumption in the literature of nonparametric density estimation and also more realistic in the dynamical system context. Last but not least, we prove that the same convergence rates of the estimator under L-infinity-norm and L-1-norm can be achieved when the density function is Holder continuous, compactly supported, and bounded. The bandwidth selection problem of the kernel density estimator for dynamical system is also discussed in our study via numerical simulations.",,,,,"Suykens, Johan/C-9781-2014","Suykens, Johan/0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,35,,,,,,,,,,,,,,,WOS:000444404600001,0
J,"Patrascu, A; Necoara, I",,,,"Patrascu, Andrei; Necoara, Ion",,,Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order O(1/k(1/2)) where k is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order O(1/k). Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems.",,,,,,"Patrascu, Andrei/0000-0002-9293-9386",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,198,,,,,,,,,,,,,,,WOS:000435455100001,0
J,"Ronan, T; Anastasio, S; Qi, ZJ; Sloutsky, R; Naegle, KM; Tavares, PHSV",,,,"Ronan, Tom; Anastasio, Shawn; Qi, Zhijie; Sloutsky, Roman; Naegle, Kristen M.; Tavares, Pedro Henrique S. Vieira",,,OpenEnsembles: A Python Resource for Ensemble Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles.",,,,,"Qi, Zhijie/HGA-4124-2022","Qi, Zhijie/0000-0002-0022-8844",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,26,,,,,,,,,,,,,,,WOS:000443230500001,0
J,"Huang, HW",,,,"Huang, Hanwen",,,Asymptotic behavior of Support Vector Machine for spiked population model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For spiked population model, we investigate the large dimension N and large sample size M asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of N, M -> infinity at fixed a = M/N. We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite -size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation.",,,,,"Huang, Hanwen/AAS-6534-2020",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,21,,,,,,,,,,,,,,,,WOS:000405963900001,0
J,"Ishiguro, K; Sato, I; Ueda, N",,,,"Ishiguro, Katsuhiko; Sato, Issei; Ueda, Naonori",,,Averaged Collapsed Variational Bayes Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,1,,,,,,,,,,,,,,,WOS:000397016900001,0
J,"Shamir, O",,,,"Shamir, Ohad",,,An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on Duchi et al. (2015), which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,52,,,,,,,,,,,,,,,WOS:000405980300001,0
J,"Farahmand, AM; Ghavamzadeh, M; Szepesvari, C; Mannor, S",,,,"Farahmand, Amir-Massoud; Ghavamzadeh, Mohammad; Szepesvari, Csaba; Mannor, Shie",,,Regularized Policy Iteration with Nonparametric Function Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study two regularization-based approximate policy iteration algorithms, namely REG-LSPI and REG-BRM, to solve reinforcement learning and planning problems in discounted Markov Decision Processes with large state and finite action spaces. The core of these algorithms are the regularized extensions of the Least-Squares Temporal Difference (LSTD) learning and Bellman Residual Minimization (BRM), which are used in the algorithms' policy evaluation steps. Regularization provides a convenient way to control the complexity of the function space to which the estimated value function belongs and as a result enables us to work with rich nonparametric function spaces. We derive efficient implementations of our methods when the function space is a reproducing kernel Hilbert space. We analyze the statistical properties of REG-LSPI and provide an upper bound on the policy evaluation error and the performance loss of the policy returned by this method. Our bound shows the dependence of the loss on the number of samples, the capacity of the function space, and some intrinsic properties of the underlying Markov Decision Process. The dependence of the policy evaluation bound on the number of samples is minimax optimal. This is the first work that provides such a strong guarantee for a nonparametric approximate policy iteration algorithm.(1)",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,139,,,,,,,,,,,,,,,WOS:000391659700001,0
J,"Gulcehre, C; Bengio, Y",,,,"Gulcehre, Caglar; Bengio, Yoshua",,,Knowledge Matters: Importance of Prior Information for Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-theart machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two-tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is 1 if all of three shapes belong to the same category and otherwise the class is 0. In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,8,,,,,,,,,,,,,,,WOS:000391467000001,0
J,"Heller, R; Heller, Y; Kaufman, S; Brill, B; Gorfine, M",,,,"Heller, Ruth; Heller, Yair; Kaufman, Shachar; Brill, Barak; Gorfine, Malka",,,Consistent Distribution-Free K-Sample and Independence Tests for Univariate Random Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A popular approach for testing if two univariate random variables are statistically independent consists of partitioning the sample space into bins, and evaluating a test statistic on the binned data. The partition size matters, and the optimal partition size is data dependent. While for detecting simple relationships coarse partitions may be best, for detecting complex relationships a great gain in power can be achieved by considering finer partitions. We suggest novel consistent distributionfree tests that are based on summation or maximization aggregation of scores over all partitions of a fixed size. We show that our test statistics based on summation can serve as good estimators of the mutual information. Moreover, we suggest regularized tests that aggregate over all partition sizes, and prove those are consistent too. We provide polynomial- time algorithms, which are critical for computing the suggested test statistics efficiently. We show that the power of the regularized tests is excellent compared to existing tests, and almost as powerful as the tests based on the optimal (yet unknown in practice) partition size, in simulations as well as on a real data example.",,,,,"Gorfine, Malka/AAD-1468-2022; Gorfine, Malka/AAV-1242-2021",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,29,,,,,,,,,,,,,,,WOS:000391477900001,0
J,"Lizotte, DJ; Laber, EB",,,,"Lizotte, Daniel J.; Laber, Eric B.",,,Multi-Objective Markov Decision Processes for Data-Driven Decision Support,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present new methodology based on Multi-Objective Markov Decision Processes for developing sequential decision support systems from data. Our approach uses sequential decision-making data to provide support that is useful to many different decision-makers, each with different, potentially time-varying preference. To accomplish this, we develop an extension of fitted-Q iteration for multiple objectives that computes policies for all scalarization functions, i.e. preference functions, simultaneously from continuous-state, finite-horizon data. We identify and address several conceptual and computational challenges along the way, and we introduce a new solution concept that is appropriate when different actions have similar expected outcomes. Finally, we demonstrate an application of our method using data from the Clinical Antipsychotic Trials of Intervention Effectiveness and show that our approach offers decision-makers increased choice by a larger class of optimal policies.",,,,,"Lizotte, Daniel/AAT-3170-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,211,,,,,,,,,,28018133,,,,,WOS:000391834400001,0
J,"Tamar, A; Di Castro, D; Mannor, S",,,,"Tamar, Aviv; Di Castro, Dotan; Mannor, Shie",,,Learning the Variance of the Reward-To-Go,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In Markov decision processes (MDPs), the variance of the reward-to-go is a natural measure of uncertainty about the long term performance of a policy, and is important in domains such as finance, resource allocation, and process control. Currently however, there is no tractable procedure for calculating it in large scale MDPs. This is in contrast to the case of the expected reward-to-go, also known as the value function, for which effective simulation-based algorithms are known, and have been used successfully in various domains. In this paper 1 we extend temporal difference (TD) learning algorithms to estimating the variance of the reward-to-go for a fixed policy. We propose variants of both TD(0) and LSTD(lambda) with linear function approximation, prove their convergence, and demonstrate their utility in an option pricing problem. Our results show a dramatic improvement in terms of sample efficiency over standard Monte-Carlo methods, which are currently the state-of-the-art.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,13,,,,,,,,,,,,,,,WOS:000391471100001,0
J,"Zhang, X; Wu, YC; Wang, L; Li, RZ",,,,"Zhang, Xiang; Wu, Yichao; Wang, Lan; Li, Runze",,,A Consistent Information Criterion for Support Vector Machines in Diverging Model Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information criteria have been popularly used in model selection and proved to possess nice theoretical properties. For classification, Claeskens et al. (2008) proposed support vector machine information criterion for feature selection and provided encouraging numerical evidence. Yet no theoretical justification was given there. This work aims to fill the gap and to provide some theoretical justifications for support vector machine information criterion in both fixed and diverging model spaces. We first derive a uniform convergence rate for the support vector machine solution and then show that a modification of the support vector machine information criterion achieves model selection consistency even when the number of features diverges at an exponential rate of the sample size. This consistency result can be further applied to selecting the optimal tuning parameter for various penalized support vector machine methods. Finite-sample performance of the proposed information criterion is investigated using Monte Carlo studies and one real-world gene selection problem.",,,,,"Li, Runze/HCH-8063-2022; Li, Runze/C-5444-2013; Li, Runze/ABF-1320-2020","Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,16,,,,,,,,,,27239164,,,,,WOS:000391472100001,0
J,"Fox, EB; Dunson, DB",,,,"Fox, Emily B.; Dunson, David B.",,,Bayesian Nonparametric Covariance Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Capturing predictor-dependent correlations amongst the elements of a multivariate response vector is fundamental to numerous applied domains, including neuroscience, epidemiology, and finance. Although there is a rich literature on methods for allowing the variance in a univariate regression model to vary with predictors, relatively little has been done in the multivariate case. As a motivating example, we consider the Google Flu Trends data set, which provides indirect measurements of influenza incidence at a large set of locations over time (our predictor). To accurately characterize temporally evolving influenza incidence across regions, it is important to develop statistical methods for a time-varying covariance matrix. Importantly, the locations provide a redundant set of measurements and do not yield a sparse nor static spatial dependence structure. We propose to reduce dimensionality and induce a flexible Bayesian nonparametric covariance regression model by relating these location-specific trajectories to a lower-dimensional subspace through a latent factor model with predictor-dependent factor loadings. These loadings are in terms of a collection of basis functions that vary nonparametrically over the predictor space. Such low-rank approximations are in contrast to sparse precision assumptions, and are appropriate in a wide range of applications. Our formulation aims to address three challenges: scaling to large p domains, coping with missing values, and allowing an irregular grid of observations. The model is shown to be highly flexible, while leading to a computationally feasible implementation via Gibbs sampling. The ability to scale to large p domains and cope with missing values is fundamental in analyzing the Google Flu Trends data.",,,,,,"Fox, Emily/0000-0003-3188-9685",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2501,2542,,,,,,,,,,,,,,,,WOS:000369888000006,0
J,"Wiener, Y; Hanneke, S; El-Yaniv, R",,,,"Wiener, Yair; Hanneke, Steve; El-Yaniv, Ran",,,A Compression Technique for Analyzing Disagreement-Based Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new and improved characterization of the label complexity of disagreement-based active learning, in which the leading quantity is the version space compression set size. This quantity is defined as the size of the smallest subset of the training data that induces the same version space. We show various applications of the new characterization, including a tight analysis of CAL and refined label complexity bounds for linear separators under mixtures of Gaussians and axis-aligned rectangles under product densities. The version space compression set size, as well as the new characterization of the label complexity, can be naturally extended to agnostic learning problems, for which we show new speedup results for two well known active learning algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,713,745,,,,,,,,,,,,,,,,WOS:000369886300003,0
J,"Ailon, N; Chen, YD; Xu, H",,,,"Ailon, Nir; Chen, Yudong; Xu, Huan",,,Iterative and Active Graph Clustering Using Trace Norm Minimization Without Cluster Size Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates graph clustering under the planted partition model in the presence of small clusters. Traditional results dictate that for an algorithm to provably correctly recover the underlying clusters, all clusters must be sufficiently large-in particular, the cluster sizes need to be (Omega) over tilde(root n), where n is the number of nodes of the graph. We show that this is not really a restriction: by a refined analysis of a convex-optimization-based recovery approach, we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to provably recover almost all clusters via a peeling strategy: we recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the partial observation setting, in which only a (chosen) part of the graph is observed. The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often after large clusters are learned (and removed). We expect that the idea of iterative peeling-that is, sequentially identifying a subset of the clusters and reducing the problem to a smaller one-is useful more broadly beyond the specific implementations (based on convex optimization) used in this paper.",,,,,"xu, huan/R-5436-2016","xu, huan/0000-0002-5712-0308",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,455,490,,,,,,,,,,,,,,,,WOS:000369886000004,0
J,"van Laarhoven, T; Marchiori, E",,,,"van Laarhoven, Twan; Marchiori, Elena",,,Axioms for Graph Clustering Quality Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate properties that intuitively ought to be satisfied by graph clustering quality functions, that is, functions that assign a score to a clustering of a graph. Graph clustering, also known as network community detection, is often performed by optimizing such a function. Two axioms tailored for graph clustering quality functions are introduced, and the four axioms introduced in previous work on distance based clustering are reformulated and generalized for the graph setting. We show that modularity, a standard quality function for graph clustering, does not satisfy all of these six properties. This motivates the derivation of a new family of quality functions, adaptive scale modularity, which does satisfy the proposed axioms. Adaptive scale modularity has two parameters, which give greater flexibility in the kinds of clusterings that can be found. Standard graph clustering quality functions, such as normalized cut and unnormalized cut, are obtained as special cases of adaptive scale modularity. In general, the results of our investigation indicate that the considered axiomatic framework covers existing 'good' quality functions for graph clustering, and can be used to derive an interesting new family of quality functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,193,215,,,,,,,,,,,,,,,,WOS:000335457400006,0
J,"Cai, T; Zhou, WX",,,,"Cai, Tony; Zhou, Wen-Xin",,,A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed.,,,,,"Zhou, Wenxin/ABB-9228-2020; Zhou, Wenxin/N-9886-2015","Zhou, Wenxin/0000-0002-2761-485X; Zhou, Wenxin/0000-0002-2761-485X; Cai, Tommaso/0000-0002-7234-3526",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3619,3647,,,,,,,,,,,,,,,,WOS:000335457100005,0
J,"Lewis, JM; de Sa, VR; van der Maaten, L",,,,"Lewis, Joshua M.; de Sa, Virginia R.; van der Maaten, Laurens",,,Divvy: Fast and Intuitive Exploratory Data Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning field have a greater impact on research practices elsewhere.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3159,3163,,,,,,,,,,,,,,,,WOS:000328603600009,0
J,"Cai, T; Fan, JQ; Jiang, TF",,,,"Cai, Tony; Fan, Jianqing; Jiang, Tiefeng",,,Distributions of Angles in Random Packing on Spheres,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R-p as the number of points n -> infinity, while the dimension p is either fixed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that all high-dimensional random vectors are almost always nearly orthogonal to each other. Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed.",,,,,"Fan, Jianqing/B-2115-2008","Cai, Tommaso/0000-0002-7234-3526",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1837,1864,,,,,,,,,,,25324693,,,,,WOS:000323367000006,0
J,"Pan, W; Shen, XT; Liu, BH",,,,"Pan, Wei; Shen, Xiatong; Liu, Binghui",,,Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method's promising performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1865,1889,,,,,,,,,,,24358018,,,,,WOS:000323367000007,0
J,"Wang, Z; Crammer, K; Vucetic, S",,,,"Wang, Zhuang; Crammer, Koby; Vucetic, Slobodan",,,Breaking the Curse of Kernelization: Budgeted Stochastic Gradient Descent for Large-Scale SVM Training,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Online algorithms that process one example at a time are advantageous when dealing with very large data or with data streams. Stochastic Gradient Descent (SGD) is such an algorithm and it is an attractive choice for online Support Vector Machine (SVM) training due to its simplicity and effectiveness. When equipped with kernel functions, similarly to other SVM learning algorithms, SGD is susceptible to the curse of kernelization that causes unbounded linear growth in model size and update time with data size. This may render SGD inapplicable to large data sets. We address this issue by presenting a class of Budgeted SGD (BSGD) algorithms for large-scale kernel SVM training which have constant space and constant time complexity per update. Specifically, BSGD keeps the number of support vectors bounded during training through several budget maintenance strategies. We treat the budget maintenance as a source of the gradient error, and show that the gap between the BSGD and the optimal SVM solutions depends on the model degradation due to budget maintenance. To minimize the gap, we study greedy budget maintenance methods based on removal, projection, and merging of support vectors. We propose budgeted versions of several popular online SVM algorithms that belong to the SGD family. We further derive BSGD algorithms for multi-class SVM training. Comprehensive empirical results show that BSGD achieves higher accuracy than the state-of-the-art budgeted online algorithms and comparable to non-budget algorithms, while achieving impressive computational efficiency both in time and space during training and prediction.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,3103,3131,,,,,,,,,,,,,,,,WOS:000313200000010,0
J,"Zhang, ZH; Liu, DH; Dai, G; Jordan, MI",,,,"Zhang, Zhihua; Liu, Dehua; Dai, Guang; Jordan, Michael I.",,,Coherence Functions with Applications in Large-Margin Classification Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classification as C-learning, and we present efficient coordinate descent algorithms for the training of regularized C-learning models.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2705,2734,,,,,,,,,,,,,,,,WOS:000309580600008,0
J,"De Smedt, T; Daelemans, W",,,,"De Smedt, Tom; Daelemans, Walter",,,Pattern for Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern.",,,,,"Daelemans, Walter/N-5785-2014; De Smedt, Tom/F-1590-2018","De Smedt, Tom/0000-0001-8295-6660; Daelemans, Walter/0000-0002-9832-7890",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,2063,2067,,,,,,,,,,,,,,,,WOS:000307020700012,0
J,"Larochelle, H; Mandel, M; Pascanu, R; Bengio, Y",,,,"Larochelle, Hugo; Mandel, Michael; Pascanu, Razvan; Bengio, Yoshua",,,Learning Algorithms for the Classification Restricted Boltzmann Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.",,,,,"Michael Mandel, Professor/AAK-1239-2021","Michael Mandel, Professor/0000-0003-2073-5357",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,643,669,,,,,,,,,,,,,,,,WOS:000303772100006,0
J,"Mazumder, R; Hastie, T",,,,"Mazumder, Rahul; Hastie, Trevor",,,Exact Covariance Thresholding into Connected Components for Large-Scale Graphical Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the sparse inverse covariance regularization problem or graphical lasso with regularization parameter lambda. Suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at lambda is decomposed into connected components. We show that the vertex-partition induced by the connected components of the thresholded sample covariance graph (at lambda) is exactly equal to that induced by the connected components of the estimated concentration graph, obtained by solving the graphical lasso problem for the same lambda. This characterizes a very interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance gains. For a range of values of lambda, our proposal splits a large graphical lasso problem into smaller tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,781,794,,,,,,,,,,,25392704,,,,,WOS:000303772100011,0
J,"Balasubramanian, K; Donmez, P; Lebanon, G",,,,"Balasubramanian, Krishnakumar; Donmez, Pinar; Lebanon, Guy",,,Unsupervised Supervised Learning II: Margin-Based Classification Without Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many popular linear classifiers, such as logistic regression, boosting, or SVM, are trained by optimizing a margin-based risk function. Traditionally, these risk functions are computed based on a labeled data set. We develop a novel technique for estimating such risks using only unlabeled data and the marginal label distribution. We prove that the proposed risk estimator is consistent on high-dimensional data sets and demonstrate it on synthetic and real-world data. In particular, we show how the estimate is used for evaluating classifiers in transfer learning, and for training classifiers with no labeled data whatsoever.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3119,3145,,,,,,,,,,,,,,,,WOS:000298103700003,0
J,"Zhou, SH; Rutimann, P; Xu, M; Buhlmann, P",,,,"Zhou, Shuheng; Ruetimann, Philipp; Xu, Min; Buehlmann, Peter",,,High-dimensional Covariance Estimation Based On Gaussian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using l(1)-penalization methods. We propose and study the following method. We combine a multiple regression approach with ideas of thresholding and refitting: first we infer a sparse undirected graphical model structure via thresholding of each among many l(1)-norm penalized regression functions; we then estimate the covariance matrix and its inverse using the maximum likelihood estimator. We show that under suitable conditions, this approach yields consistent estimation in terms of graphical structure and fast convergence rates with respect to the operator and Frobenius norm for the covariance matrix and its inverse. We also derive an explicit bound for the Kullback Leibler divergence.",,,,,"B√ºhlmann, Peter/A-2107-2013; Zhou, Shuheng/FLN-6143-2022","B√ºhlmann, Peter/0000-0002-1782-6015; ",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2975,3026,,,,,,,,,,,,,,,,WOS:000298103200009,0
J,"de Campos, CP; Ji, Q",,,,"de Campos, Cassio P.; Ji, Qiang",,,Efficient Structure Learning of Bayesian Networks using Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-and-bound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the benefits of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,663,689,,,,,,,,,,,,,,,,WOS:000289635000001,0
J,"Taylor, GW; Hinton, GE; Roweis, ST",,,,"Taylor, Graham W.; Hinton, Geoffrey E.; Roweis, Sam T.",,,Two Distributed-State Models For Generating High-Dimensional Time Series,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued visible variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This conditional RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/(similar to)gwtaylor/publications/ jmlr2011.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,1025,1068,,,,,,,,,,,,,,,,WOS:000289635000009,0
J,"Srinivasan, A; Ramakrishnan, G",,,,"Srinivasan, Ashwin; Ramakrishnan, Ganesh",,,Parameter Screening and Optimisation for ILP using Designed Experiments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reports of experiments conducted with an Inductive Logic Programming system rarely describe how specific values of parameters of the system are arrived at when constructing models. Usually, no attempt is made to identify sensitive parameters, and those that are used are often given factory-supplied default values, or values obtained from some non-systematic exploratory analysis. The immediate consequence of this is, of course, that it is not clear if better models could have been obtained if some form of parameter selection and optimisation had been performed. Questions follow inevitably on the experiments themselves: specifically, are all algorithms being treated fairly, and is the exploratory phase sufficiently well-defined to allow the experiments to be replicated? In this paper, we investigate the use of parameter selection and optimisation techniques grouped under the study of experimental design. Screening and response surface methods determine, in turn, sensitive parameters and good values for these parameters. Screening is done here by constructing a stepwise regression model relating the utility of an ILP system's hypothesis to its input parameters, using systematic combinations of values of input parameters (technically speaking, we use a two-level fractional factorial design of the input parameters). The parameters used by the regression model are taken to be the sensitive parameters for the system for that application. We then seek an assignment of values to these sensitive parameters that maximise the utility of the ILP model. This is done using the technique of constructing a local response surface. The parameters are then changed following the path of steepest ascent until a locally optimal value is reached. This combined use of parameter selection and response surface-driven optimisation has a long history of application in industrial engineering, and its role in ILP is demonstrated using well-known benchmarks. The results suggest that computational overheads from this preliminary phase are not substantial, and that much can be gained, both on improving system performance and on enabling controlled experimentation, by adopting well-established procedures such as the ones proposed here.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,627,662,,,,,,,,,,,,,,,,WOS:000288896800009,0
J,"Seldin, Y; Tishby, N",,,,"Seldin, Yevgeny; Tishby, Naftali",,,PAC-Bayesian Analysis of Co-clustering and Beyond,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering, and pairwise clustering.(1) We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for finding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved state-of-the-art performance in the MovieLens collaborative filtering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of tree-shaped graphical models depend on a trade-off between their empirical data fit and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily extends to this problem and suggests that graph clustering should optimize the trade-off between empirical data fit and the mutual information that clusters preserve on graph nodes.",,,,,"Seldin, Yevgeny/G-8955-2015","Seldin, Yevgeny/0000-0003-3152-4635",,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3595,3646,,,,,,,,,,,,,,,,WOS:000286637200011,0
J,"Blanchard, G; Lee, G; Scott, C",,,,"Blanchard, Gilles; Lee, Gyemin; Scott, Clayton",,,Semi-Supervised Novelty Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common setting for novelty detection assumes that labeled examples from the nominal class are available, but that labeled examples of novelties are unavailable. The standard ( inductive) approach is to declare novelties where the nominal density is low, which reduces the problem to density level set estimation. In this paper, we consider the setting where an unlabeled and possibly contaminated sample is also available at learning time. We argue that novelty detection in this semi-supervised setting is naturally solved by a general reduction to a binary classification problem. In particular, a detector with a desired false positive rate can be achieved through a reduction to Neyman-Pearson classification. Unlike the inductive approach, semi-supervised novelty detection (SSND) yields detectors that are optimal (e.g., statistically consistent) regardless of the distribution on novelties. Therefore, in novelty detection, unlabeled data have a substantial impact on the theoretical properties of the decision rule. We validate the practical utility of SSND with an extensive experimental study. We also show that SSND provides distribution-free, learning-theoretic solutions to two well known problems in hypothesis testing. First, our results provide a general solution to the general two-sample problem, that is, the problem of determining whether two random samples arise from the same distribution. Second, a specialization of SSND coincides with the standard p-value approach to multiple testing under the so-called random effects model. Unlike standard rejection regions based on thresholded p-values, the general SSND framework allows for adaptation to arbitrary alternative distributions in multiple dimensions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,2973,3009,,,,,,,,,,,,,,,,WOS:000285643600001,0
J,"Theodorou, EA; Buchli, J; Schaal, S",,,,"Theodorou, Evangelos A.; Buchli, Jonas; Schaal, Stefan",,,A Generalized Path Integral Control Approach to Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the goal to generate more scalable algorithms with higher efficiency and fewer open parameters, reinforcement learning (RL) has recently moved towards combining classical techniques from optimal control and dynamic programming with modern learning techniques from statistical estimation theory. In this vein, this paper suggests to use the framework of stochastic optimal control with path integrals to derive a novel approach to RL with parameterized policies. While solidly grounded in value function estimation and optimal control based on the stochastic Hamilton-Jacobi-Bellman (HJB) equations, policy improvements can be transformed into an approximation problem of a path integral which has no open algorithmic parameters other than the exploration noise. The resulting algorithm can be conceived of as model-based, semi-model-based, or even model free, depending on how the learning problem is structured. The update equations have no danger of numerical instabilities as neither matrix inversions nor gradient learning rates are required. Our new algorithm demonstrates interesting similarities with previous RL research in the framework of probability matching and provides intuition why the slightly heuristically motivated probability matching approach can actually perform well. Empirical evaluations demonstrate significant performance improvements over gradient-based policy learning and scalability to high-dimensional control problems. Finally, a learning experiment on a simulated 12 degree-of-freedom robot dog illustrates the functionality of our algorithm in a complex robot learning scenario. We believe that Policy Improvement with Path Integrals (PI(2)) offers currently one of the most efficient, numerically robust, and easy to implement algorithms for RL based on trajectory roll-outs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3137,3181,,,,,,,,,,,,,,,,WOS:000285643600006,0
J,"Reid, MD; Williamson, RC",,,,"Reid, Mark D.; Williamson, Robert C.",,,Composite Binary Losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study losses for binary classification and class probability estimation and extend the understanding of them from margin losses to general composite losses which are the composition of a proper loss with a link function. We characterise when margin losses can be proper composite losses, explicitly show how to determine a symmetric loss in full from half of one of its partial losses, introduce an intrinsic parametrisation of composite binary losses and give a complete characterisation of the relationship between proper losses and classification calibrated losses. We also consider the question of the best surrogate binary loss. We introduce a precise notion of best and show there exist situations where two convex surrogate losses are incommensurable. We provide a complete explicit characterisation of the convexity of composite binary losses in terms of the link function and the weight function associated with the proper loss which make up the composite loss. This characterisation suggests new ways of surrogate tuning as well as providing an explicit characterisation of when Bregman divergences on the unit interval are convex in their second argument. Finally, in an appendix we present some new algorithm-independent results on the relationship between properness, convexity and robustness to misclassification noise for binary losses and show that all convex proper losses are non-robust to misclassification noise.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2010,11,,,,,,2387,2422,,,,,,,,,,,,,,,,WOS:000282523400001,0
J,"Omidiran, D; Wainwright, MJ",,,,"Omidiran, Dapo; Wainwright, Martin J.",,,High-dimensional Variable Selection with Sparse Random Projections: Measurement Sparsity and Statistical Efficiency,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of high-dimensional variable selection: given n noisy observations of a k-sparse vector beta* is an element of R(p), estimate the subset of non-zero entries of beta*. A significant body of work has studied behavior of l(1)-relaxations when applied to random measurement matrices that are dense (e. g., Gaussian, Bernoulli). In this paper, we analyze sparsified measurement ensembles, and consider the trade-off between measurement sparsity, as measured by the fraction gamma of non-zero entries, and the statistical efficiency, as measured by the minimal number of observations n required for correct variable selection with probability converging to one. Our main result is to prove that it is possible to let the fraction on non-zero entries gamma -> 0 at some rate, yielding measurement matrices with a vanishing fraction of non-zeros per row, while retaining the same statistical efficiency as dense ensembles. A variety of simulation results confirm the sharpness of our theoretical predictions.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2361,2386,,,,,,,,,,,,,,,,WOS:000282523300012,0
J,"Pernkopf, F; Bilmes, JA",,,,"Pernkopf, Franz; Bilmes, Jeff A.",,,Efficient Heuristics for Discriminative Structure Learning of Bayesian Network Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a simple order-based greedy heuristic for learning discriminative structure within generative Bayesian network classifiers. We propose two methods for establishing an order of N features. They are based on the conditional mutual information and classification rate (i.e., risk), respectively. Given an ordering, we can find a discriminative structure with O(Nk+1) score evaluations (where constant k is the tree-width of the sub-graph over the attributes). We present results on 25 data sets from the UCI repository, for phonetic classification using the TIMIT database, for a visual surface inspection task, and for two handwritten digit recognition tasks. We provide classification performance for both discriminative and generative parameter learning on both discriminatively and generatively structured networks. The discriminative structure found by our new procedures significantly outperforms generatively produced structures, and achieves a classification accuracy on par with the best discriminative (greedy) Bayesian network learning approach, but does so with a factor of similar to 10-40 speedup. We also show that the advantages of generative discriminatively structured Bayesian network classifiers still hold in the case of missing features, a case where generative classifiers have an advantage over discriminative classifiers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2323,2360,,,,,,,,,,,,,,,,WOS:000282523300011,0
J,"Wu, QA; Guinney, J; Maggioni, M; Mukherjee, S",,,,"Wu, Qiang; Guinney, Justin; Maggioni, Mauro; Mukherjee, Sayan",,,Learning Gradients: Predictive Models that Infer Geometry and Statistical Dependence,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The problems of dimension reduction and inference of statistical dependence are addressed by the modeling framework of learning gradients. The models we propose hold for Euclidean spaces as well as the manifold setting. The central quantity in this approach is an estimate of the gradient of the regression or classification function. Two quadratic forms are constructed from gradient estimates: the gradient outer product and gradient based diffusion maps. The first quantity can be used for supervised dimension reduction on manifolds as well as inference of a graphical model encoding dependencies that are predictive of a response variable. The second quantity can be used for nonlinear projections that incorporate both the geometric structure of the manifold as well as variation of the response variable on the manifold. We relate the gradient outer product to standard statistical quantities such as covariances and provide a simple and precise comparison of a variety of supervised dimensionality reduction methods. We provide rates of convergence for both inference of informative directions as well as inference of a graphical model of variable dependencies.,,,,,"Wu, Qiang/B-1620-2008","Wu, Qiang/0000-0002-4698-6966; Maggioni, Mauro/0000-0003-3258-9297; Mukherjee, Sayan/0000-0002-6715-3920",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2175,2198,,,,,,,,,,,,,,,,WOS:000282523300005,0
J,"Jaksch, T; Ortner, R; Auer, P",,,,"Jaksch, Thomas; Ortner, Ronald; Auer, Peter",,,Near-optimal Regret Bounds for Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s, s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret (O) over tilde (DS root AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of Omega (root DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of (O) over tilde (l(1/3)T(2/3)DS root A).",,,,,"Auer, Peter/AAC-1314-2019","Auer, Peter/0000-0001-8385-9635; Ortner, Ronald/0000-0001-6033-2208",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1563,1600,,,,,,,,,,,,,,,,WOS:000282521500010,0
J,"Biau, G; Cerou, F; Guyader, A",,,,"Biau, Gerard; Cerou, Frederic; Guyader, Arnaud",,,On the Rate of Convergence of the Bagged Nearest Neighbor Estimate,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bagging is a simple way to combine estimates in order to improve their performance. This method, suggested by Breiman in 1996, proceeds by resampling from the original data set, constructing a predictor from each subsample, and decide by combining. By bagging an n-sample, the crude nearest neighbor regression estimate is turned into a consistent weighted nearest neighbor regression estimate, which is amenable to statistical analysis. Letting the resampling size k(n) grows appropriately with n, it is shown that this estimate may achieve optimal rate of convergence, independently from the fact that resampling is done with or without replacement. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, adaptation results by data-splitting are presented.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,687,712,,,,,,,,,,,,,,,,WOS:000277186500010,0
J,"Schaul, T; Bayer, J; Wierstra, D; Sun, Y; Felder, M; Sehnke, F; Ruckstiess, T; Schmidhuber, J",,,,"Schaul, Tom; Bayer, Justin; Wierstra, Daan; Sun, Yi; Felder, Martin; Sehnke, Frank; Rueckstiess, Thomas; Schmidhuber, Juergen",,,PyBrain,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easy-to-use yet still powerful algorithms for machine learning tasks, including a variety of predefined environments and benchmarks to test and compare algorithms. Implemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (multidimensional) recurrent neural networks and deep belief networks.",,,,,"Peters, Jan/P-6027-2019; Schaul, Tom/C-4349-2011","Peters, Jan/0000-0002-5266-8091; Schaul, Tom/0000-0002-2961-8782",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,743,746,,,,,,,,,,,,,,,,WOS:000277186500012,0
J,"Greenshtein, E; Park, J",,,,"Greenshtein, Eitan; Park, Junyong",,,Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of classification using high dimensional features' space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classifiers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classification. Fan and Fan (2008), suggested a variable selection and classification method, called FAIR. The FAIR method improves the design of naive-Bayes classifiers in sparse setups. The improvement is due to reducing the noise in estimating the features' means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classifiers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efficient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many weakly informative variables, which variable selection type of classification procedures give up on using. We compare our method with FAIR and other classification methods in simulation for sparse and non sparse setups, and in real data examples involving classification of normal versus malignant tissues based on microarray data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1687,1704,,,,,,,,,,,,,,,,WOS:000270825000013,0
J,"Hoyle, DC",,,,"Hoyle, David C.",,,Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d -> infinity at fixed ratio alpha = N/d, with alpha < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach.",,,,,,"Hoyle, David/0000-0003-3483-5885",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2733,2759,,,,,,,,,,,,,,,,WOS:000263240700005,0
J,"Koo, I; Kil, RM",,,,"Koo, Imhoi; Kil, Rhee Man",,,Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multi-layer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data.",,,,,"Koo, Imhoi/E-5759-2011","Koo, Imhoi/0000-0002-5816-0627",,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2607,2633,,,,,,,,,,,,,,,,WOS:000262637600008,0
J,"Debruyne, M; Hubert, M; Suykens, JAK",,,,"Debruyne, Michiel; Hubert, Mia; Suykens, Johan A. K.",,,Model Selection in Kernel Based Regression using the Influence Function,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent results about the robustness of kernel methods involve the analysis of influence functions. By definition the influence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the influence function is used in a similar way to analyze the statistical efficiency of a method. Links between both worlds are explored. The influence function is related to the first term of a Taylor expansion. Higher order influence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate influence functions at a specific sample distribution to obtain an approximation of the leave-one-out error. A specific implementation is proposed using a L(1) loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data.",,,,,"Hubert, Mia/B-7434-2011; Suykens, Johan A.K./C-9781-2014","Hubert, Mia/0000-0001-6398-4850; Suykens, Johan A.K./0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2377,2400,,,,,,,,,,,,,,,,WOS:000262637300012,0
J,"Collins, M; Globerson, A; Koo, T; Carreras, X; Bartlett, PL",,,,"Collins, Michael; Globerson, Amir; Koo, Terry; Carreras, Xavier; Bartlett, Peter L.",,,Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efficient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or max-margin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O (1/epsilon) EG updates are required to reach a given accuracy e in the dual; in contrast, for log-linear models only O (log (1/epsilon)) updates are required. For both the max-margin and log-linear cases, our bounds suggest that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments confirm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efficiently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods.",,,,,,"Bartlett, Peter/0000-0002-8760-3140; Carreras, Xavier/0000-0001-7432-4540",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1775,1822,,,,,,,,,,,,,,,,WOS:000262636800006,0
J,"Xie, XC; Geng, Z",,,,"Xie, Xianchao; Geng, Zhi",,,A recursive method for structural learning of directed acyclic graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is first decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efficiency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,459,483,,,,,,,,,,,,,,,,WOS:000256642000004,0
J,"Buhlmann, P; Yu, B",,,,"Buehlmann, Peter; Yu, Bin",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,187,194,,,,,,,,,,,,,,,,WOS:000256641800007,0
J,"Buja, A; Stuetzle, W",,,,"Buja, Andreas; Stuetzle, Werner",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,165,170,,,,,,,,,,,,,,,,WOS:000256641800003,0
J,"Cawley, GC; Talbot, NLC",,,,"Cawley, Gavin C.; Talbot, Nicola L. C.",,,Preventing over-fitting during model selection via Bayesian regularisation of the hyper-parameters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efficiently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-fitting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the benefit of this approach.",,,,,,"Cawley, Gavin/0000-0002-4118-9095",,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,841,861,,,,,,,,,,,,,,,,WOS:000247002800006,0
J,"Grauman, K; Darrell, T",,,,"Grauman, Kristen; Darrell, Trevor",,,The pyramid match kernel: Efficient learning with sets of features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences-generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to find implicit correspondences based on the finest resolution histogram cell where a matched pair first appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classification and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and significantly more efficient than current approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,725,760,,,,,,,,,,,,,,,,WOS:000247002800001,0
J,"Melnik, O; Vardi, Y; Zhang, CH",,,,"Melnik, Ofer; Vardi, Yehuda; Zhang, Cun-Hui",,,Concave learners for rankboost,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Rankboost has been shown to be an effective algorithm for combining ranks. However, its ability to generalize well and not overfit is directly related to the choice of weak learner, in the sense that regularization of the rank function is due to the regularization properties of its weak learners. We present a regularization property called consistency in preference and confidence that mathematically translates into monotonic concavity, and describe a new weak ranking learner ( MWGR) that generates ranking functions with this property. In experiments combining ranks from multiple face recognition algorithms and an experiment combining text information retrieval systems, rank functions using MWGR proved superior to binary weak learners.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,791,812,,,,,,,,,,,,,,,,WOS:000247002800004,0
J,"Arias, M; Khardon, R; Maloberti, J",,,,"Arias, Marta; Khardon, Roni; Maloberti, Jerome",,,Learning horn expressions with LOGAN-H,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The paper introduces LOGAN-H-a system for learning first-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The LOGAN-H system implements these algorithms and adds several facilities and optimizations that allow efficient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classification accuracy.",,,,,"Arias, Marta/C-4242-2008","Arias, Marta/0000-0001-7359-1815",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,549,587,,,,,,,,,,,,,,,,WOS:000247002700006,0
J,"Raiko, T; Valpola, H; Harva, M; Karhunen, J",,,,"Raiko, Tapani; Valpola, Harri; Harva, Markus; Karhunen, Juha",,,Building blocks for variational Bayesian learning of latent variable models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method.",,,,,"Raiko, Tapani/E-7237-2012","Raiko, Tapani/0000-0002-0321-304X",,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,155,201,,,,,,,,,,,,,,,,WOS:000247002500006,0
J,"Braun, ML",,,,"Braun, Mikio L.",,,Accurate error bounds for the eigenvalues of the kernel matrix,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The eigenvalues of the kernel matrix play an important role in a number of kernel methods, in particular, in kernel principal component analysis. It is well known that the eigenvalues of the kernel matrix converge as the number of samples tends to infinity. We derive probabilistic finite sample size bounds on the approximation error of individual eigenvalues which have the important property that the bounds scale with the eigenvalue under consideration, reflecting the actual behavior of the approximation errors as predicted by asymptotic results and observed in numerical simulations. Such scaling bounds have so far only been known for tail sums of eigenvalues. Asymptotically, the bounds presented here have a slower than stochastic rate, but the number of sample points necessary to make this disadvantage noticeable is often unrealistically large. Therefore, under practical conditions, and for all but the largest few eigenvalues, the bounds presented here form a significant improvement over existing non-scaling bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2303,2328,,,,,,,,,,,,,,,,WOS:000245390700002,0
J,"Chen, DR; Sun, T",,,,"Chen, Di-Rong; Sun, Tao",,,Consistency of multiclass empirical risk minimization methods based on convex loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The consistency of classification algorithm plays a central role in statistical learning theory. A consistent algorithm guarantees us that taking more samples essentially suffices to roughly reconstruct the unknown distribution. We consider the consistency of ERM scheme over classes of combinations of very simple rules (base classifiers) in multiclass classification. Our approach is, under some mild conditions, to establish a quantitative relationship between classification errors and convex risks. In comparison with the related previous work, the feature of our result is that the conditions are mainly expressed in terms of the differences between some values of the convex function.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2435,2447,,,,,,,,,,,,,,,,WOS:000245390700006,0
J,"Pe'er, D; Tanay, A; Regev, A",,,,"Pe'er, D; Tanay, A; Regev, A",,,MinReg: A scalable algorithm for learning parsimonious regulatory networks in yeast and mammals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, there has been a growing interest in applying Bayesian networks and their extensions to reconstruct regulatory networks from gene expression data. Since the gene expression domain involves a large number of variables and a limited number of samples, it poses both computational and statistical challenges to Bayesian network learning algorithms. Here we define a constrained family of Bayesian network structures suitable for this domain and devise an efficient search algorithm that utilizes these structural constraints to find high scoring networks from data. Interestingly, under reasonable assumptions on the underlying probability distribution, we can provide performance guarantees on our algorithm. Evaluation on real data from yeast and mouse, demonstrates that our method cannot only reconstruct a high quality model of the yeast regulatory network, but is also the first method to scale to the complexity of mammalian networks and successfully reconstructs a reasonable model over thousands of variables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,167,189,,,,,,,,,,,,,,,,WOS:000236331700001,0
J,"Maurer, A",,,,"Maurer, A",,,Bounds for linear multi-task learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task specific linear-thresholding classifiers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-specific classifiers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,117,139,,,,,,,,,,,,,,,,WOS:000236331400005,0
J,"Banerjee, Arindam; Dhillon, IS; Ghosh, J; Sra, S",,,,"Banerjee, A; Dhillon, IS; Ghosh, J; Sra, S",,,Clustering on the unit hypersphere using von Mises-Fisher distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Several large scale data mining applications, such as text categorization and gene expression analysis, involve high-dimensional data that is also inherently directional in nature. Often such data is L-2 normalized so that it lies on the surface of a unit hypersphere. Popular models such as (mixtures of) multi-variate Gaussians are inadequate for characterizing such data. This paper proposes a generative mixture-model approach to clustering directional data based on the von Mises-Fisher (vMF) distribution, which arises naturally for data distributed on the unit hypersphere. In particular, we derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the mean and concentration parameters of this mixture. Numerical estimation of the concentration parameters is non-trivial in high dimensions since it involves functional inversion of ratios of Bessel functions. We also formulate two clustering algorithms corresponding to the variants of EM that we derive. Our approach provides a theoretical basis for the use of cosine similarity that has been widely employed by the information retrieval community, and obtains the spherical kmeans algorithm (kmeans with cosine similarity) as a special case of both variants. Empirical results on clustering of high-dimensional text and gene-expression data based on a mixture of vMF distributions show that the ability to estimate the concentration parameter for each vMF component, which is not present in existing approaches, yields superior results, especially for difficult clustering tasks in high-dimensional spaces.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1345,1382,,,,,,,,,,,,,,,,WOS:000236330100004,0
J,"Segal, E; Pe'er, D; Regev, A; Koller, D; Friedman, N",,,,"Segal, E; Pe'er, D; Regev, A; Koller, D; Friedman, N",,,Learning module networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Methods for learning Bayesian networks can discover dependency structure between observed variables. Although these methods are useful in many applications, they run into computational and statistical problems in domains that involve a large number of variables. In this paper, 1 we consider a solution that is applicable when many variables have similar behavior. We introduce a new class of models, module networks, that explicitly partition the variables into modules, so that the variables in each module share the same parents in the network and the same conditional probability distribution. We define the semantics of module networks, and describe an algorithm that learns the modules' composition and their dependency structure from data. Evaluation on real data in the domains of gene expression and the stock market shows that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks.",,,,,"Segal, Eran/AAF-4855-2019; Friedman, Nir/H-9692-2012","Friedman, Nir/0000-0002-9678-3550",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,557,588,,,,,,,,,,,,,,,,WOS:000236329600007,0
J,"Ye, JP",,,,"Ye, JP",,,Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A generalized discriminant analysis based on a new optimization criterion is presented. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) when the scatter matrices are singular. An efficient algorithm for the new optimization problem is presented. The solutions to the proposed criterion form a family of algorithms for generalized LDA, which can be characterized in a closed form. We study two specific algorithms, namely Uncorrelated LDA (ULDA) and Orthogonal LDA (OLDA). ULDA was previously proposed for feature extraction and dimension reduction, whereas OLDA is a novel algorithm proposed in this paper. The features in the reduced space of ULDA are uncorrelated, while the discriminant vectors of OLDA are orthogonal to each other. We have conducted a comparative study on a variety of real-world data sets to evaluate ULDA and OLDA in terms of classification accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,483,502,,,,,,,,,,,,,,,,WOS:000236329600005,0
J,"Keerthi, SS; DeCoste, D",,,,"Keerthi, SS; DeCoste, D",,,A modified finite newton method for fast solution of large scale linear SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops a fast method for solving linear SVMs with L-2 loss function that is suited for large scale data mining tasks such as text classification. This is done by modifying the finite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight, SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modified Huber's loss function and the L-1 loss function, and also for solving ordinal regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2005,6,,,,,,341,361,,,,,,,,,,,,,,,,WOS:000236329400004,0
J,"Steinwart, I; Hush, D; Scovel, C",,,,"Steinwart, I; Hush, D; Scovel, C",,,A classification framework for anomaly detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of finding level sets for the data generating density. We interpret this learning problem as a binary classification problem and compare the corresponding classification risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classification risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i. e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justification for the well-known heuristic of artificially sampling labeled samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM.",,,,,,"Steinwart, Ingo/0000-0002-4436-7109",,,,,,,,,,,,,1532-4435,,,,,FEB,2005,6,,,,,,211,232,,,,,,,,,,,,,,,,WOS:000236329000002,0
J,"Huang, KZ; Yang, HQ; King, I; Lyu, MR; Chan, LW",,,,"Huang, KZ; Yang, HQ; King, I; Lyu, MR; Chan, LW",,,The minimum error minimax probability machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We construct a distribution-free Bayes optimal classifier called the Minimum Error Minimax Probability Machine ( MEMPM) in a worst-case setting, i.e., under all possible choices of class-conditional densities with a given mean and covariance matrix. By assuming no specific distributions for the data, our model is thus distinguished from traditional Bayes optimal approaches, where an assumption on the data distribution is a must. This model is extended from the Minimax Probability Machine (MPM), a recently-proposed novel classifier, and is demonstrated to be the general case of MPM. Moreover, it includes another special case named the Biased Minimax Probability Machine, which is appropriate for handling biased classification. One appealing feature of MEMPM is that it contains an explicit performance indicator, i.e., a lower bound on the worst-case accuracy, which is shown to be tighter than that of MPM. We provide conditions under which the worst-case Bayes optimal classifier converges to the Bayes optimal classifier. We demonstrate how to apply a more general statistical framework to estimate model input parameters robustly. We also show how to extend our model to nonlinear classification by exploiting kernelization techniques. A series of experiments on both synthetic data sets and real world benchmark data sets validates our proposition and demonstrates the effectiveness of our model.",,,,,"King, Irwin/C-9681-2015; Yang, Haiqin/V-4250-2019; Huang, Kaizhu/O-4721-2014","King, Irwin/0000-0001-8106-6447; Huang, Kaizhu/0000-0002-3034-9639",,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1253,1286,,,,,,,,,,,,,,,,WOS:000236328300003,0
J,"Laub, J; Muller, KR",,,,"Laub, J; Muller, KR",,,Feature discovery in non-metric pairwise data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Pairwise proximity data, given as similarity or dissimilarity matrix, can violate metricity. This occurs either due to noise, fallible estimates, or due to intrinsic non-metric features such as they arise from human judgments. So far the problem of non-metric pairwise data has been tackled by essentially omitting the negative eigenvalues or shifting the spectrum of the associated (pseudo) covariance matrix for a subsequent embedding. However, little attention has been paid to the negative part of the spectrum itself. In particular no answer was given to whether the directions associated to the negative eigenvalues would at all code variance other than noise related. We show by a simple, exploratory analysis that the negative eigenvalues can code for relevant structure in the data, thus leading to the discovery of new features, which were lost by conventional data analysis techniques. The information hidden in the negative eigenvalue part of the spectrum is illustrated and discussed for three data sets, namely USPS handwritten digits, text-mining and data from cognitive psychology.",,,,,"Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013","Mueller, Klaus-Robert/0000-0002-3861-7685; ",,,,,,,,,,,,,1532-4435,,,,,JUL,2004,5,,,,,,801,818,,,,,,,,,,,,,,,,WOS:000236327800003,0
J,"Mendelson, S",,,,"Mendelson, S",,,On the performance of kernel classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,We present sharp bounds on the localized Rademacher averages of the unit ball in a reproducing kernel Hilbert space in terms of the eigenvalues of the integral operator associated with the kernel. We use this result to estimate the performance of the empirical minimization algorithm when the base class is the unit ball of the reproducing kernel Hilbert space.,,,,,,"Mendelson, Shahar/0000-0002-5673-7576",,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,759,771,,10.1162/1532443041424337,0,,,,,,,,,,,,,WOS:000223238800002,0
J,"Takimoto, E; Warmuth, MK",,,,"Takimoto, E; Warmuth, MK",,,Path kernels and multiplicative updates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,"Kernels are typically applied to linear algorithms whose weight vector is a linear combination of the feature vectors of the examples. On-line versions of these algorithms are sometimes called additive updates because they add a multiple of the last feature vector to the current weight vector. In this paper we have found a way to use special convolution kernels to efficiently implement,'multiplicative updates. The kernels are defined by a directed graph. Each edge contributes an input. The inputs along a path form a product feature and all such products build the feature vector associated with the inputs. We also have a set of probabilities on the edges so that the outflow from each vertex is one. We then discuss multiplicative updates on these graphs where the prediction is essentially a kernel computation and the update contributes a factor to each edge. After adding the factors to the edges, the total outflow out of each vertex is not one any more. However some clever algorithms re-normalize the weights on the paths so that the total outflow out of each vertex is one again. Finally, we show that if the digraph is built from a regular expressions, then this can be used for speeding up the kernel and re-normalization computations. We reformulate a large number of multiplicative update algorithms using path kernels and characterize the applicability of our method. The examples include efficient algorithms for learning disjunctions and a recent algorithm that predicts as well as the best pruning of a series parallel digraphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,773,818,,10.1162/1532443041424328,0,,,,,,,,,,,,,WOS:000223238800003,0
J,"Haddawy, P; Ha, V; Restificar, A; Geisler, B; Miyamoto, J",,,,"Haddawy, P; Ha, V; Restificar, A; Geisler, B; Miyamoto, J",,,Preference elicitation via theory refinement,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Knowledge-Data Fusion,"JUN, 2000","Stanford Univ, Stanford, CA",,Stanford Univ,,,"We present an approach to elicitation of user preference models in which assumptions can be used to guide but not constrain the elicitation process. We demonstrate that when domain knowledge is available, even in the form of weak and somewhat inaccurate assumptions, significantly less data is required to build an accurate model of user preferences than when no domain knowledge is provided. This approach is based on the KBANN (Knowledge-Based Artificial Neural Network) algorithm pioneered by Shavlik and Towell (1989). We demonstrate this approach through two examples, one involves preferences under certainty, and the other involves preferences under uncertainty. In the case of certainty, we show how to encode assumptions concerning preferential independence and monotonicity in a KBANN network, which can be trained using a variety of preferential information including simple binary classification. In the case of uncertainty, we show how to construct a KBANN network that encodes certain types of dominance relations and attitude toward risk. The resulting network can be trained using answers to standard gamble questions and can be used as an approximate representation of a person's preferences. We empirically evaluate our claims by comparing the KBANN networks with simple backpropagation artificial neural networks in terms of learning rate and accuracy. For the case of uncertainty, the answers to standard gamble questions used in the experiment are taken from an actual medical data set first used by Miyamoto and Eraker (1988). In the case of certainty, we define a measure to which a set of preferences violate a domain theory, and examine the robustness of the KBANN network as this measure of domain theory violation varies.",,,,,,"Haddawy, Peter/0000-0003-2203-006X",,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,317,337,,10.1162/153244304773633843,0,,,,,,,,,,,,,WOS:000221043900004,0
J,"Anthony, M",,,,"Anthony, M",,,Generalization error bounds for threshold decision lists,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we consider the generalization accuracy of classification methods based on the iterative use of linear classifiers. The resulting classifiers, which we call threshold decision lists act as follows. Some points of the data set to be classified are given a particular classification according to a linear threshold function (or hyperplane). These are then removed from consideration, and the procedure is iterated until all points are classified. Geometrically, we can imagine that at each stage, points of the same classification are successively chopped off from the data set by a hyperplane. We analyse theoretically the generalization properties of data classification techniques that are based on the use of threshold decision lists and on the special subclass of multilevel threshold functions. We present bounds on the generalization error in a standard probabilistic learning framework. The primary focus in this paper is on obtaining generalization error bounds that depend on the levels of separation - or margins - achieved by the successive linear classifiers. We also improve and extend previously published theoretical bounds on the generalization ability of perceptron decision trees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2004,5,,,,,,189,217,,,,,,,,,,,,,,,,WOS:000236327000003,0
J,"Lanckriet, GRG; Cristianini, N; Bartlett, P; El Ghaoui, L; Jordan, MI",,,,"Lanckriet, GRG; Cristianini, N; Bartlett, P; El Ghaoui, L; Jordan, MI",,,Learning the kernel matrix with semidefinite programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space-classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm- using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.",,,,,"Jordan, Michael I/C-5253-2013","Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,JAN,2004,5,,,,,,27,72,,,,,,,,,,,,,,,,WOS:000236326900001,0
J,"Blockeel, H; Struyf, J",,,,"Blockeel, H; Struyf, J",,,Efficient algorithms for decision tree cross-validation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. We identify a number of parameters that influence the obtainable speedups, and validate and refine our analysis with experiments on a variety of data sets with two different implementations. Besides cross-validation, we also briefly explore the usefulness of these techniques for bagging. We conclude with some guidelines concerning when these optimizations should be considered.",,,,,"Blockeel, Hendrik LW/L-6993-2013","Blockeel, Hendrik LW/0000-0003-0378-3699",,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,621,650,,10.1162/jmlr.2003.3.4-5.621,0,,,,,,,,,,,,,WOS:000184926200002,0
J,"Driggs, D; Liang, JW; Schonlieb, CB",,,,"Driggs, Derek; Liang, Jingwei; Schonlieb, Carola-Bibiane",,,On Biased Stochastic Gradient Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a uniform analysis of biased stochastic gradient methods for minimizing convex, strongly convex, and non-convex composite objectives, and identify settings where bias is useful in stochastic gradient estimation. The framework we present allows us to extend proximal support to biased algorithms, including SAG and SARAH, for the first time in the convex setting. We also use our framework to develop a new algorithm, Stochastic Average Recursive GradiEnt (SARGE), that achieves the oracle complexity lower-bound for nonconvex, finite-sum objectives and requires strictly fewer calls to a stochastic gradient oracle per iteration than SVRG and SARAH. We support our theoretical results with numerical experiments that demonstrate the benefits of certain biased gradient estimators.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752357400001,0
J,"Manita, OA; Peletier, MA; Portegies, JW; Sanders, J; Senen-Cerda, A",,,,"Manita, Oxana A.; Peletier, Mark A.; Portegies, Jacobus W.; Sanders, Jaron; Senen-Cerda, Albert",,,Universal Approximation in Dropout Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We prove two universal approximation theorems for a range of dropout neural networks. These are feed-forward neural networks in which each edge is given a random {0, 1}-valued filter, that have two modes of operation: in the first each edge output is multiplied by its random filter, resulting in a random output, while in the second each edge output is multiplied by the expectation of its filter, leading to a deterministic output. It is common to use the random mode during training and the deterministic mode during testing and prediction. Both theorems are of the following form: Given a function to approximate and a threshold epsilon > 0, there exists a dropout network that is epsilon-close in probability and in L-q. The first theorem applies to dropout networks in the random mode. It assumes little on the activation function, applies to a wide class of networks, and can even be applied to approximation schemes other than neural networks. The core is an algebraic property that shows that deterministic networks can be exactly matched in expectation by random networks. The second theorem makes stronger assumptions and gives a stronger result. Given a function to approximate, it provides existence of a network that approximates in both modes simultaneously. Proof components are a recursive replacement of edges by independent copies, and a special first-layer replacement that couples the resulting larger network to the input. The functions to be approximated are assumed to be elements of general normed spaces, and the approximations are measured in the corresponding norms. The networks are constructed explicitly. Because of the different methods of proof, the two results give independent insight into the approximation properties of random dropout networks. With this, we establish that dropout neural networks broadly satisfy a universal-approximation property.",,,,,,"Sanders, Jaron/0000-0003-0187-2065",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752299000001,0
J,"Liu, HF; Jing, LP; Wen, JX; Xu, PY; Wang, JQ; Yu, J; Ng, MK",,,,"Liu, Huafeng; Jing, Liping; Wen, Jingxuan; Xu, Pengyu; Wang, Jiaqi; Yu, Jian; Ng, Michael K.",,,Interpretable Deep Generative Recommendation Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"User preference modeling in recommendation system aims to improve customer experience through discovering users' intrinsic preference based on prior user behavior data. This is a challenging issue because user preferences usually have complicated structure, such as inter-user preference similarity and intra-user preference diversity. Among them, inter-user similarity indicates different users may share similar preference, while intra-user diversity indicates one user may have several preferences. In literatures, deep generative models have been successfully applied in recommendation systems due to its flexibility on statistical dis-tributions and strong ability for non-linear representation learning. However, they suffer from the simple generative process when handling complex user preferences. Meanwhile, the latent representations learned by deep generative models are usually entangled, and may range from observed-level ones that dominate the complex correlations between users, to latent-level ones that characterize a user's preference, which makes the deep model hard to explain and unfriendly for recommendation. Thus, in this paper, we propose an Interpretable Deep Generative Recommendation Model (InDGRM) to characterize inter-user preference similarity and intra-user preference diversity, which will simultane-ously disentangle the learned representation from observed-level and latent-level. In InD-GRM, the observed-level disentanglement on users is achieved by modeling the user-cluster structure (i.e., inter-user preference similarity) in a rich multimodal space, so that users with similar preferences are assigned into the same cluster. The observed-level disentangle-ment on items is achieved by modeling the intra-user preference diversity in a prototype learning strategy, where different user intentions are captured by item groups (one group refers to one intention). To promote disentangled latent representations, InDGRM adopts structure and sparsity-inducing penalty and integrates them into the generative procedure, which has ability to enforce each latent factor focus on a limited subset of items (e.g., one item group) and benefit latent-level disentanglement. Meanwhile, it can be efficiently inferred by minimizing its penalized upper bound with the aid of local variational optimiza-tion technique. Theoretically, we analyze the generalization error bound of InDGRM to guarantee its performance. A series of experimental results on four widely-used benchmark datasets demonstrates the superiority of InDGRM on recommendation performance and interpretability.(1)",,,,,"Ng, Michael/B-7189-2009","Ng, Michael/0000-0001-6833-5227",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706888200001,0
J,"Luo, T; Xu, ZQO; Ma, Z; Zhang, YY",,,,"Luo, Tao; Xu, Zhi-Qin John; Ma, Zheng; Zhang, Yaoyu",,,Phase Diagram for Two-layer ReLU Neural Networks at Infinite-width Limit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"How neural network behaves during the training over different choices of hyperparameters is an important question in the study of neural networks. In this work, inspired by the phase diagram in statistical mechanics, we draw the phase diagram for the two-layer ReLU neural network at the infinite-width limit for a complete characterization of its dynamical regimes and their dependence on hyperparameters related to initialization. Through both experimental and theoretical approaches, we identify three regimes in the phase diagram, i.e., linear regime, critical regime and condensed regime, based on the relative change of input weights as the width approaches infinity, which tends to 0, O (1) and +infinity, respectively. In the linear regime, NN training dynamics is approximately linear similar to a random feature model with an exponential loss decay. In the condensed regime, we demonstrate through experiments that active neurons are condensed at several discrete orientations. The critical regime serves as the boundary between above two regimes, which exhibits an intermediate nonlinear behavior with the mean-field model as a typical example. Overall, our phase diagram for the two-layer ReLU NN serves as a map for the future studies and is a first step towards a more systematical investigation of the training behavior and the implicit regularization of NNs of different structures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,47,,,,,,,,,,,,,,,,WOS:000656374200001,0
J,"Muthukumar, V; Narang, A; Subramanian, V; Belkin, M; Hsu, D; Sahai, A",,,,"Muthukumar, Vidya; Narang, Adhyyan; Subramanian, Vignesh; Belkin, Mikhail; Hsu, Daniel; Sahai, Anant",,,Classification vs regression in overparameterized regimes: Does the loss function matter?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We compare classification and regression tasks in an overparameterized linear model with Gaussian features. On the one hand, we show that with sufficient overparameterization all training points are support vectors: solutions obtained by least-squares minimum-norm interpolation, typically used for regression, are identical to those produced by the hard margin support vector machine (SVM) that minimizes the hinge loss, typically used for training classifiers. On the other hand, we show that there exist regimes where these interpolating solutions generalize well when evaluated by the 0-1 test loss function, but do not generalize if evaluated by the square loss function, i.e. they approach the null risk. Our results demonstrate the very different roles and properties of loss functions used at the training phase (optimization) and the testing phase (generalization).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706447500001,0
J,"Puolamaki, K; Oikarinen, E; Henelius, A",,,,"Puolamaki, Kai; Oikarinen, Emilia; Henelius, Andreas",,,Guided Visual Exploration of Relations in Data Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Efficient explorative data analysis systems must take into account both what a user knows and wants to know. This paper proposes a principled framework for interactive visual exploration of relations in data, through views most informative given the user's current knowledge and objectives. The user can input pre-existing knowledge of relations in the data and also formulate specific exploration interests, which are then taken into account in the exploration. The idea is to steer the exploration process towards the interests of the user, instead of showing uninteresting or already known relations. The user's knowledge is modelled by a distribution over data sets parametrised by subsets of rows and columns of data, called tile constraints. We provide a computationally efficient implementation of this concept based on constrained randomisation. Furthermore, we describe a novel dimensionality reduction method for finding the views most informative to the user, which at the limit of no background knowledge and with generic objectives reduces to PCA. We show that the method is suitable for interactive use and is robust to noise, outperforms standard projection pursuit visualisation methods, and gives understandable and useful results in analysis of real-world data. We provide an open-source implementation of the framework.",,,,,"Puolamaki, Kai/C-9016-2017","Puolamaki, Kai/0000-0003-1819-1047",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,96,,,,,,,,,,,,,,,WOS:000663148900001,0
J,"Tuck, J; Barratt, S; Boyd, S",,,,"Tuck, Jonathan; Barratt, Shane; Boyd, Stephen",,,A Distributed Method for Fitting Laplacian Regularized Stratified Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stratified models are models that depend in an arbitrary way on a set of selected categorical features, and depend linearly on the other features. In a basic and traditional formulation a separate model is fit for each value of the categorical feature, using only the data that has the specific categorical value. To this formulation we add Laplacian regularization, which encourages the model parameters for neighboring categorical values to be similar. Laplacian regularization allows us to specify one or more weighted graphs on the stratification feature values. For example, stratifying over the days of the week, we can specify that the Sunday model parameter should be close to the Saturday and Monday model parameters. The regularization improves the performance of the model over the traditional stratified model, since the model for each value of the categorical 'borrows strength' from its neighbors. In particular, it produces a model even for categorical values that did not appear in the training data set. We propose an efficient distributed method for fitting stratified models, based on the alternating direction method of multipliers (ADMM). When the fitting loss functions are convex, the stratified model fitting problem is convex, and our method computes the global minimizer of the loss plus regularization; in other cases it computes a local minimizer. The method is very efficient, and naturally scales to large data sets or numbers of stratified feature values. We illustrate our method with a variety of examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656358900001,0
J,"Wang, YF; Huang, HY; Rudin, C; Shaposhnik, Y",,,,"Wang, Yingfan; Huang, Haiyang; Rudin, Cynthia; Shaposhnik, Yaron",,,"Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dimension reduction (DR) techniques such as t-SNE, UMAP, and TriMap have demonstrated impressive visualization performance on many real-world datasets. One tension that has always faced these methods is the trade-off between preservation of global structure and preservation of local structure: these methods can either handle one or the other, but not both. In this work, our main goal is to understand what aspects of DR methods are important for preserving both local and global structure: it is difficult to design a better method without a true understanding of the choices we make in our algorithms and their empirical impact on the low-dimensional embeddings they produce. Towards the goal of local structure preservation, we provide several useful design principles for DR loss functions based on our new understanding of the mechanisms behind successful DR methods. Towards the goal of global structure preservation, our analysis illuminates that the choice of which components to preserve is important. We leverage these insights to design a new algorithm for DR, called Pairwise Controlled Manifold Approximation Projection (PaCMAP), which preserves both local and global structure. Our work provides several unexpected insights into what design choices both to make and avoid when constructing DR algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706883000001,0
J,"Xiao, D; Ke, Y; Li, RZ",,,,"Xiao, Di; Ke, Yuan; Li, Runze",,,Homogeneity Structure Learning in Large-scale Panel Data with Heavy-tailed Errors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale panel data is ubiquitous in many modern data science applications. Conventional panel data analysis methods fail to address the new challenges, like individual impacts of covariates, endogeneity, embedded low-dimensional structure, and heavy-tailed errors, arising from the innovation of data collection platforms on which applications operate. In response to these challenges, this paper studies large-scale panel data with an interactive effects model. This model takes into account the individual impacts of covariates on each spatial node and removes the exogenous condition by allowing latent factors to affect both covariates and errors. Besides, we waive the sub-Gaussian assumption and allow the errors to be heavy-tailed. Further, we propose a data-driven procedure to learn a parsimonious yet flexible homogeneity structure embedded in high-dimensional individual impacts of covariates. The homogeneity structure assumes that there exists a partition of regression coefficients where the coefficients are the same within each group but different between the groups. The homogeneity structure is flexible as it contains many widely assumed low-dimensional structures (sparsity, global impact, etc.) as its special cases. Non-asymptotic properties are established to justify the proposed learning procedure. Extensive numerical experiments demonstrate the advantage of the proposed learning procedure over conventional methods especially when the data are generated from heavy-tailed distributions.",,,,,"Li, Runze/HCH-8063-2022; Li, Runze/C-5444-2013; Ke, Yuan/AFK-3195-2022","Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202; ",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500013,0
J,"Al-Shedivat, M; Dubey, A; Xing, E",,,,"Al-Shedivat, Maruan; Dubey, Avinava; Xing, Eric",,,Contextual Explanation Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modern learning algorithms excel at producing accurate but complex models of the data. However, deploying such models in the real-world requires extra care: we must ensure their reliability, robustness, and absence of undesired biases. This motivates the development of models that are equally accurate but can be also easily inspected and assessed beyond their predictive performance. To this end, we introduce contextual explanation networks (CENs)-a class of architectures that learn to predict by generating and utilizing intermediate, simplified probabilistic models. Specifically, CENs generate parameters for intermediate graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain simultaneously. Our approach offers two major advantages: (i) for each prediction, valid, instance-specific explanation is generated with no computational overhead and (ii) prediction via explanation acts as a regularizer and boosts performance in data-scarce settings. We analyze the proposed framework theoretically and experimentally. Our results on image and text classification and survival analysis tasks demonstrate that CENs are not only competitive with the state-of-the-art methods but also offer additional insights behind each prediction, that can be valuable for decision support. We also show that while post, hoc methods may produce misleading explanations in certain cases, CENs are consistent and allow to detect such cases systematically.",,,,,"salama, khaled Nabil/K-3689-2019","salama, khaled Nabil/0000-0001-7742-1282",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,194,,,,,,,,,,,,,,,WOS:000590001500001,0
J,"Andreux, M; Angles, T; Exarchakis, G; Leonarduzzi, R; Rochette, G; Thiry, L; Zarka, J; Mallat, S; Anden, J; Belilovsky, E; Bruna, J; Lostanlen, V; Chaudhary, M; Hirn, MJ; Oyallon, E; Zhang, SX; Cella, C; Eickenberg, M",,,,"Andreux, Mathieu; Angles, Tomas; Exarchakis, Georgios; Leonarduzzi, Roberto; Rochette, Gaspar; Thiry, Louis; Zarka, John; Mallat, Stephane; Anden, Joakim; Belilovsky, Eugene; Bruna, Joan; Lostanlen, Vincent; Chaudhary, Muawiz; Hirn, Matthew J.; Oyallon, Edouard; Zhang, Sixin; Cella, Carmine; Eickenberg, Michael",,,Kymatio: Scattering Transforms in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The wavelet scattering transform is an invariant and stable signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks, including PyTorch and TensorFlow/Keras. The transforms are implemented on both CPUs and GPUs, the latter offering a significant speedup over the former. The package also has a small memory footprint. Source code, documentation, and examples are available under a BSD license at https://www.kymat.io.",,,,,,"Hirn, Matthew/0000-0003-0290-4292",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000012,0
J,"Ciosek, K; Whiteson, S",,,,"Ciosek, Kamil; Whiteson, Shimon",,,Expected Policy Gradients for Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to eH, where H is the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000005,0
J,"Diakonikolas, J; Guzman, C",,,,"Diakonikolas, Jelena; Guzman, Cristobal",,,Lower Bounds for Parallel and Randomized Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the question of whether parallelization in the exploration of the feasible set can be used to speed up convex optimization, in the local oracle model of computation and in the high-dimensional regime. We show that the answer is negative for both deterministic and randomized algorithms applied to essentially any of the interesting geometries and nonsmooth, weakly-smooth, or smooth objective functions. In particular, we show that it is not possible to obtain a polylogarithmic (in the sequential complexity of the problem) number of parallel rounds with a polynomial (in the dimension) number of queries per round. In the majority of these settings and when the dimension of the space is polynomial in the inverse target accuracy, our lower bounds match the oracle complexity of sequential convex optimization, up to at most a logarithmic factor in the dimension, which makes them (nearly) tight. Another conceptual contribution of our work is in providing a general and streamlined framework for proving lower bounds in the setting of parallel convex optimization. Prior to our work, lower bounds for parallel convex optimization algorithms were only known in a small fraction of the settings considered in this paper, mainly applying to Euclidean (l(2)) and l(infinity) spaces.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300005,0
J,"Fattahi, S; Sojoudi, S",,,,"Fattahi, Salar; Sojoudi, Somayeh",,,Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Rank-1 Robust Principal Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work is concerned with the non-negative rank-1 robust principal component analysis (RPCA), where the goal is to recover the dominant non-negative principal components of a data matrix precisely, where a number of measurements could be grossly corrupted with sparse and arbitrary large noise. Most of the known techniques for solving the RPCA rely on convex relaxation methods by lifting the problem to a higher dimension, which significantly increase the number of variables. As an alternative, the well-known Burer-Monteiro approach can be used to cast the RPCA as a non-convex and non-smooth '1 optimization problem with a significantly smaller number of variables. In this work, we show that the low-dimensional formulation of the symmetric and asymmetric positive rank-1 RPCA based on the Burer-Monteiro approach has benign landscape, i.e., 1) it does not have any spurious local solution, 2) has a unique global solution, and 3) its unique global solution coincides with the true components. An implication of this result is that simple local search algorithms are guaranteed to achieve a zero global optimality gap when directly applied to the low-dimensional formulation. Furthermore, we provide strong deterministic and probabilistic guarantees for the exact recovery of the true principal components. In particular, it is shown that a constant fraction of the measurements could be grossly corrupted and yet they would not create any spurious local solution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000011,0
J,"Johndrow, J; Orenstein, P; Bhattacharya, A",,,,"Johndrow, James; Orenstein, Paulo; Bhattacharya, Anirban",,,Scalable Approximate MCMC Algorithms for the Horseshoe Prior,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have significantly improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove guarantees for the accuracy of the approximate algorithm, and show that gradually decreasing the approximation error as the chain extends results in an exact algorithm. The scalability of the algorithm is illustrated in simulations with problem size as large as N = 5; 000 observations and p = 50; 000 predictors, and an application to a genome-wide association study with N = 2; 267 and p = 98; 385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000025,0
J,"Manchev, N; Spratling, M",,,,"Manchev, Nikolay; Spratling, Michael",,,Target Propagation in Recurrent Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recurrent Neural Networks have been widely used to process sequence data, but have long been criticized for their biological implausibility and training difficulties related to vanishing and exploding gradients. This paper presents a novel algorithm for training recurrent networks, target propagation through time (TPTT), that outperforms standard backpropagation through time (BPTT) on four out of the five problems used for testing. The proposed algorithm is initially tested and compared to BPTT on four synthetic time lag tasks, and its performance is also measured using the sequential MNIST data set. In addition, as TPTT uses target propagation, it allows for discrete nonlinearities and could potentially mitigate the credit assignment problem in more complex recurrent architectures.",,,,,"Spratling, Michael/G-7689-2011","Spratling, Michael/0000-0001-9531-2813",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300007,0
J,"Narvekar, S; Peng, B; Leonetti, M; Sinapov, J; Taylor, ME; Stone, P",,,,"Narvekar, Sanmit; Peng, Bei; Leonetti, Matteo; Sinapov, Jivko; Taylor, Matthew E.; Stone, Peter",,,Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,181,,,,,,,,,,,,,,,WOS:000570234700001,0
J,"Salah, A; Truong, QT; Lauw, HW",,,,"Salah, Aghiles; Quoc-Tuan Truong; Lauw, Hady W.",,,Cornac: A Comparative Framework for Multimodal Recommender Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Cornac is an open-source Python framework for multimodal recommender systems. In addition to core utilities for accessing, building, evaluating, and comparing recommender models, Cornac is distinctive in putting emphasis on recommendation models that leverage auxiliary information in the form of a social network, item textual descriptions, product images, etc. Such multimodal auxiliary data supplement user-item interactions (e.g., ratings, clicks), which tend to be sparse in practice. To facilitate broad adoption and community contribution, Cornac is publicly available at https://github.com/PreferredAI/cornac, and it can be installed via Anaconda or the Python Package Index (pip). Not only is it well-covered by unit tests to ensure code quality, but it is also accompanied with a detailed documentation(1), tutorials, examples, and several built-in benchmarking data sets.",,,,,"Truong, Quoc-Tuan/AAJ-3043-2021; LAUW, Hady Wirawan/E-8557-2012","Truong, Quoc-Tuan/0000-0003-2291-1385; LAUW, Hady Wirawan/0000-0002-8245-8677",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,95,,,,,,,,,,,,,,,WOS:000545028300001,0
J,"Tang, PP; Wang, CJ; Sun, DF; Toh, KC",,,,"Tang, Peipei; Wang, Chengjing; Sun, Defeng; Toh, Kim-Chuan",,,A Sparse Semismooth Newton Based Proximal Majorization-Minimization Algorithm for Nonconvex Square-Root-Loss Regression Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider high-dimensional nonconvex square-root-loss regression problems and introduce a proximal majorization-minimization (PMM) algorithm for solving these problems. Our key idea for making the proposed PMM to be efficient is to develop a sparse semismooth Newton method to solve the corresponding subproblems. By using the Kurdyka-Lojasiewicz property exhibited in the underlining problems, we prove that the PMM algorithm converges to a d-stationary point. We also analyze the oracle property of the initial subproblem used in our algorithm. Extensive numerical experiments are presented to demonstrate the high efficiency of the proposed PMM algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,226,,,,,,,,,,,,,,,WOS:000605745400001,0
J,"Zhou, F; Li, ZD; Fan, XH; Wang, Y; Sowmya, A; Chen, F",,,,"Zhou, Feng; Li, Zhidong; Fan, Xuhui; Wang, Yang; Sowmya, Arcot; Chen, Fang",,,Efficient Inference for Nonparametric Hawkes Processes Using Auxiliary Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The expressive ability of classic Hawkes processes is limited due to the parametric assumption on the baseline intensity and triggering kernel. Therefore, it is desirable to perform inference in a data-driven, nonparametric approach. Many recent works have proposed nonparametric Hawkes process models based on Gaussian processes (GP). However, the likelihood is non-conjugate to the prior resulting in a complicated and time-consuming inference procedure. To address the problem, we present the sigmoid Gaussian Hawkes process model in this paper: the baseline intensity and triggering kernel are both modeled as the sigmoid transformation of random trajectories drawn from a GP. By introducing auxiliary latent random variables (branching structure, Polya-Gamma random variables and latent marked Poisson processes), the likelihood is converted to two decoupled components with a Gaussian form which allows for an efficient conjugate analytical inference. Using the augmented likelihood, we derive an efficient Gibbs sampling algorithm to sample from the posterior; an efficient expectation-maximization (EM) algorithm to obtain the maximum a posteriori (MAP) estimate and furthermore an efficient mean-field variational inference algorithm to approximate the posterior. To further accelerate the inference, a sparse GP approximation is introduced to reduce complexity. We demonstrate the performance of our three algorithms on both simulated and real data. The experiments show that our proposed inference algorithms can recover well the underlying prompting characteristics efficiently.",,,,,"Fan, Xu/GSE-2196-2022","Li, Zhidong/0000-0003-3288-5547; Fan, Xuhui/0000-0002-7558-7200; Wang, Yang/0000-0002-6815-0879; , Zhidong/0000-0002-0784-157X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,241,,,,,,,,,,,,,,,WOS:000608912900001,0
J,"Bietti, A; Mairal, J",,,,"Bietti, Alberto; Mairal, Julien",,,"Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they a ff ect learning guarantees is still missing. In this paper, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of di ff eomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as recti fi ed linear units, and we discuss its relationship with recent generalization bounds based on spectral norms.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,25,,,,,,,,,,,,,,,WOS:000458668800001,0
J,"Gautier, G; Polito, G; Bardenet, R; Valko, M",,,,"Gautier, Guillaume; Polito, Guillermo; Bardenet, Remi; Valko, Michal",,,DPPy: DPP Sampling with Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms for both finite and continuous DPPs. The project is hosted on GitHubc) and equipped with an extensive documentation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,180,,,,,,,,,,,,,,,WOS:000506403100020,0
J,"Krishnamurthy, A; Agarwal, A; Huang, TK; Daume, H; Langford, J",,,,"Krishnamurthy, Akshay; Agarwal, Alekh; Huang, Tzu-Kuo; Daume, Hal, III; Langford, John",,,Active Learning for Cost-Sensitive Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. We empirically compare COAL to passive learning and several active learning baselines, showing significant improvements in labeling effort and test cost on real-world datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,65,,,,,,,,,,,,,,,WOS:000467894100001,0
J,"Zhang, Z; Xia, YQ",,,,"Zhang, Zhenyue; Xia, Yuqing",,,Minimal Sample Subspace Learning: Theory and Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Subspace segmentation, or subspace learning, is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. The existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via the self-expressiveness of samples. A closed form of the representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, therein considering solutions to both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method to retrieve minimal sample subspaces that are heavily intersected.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,143,,,,,,,,,,,,,,,WOS:000491132200007,0
J,"Zhao, Y; Nasrullah, Z; Li, Z",,,,"Zhao, Yue; Nasrullah, Zain; Li, Zheng",,,PyOD: A Python Toolbox for Scalable Outlier Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https : //github . com/yzhao062/pyod.",,,,,"Zhao, Yue/AAM-2787-2020","Zhao, Yue/0000-0003-3401-4921",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,96,,,,,,,,,,,,,,,WOS:000470908800001,0
J,"Chiang, KY; Dhillon, IS; Hsieh, CJ",,,,"Chiang, Kai-Yang; Dhillon, Inderjit S.; Hsieh, Cho-Jui",,,Using Side Information to Reliably Learn Low-Rank Matrices from Missing and Corrupted Observations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning a low-rank matrix from missing and corrupted observations is a fundamental problem in many machine learning applications. However, the role of side information in low-rank matrix learning has received little attention, and most current approaches are either ad-hoc or only applicable in certain restrictive cases. In this paper, we propose a general model that exploits side information to better learn low-rank matrices from missing and corrupted observations, and show that the proposed model can be further applied to several popular scenarios such as matrix completion and robust PCA. Furthermore, we study the e ff ect of side information on sample complexity and show that by using our model, the e ffi ciency for learning can be improved given su ffi ciently informative side information. This result thus provides theoretical insight into the usefulness of side information in our model. Finally, we conduct comprehensive experiments in three real-world applications| relationship prediction, semi-supervised clustering and noisy image classi fi cation, showing that our proposed model is able to properly exploit side information for more e ff ective learning both in theory and practice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454478400001,0
J,"Thanei, GA; Meinshausen, N; Shah, RD",,,,"Thanei, Gian-Andrea; Meinshausen, Nicolai; Shah, Rajen D.",,,The xyz algorithm for fast interaction search in high-dimensional data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When performing regression on a data set with p variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least O (p(2)) if done naively. This cost can be prohibitive if p is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in p. We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires O (p(alpha)) operations for 1 < alpha < 2 depending on their strength. The underlying idea is to transform interaction search into a closest pair problem which can be solved efficiently in subquadratic time. The algorithm is called xyz and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than 10(11) interactions can be screened in under 280 seconds with a single-core 1.2 GHz CPU.",,,,,,"Shah, Rajen/0000-0001-9073-3782",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,37,,,,,,,,,,,,,,,WOS:000444405000001,0
J,"Adams, H; Emerson, T; Kirby, M; Neville, R; Peterson, C; Shipman, P; Chepushtanova, S; Hanson, E; Motta, F; Ziegelmeier, L",,,,"Adams, Henry; Emerson, Tegan; Kirby, Michael; Neville, Rachel; Peterson, Chris; Shipman, Patrick; Chepushtanova, Sofya; Hanson, Eric; Motta, Francis; Ziegelmeier, Lori",,,Persistence Images: A Stable Vector Representation of Persistent Homology,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finitedimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.",,,,,,"Neville, Rachel/0000-0002-7029-7044",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000397019700001,0
J,"Leppaaho, E; Ammad-ud-din, M; Kaski, S",,,,"Leppaaho, Eemeli; Ammad-ud-din, Muhammad; Kaski, Samuel",,,GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data.",,,,,"Kaski, Samuel/B-6684-2008","Kaski, Samuel/0000-0003-1925-9154",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,39,,,,,,,,,,,,,,,WOS:000400520800001,0
J,"Norton, M; Mafusalov, A; Uryasev, S",,,,"Norton, Matthew; Mafusalov, Alexander; Uryasev, Stan",,,Soft Margin Support Vector Classification as Buffered Probability Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we show that the popular C-SVM, soft-margin support vector classifier is equivalent to minimization of Buffered Probability of Exceedance (bPOE), a recently introduced characterization of uncertainty. To show this, we introduce a new SVM formulation, called the EC-SVM, which is derived from a simple bPOE minimization problem that is easy to interpret with a meaningful free parameter, optimal objective value, and probabilistic derivation. Over the range of its free parameter, the EC-SVM has both a convex and non-convex case which we connect to existing SVM formulations. We first show that the C-SVM, formulated with any regularization norm, is equivalent to the convex EC-SVM. Similarly, we show that the E nu-SVM is equivalent to the EC-SVM over its entire parameter range, which includes both the convex and non-convex case. These equivalences, coupled with the interpretability of the EC-SVM, allow us to gain surprising new insights into the C-SVM and fully connect soft margin support vector classification with superquantile and bPOE concepts. We also show that the EC-SVM can easily be cast as a robust optimization problem, where bPOE is minimized with data lying in a fixed uncertainty set. This reformulation allows us to clearly differentiate between the convex and non-convex case, with convexity associated with pessimistic views of uncertainty and non-convexity associated with optimistic views of uncertainty. Finally, we address some practical considerations. First, we show that these new insights can assist in making parameter selection more efficient. Second, we discuss optimization approaches for solving the EC-SVM. Third, we address the issue of generalization, providing generalization bounds for both bPOE and misclassification rate.",,,,,"URYASEV, Stan/O-5231-2014","URYASEV, Stan/0000-0001-6950-3966",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,43,68,,,,,,,,,,,,,,,WOS:000412057900001,0
J,"Pedregosa, F; Bach, F; Gramfort, A",,,,"Pedregosa, Fabian; Bach, Francis; Gramfort, Alexandre",,,On the Consistency of Ordinal Regression Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets.",,,,,"Pedregosa, Fabian/U-3477-2019","Gramfort, Alexandre/0000-0001-9791-4404",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,35,,,,,,,,,,,,,,,,WOS:000405980600001,0
J,"Silva, R; Shimizu, S",,,,"Silva, Ricardo; Shimizu, Shohei",,,Learning Instrumental Variables with Structural and Non-Gaussianity Assumptions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning a causal effect from observational data requires strong assumptions. One possible method is to use instrumental variables, which are typically justified by background knowledge. It is possible, under further assumptions, to discover whether a variable is structurally instrumental to a target causal effect X -> Y. However, the few existing approaches are lacking on how general these assumptions can be, and how to express possible equivalence classes of solutions. We present instrumental variable discovery methods that systematically characterize which set of causal effects can and cannot be discovered under local graphical criteria that define instrumental variables, without reconstructing full causal graphs. We also introduce the first methods to exploit non-Gaussianity assumptions, highlighting identifiability problems and solutions. Due to the difficulty of estimating such models from finite data, we investigate how to strengthen assumptions in order to make the statistical problem more manageable.",,,,,"Shimizu, Shohei/B-4425-2010","Shimizu, Shohei/0000-0002-1931-0733",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,120,,,,,,,,,,,,,,,WOS:000424541100001,0
J,"Singer, M; Krivobokova, T; Munk, A",,,,"Singer, Marco; Krivobokova, Tatyana; Munk, Axel",,,Kernel Partial Least Squares for Stationary Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider the kernel partial least squares algorithm for non-parametric regression with stationary dependent data. Probabilistic convergence rates of the kernel partial least squares estimator to the true regression function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares.,,,,,,"Krivobokova, Tatyana/0000-0002-6389-2312",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,41,,,,,,,,,,,,,,,WOS:000424541800001,0
J,"Zheng, S; Wang, JL; Xia, F; Xu, W; Zhang, T",,,,"Zheng, Shun; Wang, Jialei; Xia, Fen; Xu, Wei; Zhang, Tong",,,A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In modern large-scale machine learning applications, the training data are often partitioned and stored on multiple machines. It is customary to employ the data parallelism approach, where the aggregated training loss is minimized without moving data across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle data parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in (Boyd et al., 2011; Ma et al., 2017; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical analyses. Moreover, with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version, and the new result improves previous ones (Yang, 2013; Ma et al., 2017) whose iteration complexities grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state-of-the-art distributed dual coordinate optimization algorithms.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,115,,,,,,,,,,,,,,,WOS:000424539700001,0
J,"Arjevani, Y; Shalev-Shwartz, S; Shamir, O",,,,"Arjevani, Yossi; Shalev-Shwartz, Shai; Shamir, Ohad",,,On Lower and Upper Bounds in Smooth and Strongly Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a novel framework to study smooth and strongly convex optimization algorithms. Focusing on quadratic functions we are able to examine optimization algorithms as a recursive application of linear operators. This, in turn, reveals a powerful connection between a class of optimization algorithms and the analytic theory of polynomials whereby new lower and upper bounds are derived. Whereas existing lower bounds for this setting are only valid when the dimensionality scales with the number of iterations, our lower bound holds in the natural regime where the dimensionality is fixed. Lastly, expressing it as an optimal solution for the corresponding optimization problem over polynomials, as formulated by our framework, we present a novel systematic derivation of Nesterov's well-known Accelerated Gradient Descent method. This rather natural interpretation of AGD contrasts with earlier ones which lacked a simple, yet solid, motivation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,126,,,,,,,,,,,,,,,WOS:000391655100001,0
J,"Chen, X; Guntuboyina, A; Zhang, YC",,,,"Chen, Xi; Guntuboyina, Adityanand; Zhang, Yuchen",,,On Bayes Risk Lower Bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper provides a general technique for lower bounding the Bayes risk of statistical estimation, applicable to arbitrary loss functions and arbitrary prior distributions. A lower bound on the Bayes risk not only serves as a lower bound on the minimax risk, but also characterizes the fundamental limit of any estimator given the prior knowledge. Our bounds are based on the notion of f-informativity (Csiszar, 1972), which is a function of the underlying class of probability measures and the prior. Application of our bounds requires upper bounds on the f-informativity, thus we derive new upper bounds on f-informativity which often lead to tight Bayes risk lower bounds. Our technique leads to generalizations of a variety of classical minimax bounds (e.g., generalized Fano's inequality). Our Bayes risk lower bounds can be directly applied to several concrete estimation problems, including Gaussian location models, generalized linear models, and principal component analysis for spiked covariance models. To further demonstrate the applications of our Bayes risk lower bounds to machine learning problems, we present two new theoretical results: (1) a precise characterization of the minimax risk of learning spherical Gaussian mixture models under the smoothed analysis framework, and (2) lower bounds for the Bayes risk under a natural prior for both the prediction and estimation errors for high-dimensional sparse linear regression under an improper learning setting.",,,,,"Zhang, Yuchen/GYI-8858-2022; Zhang, ShiLiang/AAA-4638-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,58,218,,,,,,,,,,,,,,,WOS:000391838200001,0
J,"Chen, X; Jiao, KV; Lin, QH",,,,"Chen, Xi; Jiao, Kevin; Lin, Qihang",,,Bayesian Decision Process for Cost-Efficient Dynamic Ranking via Crowdsourcing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Rank aggregation based on pairwise comparisons over a set of items has a wide range of applications. Although considerable research has been devoted to the development of rank aggregation algorithms, one basic question is how to efficiently collect a large amount of high-quality pairwise comparisons for the ranking purpose. Because of the advent of many crowdsourcing services, a crowd of workers are often hired to conduct pairwise comparisons with a small monetary reward for each pair they compare. Since different workers have different levels of reliability and different pairs have different levels of ambiguity, it is desirable to wisely allocate the limited budget for comparisons among the pairs of items and workers so that the global ranking can be accurately inferred from the comparison results. To this end, we model the active sampling problem in crowdsourced ranking as a Bayesian Markov decision process, which dynamically selects item pairs and workers to improve the ranking accuracy under a budget constraint. We further develop a computationally efficient sampling policy based on knowledge gradient as well as a moment matching technique for posterior approximation. Experimental evaluations on both synthetic and real data show that the proposed policy achieves high ranking accuracy with a lower labeling cost.",,,,,"Zhang, Yuchen/GYI-8858-2022; Zhang, ShiLiang/AAA-4638-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,217,,,,,,,,,,,,,,,WOS:000391837900001,0
J,"Ganin, Y; Ustinova, E; Ajakan, H; Germain, P; Larochelle, H; Laviolette, F; Marchand, M; Lempitsky, V",,,,"Ganin, Yaroslav; Ustinova, Evgeniya; Ajakan, Hana; Germain, Pascal; Larochelle, Hugo; Laviolette, Francois; Marchand, Mario; Lempitsky, Victor",,,Domain-Adversarial Training of Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little e ff ort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classi fi cation problems (document sentiment analysis and image classi fi cation), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identi fi cation application.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,59,,,,,,,,,,,,,,,WOS:000391492800001,0
J,"Hoffman, J; Pathak, D; Tzeng, E; Long, J; Guadarrama, S; Darrell, T; Saenko, K",,,,"Hoffman, Judy; Pathak, Deepak; Tzeng, Eric; Long, Jonathan; Guadarrama, Sergio; Darrell, Trevor; Saenko, Kate",,,Large Scale Visual Recognition through Adaptation using Joint Representation and Multiple Instance Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a >7.6K category detector and release code and models at lsda.berkeleyvision.org.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,142,,,,,,,,,,,,,,,WOS:000391660800001,0
J,"Kwon, J; Perchet, V",,,,"Kwon, Joon; Perchet, Vianney",,,Gains and Losses are Fundamentally Different in Regret Minimization: The Sparse Case,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We demonstrate that, in the classical non-stochastic regret minimization problem with d decisions, gains and losses to be respectively maximized or minimized are fundamentally different. Indeed, by considering the additional sparsity assumption (at each stage, at most s decisions incur a nonzero outcome), we derive optimal regret bounds of different orders. Specifically, with gains, we obtain an optimal regret guarantee after T stages of order root T logs, so the classical dependency in the dimension is replaced by the sparsity size. With losses, we provide matching upper and lower bounds of order root Ts log(d)/d, which is decreasing in d. Eventually, we also study the bandit setting, and obtain an upper bound of order root Ts log(d/s) when outcomes are losses. This bound is proven to be optimal up to the logarithmic factor root log(d/s).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,229,,,,,,,,,,,,,,,WOS:000391916200001,0
J,"Ma, CX; Tappenden, R; Takac, M",,,,"Ma, Chenxin; Tappenden, Rachael; Takac, Martin",,,Linear Convergence of Randomized Feasible Descent Methods Under the Weak Strong Convexity Assumption,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we generalize the framework of the Feasible Descent Method (FDM) to a Randomized (R-FDM) and a Randomized Coordinate-wise Feasible Descent Method (RC-FDM) framework. We show that many machine learning algorithms, including the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem.",,,,,"Takac, Martin/AAA-8564-2022","Takac, Martin/0000-0001-7455-2025",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,228,,,,,,,,,,,,,,,WOS:000391916700001,0
J,"Teh, YW; Thiery, AH; Vollmer, SJ",,,,"Teh, Yee Whye; Thiery, Alexandre H.; Vollmer, Sebastian J.",,,Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally expensive. Both the calculation of the acceptance probability and the creation of informed proposals usually require an iteration through the whole data set. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem by generating proposals which are only based on a subset of the data, by skipping the accept-reject step and by using decreasing step-sizes sequence (delta(m))(m >= 0). We provide in this article a rigorous mathematical framework for analysing this algorithm. We prove that, under verifiable assumptions, the algorithm is consistent, satisfies a central limit theorem (CLT) and its asymptotic bias-variance decomposition can be characterized by an explicit functional of the step-sizes sequence (delta(m))(m >= 0). We leverage this analysis to give practical recommendations for the notoriously difficult tuning of this algorithm: it is asymptotically optimal to use a step-size sequence of the type delta(m) asymptotic to m(-1/3), leading to an algorithm whose mean squared error (MSE) decreases at rate O(m(-1/3)).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,7,,,,,,,,,,,,,,,WOS:000391464800001,0
J,"Daniely, A; Sabato, S; Ben-David, S; Shalev-Shwartz, S",,,,"Daniely, Amit; Sabato, Sivan; Ben-David, Shai; Shalev-Shwartz, Shai",,,Multiclass Learnability and the ERM Principle,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the sample complexity of multiclass prediction in several learning settings. For the PAC setting our analysis reveals a surprising phenomenon: In sharp contrast to binary classification, we show that there exist multiclass hypothesis classes for which some Empirical Risk Minimizers (ERM learners) have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learners will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes-classes that are invariant under permutations of label names. We further provide a characterization of mistake and regret bounds for multiclass learning in the online setting and the bandit setting, using new generalizations of Littlestone's dimension.",,,,,"Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2377,2404,,,,,,,,,,,,,,,,WOS:000369888000001,0
J,"Hastie, T; Mazumder, R; Lee, JD; Zadeh, R",,,,"Hastie, Trevor; Mazumder, Rahul; Lee, Jason D.; Zadeh, Reza",,,Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The matrix-completion problem has attracted a lot of attention, largely as a result of the celebrated Netflix competition. Two popular approaches for solving the problem are nuclear-norm-regularized matrix approximation (Candes and Tao, 2009; Mazumder et al., 2010), and maximum-margin matrix factorization (Srebro et al., 2005). These two procedures are in some cases solving equivalent problems, but with quite different algorithms. In this article we bring the two approaches together, leading to an efficient algorithm for large matrix factorization and completion that outperforms both of these. We develop a software package softImpute in R for implementing our approaches, and a distributed version for very large matrices using the Spark cluster programming environment",,,,,,"Lee, Jason/0000-0003-0064-7800; Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3367,3402,,,,,,,,,,,31130828,,,,,WOS:000369888000033,0
J,"Jorgensen, P; Tian, F",,,,"Jorgensen, Palle; Tian, Feng",,,Discrete Reproducing Kernel Hilbert Spaces: Sampling and Distribution of Dirac-masses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs) H over infinite, discrete and countable sets V. In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of V. Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural network configuration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs H of functions defined on a prescribed countable infinite discrete set V, we characterize those which contain the Dirac masses (delta(x) for all points x in V. Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set V of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3079,3114,,,,,,,,,,,,,,,,WOS:000369888000025,0
J,"Lowd, D; Rooshenas, A",,,,"Lowd, Daniel; Rooshenas, Amirmohammad",,,The Libra Toolkit for Probabilistic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2459,2463,,,,,,,,,,,,,,,,WOS:000369888000004,0
J,"Bellec, PC; Tsybakov, AB",,,,"Bellec, Pierre C.; Tsybakov, Alexandre B.",,,Sharp Oracle Bounds for Monotone and Convex Regression Through Aggregation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive oracle inequalities for the problems of isotonic and convex regression using the combination of Q-aggregation procedure and sparsity pattern aggregation. This improves upon the previous results including the oracle inequalities for the constrained least squares estimator. One of the improvements is that our oracle inequalities are sharp, i.e., with leading constant 1. It allows us to obtain bounds for the minimax regret thus accounting for model misspecification, which was not possible based on the previous results. Another improvement is that we obtain oracle inequalities both with high probability and in expectation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1879,1892,,,,,,,,,,,,,,,,WOS:000369887300007,0
J,"Fearnley, J; Gairing, M; Goldberg, PW; Savani, R",,,,"Fearnley, John; Gairing, Martin; Goldberg, Paul W.; Savani, Rahul",,,Learning Equilibria of Games via Payoff Queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A recent body of experimental literature has studied empirical game-theoretical analysis, in which we have partial knowledge of a game, consisting of observations of a subset of the pure-strategy profiles and their associated payoffs to players. The aim is to find an exact or approximate Nash equilibrium of the game, based on these observations. It is usually assumed that the strategy profiles may be chosen in an on-line manner by the algorithm. We study a corresponding computational learning model, and the query complexity of learning equilibria for various classes of games. We give basic results for exact equilibria of bimatrix and graphical games. We then study the query complexity of approximate equilibria in bimatrix games. Finally, we study the query complexity of exact equilibria in symmetric network congestion games. For directed acyclic networks, we can learn the cost functions (and hence compute an equilibrium) while querying just a small fraction of pure-strategy profiles. For the special case of parallel links, we have the stronger result that an equilibrium can be identified while only learning a small fraction of the cost values.",,,,,,"Savani, Rahul/0000-0003-1262-7831",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1305,1344,,,,,,,,,,,,,,,,WOS:000369887100001,0
J,"Kiraly, FJ; Theran, L; Tomioka, R",,,,"Kiraly, Franz J.; Theran, Louis; Tomioka, Ryota",,,The Algebraic Combinatorial Approach for Low-Rank Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel algebraic combinatorial view on low-rank matrix completion based on studying relations between a few entries with tools from algebraic geometry and matroid theory. The intrinsic locality of the approach allows for the treatment of single entries in a closed theoretical and practical framework. More specifically, apart from introducing an algebraic combinatorial theory of low-rank matrix completion, we present probability one algorithms to decide whether a particular entry of the matrix can be completed. We also describe methods to complete that entry from a few others, and to estimate the error which is incurred by any method completing that entry. Furthermore, we show how known results on matrix completion and their sampling assumptions can be related to our new perspective and interpreted in terms of a completability phase transition.",,,,,,"Theran, Louis/0000-0001-5282-4800",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1391,1436,,,,,,,,,,,,,,,,WOS:000369887100003,0
J,"Javanmard, A; Montanari, A",,,,"Javanmard, Adel; Montanari, Andrea",,,Confidence Intervals and Hypothesis Testing for High-Dimensional Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values for these models. We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a highthroughput genomic data set about riboflavin production rate, made publicly available by Biihlmann et al. (2014).",,,,,"Javanmard, Adel/ABB-5000-2020",,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,2869,2909,,,,,,,,,,,,,,,,WOS:000344638800002,0
J,"Anandkumar, A; Ge, R; Hsu, D; Kakade, SM; Telgarsky, M",,,,"Anandkumar, Animashree; Ge, Rong; Hsu, Daniel; Kakade, Sham M.; Telgarsky, Matus",,,Tensor Decompositions for Learning Latent Variable Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models-including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation-which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.",,,,,,"Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,1532-4435,,,,,AUG,2014,15,,,,,,2773,2832,,,,,,,,,,,,,,,,WOS:000344638600005,0
J,"Cohen, SB; Stratos, K; Collins, M; Foster, DP; Ungar, L",,,,"Cohen, Shay B.; Stratos, Karl; Collins, Michael; Foster, Dean P.; Ungar, Lyle",,,Spectral Learning of Latent-Variable PCFGs: Algorithms and Sample Complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a spectral learning algorithm for latent-variable PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides statistically consistent parameter estimates. Our result rests on three theorems: the first gives a tensor form of the inside-outside algorithm for PCFGs; the second shows that the required tensors can be estimated directly from training examples where hidden-variable values are missing; the third gives a PAC-style convergence bound for the estimation method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2399,2449,,,,,,,,,,,,,,,,WOS:000344638400002,0
J,"Dhurandhar, A; Petrik, M",,,,"Dhurandhar, Amit; Petrik, Marek",,,Efficient and Accurate Methods for Updating Generalized Linear Models with Multiple Feature Additions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose an approach for learning regression models efficiently in an environment where multiple features and data-points are added incrementally in a multi-step process. At each step, any finite number of features maybe added and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares, weighted least squares, generalized least squares and ridge regression, but also more generally for generalized linear models and lasso regression that use iterated re-weighted least squares for maximum likelihood estimation. Our approach instantiated to linear settings has close relations to the partitioned matrix inversion mechanism based on Schur's complement. For arbitrary regression methods, even a relaxation of the approach is no worse than using the model from the previous step or using a model that learns on the additional features and optimizes the residual of the model at the previous step. Such problems are commonplace in complex manufacturing operations consisting of hundreds of steps, where multiple measurements are taken at each step to monitor the quality of the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate steps can be extremely useful in enhancing productivity. We further validate our claims through experiments on synthetic and real industrial data sets.",,,,,,"Petrik, Marek/0000-0002-4568-7948",,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2607,2627,,,,,,,,,,,,,,,,WOS:000344638400008,0
J,"Konecny, J; Hagara, M",,,,"Konecny, Jakub; Hagara, Michal",,,One-Shot-Learning Gesture Recognition using HOG-HOF Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The purpose of this paper is to describe one-shot-learning gesture recognition systems developed on the ChaLearn Gesture Dataset (ChaLearn). We use RGB and depth images and combine appearance (Histograms of Oriented Gradients) and motion descriptors (Histogram of Optical Flow) for parallel temporal segmentation and recognition. The Quadratic-Chi distance family is used to measure differences between histograms to capture cross-bin relationships. We also propose a new algorithm for trimming videos to remove all the unimportant frames from videos. We present two methods that use a combination of HOG-HOF descriptors together with variants of a Dynamic Time Warping technique. Both methods outperform other published methods and help narrow the gap between human performance and algorithms on this task. The code is publicly available in the MLOSS repository.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2513,2532,,,,,,,,,,,,,,,,WOS:000344638400005,0
J,"Durante, D; Scarpa, B; Dunson, DB",,,,"Durante, Daniele; Scarpa, Bruno; Dunson, David B.",,,Locally Adaptive Factor Processes for Multivariate Time Series,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.",,,,,"Scarpa, Bruno/AAK-1087-2021; Durante, Daniele/O-8277-2017","Scarpa, Bruno/0000-0002-9628-5164; Durante, Daniele/0000-0002-8595-6719",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1493,1522,,,,,,,,,,,,,,,,WOS:000338420000010,0
J,"Mohan, K; London, P; Fazei, M; Witten, D; Lee, SI",,,,"Mohan, Karthik; London, Palma; Fazei, Maryan; Witten, Daniela; Lee, Su-In",,,Node-Based Learning of Multiple Gaussian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a node-based approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the K networks are due to individual nodes that are perturbed across conditions, or (2) similarities among the K networks are due to the presence of common hub nodes that are shared across all K networks. Using a row-column overlap norm penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,445,488,,,,,,,,,,,25309137,,,,,WOS:000335457700002,0
J,"Nandan, M; Khargonekar, PP; Talathi, SS",,,,"Nandan, Manu; Khargonekar, Pramod P.; Talathi, Sachin S.",,,Fast SVM Training Using Approximate Extreme Points,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Applications of non-linear kernel support vector machines (SVMs) to large data sets is seriously hampered by its excessive training time. We propose a modification, called the approximate extreme points support vector machine (AESVM), that is aimed at overcoming this burden. Our approach relies on conducting the SVM optimization over a carefully selected subset, called the representative set, of the training data set. We present analytical results that indicate the similarity of AESVM and SVM solutions. A linear time algorithm based on convex hulls and extreme points is used to compute the representative set in kernel space. Extensive computational experiments on nine data sets compared AESVM to LIBSVM (Chang and Lin, 2011), CVM (Tsang et al., 2005), BVM (Tsang et al., 2007), LASVM (Bordes et al., 2005), SVMperf (Joachims and Yu, 2009), and the random features method (Rahimi and Recht, 2007). Our AESVM implementation was found to train much faster than the other methods, while its classification accuracy was similar to that of LIBSVM in all cases. In particular, for a seizure detection data set, AESVM training was almost 500 times faster than LIBSVM and LASVM and 20 times faster than CVM and BVM. Additionally, AESVM also gave competitively fast classification times.",,,,,,"Khargonekar, Pramod/0000-0001-6634-6950",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,59,98,,,,,,,,,,,,,,,,WOS:000335457400002,0
J,"Sun, TN; Zhang, CH",,,,"Sun, Tingni; Zhang, Cun-Hui",,,Sparse Matrix Inversion with Scaled Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new method of learning a sparse nonnegative-definite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm first estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other l(1) regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the l(1) and spectrum norms of the target inverse matrix diverges to infinity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3385,3418,,,,,,,,,,,,,,,,WOS:000329786900006,0
J,"Joseph, A",,,,"Joseph, Antony",,,Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefficient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefficient vector, whereby one has only control on the l(1) norm of the smaller coefficients, is also analyzed. As consequence of these results, we also show that the coefficient estimate satisfies strong oracle type inequalities.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1771,1800,,,,,,,,,,,,,,,,WOS:000323367000004,0
J,"Bahmani, S; Raj, B; Boufounos, PT",,,,"Bahmani, Sohail; Raj, Bhiksha; Boufounos, Petros T.",,,Greedy Sparsity-Constrained Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparsity-constrained optimization has wide applicability in machine learning, statistics, and signal processing problems such as feature selection and Compressed Sensing. A vast body of work has studied the sparsity-constrained optimization from theoretical, algorithmic, and application aspects in the context of sparse estimation in linear models where the fidelity of the estimate is measured by the squared error. In contrast, relatively less effort has been made in the study of sparsity-constrained optimization in cases where nonlinear models are involved or the cost function is not quadratic. In this paper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to approximate sparse minima of cost functions of arbitrary form. Should a cost function have a Stable Restricted Hessian (SRH) or a Stable Restricted Linearization (SRL), both of which are introduced in this paper, our algorithm is guaranteed to produce a sparse vector within a bounded distance from the true sparse optimum. Our approach generalizes known results for quadratic cost functions that arise in sparse linear regression and Compressed Sensing. We also evaluate the performance of GraSP through numerical simulations on synthetic and real data, where the algorithm is employed for sparse logistic regression with and without l(2)-regularization.",,,,,"Boufounos, Petros/C-3602-2013","Boufounos, Petros/0000-0003-1369-0947; Bahmani, Sohail/0000-0002-8316-8313",,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,807,841,,,,,,,,,,,,,,,,WOS:000317461700004,0
J,"Gerchinovitz, S",,,,"Gerchinovitz, Sebastien",,,Sparsity Regret Bounds for Individual Sequences in Online Linear Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T. We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same flavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with fixed design.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,729,769,,,,,,,,,,,,,,,,WOS:000317461700001,0
J,"Wang, J; Jebara, T; Chang, SF",,,,"Wang, Jun; Jebara, Tony; Chang, Shih-Fu",,,Semi-Supervised Learning Using Greedy Max-Cut,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classification function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classification function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efficient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artificial and standard benchmark data sets where it obtains superior classification accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2013,14,,,,,,771,800,,,,,,,,,,,,,,,,WOS:000317461700002,0
J,"Hyttinen, A; Eberhardt, F; Hoyer, PO",,,,"Hyttinen, Antti; Eberhardt, Frederick; Hoyer, Patrik O.",,,Learning Linear Cyclic Causal Models with Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Identifying cause-effect relationships between variables of interest is a central problem in science. Given a set of experiments we describe a procedure that identifies linear models that may contain cycles and latent variables. We provide a detailed description of the model family, full proofs of the necessary and sufficient conditions for identifiability, a search algorithm that is complete, and a discussion of what can be done when the identifiability conditions are not satisfied. The algorithm is comprehensively tested in simulations, comparing it to competing algorithms in the literature. Furthermore, we adapt the procedure to the problem of cellular network inference, applying it to the biologically realistic data of the DREAM challenges. The paper provides a full theoretical foundation for the causal discovery procedure first presented by Eberhardt et al. (2010) and Hyttinen et al. (2010).",,,,,,"Hyttinen, Antti/0000-0002-6649-3229",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3387,3439,,,,,,,,,,,,,,,,WOS:000313200200009,0
J,"Lee, S; Wright, SJ",,,,"Lee, Sangkyun; Wright, Stephen J.",,,Manifold Identification in Dual Averaging for Regularized Stochastic Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Iterative methods that calculate their steps from approximate subgradient directions have proved to be useful for stochastic learning problems over large and streaming data sets. When the objective consists of a loss function plus a nonsmooth regularization term, the solution often lies on a low-dimensional manifold of parameter space along which the regularizer is smooth. (When an l(1) regularizer is used to induce sparsity in the solution, for example, this manifold is defined by the set of nonzero components of the parameter vector.) This paper shows that a regularized dual averaging algorithm can identify this manifold, with high probability, before reaching the solution. This observation motivates an algorithmic strategy in which, once an iterate is suspected of lying on an optimal or near-optimal manifold, we switch to a local phase that searches in this manifold, thus converging rapidly to a near-optimal point. Computational results are presented to verify the identification property and to illustrate the effectiveness of this approach.",,,,,"Lee, Sangkyun/AAE-5272-2019","Lee, Sangkyun/0000-0001-8415-6368",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1705,1744,,,,,,,,,,,,,,,,WOS:000307020700001,0
J,"Zitnik, M; Zupan, B",,,,"Zitnik, Marinka; Zupan, Blaz",,,NIMFA: A Python Library for Nonnegative Matrix Factorization.,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks.",,,,,"Zupan, Blaz/L-1595-2019","Zupan, Blaz/0000-0002-5864-7056",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,849,853,,,,,,,,,,,,,,,,WOS:000303772100014,0
J,"Recht, B",,,,"Recht, Benjamin",,,A Simpler Approach to Matrix Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Candes and Recht (2009), Candes and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2011,12,,,,,,3413,3430,,,,,,,,,,,,,,,,WOS:000299681200001,0
J,"Rigollet, P; Tong, X",,,,"Rigollet, Philippe; Tong, Xin",,,"Neyman-Pearson Classification, Convexity and Stochastic Constraints",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by problems of anomaly detection, this paper implements the Neyman-Pearson paradigm to deal with asymmetric errors in binary classification with a convex loss phi. Given a finite collection of classifiers, we combine them and obtain a new classifier that satisfies simultaneously the two following properties with high probability: (i) its phi-type I error is below a pre-specified level and (ii), it has phi-type II error close to the minimum possible. The proposed classifier is obtained by minimizing an empirical convex objective with an empirical convex constraint. The novelty of the method is that the classifier output by this computationally feasible program is shown to satisfy the original constraint on type I error. New techniques to handle such problems are developed and they have consequences on chance constrained programming. We also evaluate the price to pay in terms of type II error for being conservative on type I error.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2831,2855,,,,,,,,,,,,,,,,WOS:000298103200004,0
J,"Choi, MJ; Tan, VYF; Anandkumar, A; Willsky, AS",,,,"Choi, Myung Jin; Tan, Vincent Y. F.; Anandkumar, Animashree; Willsky, Alan S.",,,Learning Latent Tree Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning a latent tree graphical model where samples are available only from a subset of variables. We propose two consistent and computationally efficient algorithms for learning minimal latent trees, that is, trees without any redundant hidden nodes. Unlike many existing methods, the observed nodes (or variables) are not constrained to be leaf nodes. Our algorithms can be applied to both discrete and Gaussian random variables and our learned models are such that all the observed and latent variables have the same domain (state space). Our first algorithm, recursive grouping, builds the latent tree recursively by identifying sibling groups using so-called information distances. One of the main contributions of this work is our second algorithm, which we refer to as CLGrouping. CLGrouping starts with a pre-processing procedure in which a tree over the observed variables is constructed. This global step groups the observed nodes that are likely to be close to each other in the true latent tree, thereby guiding subsequent recursive grouping (or equivalent procedures such as neighbor-joining) on much smaller subsets of variables. This results in more accurate and efficient learning of latent trees. We also present regularized versions of our algorithms that learn latent tree approximations of arbitrary distributions. We compare the proposed algorithms to other methods by performing extensive numerical experiments on various latent tree graphical models such as hidden Markov models and star graphs. In addition, we demonstrate the applicability of our methods on real-world data sets by modeling the dependency structure of monthly stock returns in the S&P index and of the words in the 20 newsgroups data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1771,1812,,,,,,,,,,,,,,,,WOS:000292304000010,0
J,"Ross, S; Pineau, J; Chaib-draa, B; Kreitmann, P",,,,"Ross, Stephane; Pineau, Joelle; Chaib-draa, Brahim; Kreitmann, Pierre",,,A Bayesian Approach for Learning and Planning in Partially Observable Markov Decision Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.",,,,,"Chaib-draa, Brahim/A-1157-2008","Chaib-draa, Brahim/0000-0001-7615-5154; Salguero Tejada, Carlos/0000-0003-0930-9277",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1729,1770,,,,,,,,,,,,,,,,WOS:000292304000009,0
J,"Zilles, S; Lange, S; Holte, R; Zinkevich, M",,,,"Zilles, Sandra; Lange, Steffen; Holte, Robert; Zinkevich, Martin",,,Models of Cooperative Teaching and Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While most supervised machine learning models assume that training examples are sampled at random or adversarially, this article is concerned with models of learning from a cooperative teacher that selects helpful training examples. The number of training examples a learner needs for identifying a concept in a given class C of possible target concepts (sample complexity of C) is lower in models assuming such teachers, that is, helpful examples can speed up the learning process. The problem of how a teacher and a learner can cooperate in order to reduce the sample complexity, yet without using coding tricks, has been widely addressed. Nevertheless, the resulting teaching and learning protocols do not seem to make the teacher select intuitively helpful examples. The two models introduced in this paper are built on what we call subset teaching sets and recursive teaching sets. They extend previous models of teaching by letting both the teacher and the learner exploit knowing that the partner is cooperative. For this purpose, we introduce a new notion of coding trick/collusion. We show how both resulting sample complexity measures (the subset teaching dimension and the recursive teaching dimension) can be arbitrarily lower than the classic teaching dimension and known variants thereof, without using coding tricks. For instance, monomials can be taught with only two examples independent of the number of variables. The subset teaching dimension turns out to be nonmonotonic with respect to subclasses of concept classes. We discuss why this nonmonotonicity might be inherent in many interesting cooperative teaching and learning scenarios.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,349,384,,,,,,,,,,,,,,,,WOS:000288896800001,0
J,"V'yugin, VV",,,,"V'yugin, Vladimir V.",,,Online Learning in Case of Unbounded Losses Using Follow the Perturbed Leader Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,In this paper the sequential prediction problem with expert advice is considered for the case where losses of experts suffered at each step cannot be bounded in advance. We present some modification of Kalai and Vempala algorithm of following the perturbed leader where weights depend on past losses of the experts. New notions of a volume and a scaled fluctuation of a game are introduced. We present a probabilistic algorithm protected from unrestrictedly large one-step losses. This algorithm has the optimal performance in the case when the scaled fluctuations of one-step losses of experts of the pool tend to zero.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,241,266,,,,,,,,,,,,,,,,WOS:000287938500008,0
J,"Lucke, J; Eggert, J",,,,"Luecke, Joerg; Eggert, Julian",,,Expectation Truncation and the Benefits of Preselection In Training Generative Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show how a preselection of hidden variables can be used to efficiently train generative models with binary hidden variables. The approach is based on Expectation Maximization (EM) and uses an efficiently computable approximation to the sufficient statistics of a given model. The computational cost to compute the sufficient statistics is strongly reduced by selecting, for each data point, the relevant hidden causes. The approximation is applicable to a wide range of generative models and provides an interpretation of the benefits of preselection in terms of a variational EM approximation. To empirically show that the method maximizes the data likelihood, it is applied to different types of generative models including: a version of non-negative matrix factorization (NMF), a model for non-linear component extraction (MCA), and a linear generative model similar to sparse coding. The derived algorithms are applied to both artificial and realistic data, and are compared to other models in the literature. We find that the training scheme can reduce computational costs by orders of magnitude and allows for a reliable extraction of hidden causes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2855,2900,,,,,,,,,,,,,,,,WOS:000284040000009,0
J,"Xiao, L",,,,"Xiao, Lin",,,Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as l1-norm for promoting sparsity. We develop extensions of Nesterov's dual averaging method, that can exploit the regularization structure in an online setting. At each iteration of these methods, the learning variables are adjusted by solving a simple minimization problem that involves the running average of all past subgradients of the loss function and the whole regularization term, not just its subgradient. In the case of l1-regularization, our method is particularly effective in obtaining sparse solutions. We show that these methods achieve the optimal convergence rates or regret bounds that are standard in the literature on stochastic and online convex optimization. For stochastic learning problems in which the loss functions have Lipschitz continuous gradients, we also present an accelerated version of the dual averaging method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2543,2596,,,,,,,,,,,,,,,,WOS:000284040000001,0
J,"Koltchinskii, V",,,,"Koltchinskii, Vladimir",,,Rademacher Complexities and Bounding the Excess Risk in Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Sequential algorithms of active learning based on the estimation of the level sets of the empirical risk are discussed in the paper. Localized Rademacher complexities are used in the algorithms to estimate the sample sizes needed to achieve the required accuracy of learning in an adaptive way. Probabilistic bounds on the number of active examples have been proved and several applications to binary classification problems are considered.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2010,11,,,,,,2457,2485,,,,,,,,,,,,,,,,WOS:000282523400003,0
J,"Fan, Y; Xu, J; Shelton, CR",,,,"Fan, Yu; Xu, Jing; Shelton, Christian R.",,,Importance Sampling for Continuous Time Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A continuous time Bayesian network (CTBN) uses a structured representation to describe a dynamic system with a finite number of states which evolves in continuous time. Exact inference in a CTBN is often intractable as the state space of the dynamic system grows exponentially with the number of variables. In this paper, we first present an approximate inference algorithm based on importance sampling. We then extend it to continuous-time particle filtering and smoothing algorithms. These three algorithms can estimate the expectation of any function of a trajectory, conditioned on any evidence set constraining the values of subsets of the variables over subsets of the time line. We present experimental results on both synthetic networks and a network learned from a real data set on people's life history events. We show the accuracy as well as the time efficiency of our algorithms, and compare them to other approximate algorithms: expectation propagation and Gibbs sampling.",,,,,"Shelton, Christian/GQJ-1146-2022","Shelton, Christian/0000-0001-6698-7838",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2115,2140,,,,,,,,,,,,,,,,WOS:000282523300002,0
J,"Keshavan, RH; Montanari, A; Oh, S",,,,"Keshavan, Raghunandan H.; Montanari, Andrea; Oh, Sewoong",,,Matrix Completion from Noisy Entries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a matrix M of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries. The problem arises in a variety of applications, from collaborative filtering (the 'Netflix problem') to structure-from-motion and positioning. We study a low complexity algorithm introduced by Keshavan, Montanari, and Oh (2010), based on a combination of spectral techniques and manifold optimization, that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,2057,2078,,,,,,,,,,,,,,,,WOS:000282523000005,0
J,"Gunawardana, A; Shani, G",,,,"Gunawardana, Asela; Shani, Guy",,,A Survey of Accuracy Evaluation Metrics of Recommendation Tasks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms offline using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of offline experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing offline experiments.",,,,,"Shani, Guy/F-1634-2012","Shani, Guy/0000-0003-4131-0382",,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2935,2962,,,,,,,,,,,,,,,,WOS:000273877300007,0
J,"Bickel, S; Bruckner, M; Scheffer, T",,,,"Bickel, Steffen; Brueckner, Michael; Scheffer, Tobias",,,Discriminative Learning Under Covariate Shift,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution-problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions; our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2137,2155,,,,,,,,,,,,,,,,WOS:000272346100007,0
J,"Poczos, B; Lorincz, A",,,,"Poczos, Barnabas; Lorincz, Andras",,,Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce novel online Bayesian methods for the identification of a family of noisy recurrent neural networks (RNNs). We present Bayesian active learning techniques for stimulus selection given past experiences. In particular, we consider the unknown parameters as stochastic variables and use A-optimality and D-optimality principles to choose optimal stimuli. We derive myopic cost functions in order to maximize the information gain concerning network parameters at each time step. We also derive the A-optimal and D-optimal estimations of the additive noise that perturbs the dynamical system of the RNN. Here we investigate myopic as well as non-myopic estimations, and study the problem of simultaneous estimation of both the system parameters and the noise. Employing conjugate priors our derivations remain approximation-free and give rise to simple update rules for the online learning of the parameters. The efficiency of our method is demonstrated for a number of selected cases, including the task of controlled independent component analysis.",,,,,"Lorincz, Andras/H-4125-2012","Lorincz, Andras/0000-0002-1280-3447",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,515,554,,,,,,,,,,,,,,,,WOS:000270824200014,0
J,"Chang, KW; Hsieh, CJ; Lin, CJ",,,,"Chang, Kai-Wei; Hsieh, Cho-Jui; Lin, Chih-Jen",,,Coordinate descent method for large-scale L2-loss linear support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classification and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while fixing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efficient and stable than state of the art methods such as Pegasos and TRON.",,,,,"Chang, Kai-Wei/M-6055-2016","Chang, Kai-Wei/0000-0001-5365-0072; Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1369,1398,,,,,,,,,,,,,,,,WOS:000258646800004,0
J,"Rieck, K; Laskov, P",,,,"Rieck, Konrad; Laskov, Pavel",,,Linear-time computation of similarity measures for sequential data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Efficient and expressive comparison of sequences is an essential procedure for learning with sequential data. In this article we propose a generic framework for computation of similarity measures for sequences, covering various kernel, distance and non-metric similarity functions. The basis for comparison is embedding of sequences using a formal language, such as a set of natural words, k-grams or all contiguous subsequences. As realizations of the framework we provide linear-time algorithms of different complexity and capabilities using sorted arrays, tries and suffix trees as underlying data structures. Experiments on data sets from bioinformatics, text processing and computer security illustrate the efficiency of the proposed algorithms - enabling peak performances of up to 10(6) pairwise comparisons per second. The utility of distances and non-metric similarity measures for sequences as alternatives to string kernels is demonstrated in applications of text categorization, network intrusion detection and transcription site recognition in DNA.",,,,,"Rieck, Konrad/F-2233-2010","Rieck, Konrad/0000-0002-5054-8758",,,,,,,,,,,,,1532-4435,,,,,JAN,2008,9,,,,,,23,48,,,,,,,,,,,,,,,,WOS:000256641400002,0
J,"Gomez, V; Mooij, JM; Kappen, HJ",,,,"Gomez, Vicenc; Mooij, Joris M.; Kappen, Hilbert J.",,,Truncating the loop series expansion for belief propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each generalized loop in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to significant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered.",,,,,"G√≥mez, Vicen√ß/D-1984-2009; Kappen, H.J./L-4425-2015","G√≥mez, Vicen√ß/0000-0001-5146-7645; ",,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,1987,2016,,,,,,,,,,,,,,,,WOS:000252744600001,0
J,"Ross, DA; Zemel, RS",,,,"Ross, David A.; Zemel, Richard S.",,,Learning parts-based representations of data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many perceptual models and theories hinge on treating objects as a collection of constituent parts. When applying these approaches to data, a fundamental problem arises: how can we determine what are the parts? We attack this problem using learning, proposing a form of generative latent factor model, in which each data dimension is allowed to select a different factor or part as its explanation. This approach permits a range of variations that posit different models for the appearance of a part. Here we provide the details for two such models: a discrete and a continuous one. Further, we show that this latent factor model can be extended hierarchically to account for correlations between the appearances of different parts. This permits modeling of data consisting of multiple categories, and learning these categories simultaneously with the parts when they are unobserved. Experiments demonstrate the ability to learn parts-based representations, and categories, of facial images and user-preference data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2369,2397,,,,,,,,,,,,,,,,WOS:000245390700004,0
J,"Shimizu, S; Hoyer, PO; Hyvarinen, A; Kerminen, A",,,,"Shimizu, Shohei; Hoyer, Patrik O.; Hyvarinen, Aapo; Kerminen, Antti",,,A linear non-Gaussian acyclic model for causal discovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data and real-world data.",,,,,"Shimizu, Shohei/B-4425-2010; Hyvarinen, Aapo/E-9006-2012","Shimizu, Shohei/0000-0002-1931-0733; Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2003,2030,,,,,,,,,,,,,,,,WOS:000245390500003,0
J,"Laskov, P; Gehl, C; Kruger, S; Muller, KR",,,,"Laskov, Pavel; Gehl, Christian; Krueger, Stefan; Mueller, Klaus-Robert",,,"Incremental support vector learning: analysis, implementation and applications",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network traffic, can be foreseen.",,,,,"Muller, Klaus R/C-3196-2013; Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,SEP,2006,7,,,,,,1909,1936,,,,,,,,,,,,,,,,WOS:000245389400005,0
J,"Wainwright, MJ",,,,"Wainwright, Martin J.",,,Estimating the wrong graphical model: Benefits in the computation-limited setting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider the problem of joint parameter estimation and prediction in a Markov random field: that is, the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the wrong model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious benefit of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,SEP,2006,7,,,,,,1829,1859,,,,,,,,,,,,,,,,WOS:000245389400002,0
J,"De Bie, T; Cristianini, N",,,,"De Bie, Tijl; Cristianini, Nello",,,"Fast SDP relaxations of graph cut clustering, transduction, and other combinatorial problems",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The rise of convex programming has changed the face of many research fields in recent years, machine learning being one of the ones that benefitted the most. A very recent developement, the relaxation of combinatorial problems to semi-definite programs ( SDP), has gained considerable attention over the last decade (Helmberg, 2000; De Bie and Cristianini, 2004a). Although SDP problems can be solved in polynomial time, for many relaxations the exponent in the polynomial complexity bounds is too high for scaling to large problem sizes. This has hampered their uptake as a powerful new tool in machine learning. In this paper, we present a new and fast SDP relaxation of the normalized graph cut problem, and investigate its usefulness in unsupervised and semi-supervised learning. In particular, this provides a convex algorithm for transduction, as well as approaches to clustering. We further propose a whole cascade of fast relaxations that all hold the middle between older spectral relaxations and the new SDP relaxation, allowing one to trade off computational cost versus relaxation accuracy. Finally, we discuss how the methodology developed in this paper can be applied to other combinatorial problems in machine learning, and we treat the max-cut problem as an example.",,,,,"De Bie, Tijl/B-2920-2013","De Bie, Tijl/0000-0002-2692-7504",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1409,1436,,,,,,,,,,,,,,,,WOS:000245388800011,0
J,"Glasmachers, T; Igel, C",,,,"Glasmachers, Tobias; Igel, Christian",,,Maximum-gain working set selection for SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Support vector machines are trained by solving constrained quadratic optimization problems. This is usually done with an iterative decomposition algorithm operating on a small working set of variables in every iteration. The training time strongly depends on the selection of these variables. We propose the maximum-gain working set selection algorithm for large scale quadratic programming. It is based on the idea to greedily maximize the progress in each single iteration. The algorithm takes second order information from cached kernel matrix entries into account. We prove the convergence to an optimal solution of a variant termed hybrid maximum-gain working set selection. This method is empirically compared to the prominent most violating pair selection and the latest algorithm using second order information. For large training sets our new selection scheme is significantly faster.,,,,,"Igel, Christian/B-4091-2009","Igel, Christian/0000-0003-2868-0856",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1437,1466,,,,,,,,,,,,,,,,WOS:000245388800012,0
J,"Even-Dar, E; Mannor, S; Mansour, Y",,,,"Even-Dar, Eyal; Mannor, Shie; Mansour, Yishay",,,Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O((n/epsilon(2)) log(1/delta)) times to find an epsilon-optimal arm with probability of at least 1-delta. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over epsilon-greedy Q-learning.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1079,1105,,,,,,,,,,,,,,,,WOS:000245388400008,0
J,"Ryabko, D",,,,"Ryabko, D",,,Pattern recognition for conditionally independent data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,15th Annual International Conference on Algorithmic Learning Theory (ALT 2004),"OCT 02-05, 2004","Padua, ITALY","Univ Lubeck, Inst Theoret Comp Sci,Hokkaido Univ, Div Comp Sci",,,,"In this work we consider the task of relaxing the i.i.d. assumption in pattern recognition ( or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples ( pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold. We find a broad class of learning algorithms for which estimations of the probability of the classification error achieved under the classical i.i.d. assumption can be generalized to the similar estimates for case of conditionally i.i.d. examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2006,7,,,,,,645,664,,,,,,,,,,,,,,,,WOS:000237359100004,0
J,"Micchelli, CA; Pontil, M",,,,"Micchelli, CA; Pontil, M",,,Learning the kernel function via regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of finding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m+2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1099,1125,,,,,,,,,,,,,,,,WOS:000236329900004,0
J,"Tsuda, K; Ratsch, G; Warmuth, MK",,,,"Tsuda, K; Ratsch, G; Warmuth, MK",,,Matrix exponentiated gradient updates for on-line learning and Bregman projection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address the problem of learning a symmetric positive definite matrix. The central issue is to design parameter updates that preserve positive definiteness. Our updates are motivated with the von Neumann divergence. Rather than treating the most general case, we focus on two key applications that exemplify our methods: on-line learning with a simple square loss, and finding a symmetric positive definite matrix subject to linear constraints. The updates generalize the exponentiated gradient (EG) update and AdaBoost, respectively: the parameter is now a symmetric positive definite matrix of trace one instead of a probability vector ( which in this context is a diagonal positive definite matrix with trace one). The generalized updates use matrix logarithms and exponentials to preserve positive definiteness. Most importantly, we show how the derivation and the analyses of the original EG update and AdaBoost generalize to the non-diagonal case. We apply the resulting matrix exponentiated gradient ( MEG) update and DefiniteBoost to the problem of learning a kernel matrix from distance measurements.",,,,,"R√§tsch, Gunnar/B-8182-2009; R√§tsch, Gunnar/O-5914-2017","R√§tsch, Gunnar/0000-0001-5486-8532",,,,,,,,,,,,,1532-4435,,,,,JUN,2005,6,,,,,,995,1018,,,,,,,,,,,,,,,,WOS:000236329800003,0
J,"Langford, J",,,,"Langford, J",,,Tutorial on practical prediction theory for classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discuss basic prediction theory and its impact on classification success evaluation, implications for learning algorithm design, and uses in learning algorithm execution. This tutorial is meant to be a comprehensive compilation of results which are both theoretically rigorous and quantitatively useful. There are two important implications of the results presented here. The first is that common practices for reporting results in classification should change to use the test set bound. The second is that train set bounds can sometimes be used to directly motivate learning algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2005,6,,,,,,273,306,,,,,,,,,,,,,,,,WOS:000236329400002,0
J,"Rusakov, D; Geiger, D",,,,"Rusakov, D; Geiger, D",,,Asymptotic model selection for naive Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally incorrect for statistical models that belong to stratified exponential families. This claim stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct asymptotic approximation for the marginal likelihood.",,,,,"Rusakov, Dmitri A/E-1987-2011","Rusakov, Dmitri A/0000-0001-9539-9947",,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,1,35,,,,,,,,,,,,,,,,WOS:000236328800001,0
J,"Bengio, Y; Grandvalet, Y",,,,"Bengio, Y; Grandvalet, Y",,,No unbiased estimator of the variance of K-fold cross-validation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms ( in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal ( valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators ( that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2004,5,,,,,,1089,1105,,,,,,,,,,,,,,,,WOS:000236328100001,0
J,"Costa, VS; Srinivasan, A; Camacho, R; Blockeel, H; Demoen, B; Janssens, G; Struyf, J; Vandecasteele, H; Van Laer, W",,,,"Costa, VS; Srinivasan, A; Camacho, R; Blockeel, H; Demoen, B; Janssens, G; Struyf, J; Vandecasteele, H; Van Laer, W",,,Query transformations for improving the efficiency of ILP systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,10th International Conference on Inductive Logic Programming (ILP2000),"JUL, 2000","London, ENGLAND",,,,,"Relatively simple transformations can speed up the execution of queries for data mining considerably. While some ILP systems use such transformations, relatively little is known about them or how they relate to each other. This paper describes a number of such transformations. Not all of them are novel, but there have been no studies comparing their efficacy. The main contributions of the paper are: (a) it clarifies the relationship between the transformations; (b) it contains an empirical study of what can be gained by applying the transformations; and (c) it provides some guidance on the kinds of problems that are likely to benefit from the transformations.",,,,,"Costa, Vitor Santos/B-2859-2012; Blockeel, Hendrik LW/L-6993-2013; INESC-TEC, CRACS/F-7527-2012; Janssens, Gerda/N-2662-2013; Camacho, Rui/ABF-4895-2021","Costa, Vitor Santos/0000-0002-3344-8237; Blockeel, Hendrik LW/0000-0003-0378-3699; Camacho, Rui/0000-0003-0940-3554",,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,465,491,,10.1162/153244304773936027,0,,,,,,,,,,,,,WOS:000221345700004,0
J,"Crammer, K; Singer, Y",,,,"Crammer, K; Singer, Y",,,A family of additive online algorithms for category ranking,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Machine Learning Methods for Text and Images,2001,"VANCOUVER, CANADA",,,,,"We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stem from recent advances in online learning algorithms. The algorithms are simple to implement and are also time and memory efficient. We provide a unified analysis of the family of algorithms in the mistake bound model. We then discuss experiments with the proposed family of topic-ranking algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora, the algorithms we present achieve state-of-the-art results and outperforms topic-ranking adaptations of Rocchio's algorithm and of the Perceptron algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1025,1058,,10.1162/153244303322533188,0,,,,,,,,,,,,,WOS:000186002400002,0
J,"Nevo, Z; El-Yaniv, R",,,,"Nevo, Z; El-Yaniv, R",,,On Online learning of decision lists,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A fundamental open problem in computational learning theory is whether there is an attribute efficient learning algorithm for the concept class of decision lists (Rivest, 1987; Blum, 1996). We consider a weaker problem, where the concept class is restricted to decision lists with D alternations. For this class, we present a novel online algorithm that achieves a mistake bound of O(r(D) log n), where r is the number of relevant variables, and n is the total number of variables. The algorithm can be viewed as a strict generalization of the famous Winnow algorithm by Littlestone (1988), and improves the O(r(2D) log n) mistake bound of Balanced Winnow. Our bound is stronger than a similar PAC-learning result of Dhagat and Hellerstein (1994). A combination of our algorithm with the algorithm suggested by Rivest (1987) might achieve even better bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,271,301,,10.1162/153244303765208395,0,,,,,,,,,,,,,WOS:000182488500004,0
J,"Mendelson, S",,,,"Mendelson, S",,,On the size of convex hulls of small sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate two different notions of size which appear naturally in Statistical Learning Theory. We present quantitative estimates on the fat-shattering dimension and on the covering numbers of convex hulls of sets of functions, given the necessary data on the original sets. The proofs we present are relatively simple since they do not require extensive background in convex geometry.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,1,,,,,1,18,,,,,,,,,,,,,,,,WOS:000173838200001,0
J,"Bousquet, O; Elisseeff, A",,,,"Bousquet, O; Elisseeff, A",,,Stability and generalization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when the classifier is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classification.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,3,,,,,499,526,,10.1162/153244302760200704,0,,,,,,,,,,,,,WOS:000178101500007,0
J,"Downs, T; Gates, KE; Masters, A",,,,"Downs, T; Gates, KE; Masters, A",,,Exact simplification of support vector solutions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, COLORADO",,,,,This paper demonstrates that standard algorithms for training support vector machines generally produce solutions with a greater number of support vectors than are strictly necessary. An algorithm is presented that allows unnecessary support vectors to be recognized and eliminated while leaving the solution otherwise unchanged. The algorithm is applied to a variety of benchmark data sets (for both classification and regression) and in most cases the procedure leads to a reduction in the number of support vectors. In some cases the reduction is substantial.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,293,297,,10.1162/15324430260185637,0,,,,,,,,,,,,,WOS:000176055300010,0
J,"Smola, AJ; Mika, S; Scholkopf, B; Williamson, RC",,,,"Smola, AJ; Mika, S; Scholkopf, B; Williamson, RC",,,Regularized principal manifolds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding. We explore connection in two ways: (1) we propose an algorithm for finding principal manifolds that can be regularized in a variety of ways; and (2) we derive uniform convergence bounds and hence bounds on the learning rates of the algorithm. In particular, we give bounds on the covering numbers which allows us to obtain nearly optimal learning rates for certain types of regularization operators. Experimental results demonstrate the feasibility of the approach.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,1532-4435,,,,,JUN,2001,1,3,,,,,179,209,,10.1162/15324430152748227,0,,,,,,,,,,,,,WOS:000173336900002,0
J,"Boyan, JA; Moore, AW",,,,"Boyan, JA; Moore, AW",,,Learning evaluation functions to improve optimization by local search,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes algorithms that learn to improve search performance on large-scale optimization tasks. The main algorithm, STAG, works by learning an evaluation function that predicts the outcome of a local search algorithm, such as hillclimbing or WALKSAT, from features of states visited during search. The learned evaluation function is then used to bias future search trajectories toward better optima on the same problem. Another algorithm, X-STAGE, transfers previously learned evaluation functions to new, similar optimization problems. Empirical results are provided on seven large-scale optimization domains: bin-packing, channel routing, Bayesian network structure-finding, radiotherapy treatment planning, cartogram design, Boolean satisfiability, and Boggle board setup.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2001,1,2,,,,,77,112,,10.1162/15324430152733124,0,,,,,,,,,,,,,WOS:000173336800001,0
J,"Mania, H; Jordan, MI; Recht, B",,,,"Mania, Horia; Jordan, Michael, I; Recht, Benjamin",,,Active Learning for Nonlinear System Identification with Guarantees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While the identification of nonlinear dynamical systems is a fundamental building block of modelbased reinforcement learning and feedback control, its sample complexity is only understood for systems that either have discrete states and actions or for systems that can be identified from data generated by i.i.d. random inputs. Nonetheless, many interesting dynamical systems have continuous states and actions and can only be identified through a judicious choice of inputs. Motivated by practical settings, we study a class of nonlinear dynamical systems whose state transitions depend linearly on a known feature embedding of state-action pairs. To estimate such systems in finite time identification methods must explore all directions in feature space. We propose an active learning approach that achieves this by repeating three steps: trajectory planning, trajectory tracking, and re-estimation of the system from all available data. We show that our method estimates nonlinear dynamical systems at a parametric rate, similar to the statistical rate of standard linear regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752388600001,0
J,"Bengs, V; Busa-Fekete, R; El Mesaoudi-Paul, A; Hullermeier, E",,,,"Bengs, Viktor; Busa-Fekete, Robert; El Mesaoudi-Paul, Adil; Huellermeier, Eyke",,,Preference-based Online Learning with Dueling Bandits: A Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In machine learning, the notion of multi-armed bandits refers to a class of online learning problems, in which an agent is supposed to simultaneously explore and exploit a given set of choice alternatives in the course of a sequential decision process. In the standard setting, the agent learns from stochastic feedback in the form of real-valued rewards. In many applications, however, numerical reward signals are not readily available-instead, only weaker information is provided, in particular relative preferences in the form of qualitative comparisons between pairs of alternatives. This observation has motivated the study of variants of the multi-armed bandit problem, in which more general representations are used both for the type of feedback to learn from and the target of prediction. The aim of this paper is to provide a survey of the state of the art in this field, referred to as preference-based multi-armed bandits or dueling bandits. To this end, we provide an overview of problems that have been considered in the literature as well as methods for tackling them. Our taxonomy is mainly based on the assumptions made by these methods about the data-generating process and, related to this, the properties of the preference-based feedback.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500007,0
J,"Biscione, V; Bowers, JS",,,,"Biscione, Valerio; Bowers, Jeffrey S.",,,"Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When seeing a new object, humans can immediately recognize it across different retinal locations: the internal object representation is invariant to translation. It is commonly believed that Convolutional Neural Networks (CNNs) are architecturally invariant to translation thanks to the convolution and/or pooling operations they are endowed with. In fact, several studies have found that these networks systematically fail to recognise new objects on untrained locations. In this work, we test a wide variety of CNNs architectures showing how, apart from DenseNet-121, none of the models tested was architecturally invariant to translation. Nevertheless, all of them could learn to be invariant to translation. We show how this can be achieved by pretraining on ImageNet, and it is sometimes possible with much simpler data sets when all the items are fully translated across the input canvas. At the same time, this invariance can be disrupted by further training due to catastrophic forgetting/interference. These experiments show how pretraining a network on an environment with the right 'latent' characteristics (a more naturalistic environment) can result in the network learning deep perceptual rules which would dramatically improve subsequent generalization.",,,,,,"Bowers, Jeffrey/0000-0001-9558-5010",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706446700001,0
J,"Charlier, B; Feydy, J; Glaunes, JA; Collin, FD; Durif, G",,,,"Charlier, Benjamin; Feydy, Jean; Glaunes, Joan Alexis; Collin, Francois-David; Durif, Ghislain",,,"Kernel Operations on the GPU, with Autodiff, without Memory Overflows",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The KeOps library provides a fast and memory-efficient GPU support for tensors whose entries are given by a mathematical formula, such as kernel and distance matrices. KeOps alleviates the main bottleneck of tensor-centric libraries for kernel and geometric applications: memory consumption. It also supports automatic differentiation and outperforms standard GPU baselines, including PyTorch CUDA tensors or the Halide and TVM libraries. KeOps combines optimized C++/CUDA schemes with binders for high-level languages: Python (Numpy and PyTorch), Matlab and GNU R. As a result, high-level quadratic codes can now scale up to large data sets with millions of samples processed in seconds. KeOps brings graphics-like performances for kernel methods and is freely available on standard repositories (PyPi, CRAN). To showcase its versatility, we provide tutorials in a wide range of settings online at www.kernel-operations.io.",,,,,,"DURIF, Ghislain/0000-0003-2567-1401",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,2021,,,,,,,,,,,,,,,WOS:000656386100001,0
J,"Chen, X; Chernozhukov, V; Fernandez-Val, I; Kostyshak, S; Luo, Y",,,,"Chen, Xi; Chernozhukov, Victor; Fernandez-Val, Ivan; Kostyshak, Scott; Luo, Ye",,,Shape-Enforcing Operators for Generic Point and Interval Estimators of Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common problem in econometrics, statistics, and machine learning is to estimate and make inference on functions that satisfy shape restrictions. For example, distribution functions are nondecreasing and range between zero and one, height growth charts are nondecreasing in age, and production functions are nondecreasing and quasi-concave in input quantities. We propose a method to enforce these restrictions ex post on generic unconstrained point and interval estimates of the target function by applying functional operators. The interval estimates could be either frequentist confidence bands or Bayesian credible regions. If an operator has reshaping, invariance, order-preserving, and distance reducing properties, the shape-enforced point estimates are closer to the target function than the original point estimates and the shape-enforced interval estimates have greater coverage and shorter length than the original interval estimates. We show that these properties hold for six different operators that cover commonly used shape restrictions in practice: range, convexity, monotonicity, monotone convexity, quasi-convexity, and monotone quasi-convexity, with the latter two restrictions being of paramount importance. The main attractive property of the post-processing approach is that it works in conjunction with any generic initial point or interval estimate, obtained using any of parametric, semi parametric or nonparametric learning methods, including recent methods that are able to exploit either smoothness, sparsity, or other forms of structured parsimony of target functions. The post-processed point and interval estimates automatically inherit and provably improve these properties in finite samples, while also enforcing qualitative shape restrictions brought by scientific reasoning. We illustrate the results with two empirical applications to the estimation of a height growth chart for infants in India and a production function for chemical firms in China.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706448800001,0
J,"Damianou, A; Lawrence, ND; Ek, CH",,,,"Damianou, Andreas; Lawrence, Neil D.; Ek, Carl Henrik",,,Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Factor analysis aims to determine latent factors, or traits, which summarize a given data set. Inter-battery factor analysis extends this notion to multiple views of the data. In this paper we show how a nonlinear, nonparametric version of these models can be recovered through the Gaussian process latent variable model. This gives us a flexible formalism for multi-view learning where the latent variables can be used both for exploratory purposes and for learning representations that enable efficient inference for ambiguous estimation tasks. Learning is performed in a Bayesian manner through the formulation of a variational compression scheme which gives a rigorous lower bound on the log likelihood. Our Bayesian framework provides strong regularization during training, allowing the structure of the latent space to be determined efficiently and automatically. We demonstrate this by producing the first (to our knowledge) published results of learning from dozens of views, even when data is scarce. We further show experimental results on several different types of multi-view data sets and for different kinds of tasks, including exploratory data analysis, generation, ambiguity modelling through latent priors and classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,86,,,,,,,,,,,,,,,WOS:000663143700001,0
J,"Fan, A; Bhosale, S; Schwenk, H; Ma, ZY; El-Kishky, A; Goyal, S; Baines, M; Celebi, O; Wenzek, G; Chaudhary, V; Goyal, N; Birch, T; Liptchinsky, V; Edunov, S; Grave, E; Auli, M; Joulin, A",,,,"Fan, Angela; Bhosale, Shruti; Schwenk, Holger; Ma, Zhiyi; El-Kishky, Ahmed; Goyal, Siddharth; Baines, Mandeep; Celebi, Onur; Wenzek, Guillaume; Chaudhary, Vishrav; Goyal, Naman; Birch, Tom; Liptchinsky, Vitaliy; Edunov, Sergey; Grave, Edouard; Auli, Michael; Joulin, Armand",,,Beyond English-Centric Multilingual Machine Translation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model: https://github.com/pytorch/fairseq/tree/master/examples/m2m_100.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,107,,,,,,,,,,,,,,,WOS:000663162000001,0
J,"Galvan, G; Lapucci, M; Lin, CJ; Sciandrone, M",,,,"Galvan, Giulio; Lapucci, Matteo; Lin, Chih-Jen; Sciandrone, Marco",,,A Two-Level Decomposition Framework Exploiting First and Second Order Information for SVM Training Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this work we present a novel way to solve the sub-problems that originate when using decomposition algorithms to train Support Vector Machines (SVMs). State-of-the-art Sequential Minimization Optimization (SMO) solvers reduce the original problem to a sequence of sub-problems of two variables for which the solution is analytical. Although considering more than two variables at a time usually results in a lower number of iterations needed to train an SVM model, solving the sub-problem becomes much harder and the overall computational gains are limited, if any. We propose to apply the two-variables decomposition method to solve the sub-problems themselves and experimentally show that it is a viable and efficient way to deal with sub-problems of up to 50 variables. As a second contribution we explore different ways to select the working set and its size, combining first-order and second-order working set selection rules together with a strategy for exploiting cached elements of the Hessian matrix. An extensive numerical comparison shows that the method performs considerably better than state-of-the-art software.",,,,,"Lapucci, Matteo/AAD-4672-2019","Lapucci, Matteo/0000-0002-2488-5486",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500023,0
J,"Liang, TY",,,,"Liang, Tengyuan",,,How Well Generative Adversarial Networks Learn Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the rates of convergence for learning distributions implicitly with the adversarial framework and Generative Adversarial Networks (GANs), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated Method of Moments (GMM/SMM) as special cases. We study a wide range of parametric and nonparametric target distributions under a host of objective evaluation metrics. We investigate how to obtain valid statistical guarantees for GANs through the lens of regularization. On the nonparametric end, we derive the optimal minimax rates for distribution estimation under the adversarial framework. On the parametric end, we establish a theory for general neural network classes (including deep leaky ReLU networks) that characterizes the interplay on the choice of generator and discriminator pair. We discover and isolate a new notion of regularization, called the generator-discriminator-pair regularization, that sheds light on the advantage of GANs compared to classical parametric and nonparametric approaches for explicit distribution estimation. We develop novel oracle inequalities as the main technical tools for analyzing GANs, which are of independent interest.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706866700001,0
J,"Loper, J; Blei, D; Cunningham, JP; Paninski, L",,,,"Loper, Jackson; Blei, David; Cunningham, John P.; Paninski, Liam",,,A general linear-time inference method for Gaussian Processes on one dimension,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian Processes (GPs) provide powerful probabilistic frameworks for interpolation, forecasting, and smoothing, but have been hampered by computational scaling issues. Here we investigate data sampled on one dimension (e.g., a scalar or vector time series sampled at arbitrarily-spaced intervals), for which state-space models are popular due to their linearly-scaling computational costs. It has long been conjectured that state-space models are general, able to approximate any one-dimensional GP. We provide the first general proof of this conjecture, showing that any stationary GP on one dimension with vector-valued observations governed by a Lebesgue-integrable continuous kernel can be approximated to any desired precision using a specifically-chosen state-space model: the Latent Exponentially Generated (LEG) family. This new family offers several advantages compared to the general state-space model: it is always stable (no unbounded growth), the covariance can be computed in closed form, and its parameter space is unconstrained (allowing straightforward estimation via gradient descent). The theorem's proof also draws connections to Spectral Mixture Kernels, providing insight about this popular family of kernels. We develop parallelized algorithms for performing inference and learning in the LEG model, test the algorithm on real and synthetic data, and demonstrate scaling to datasets with billions of samples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706881500001,0
J,"Ramezani-Kebrya, A; Faghri, F; Markov, I; Aksenov, V; Alistarh, D; Roy, DM",,,,"Ramezani-Kebrya, Ali; Faghri, Fartash; Markov, Ilya; Aksenov, Vitalii; Alistarh, Dan; Roy, Daniel M.",,,NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed to perform parallel model training. One popular communication-compression method for data-parallel SGD is QSGD (Alistarh et al., 2017), which quantizes and encodes gradients to reduce communication costs. The baseline variant of QSGD provides strong theoretical guarantees, however, for practical purposes, the authors proposed a heuristic variant which we call QSGDinf, which demonstrated impressive empirical gains for distributed training of large neural networks. In this paper, we build on this work to propose a new gradient quantization scheme, and show that it has both stronger theoretical guarantees than QSGD, and matches and exceeds the empirical performance of the QSGDinf heuristic and of other compression methods.",,,,,"Aksenov, Vitaly/I-4568-2016","Aksenov, Vitaly/0000-0001-9134-5490; Ramezani-Kebrya, Ali/0000-0002-8767-5603; Alistarh, Dan/0000-0003-3650-940X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,114,,,,,,,,,,,,,,,WOS:000663169100001,0
J,"van Erven, T; Koolen, WM; van der Hoeven, D",,,,"van Erven, Tim; Koolen, Wouter M.; van der Hoeven, Dirk",,,MetaGrad: Adaptation using Multiple Learning Rates in Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide a new adaptive method for online convex optimization, MetaGrad, that is robust to general convex losses but achieves faster rates for a broad class of special functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature. We prove this by drawing a connection to the Bernstein condition, which is known to imply fast rates in offline statistical learning. MetaGrad further adapts automatically to the size of the gradients. Its main feature is that it simultaneously considers multiple learning rates, which are weighted directly proportional to their empirical performance on the data using a new meta-algorithm. We provide three versions of MetaGrad. The full matrix version maintains a full covariance matrix and is applicable to learning tasks for which we can afford update time quadratic in the dimension. The other two versions provide speed-ups for high-dimensional learning tasks with an update time that is linear in the dimension: one is based on sketching, the other on running a separate copy of the basic algorithm per coordinate. We evaluate all versions of MetaGrad on benchmark online classification and regression tasks, on which they consistently outperform both online gradient descent and AdaGrad.",,,,,,"van Erven, Tim/0000-0002-9200-1451",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687126800001,0
J,"Zimmert, J; Seldin, Y",,,,"Zimmert, Julian; Seldin, Yevgeny",,,Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive an algorithm that achieves the optimal (within constants) pseudo-regret in both adversarial and stochastic multi-armed bandits without prior knowledge of the regime and time horizon.(1) The algorithm is based on online mirror descent (OMD) with Tsallis entropy regularization with power alpha = 1/2 and reduced-variance loss estimators. More generally, we define an adversarial regime with a self-bounding constraint, which includes stochastic regime, stochastically constrained adversarial regime (Wei and Luo, 2018), and stochastic regime with adversarial corruptions (Lykouris et al., 2018) as special cases, and show that the algorithm achieves logarithmic regret guarantee in this regime and all of its special cases simultaneously with the optimal regret guarantee in the adversarial regime. The algorithm also achieves adversarial and stochastic optimality in the utility-based dueling bandit setting. We provide empirical evaluation of the algorithm demonstrating that it significantly outperforms Ucb1 and Exp3 in stochastic environments. We also provide examples of adversarial environments, where Ucb1 and THOMPSON SAMPLING exhibit almost linear regret, whereas our algorithm suffers only logarithmic regret. To the best of our knowledge, this is the first example demonstrating vulnerability of Thompson Sampling in adversarial environments. Last but not least, we present a general stochastic analysis and a general adversarial analysis of OMD algorithms with Tsallis entropy regularization for alpha is an element of [0, 1] and explain the reason why alpha = 1/2 works best.",,,,,"Seldin, Yevgeny/G-8955-2015","Seldin, Yevgeny/0000-0003-3152-4635",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500028,0
J,"Chen, YS; Dwivedi, R; Wainwright, MJ; Yu, B",,,,"Chen, Yuansi; Dwivedi, Raaz; Wainwright, Martin J.; Yu, Bin",,,Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo sampling algorithm for drawing samples from smooth probability densities over continuous spaces. We study the variant most widely used in practice, Metropolized HMC with the Stormer-Verlet or leapfrog integrator, and make two primary contributions. First, we provide a non-asymptotic upper bound on the mixing time of the Metropolized HMC with explicit choices of step-size and number of leapfrog steps. This bound gives a precise quantification of the faster convergence of Metropolized HMC relative to simpler MCMC algorithms such as the Metropolized random walk, or Metropolized Langevin algorithm. Second, we provide a general framework for sharpening mixing time bounds of Markov chains initialized at a substantial distance from the target distribution over continuous spaces. We apply this sharpening device to the Metropolized random walk and Langevin algorithms, thereby obtaining improved mixing time bounds from a non-warm initial distribution.",,,,,"Dwivedi, Raaz/AAZ-2028-2020",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,92,,,,,,,,,,,,,,,WOS:000545027500001,0
J,"De, A; Long, PM; Servedio, RA",,,,"De, Anindya; Long, Philip M.; Servedio, Rocco A.",,,Learning Sums of Independent Random Variables with Sparse Collective Support,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the learnability of sums of independent integer random variables given a bound on the size of the union of their supports. For A subset of Z(+), a sum of independent random variables with collective support A (called an A-sum in this paper) is a distribution S = X-1 + ... + X-N where the X-i's are mutually independent (but not necessarily identically distributed) integer random variables with U-i supp(X-i) subset of A. We give two main algorithmic results for learning such distributions. First, for the case vertical bar A vertical bar = 3, we give an algorithm for learning an unknown A-sum to accuracy  using poly(1/epsilon) samples and running in time poly(1/epsilon), independent of N and of the elements of A. Second, for an arbitrary constant k >= 4, if A = {a(1), ..., ak(g)} with 0 <= a(1) < ... < a(k), we give an algorithm that uses poly(1=) log log ak samples (independent of N) and runs in time poly(1/epsilon; loga(k)): We prove an essentially matching lower bound: if vertical bar A vertical bar = 4, then any algorithm must use Omega(log log a(4)) samples even for learning to constant accuracy. We also give similar-in-spirit (but quantitatively very different) algorithmic results, and essentially matching lower bounds, for the case in which A is not known to the learner. Our algorithms and lower bounds together settle the question of how the sample complexity of learning sums of independent integer random variables scales with the elements in the union of their supports, both in the known-support and unknown-support settings. Finally, all our algorithms easily extend to the semi-agnostic learning model, in which training data is generated from a distribution that is only c-close to some A-sum for a constant c > 0.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,79,221,,,,,,,,,,,,,,,WOS:000605741700001,0
J,"Heber, F; Trst'nova, Z; Leimkuhler, B",,,,"Heber, Frederik; Trst'nova, Zofia; Leimkuhler, Benedict",,,Posterior sampling strategies based on discretized stochastic differential equations for machine learning applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the advent of GPU-assisted hardware and maturing high-efficiency software platforms such as TensorFlow and PyTorch, Bayesian posterior sampling for neural networks becomes plausible. In this article we discuss Bayesian parametrization in machine learning based on Markov Chain Monte Carlo methods, specifically discretized stochastic differential equations such as Langevin dynamics and extended system methods in which an ensemble of walkers is employed to enhance sampling. We provide a glimpse of the potential of the sampling-intensive approach by studying (and visualizing) the loss landscape of a neural network applied to the MNIST data set. Moreover, we investigate how the sampling efficiency itself can be significantly enhanced through an ensemble quasi-Newton preconditioning method. This article accompanies the release of a new TensorFlow software package, the Thermodynamic Analytics ToolkIt, which is used in the computational experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,228,,,,,,,,,,,,,,,WOS:000605746000001,0
J,"Horel, E; Giesecke, K",,,,"Horel, Enguerrand; Giesecke, Kay",,,Significance Tests for Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a pivotal test to assess the statistical significance of the feature variables in a single-layer feedforward neural network regression model. We propose a gradient-based test statistic and study its asymptotics using nonparametric techniques. Under technical conditions, the limiting distribution is given by a mixture of chi-square distributions. The tests enable one to discern the impact of individual variables on the prediction of a neural network. The test statistic can be used to rank variables according to their influence. Simulation results illustrate the computational efficiency and the performance of the test. An empirical application to house price valuation highlights the behavior of the test using actual data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,227,,,,,,,,,,,,,,,WOS:000605745600001,0
J,"Thalmeier, D; Kappen, HJ; Totaro, S; Gomez, V",,,,"Thalmeier, Dominik; Kappen, Hilbert J.; Totaro, Simone; Gomez, Vicenc",,,Adaptive Smoothing for Path Integral Control,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In Path Integral control problems a representation of an optimally controlled dynamical system can be formally computed and serve as a guidepost to learn a parametrized policy. The Path Integral Cross-Entropy (PICE) method tries to exploit this, but is hampered by poor sample efficiency. We propose a model-free algorithm called ASPIC (Adaptive Smoothing of Path Integral Control) that applies an inf-convolution to the cost function to speedup convergence of policy optimization. We identify PICE as the infinite smoothing limit of such technique and show that the sample efficiency problems that PICE suffers disappear for finite levels of smoothing. For zero smoothing, ASPIC becomes a greedy optimization of the cost, which is the standard approach in current reinforcement learning. ASPIC adapts the smoothness parameter to keep the variance of the gradient estimator at a predefined level, independently of the number of samples. We show analytically and empirically that intermediate levels of smoothing are optimal, which renders the new method superior to both PICE and direct cost optimization.",,,,,"G√≥mez, Vicen√ß/D-1984-2009; Kappen, H.J./L-4425-2015","G√≥mez, Vicen√ß/0000-0001-5146-7645; ",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,191,,,,,,,,,,,,,,,WOS:000589999400001,0
J,"Daneshmand, A; Sun, Y; Scutari, G; Facchinei, F; Sadler, BM",,,,"Daneshmand, Amir; Sun, Ying; Scutari, Gesualdo; Facchinei, Francisco; Sadler, Brian M.",,,Decentralized Dictionary Learning Over Time-Varying Digraphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies Dictionary Learning problems wherein the learning task is distributed over a multi-agent network, modeled as a time-varying directed graph. This formulation is relevant, for instance, in Big Data scenarios where massive amounts of data are collected/stored in different locations (e.g., sensors, clouds) and aggregating and/or processing all data in a fusion center might be inefficient or unfeasible, due to resource limitations, communication overheads or privacy issues. We develop a unified decentralized algorithmic framework for this class of nonconvex problems, which is proved to converge to stationary solutions at a sublinear rate. The new method hinges on Successive Convex Approximation techniques, coupled with a decentralized tracking mechanism aiming at locally estimating the gradient of the smooth part of the sum-utility. To the best of our knowledge, this is the first provably convergent decentralized algorithm for Dictionary Learning and, more generally, bi-convex problems over (time-varying) (di)graphs.",,,,,"Sadler, Brian M./AAR-6018-2021; Sun, Yingyuan/HHS-7124-2022","Sadler, Brian M./0000-0002-9564-3812; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,139,,,,,,,,,,,,,,,WOS:000491132200003,0
J,"Lopes, ME; Wang, SS; Mahoney, MW",,,,"Lopes, Miles E.; Wang, Shusen; Mahoney, Michael W.",,,A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know (1) how accurate a given solution is, or (2) how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication (sketching) as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for directly estimating the accuracy as a function of the reduced dimension (as opposed to deriving worst-case bounds on the accuracy in terms of the reduced dimension). From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an extrapolation technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method.",,,,,,"Wang, Shusen/0000-0003-3928-6782",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,39,,,,,,,,,,,,,,,WOS:000463319500001,0
J,"Luo, L; Chen, C; Zhang, ZH; Li, WJ; Zhang, T",,,,"Luo, Luo; Chen, Cheng; Zhang, Zhihua; Li, Wu-Jun; Zhang, Tong",,,Robust Frequent Directions with Application in Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The frequent directions (FD) technique is a deterministic approach for online sketching that has many applications in machine learning. The conventional FD is a heuristic procedure that often outputs rank deficient matrices. To overcome the rank deficiency problem, we propose a new sketching strategy called robust frequent directions (RFD) by introducing a regularization term. RFD can be derived from an optimization problem. It updates the sketch matrix and the regularization term adaptively and jointly. RFD reduces the approximation error of FD without increasing the computational cost. We also apply RFD to online learning and propose an effective hyperparameter-free online Newton algorithm. We derive a regret bound for our online Newton algorithm based on RFD, which guarantees the robustness of the algorithm. The experimental studies demonstrate that the proposed method outperforms state-of-the-art second order online learning algorithms.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,1,41,45,,,,,,,,,,,,,,,WOS:000463320400001,0
J,"Scaman, K; Bach, F; Bubeck, S; Lee, YT; Massoulie, L",,,,"Scaman, Kevin; Bach, Francis; Bubeck, Sebastien; Lee, Yin Tat; Massoulie, Laurent",,,Optimal Convergence Rates for Convex Distributed Optimization in Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work proposes a theoretical analysis of distributed optimization of convex functions using a network of computing units. We investigate this problem under two communication schemes (centralized and decentralized) and four classical regularity assumptions: Lipschitz continuity, strong convexity, smoothness, and a combination of strong convexity and smoothness. Under the decentralized communication scheme, we provide matching upper and lower bounds of complexity along with algorithms achieving this rate up to logarithmic constants. For non-smooth objective functions, while the dominant term of the error is in O(1/root t), the structure of the communication network only impacts a second-order term in O(1/t), where t is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly convex objective functions. Such a convergence rate is achieved by the novel multi-step primal-dual (MSPD) algorithm. Under the centralized communication scheme, we show that the naive distribution of standard optimization algorithms is optimal for smooth objective functions, and provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function for non-smooth functions. We then show that DRS is within a d(1/4) multiplicative factor of the optimal convergence rate, where d is the underlying dimension.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,159,,,,,,,,,,,,,,,WOS:000491132200023,0
J,"Vaz, AF; Izbicki, R; Stern, RB",,,,"Vaz, Afonso Fernandes; Izbicki, Rafael; Stern, Rafael Bassi",,,Quantification Under Prior Probability Shift: the Ratio Estimator and its Extensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The quantification problem consists of determining the prevalence of a given label in a target population. However, one often has access to the labels in a sample from the training population but not in the target population. A common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. In this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. Complementing this lower bound, we present a new approximately minimax class of estimators, ratio estimators, which generalize several previous proposals in the literature. Using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem. We also extend the ratio estimator so that it can: (i) incorporate labels from the target population, when they are available and (ii) estimate how the prevalence of positive labels varies according to a function of certain covariates.",,,,,"Stern, Rafael/Q-6817-2019; Stern, Rafael/CAJ-3094-2022","Stern, Rafael/0000-0002-4323-4515",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,79,,,,,,,,,,,,,,,WOS:000467896900001,0
J,"Akrour, R; Abdolmaleki, A; Abdulsamad, H; Peters, J; Neumann, G",,,,"Akrour, Riad; Abdolmaleki, Abbas; Abdulsamad, Hany; Peters, Jan; Neumann, Gerhard",,,Model-Free Trajectory-based Policy Optimization with Monotonic Improvement,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many of the recent trajectory optimization algorithms alternate between linear approximation of the system dynamics around the mean trajectory and conservative policy update. One way of constraining the policy change is by bounding the Kullback-Leibler (KL) divergence between successive policies. These approaches already demonstrated great experimental success in challenging problems such as end-to-end control of physical systems. However, the linear approximation of the system dynamics can introduce a bias in the policy update and prevent convergence to the optimal policy. In this article, we propose a new model-free trajectory-based policy optimization algorithm with guaranteed monotonic improvement. The algorithm backpropagates a local, quadratic and time-dependent Q-Function learned from trajectory data instead of a model of the system dynamics. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics. We experimentally demonstrate on highly non-linear control tasks the improvement in performance of our algorithm in comparison to approaches linearizing the system dynamics. In order to show the monotonic improvement of our algorithm, we additionally conduct a theoretical analysis of our policy update scheme to derive a lower bound of the change in policy return between successive iterations.",,,,,"Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,14,,,,,,,,,,,,,,,WOS:000443225800001,0
J,"Angiulli, F",,,,"Angiulli, Fabrizio",,,"On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Over the years, different characterizations of the curse of dimensionality have been provided, usually stating the conditions under which, in the limit of the infinite dimensionality, distances become indistinguishable. However, these characterizations almost never address the form of associated distributions in the finite, although high-dimensional, case. This work aims to contribute in this respect by investigating the distribution of distances, and of direct and reverse nearest neighbors, in intrinsically high-dimensional spaces. Indeed, we derive a closed form for the distribution of distances from a given point, for the expected distance from a given point to its kth nearest neighbor, and for the expected size of the approximate set of neighbors of a given point in finite high-dimensional spaces. Additionally, the hubness problem is considered, which is related to the form of the function N-k representing the number of points that have a given point as one of their k nearest neighbors, which is also called the number of k-occurrences. Despite the extensive use of this function, the precise characterization of its form is a longstanding problem. We derive a closed form for the number of k-occurrences associated with a given point in finite high-dimensional spaces, together with the associated limiting probability distribution. By investigating the relationships with the hubness phenomenon emerging in network science, we find that the distribution of node (in-)degrees of some real-life, large-scale networks has connections with the distribution of k-occurrences described herein.",,,,,,"Angiulli, Fabrizio/0000-0002-9860-7569",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,170,,,,,,,,,,,,,,,WOS:000435441800001,0
J,"Biecek, PL",,,,"Biecek, Przemys Law",,,DALEX: Explainers for Complex Predictive Models in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454481400001,0
J,"Dai, B; Wang, Y; Aston, J; Hua, G; Wipf, D",,,,"Dai, Bin; Wang, Yu; Aston, John; Hua, Gang; Wipf, David",,,Connections with Robust PCA and the Role of Emergent Sparsity in Variational Autoencoder Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.",,,,,,"Wipf, David/0000-0002-2768-4540; Dai, Bin/0000-0003-0621-3544",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444407200001,0
J,"Kasai, H",,,,"Kasai, Hiroyuki",,,SGDLibrary: A MATLAB library for stochastic optimization algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of finding the minimizer of a function f : R-d -> R of the finite-sum form min f(w) = 1/n Sigma(n)(i) f(i)(w). This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,215,,,,,,,,,,,,,,,WOS:000435628900001,0
J,"Probst, P; Boulesteix, AL",,,,"Probst, Philipp; Boulesteix, Anne-Laure",,,To Tune or Not to Tune the Number of Trees in Random Forest,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The number of trees T in the random forest (RF) algorithm for supervised learning has to be set by the user. It is unclear whether T should simply be set to the largest computationally manageable value or whether a smaller T may be sufficient or in some cases even better. While the principle underlying bagging is that more trees are better, in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting T to a computationally feasible large number as long as classical error measures based on average loss are considered.",,,,,"Boulesteix, Anne-Laure/A-3948-2010; Probst, Philipp/GYV-1134-2022","Boulesteix, Anne-Laure/0000-0002-2729-0947; ",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,181,,,,,,,,,,,,,,,WOS:000435448800001,0
J,"Shen, J; Li, P",,,,"Shen, Jie; Li, Ping",,,A Tight Bound of Hard Thresholding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with the hard thresholding operator which sets all but the k largest absolute elements of a vector to zero. We establish a tight bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the l(1)-relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the global linear convergence for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,208,,,,,,,,,,,,,,,WOS:000435628100001,0
J,"Yang, JY; Chow, YL; Re, C; Mahoney, MW",,,,"Yang, Jiyan; Chow, Yin-Lam; Re, Christopher; Mahoney, Michael W.",,,Weighted SGD for l(p) Regression with Randomized Preconditioning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. SGD methods are easy to implement and applicable to a wide range of convex optimization problems. In contrast, RLA algorithms provide much stronger performance guarantees but are applicable to a narrower class of problems. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems-e.g., l(2) and l(1) regression problems. .We propose a hybrid algorithm named PWSGD that uses RLA techniques for preconditioning and constructing an importance sampling distribution, and then performs an SGD- like iterative process with weighted sampling on the preconditioned system. . By rewriting a deterministic l(p) regression problem as a stochastic optimization problem, we connect PWSGD to several existing l(p) solvers including RLA methods with algorithmic leveraging (RLA for short). .We prove that PWSGD inherits faster convergence rates that only depend on the lower dimension of the linear system, while maintaining low computation complexity. Such SGD convergence rates are superior to other related SGD algorithm such as the weighted randomized Kaczmarz algorithm. . Particularly, when solving l(1) regression with size n by d, PWSGD returns an approximate solution with epsilon relative error in the objective value in O (log n . nnz(A) + poly(d)/epsilon(2)) time. This complexity is uniformly better than that of RLA methods in terms of both epsilon and d when the problem is unconstrained. In the presence of constraints, PWSGD only has to solve a sequence of much simpler and smaller optimization problem over the same constraints. In general this is more efficient than solving the constrained subproblem required in RLA. .For l(2) regression, PWSGD returns an approximate solution with epsilon relative error in the objective value and the solution vector measured in prediction norm in O (log n . nnz(A)+ poly(d) log(1/epsilon) / epsilon) time. We show that for unconstrained l(2) regression, this complexity is comparable to that of RLA and is asymptotically better over several state-of-the-art solvers in the regime where the desired accuracy epsilon, high dimension n and low dimension d satisfy d >= 1/epsilon and n >= d(2)/epsilon. We also provide lower bounds on the coreset complexity for more general regression problems, indicating that still new ideas will be needed to extend similar RLA preconditioning ideas to weighted SGD algorithms for more general regression problems. Finally, the effectiveness of such algorithms is illustrated numerically on both synthetic and real datasets, and the results are consistent with our theoretical findings and demonstrate that PWSGD converges to a medium-precision solution, e. g., epsilon = 10 3, more quickly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,211,,,,,,,,,,,,,,,WOS:000435628500001,0
J,"Arias-Castro, E; Lerman, G; Zhang, T",,,,"Arias-Castro, Ery; Lerman, Gilad; Zhang, Teng",,,Spectral Clustering Based on Local PCA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal sub-spaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets.",,,,,,"Lerman, Gilad/0000-0003-4624-3115",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,57,,,,,,,,,,,,,,,,WOS:000399833400001,0
J,"Hu, YH; Li, C; Meng, KW; Qin, J; Yang, XQ",,,,"Hu, Yaohua; Li, Chong; Meng, Kaiwen; Qin, Jing; Yang, Xiaoqi",,,"Group Sparse Optimization via l(p),(q) Regularization",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we investigate a group sparse optimization problem via l(p),(q) regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order O(lambda(2/2-q)) for any point in a level set of the l(p),(q) regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order O (lambda(2)) for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the l(p,q) regularization problems, either by analytically solving some specific l(p,q) regularization subproblems, or by using the Newton method to solve general l(p),(q) regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the l(1,q) regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual l(q) regularization problem (0 < q < 1) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation.",,,,,"Meng, Kaiwen/F-6503-2013","Meng, Kaiwen/0000-0003-4333-0184",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,30,,,,,,,,,,,,,,,WOS:000399842700001,0
J,"Saeedi, A; Kulkarni, TD; Mansinghka, VK; Gershman, SJ",,,,"Saeedi, Ardavan; Kulkarni, Tejas D.; Mansinghka, Vikash K.; Gershman, Samuel J.",,,Variational Particle Approximations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper de fines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can off er appealing time/accuracy trade-offs as compared to multiple alternatives.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,29,,,,,,,,,,,,,,,,WOS:000412058100001,0
J,"Avron, H; Sindhwani, V; Yang, JY; Mahoney, MW",,,,"Avron, Haim; Sindhwani, Vikas; Yang, Jiyan; Mahoney, Michael W.",,,Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of improving the efficiency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large data sets. These approximate feature maps arise as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy based on theoretical characterizations of the integration error with respect to a given sequence. We then propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization. Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of classical and adaptive QMC techniques for this problem.",,,,,,"Avron, Haim/0000-0002-1688-9030",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,120,,,,,,,,,,,,,,,WOS:000391553000001,0
J,"Benavoli, A; Corani, G; Mangili, F",,,,"Benavoli, Alessio; Corani, Giorgio; Mangili, Francesca",,,Should We Really Use Post-Hoc Tests Based on Mean-Ranks?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms A and B depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between A and B could be declared significant if the pool comprises algorithms C,D,E and not significant if the pool comprises algorithms F,G,H. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.",,,,,,"corani, giorgio/0000-0002-1541-8384; benavoli, alessio/0000-0002-2522-7178",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,5,,,,,,,,,,,,,,,WOS:000391463700001,0
J,"Brouard, C; Szafranski, M; d'Alche-Buc, F",,,,"Brouard, Celine; Szafranski, Marie; d'Alche-Buc, Florence",,,Input Output Kernel Regression: Supervised and Semi-Supervised Structured Output Prediction with Operator-Valued Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce a novel approach, called Input Output Kernel Regression (IOKR), for learning mappings between structured inputs and structured outputs. The approach belongs to the family of Output Kernel Regression methods devoted to regression in feature space endowed with some output kernel. In order to take into account structure in input data and benefit from kernels in the input space as well, we use the Reproducing Kernel Hilbert Space theory for vector-valued functions. We first recall the ridge solution for supervised learning and then study the regularized hinge loss-based solution used in Maximum Margin Regression. Both models are also developed in the context of semi-supervised setting. In addition we derive an extension of Generalized Cross Validation for model selection in the case of the least-square model. Finally we show the versatility of the IOKR framework on two different problems: link prediction seen as a structured output problem and multi-task regression seen as a multiple and interdependent output problem. Eventually, we present a set of detailed numerical results that shows the relevance of the method on these two tasks.",,,,,,"Brouard, Celine/0000-0002-5574-7027",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,176,,,,,,,,,,,,,,,WOS:000391680400001,0
J,"Elser, V",,,,"Elser, Veit",,,A Network That Learns Strassen Multiplication,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study neural networks whose only non-linear components are multipliers, to test a new training rule in a context where the precise representation of data is paramount. These networks are challenged to discover the rules of matrix multiplication, given many examples. By limiting the number of multipliers, the network is forced to discover the Strassen multiplication rules. This is the mathematical equivalent of finding low rank decompositions of the n x n matrix multiplication tensor, Mn. We train these networks with the conservative learning rule, which makes minimal changes to the weights so as to give the correct output for each input at the time the input-output pair is received. Conservative learning needs a few thousand examples to find the rank 7 decomposition of M-2, and 10(5) for the rank 23 decomposition of M-3 (the lowest known). High precision is critical, especially for M-3, to discriminate between true decompositions and border approximations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,116,,,,,,,,,,,,,,,WOS:000391551700001,0
J,"Escalera, S; Athitsos, V; Guyon, I",,,,"Escalera, Sergio; Athitsos, Vassilis; Guyon, Isabelle",,,Challenges in multimodal gesture recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011-2015. We began right at the start of the Kinect (TM) revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research.",,,,,"Escalera, Sergio/L-2998-2015; Athitsos, Vassilis/AAF-8496-2020","Escalera, Sergio/0000-0003-0617-8873; ",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,72,,,,,,,,,,,,,,,WOS:000391523800001,0
J,"Gutmann, MU; Corander, J",,,,"Gutmann, Michael U.; Corander, Jukka",,,Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,125,,,,,,,,,,,,,,,WOS:000391655000001,0
J,"Lemeire, J",,,,"Lemeire, Jan",,,Conditional Independencies under the Algorithmic Independence of Conditionals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we analyze the relationship between faithfulness and the more recent condition of algorithmic Independence of Conditionals (IC) with respect to the Conditional Independencies (CIs) they allow. Both conditions have been extensively used for causal inference by refuting factorizations for which the condition does not hold. Violation of faithfulness happens when there are CIs that do not follow from the Markov condition. For those CIs, non-trivial constraints among some parameters of the Conditional Probability Distributions (CPDs) must hold. When such a constraint is defined over parameters of different CPDs, we prove that IC is also violated unless the parameters have a simple description. To understand which non-Markovian CIs are permitted we define a new condition closely related to IC: the Independence from Product Constraints (IPC). The condition reflects that CIs might be the result of specific parameterizations of individual CPDs but not from constraints on parameters of different CPDs. In that sense it is more restrictive than IC: parameters may have a simple description. On the other hand, IC also excludes other forms of algorithmic dependencies between CPDs. Finally, we prove that on top of the CIs permitted by the Markov condition (faithfulness), IPC allows non-minimality, deterministic relations and what we called proportional CPDs. These are the only cases in which a CI follows from a specific parameterization of a single CPD.",,,,,"Lemeire, Jan/AAJ-6474-2020","Lemeire, Jan/0000-0002-2106-448X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,20,151,,,,,,,,,,,,,,,WOS:000391663100001,0
J,"Shah, NB; Balakrishnan, S; Bradley, J; Parekh, A; Ramchandran, K; Wainwright, MJ",,,,"Shah, Nihar B.; Balakrishnan, Sivaraman; Bradley, Joseph; Parekh, Abhay; Ramchandran, Kannan; Wainwright, Martin J.",,,Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector w* is an element of R-d that represents the qualities of the d items being compared; this class of models includes the two most widely used parametric models-the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector w* under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared via its Laplacian spectrum. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,58,,,,,,,,,,,,,,,WOS:000391492300001,0
J,"van den Burg, GJJ; Groenen, PJF",,,,"van den Burg, Gerrit J. J.; Groenen, Patrick J. F.",,,GenSVM: A Generalized Multiclass Support Vector Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Traditional extensions of the binary support vector machine (SVM) to multiclass problems are either heuristics or require solving a large dual optimization problem. Here, a generalized multiclass SVM is proposed called GenSVM. In this method classification boundaries for a K-class problem are constructed in a (K-1)-dimensional space using a simplex encoding. Additionally, several different weightings of the misclassification errors are incorporated in the loss function, such that it generalizes three existing multiclass SVMs through a single optimization problem. An iterative majorization algorithm is derived that solves the optimization problem without the need of a dual formulation. This algorithm has the advantage that it can use warm starts during cross validation and during a grid search, which significantly speeds up the training phase. Rigorous numerical experiments compare linear GenSVM with seven existing multiclass SVMs on both small and large data sets. These comparisons show that the proposed method is competitive with existing methods in both predictive accuracy and training time, and that it significantly outperforms several existing methods on these criteria.",,,,,,"Groenen, Patrick/0000-0001-6683-8971",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,42,225,,,,,,,,,,,,,,,WOS:000391914600001,0
J,"Hothorn, T; Zeileis, A",,,,"Hothorn, Torsten; Zeileis, Achim",,,partykit: A Modular Toolkit for Recursive Partytioning in R,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The R package partykit provides a flexible toolkit for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) basic infrastructure for representing trees (inferred by any algorithm) so that unified print/plot/predict methods are available; (b) dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such trees (e.g., by rpart, RWeka, PMML); (c) a reimplementation of conditional inference trees (ctree, originally provided in the party package); (d) an extended reimplementation of model-based recursive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. Here, a brief overview of the package and its design is given while more detailed discussions of items (a) (d) are available in vignettes accompanying the package.",,,,,"Zeileis, Achim/K-9226-2015","Zeileis, Achim/0000-0003-0918-3766",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3905,3909,,,,,,,,,,,,,,,,WOS:000369888000047,0
J,"Berend, D; Kontorovich, A",,,,"Berend, Daniel; Kontorovich, Aryeh",,,A Finite Sample Analysis of the Naive Bayes Classifier,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We revisit, from a statistical learning perspective, the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Naive Bayes weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. We derive optimality results for our estimates and also establish some structural characterizations. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. Several challenging open problems are posed, and experimental results are provided to illustrate the theory.",,,,,"Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019; BEREND, DANIEL/AAJ-4653-2020","Kontorovich, Aryeh/0000-0001-8038-8671; Berend, Daniel/0000-0002-5756-5921",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1519,1545,,,,,,,,,,,,,,,,WOS:000369887100006,0
J,"Moreno, PG; Artes-Rodriguez, A; Teh, YW; Perez-Cruz, F",,,,"Moreno, Pablo G.; Artes-Rodriguez, Antonio; Teh, Yee Whye; Perez-Cruz, Fernando",,,Bayesian Nonparametric Crowdsourcing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Crowdsourcing has been proven to be an effective and efficient tool to annotate large data-sets. User annotations are often noisy, so methods to combine the annotations to produce reliable estimates of the ground truth are necessary. We claim that considering the existence of clusters of users in this combination step can improve the performance. This is especially important in early stages of crowdsourcing implementations, where the number of annotations is low. At this stage there is not enough information to accurately estimate the bias introduced by each annotator separately, so we have to resort to models that consider the statistical links among them. In addition, finding these clusters is interesting in itself as knowing the behavior of the pool of annotators allows implementing efficient active learning strategies. Based on this, we propose in this paper two new fully unsupervised models based on a Chinese restaurant process (CRP) prior and a hierarchical structure that allows inferring these groups jointly with the ground truth and the properties of the users. Efficient inference algorithms based on Gibbs sampling with auxiliary variables are proposed. Finally, we perform experiments, both on synthetic and real databases, to show the advantages of our models over state-of-the-art algorithms.",,,,,"Art√©s, Antonio/E-4842-2018","Art√©s, Antonio/0000-0001-6540-7109; perez-cruz, fernando/0000-0001-8996-5076",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1607,1627,,,,,,,,,,,,,,,,WOS:000369887100010,0
J,"Saberian, M; Vasconcelos, N",,,,"Saberian, Mohammad; Vasconcelos, Nuno",,,Boosting Algorithms for Detector Cascade Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of learning classifier cascades is considered. A new cascade boosting algorithm, fast cascade boosting (FCBoost), is proposed. FCBoost is shown to have a number of interesting properties, namely that it 1) minimizes a Lagrangian risk that jointly accounts for classification accuracy and speed, 2) generalizes adaboost, 3) can be made cost-sensitive to support the design of high detection rate cascades, and 4) is compatible with many predictor structures suitable for sequential decision making. It is shown that a rich family of such structures can be derived recursively from cascade predictors of two stages, denoted cascade generators. Generators are then proposed for two new cascade families, last-stage and multiplicative cascades, that generalize the two most popular cascade architectures in the literature. The concept of neutral predictors is finally introduced, enabling FCBoost to automatically determine the cascade configuration, i.e., number of stages and number of weak learners per stage, for the learned cascades. Experiments on face and pedestrian detection show that the resulting cascades outperform current state-of-the-art methods in both detection accuracy and speed.",,,,,,"Vasconcelos, Nuno/0000-0002-9024-4302",,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2569,2605,,,,,,,,,,,,,,,,WOS:000344638400007,0
J,"Lindsten, F; Jordan, MI; Schon, TB",,,,"Lindsten, Fredrik; Jordan, Michael I.; Schon, Thomas B.",,,Particle Gibbs with Ancestor Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a new PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate, for instance, the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models.",,,,,"Jordan, Michael I/C-5253-2013; Sch√∂n, Thomas B/D-4169-2009","Sch√∂n, Thomas B/0000-0001-5183-234X; Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2145,2184,,,,,,,,,,,,,,,,WOS:000344638300009,0
J,"Richard, E; Gaiffas, S; Vayatis, N",,,,"Richard, Emile; Gaiffas, Stephane; Vayatis, Nicolas",,,Link Prediction in Graphs with Autoregressive Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the paper, we consider the problem of link prediction in time-evolving graphs. We assume that certain graph features, such as the node degree, follow a vector autoregressive (VAR) model and we propose to use this information to improve the accuracy of prediction. Our strategy involves a joint optimization procedure over the space of adjacency matrices and VAR matrices. On the adjacency matrix it takes into account both sparsity and low rank properties and on the VAR it encodes the sparsity. The analysis involves oracle inequalities that illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank. The estimate is computed efficiently using proximal methods, and evaluated through numerical experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,565,593,,,,,,,,,,,,,,,,WOS:000335457700007,0
J,"Wilson, A; Fern, A; Tadepalli, P",,,,"Wilson, Aaron; Fern, Alan; Tadepalli, Prasad",,,Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, Bayesian Optimization (BO) has been used to successfully optimize parametric policies in several challenging Reinforcement Learning (RL) applications. BO is attractive for this problem because it exploits Bayesian prior information about the expected return and exploits this knowledge to select new policies to execute. Effectively, the BO framework for policy search addresses the exploration-exploitation tradeoff. In this work, we show how to more effectively apply BO to RL by exploiting the sequential trajectory information generated by RL agents. Our contributions can be broken into two distinct, but mutually beneficial, parts. The first is a new Gaussian process (GP) kernel for measuring the similarity between policies using trajectory data generated from policy executions. This kernel can be used in order to improve posterior estimates of the expected return thereby improving the quality of exploration. The second contribution, is a new GP mean function which uses learned transition and reward functions to approximate the surface of the objective. We show that the model-based approach we develop can recover from model inaccuracies when good transition and reward models cannot be learned. We give empirical results in a standard set of RL benchmarks showing that both our model-based and model-free approaches can speed up learning compared to competing methods. Further, we show that our contributions can be combined to yield synergistic improvement in some domains.",,,,,,"Tadepalli, Prasad/0000-0003-2736-3912",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,253,282,,,,,,,,,,,,,,,,WOS:000335457400008,0
J,"Escalante-B, AN; Wiskott, L",,,,"Escalante-B, Alberto N.; Wiskott, Laurenz",,,How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the final label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classification, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufficient as well as when feature selection is difficult.",,,,,"Wiskott, Laurenz/P-7715-2017","Wiskott, Laurenz/0000-0001-6237-740X",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3683,3719,,,,,,,,,,,,,,,,WOS:000335457100007,0
J,"Wan, J; Ruan, QQ; Li, W; Deng, S",,,,"Wan, Jun; Ruan, Qiuqi; Li, Wei; Deng, Shuang",,,One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2549,2582,,,,,,,,,,,,,,,,WOS:000327007400003,0
J,"Rosasco, L; Villa, S; Mosci, S; Santoro, M; Verri, A",,,,"Rosasco, Lorenzo; Villa, Silvia; Mosci, Sofia; Santoro, Matteo; Verri, Alessandro",,,Nonparametric Sparsity and Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods.",,,,,"Santoro, Matteo/N-7176-2013; Villa, Silvia/K-4572-2013; Santoro, Matteo/ABB-4593-2020; Villa, Silvia/ABD-4411-2020","rosasco, lorenzo/0000-0003-3098-383X; VILLA, SILVIA/0000-0002-6232-5631",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1665,1714,,,,,,,,,,,,,,,,WOS:000323367000001,0
J,"Thom, M; Palm, G",,,,"Thom, Markus; Palm, Guenther",,,Sparse Activity and Sparse Connectivity in Supervised Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1091,1143,,,,,,,,,,,,,,,,WOS:000318590500011,0
J,"Lang, T; Toussaint, M; Kersting, K",,,,"Lang, Tobias; Toussaint, Marc; Kersting, Kristian",,,Exploration in Relational Domains for Model-based Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A fundamental problem in reinforcement learning is balancing exploration and exploitation. We address this problem in the context of model-based reinforcement learning in large stochastic relational domains by developing relational extensions of the concepts of the E-3 and R-MAX algorithms. Efficient exploration in exponentially large state spaces needs to exploit the generalization of the learned model: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be a well-known context in which exploitation is promising. To address this we introduce relational count functions which generalize the classical notion of state and action visitation counts. We provide guarantees on the exploration efficiency of our framework using count functions under the assumption that we had a relational KWIK learner and a near-optimal planner. We propose a concrete exploration algorithm which integrates a practically efficient probabilistic rule learner and a relational planner (for which there are no guarantees, however) and employs the contexts of learned relational rules as features to model the novelty of states and actions. Our results in noisy 3D simulated robot manipulation problems and in domains of the international planning competition demonstrate that our approach is more effective than existing propositional and factored exploration techniques.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3725,3768,,,,,,,,,,,,,,,,WOS:000314529000009,0
J,"Brodersen, KH; Mathys, C; Chumbley, JR; Daunizeau, J; Ong, CS; Buhmann, JM; Stephan, KE",,,,"Brodersen, Kay H.; Mathys, Christoph; Chumbley, Justin R.; Daunizeau, Jean; Ong, Cheng Soon; Buhmann, Joachim M.; Stephan, Klaas E.",,,Bayesian Mixed-Effects Inference on Classification Performance in Hierarchical Data Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classification algorithms are frequently used on data with a natural hierarchical structure. For instance, classifiers are often trained and tested on trial-wise measurements, separately for each subject within a group. One important question is how classification outcomes observed in individual subjects can be generalized to the population from which the group was sampled. To address this question, this paper introduces novel statistical models that are guided by three desiderata. First, all models explicitly respect the hierarchical nature of the data, that is, they are mixed-effects models that simultaneously account for within-subjects (fixed-effects) and across-subjects (random-effects) variance components. Second, maximum-likelihood estimation is replaced by full Bayesian inference in order to enable natural regularization of the estimation problem and to afford conclusions in terms of posterior probability statements. Third, inference on classification accuracy is complemented by inference on the balanced accuracy, which avoids inflated accuracy estimates for imbalanced data sets. We introduce hierarchical models that satisfy these criteria and demonstrate their advantages over conventional methods using MCMC implementations for model inversion and model selection on both synthetic and empirical data. We envisage that our approach will improve the sensitivity and validity of statistical inference in future hierarchical classification studies.",,,,,"Mathys, Christoph/D-5661-2014; Buhmann, Joachim/AAU-4760-2020; Brodersen, Kay H/B-6694-2013","Mathys, Christoph/0000-0003-4079-5453; Brodersen, Kay H/0000-0002-7707-090X; Stephan, Klaas Enno/0000-0002-8594-9092",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3133,3176,,,,,,,,,,,,,,,,WOS:000313200200001,0
J,"Solnon, M; Arlot, S; Bach, F",,,,"Solnon, Matthieu; Arlot, Sylvain; Bach, Francis",,,Multi-task Regression using Minimal Penalties,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we study the kernel multiple ridge regression framework, which we refer to as multi-task regression, using penalization techniques. The theoretical analysis of this problem shows that the key element appearing for an optimal calibration is the covariance matrix of the noise between the different tasks. We present a new algorithm to estimate this covariance matrix, based on the concept of minimal penalty, which was previously used in the single-task regression framework to estimate the variance of the noise. We show, in a non-asymptotic setting and under mild assumptions on the target function, that this estimator converges towards the covariance matrix. Then plugging this estimator into the corresponding ideal penalty leads to an oracle inequality. We illustrate the behavior of our algorithm on synthetic examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2773,2812,,,,,,,,,,,,,,,,WOS:000309580600010,0
J,"Barbu, A; Lay, N",,,,"Barbu, Adrian; Lay, Nathan",,,An Introduction to Artificial Prediction Markets for Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Prediction markets are used in real life to predict outcomes of interest such as presidential elections. This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators. The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes. The market can be trained online by updating the participants' budgets using training examples. Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions. Efficient numerical algorithms are presented for solving these equations. The obtained artificial prediction market is shown to be a maximum likelihood estimator. It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods. Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances. Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI data sets. Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2012,13,,,,,,2177,2204,,,,,,,,,,,,,,,,WOS:000307496000003,0
J,"Jain, P; Kulis, B; Davis, JV; Dhillon, IS",,,,"Jain, Prateek; Kulis, Brian; Davis, Jason V.; Dhillon, Inderjit S.",,,Metric and Kernel Learning Using a Linear Transformation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework-that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,519,547,,,,,,,,,,,,,,,,WOS:000303772100001,0
J,"Lichtenwalter, RN; Chawla, NV",,,,"Lichtenwalter, Ryan N.; Chawla, Nitesh V.",,,LPmade: Link Prediction Made Easy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"LPmade is a complete cross-platform software solution for multi-core link prediction and related tasks and analysis. Its first principal contribution is a scalable network library supporting high-performance implementations of the most commonly employed unsupervised link prediction methods. Link prediction in longitudinal data requires a sophisticated and disciplined procedure for correct results and fair evaluation, so the second principle contribution of LPmade is a sophisticated GNU make architecture that completely automates link prediction, prediction evaluation, and network analysis. Finally, LPmade streamlines and automates the procedure for creating multivariate supervised link prediction models with a version of WEKA modified to operate effectively on extremely large data sets. With mere minutes of manual work, one may start with a raw stream of records representing a network and progress through hundreds of steps to generate plots, gigabytes or terabytes of output, and actionable or publishable results.",,,,,"Chawla, Nitesh/F-2690-2016","Chawla, Nitesh/0000-0003-3932-5956",,,,,,,,,,,,,1532-4435,,,,,AUG,2011,12,,,,,,2489,2492,,,,,,,,,,,,,,,,WOS:000298102200002,0
J,"Alvarez, MA; Lawrence, ND",,,,"Alvarez, Mauricio A.; Lawrence, Neil D.",,,Computationally Efficient Convolved Multiple Output Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently there has been an increasing interest in regression methods that deal with multiple outputs. This has been motivated partly by frameworks like multitask learning, multisensor networks or structured output data. From a Gaussian processes perspective, the problem reduces to specifying an appropriate covariance function that, whilst being positive semi-definite, captures the dependencies between all the data points and across all the outputs. One approach to account for non-trivial correlations between outputs employs convolution processes. Under a latent function interpretation of the convolution transform we establish dependencies between output variables. The main drawbacks of this approach are the associated computational and storage demands. In this paper we address these issues. We present different efficient approximations for dependent output Gaussian processes constructed through the convolution formalism. We exploit the conditional independencies present naturally in the model. This leads to a form of the covariance similar in spirit to the so called PITC and FITC approximations for a single output. We show experimental results with synthetic and real data, in particular, we show results in school exams score prediction, pollution prediction and gene expression data.",,,,,,"Alvarez, Mauricio A./0000-0002-8980-4472; Lawrence, Neil/0000-0001-9258-1030",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1459,1500,,,,,,,,,,,,,,,,WOS:000292304000002,0
J,"Ukkonen, A",,,,"Ukkonen, Antti",,,Clustering Algorithms for Chains,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of clustering a set of chains to k clusters. A chain is a totally ordered subset of a finite set of items. Chains are an intuitive way to express preferences over a set of alternatives, as well as a useful representation of ratings in situations where the item-specific scores are either difficult to obtain, too noisy due to measurement error, or simply not as relevant as the order that they induce over the items. First we adapt the classical k-means for chains by proposing a suitable distance function and a centroid structure. We also present two different approaches for mapping chains to a vector space. The first one is related to the planted partition model, while the second one has an intuitive geometrical interpretation. Finally we discuss a randomization test for assessing the significance of a clustering. To this end we present an MCMC algorithm for sampling random sets of chains that share certain properties with the original data. The methods are studied in a series of experiments using real and artificial data. Results indicate that the methods produce interesting clusterings, and for certain types of inputs improve upon previous work on clustering algorithms for orders.",,,,,"Ukkonen, Antti/E-8512-2013",,,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1389,1423,,,,,,,,,,,,,,,,WOS:000290096100007,0
J,"Meila, M; Bao, L",,,,"Meila, Marina; Bao, Le",,,An Exponential Model for Infinite Rankings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a statistical model for expressing preferences through rankings, when the number of alternatives (items to rank) is large. A human ranker will then typically rank only the most preferred items, and may not even examine the whole set of items, or know how many they are. Similarly, a user presented with the ranked output of a search engine, will only consider the highest ranked items. We model such situations by introducing a stagewise ranking model that operates with finite ordered lists called top-t orderings over an infinite space of items. We give algorithms to estimate this model from data, and demonstrate that it has sufficient statistics, being thus an exponential family model with continuous and discrete parameters. We describe its conjugate prior and other statistical properties. Then, we extend the estimation problem to multimodal data by introducing an Exponential-Blurring-Mean-Shift nonparametric clustering algorithm. The experiments highlight the properties of our model and demonstrate that infinite models over permutations can be simple, elegant and practical.",,,,,"Bao, Le/B-2987-2014",,,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3481,3518,,,,,,,,,,,,,,,,WOS:000286637200007,0
J,"Ilin, A; Raiko, T",,,,"Ilin, Alexander; Raiko, Tapani",,,Practical Approaches to Principal Component Analysis in the Presence of Missing Values,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Principal component analysis (PCA) is a classical data analysis technique that finds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overfitting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overfitting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artificial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the Netflix problem.",,,,,"Raiko, Tapani/E-7237-2012","Raiko, Tapani/0000-0002-0321-304X",,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,1957,2000,,,,,,,,,,,,,,,,WOS:000282523000002,0
J,"Argyriou, A; Micchelli, CA; Pontil, M",,,,"Argyriou, Andreas; Micchelli, Charles A.; Pontil, Massimiliano",,,When Is There a Representer Theorem? Vector Versus Matrix Regularizers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L-2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufficient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2507,2529,,,,,,,,,,,,,,,,WOS:000272346600004,0
J,"Dietterich, TG; Hao, GH; Ashenfelter, A",,,,"Dietterich, Thomas G.; Hao, Guohua; Ashenfelter, Adam",,,Gradient Tree Boosting for Training Conditional Random Fields,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional random fields (CRFs) provide a flexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2113,2139,,,,,,,,,,,,,,,,WOS:000262637300003,0
J,"Bax, E",,,,"Bax, Eric",,,Nearly Uniform Validation Improves Compression-Based Error Bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth's compression-based bounding technique.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1741,1755,,,,,,,,,,,,,,,,WOS:000262636800004,0
J,"Munos, R; Szepesvari, C",,,,"Munos, Remi; Szepesvari, Csaba",,,Finite-time bounds for fitted value iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we develop a theoretical analysis of the performance of sampling-based fitted value iteration (FVI) to solve infinite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of finite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufficiently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L-p-norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e. g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reflects how well the function space is aligned with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,815,857,,,,,,,,,,,,,,,,WOS:000258645300001,0
J,"Jiang, B; Zhang, XG; Cai, TX",,,,"Jiang, Bo; Zhang, Xuegong; Cai, Tianxi",,,Estimating the confidence interval for prediction errors of support vector machine classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machine (SVM) is one of the most popular and promising classification algorithms. After a classification rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with finite-dimensional kernels. A perturbation-resampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples.",,,,,"Jiang, Bo/F-6959-2014","Jiang, Bo/0000-0002-6146-5376",,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,521,540,,,,,,,,,,,,,,,,WOS:000256642000007,0
J,"Hickey, RJ",,,,"Hickey, Ray J.",,,Structure and majority classes in decision tree learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1747,1768,,,,,,,,,,,,,,,,WOS:000252744400003,0
J,"Caponnetto, A; Rakhlin, A",,,,"Caponnetto, Andrea; Rakhlin, Alexander",,,Stability properties of empirical risk minimization over Donsker classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study some stability properties of algorithms which minimize (or almost-minimize) empirical error over Donsker classes of functions. We show that, as the number n of samples grows, the L-2-diameter of the set of almost-minimizers of empirical error with tolerance xi(n) = o(n(-1/2)) converges to zero in probability. Hence, even in the case of multiple minimizers of expected error, as n increases it becomes less and less likely that adding a sample (or a number of samples) to the training set will result in a large jump to a new hypothesis. Moreover, under some assumptions on the entropy of the class, along with an assumption of Komlos-Major-Tusnady type, we derive a power rate of decay for the diameter of almost-minimizers. This rate, through an application of a uniform ratio limit inequality, is shown to govern the closeness of the expected errors of the almost-minimizers. In fact, under the above assumptions, the expected errors of almost-minimizers become closer with a rate strictly faster than n(-1/2).",,,,,,"CAPONNETTO, Andrea/0000-0002-6311-0667",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2565,2583,,,,,,,,,,,,,,,,WOS:000245390800001,0
J,"Wright, CV; Monrose, F; Masson, GM",,,,"Wright, Charles V.; Monrose, Fabian; Masson, Gerald M.",,,On inferring application protocol Behaviors in encrypted network traffic,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Several fundamental security mechanisms for restricting access to network resources rely on the ability of a reference monitor to inspect the contents of traffic as it traverses the network. However, with the increasing popularity of cryptographic protocols, the traditional means of inspecting packet contents to enforce security policies is no longer a viable approach as message contents are concealed by encryption. In this paper, we investigate the extent to which common application protocols can be identified using only the features that remain intact after encryption - namely packet size, timing, and direction. We first present what we believe to be the first exploratory look at protocol identification in encrypted tunnels which carry traffic from many TCP connections simultaneously, using only post-encryption observable features. We then explore the problem of protocol identification in individual encrypted TCP connections, using much less data than in other recent approaches. The results of our evaluation show that our classifiers achieve accuracy greater than 90% for several protocols in aggregate traffic, and, for most protocols, greater than 80% when making fine-grained classifications on single connections. Moreover, perhaps most surprisingly, we show that one can even estimate the number of live connections in certain classes of encrypted tunnels to within, on average, better than 20%.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2745,2769,,,,,,,,,,,,,,,,WOS:000245390800010,0
J,"Langley, P; Choi, D",,,,"Langley, P; Choi, D",,,Learning recursive control programs from problem solving,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a new representation for physical control - teleoreactive logic programs - along with an interpreter that uses them to achieve goals. In addition, we present a new learning method that acquires recursive forms of these structures from traces of successful problem solving. We report experiments in three different domains that demonstrate the generality of this approach. In closing, we review related work on learning complex skills and discuss directions for future research on this topic.",,,,,"Choi, Dongkyu/HCI-6907-2022","Choi, Dongkyu/0000-0001-8239-9825",,,,,,,,,,,,,1532-4435,,,,,MAR,2006,7,,,,,,493,518,,,,,,,,,,,,,,,,WOS:000237359000001,0
J,"Hamerly, G; Perelman, E; Lau, J; Calder, B; Sherwood, T",,,,"Hamerly, G; Perelman, E; Lau, J; Calder, B; Sherwood, T",,,Using machine learning to guide architecture simulation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An essential step in designing a new computer architecture is the careful examination of different design options. It is critical that computer architects have efficient means by which they may estimate the impact of various design options on the overall machine. This task is complicated by the fact that different programs, and even different parts of the same program, may have distinct behaviors that interact with the hardware in different ways. Researchers use very detailed simulators to estimate processor performance, which models every cycle of an executing program. Unfortunately, simulating every cycle of a real program can take weeks or months. To address this problem we have created a tool called SimPoint that uses data clustering algorithms from machine learning to automatically find repetitive patterns in a program's execution. By simulating one representative of each repetitive behavior pattern, simulation time can be reduced to minutes instead of weeks for standard benchmark programs, with very little cost in terms of accuracy. We describe this important problem, the data representation and preprocessing methods used by SimPoint, the clustering algorithm at the core of SimPoint, and we evaluate different options for tuning SimPoint.",,,,,,"Hamerly, Greg/0000-0002-0360-1544; Sherwood, Timothy/0000-0002-6550-6075",,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,343,378,,,,,,,,,,,,,,,,WOS:000236331700006,0
J,"Waheed, K; Salem, FM",,,,"Waheed, K; Salem, FM",,,Blind source recovery: A framework in the state space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Blind Source Recovery (BSR) denotes recovery of original source/signals from environments that may include convolution, temporal variation, and even nonlinearity. It also infers the recovery of sources even in die absence of precise environment identifiability. This paper describes, in a comprehensive fashion, a generalized BSR formulation achieved by the application of stochastic optimization principles to the Kullback-Liebler divergence as a performance functional subject to the constraints of the general (i.e., nonlinear and time-varying) state space representation. This technique is used to derive update laws for nonlinear time-varying dynamical systems, which are subsequently specialized to time-invariant and linear systems. Further, the state space demixing network structures have been exploited to develop learning rules, capable of handling most filtering paradigms, which can be conveniently extended to nonlinear models. In the special cases, distinct linear state-space algorithms are presented for the minimum phase and non-minimum phase mixing environment models. Conventional (FIR/lIR) filtering models are subsequently derived from this general structure and are compared with material in the recent literature. Illustrative simulation examples are presented to demonstrate the online adaptation capabilities of die developed algorithms. Some of this reported work has also been implemented in dedicated hardware/software platforms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1411,1446,,,,,,,,,,,,,,,,WOS:000224808300012,0
J,"McAllester, D; Ortiz, L",,,,"McAllester, D; Ortiz, L",,,Concentration inequalities for the missing mass and for histogram rule error,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,"This paper gives distribution-free concentration inequalities for the missing mass and the error rate of histogram rules. Negative association methods can be used to reduce these concentration problems to concentration questions about independent sums. Although the sums are independent, they are highly heterogeneous. Such highly heterogeneous independent sums cannot be analyzed using standard concentration inequalities such as Hoeffding's inequality, the Angluin-Valiant bound, Bernstein's inequality, Bennett's inequality, or McDiarmid's theorem. The concentration inequality for histogram rule error is motivated by the desire to construct a new class of bounds on the generalization error of decision trees.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,895,911,,10.1162/1532443041424292,0,,,,,,,,,,,,,WOS:000223238800007,0
J,"Bshouty, NH; Feldman, V",,,,"Bshouty, NH; Feldman, V",,,On using extended statistical queries to avoid membership queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Kushilevitz-Mansour (KM) algorithm is an algorithm that finds all the large Fourier coefficients of a Boolean function. It is the main tool for learning decision trees and DNF expressions in the PAC model with respect to the uniform distribution. The algorithm requires access to the membership query (MQ) oracle. The access is often unavailable in learning applications and thus the KM algorithm cannot be used. We significantly weaken this requirement by producing an analogue of the KM algorithm that uses extended statistical queries (SQ) (SQs in which the expectation is taken with respect to a distribution given by a learning algorithm). We restrict a set of distributions that a learning algorithm may use for its statistical queries to be a set of product distributions with each bit being 1 with probability rho, 1/2 or 1 - rho for a constant 1/2 > rho > 0 (we denote the resulting model by SQ-D-rho). Our analogue finds all the large Fourier coefficients of degree lower than clog n (we call it the Bounded Sieve (BS)). We use BS to learn decision trees and by adapting Freund's boosting technique we give an algorithm that learns DNF in SQ-D-rho. An important property of the model is that its algorithms can be simulated by MQs with persistent noise. With some modifications BS can also be simulated by MQs with product attribute noise (i.e., for a query x oracle changes every bit of x with some constant probability and calculates the value of the target function at the resulting point) and classification noise. This implies learnability of decision trees and weak learnability of DNF with this non-trivial noise. In the second part of this paper we develop a characterization for learnability with these extended statistical queries. We show that our characterization when applied to SQ-Dp is tight in terms of learning parity functions. We extend the result given by Blum et al. by proving that there is a class learnable in the PAC model with random classification noise and not learnable in SQ-Dp.",,,,,,"Feldman, Vitaly/0000-0002-3904-759X",,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,359,395,,10.1162/153244302760200669,0,,,,,,,,,,,,,WOS:000178101500003,0
J,"Bach, P; Chernozhukov, V; Kurz, MS; Spindler, M",,,,"Bach, Philipp; Chernozhukov, Victor; Kurz, Malte S.; Spindler, Martin",,,DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,6,,,,,,,,,,,,,,,,WOS:000766876100001,0
J,"Majumdar, S; Michailidis, G",,,,"Majumdar, Subhabrata; Michailidis, George",,,Joint Estimation and Inference for Data Integration Problems based on Multiple Multi-layered Gaussian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The rapid development of high-throughput technologies has enabled the generation of data from biological or disease processes that span multiple layers, like genomic, proteomic or metabolomic data, and further pertain to multiple sources, like disease subtypes or experimental conditions. In this work, we propose a general statistical framework based on Gaussian graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across different layers containing data on molecular compartments) integration of information in such datasets. We start with decomposing the multi-layer problem into a series of two-layer problems. For each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. We use a combination of neighborhood selection and group-penalized regression to obtain sparse estimates of all model parameters. Following this, we develop a debiasing technique and asymptotic distributions of inter-layer directed edge weights that utilize already computed neighborhood selection coefficients for nodes in the upper layer. Subsequently, we establish global and simultaneous testing procedures for these edge weights. Performance of the proposed methodology is evaluated on synthetic and real data.",,,,,,"Majumdar, Subhabrata/0000-0003-3529-7820",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,53,,,,,,,,,,,,,,,,WOS:000766878400001,0
J,"Pegorar, M; Beraha, M",,,,"Pegorar, Matteo; Beraha, Mario",,,Projected Statistical Methods for Distributional Data on the Real Line with the Wasserstein Metric,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel class of projected methods to perform statistical analysis on a data set of probability distributions on the real line, with the 2-Wasserstein metric. We focus in particular on Principal Component Analysis (PCA) and regression. To define these models, we exploit a representation of the Wasserstein space closely related to its weak Riemannian structure by mapping the data to a suitable linear space and using a metric projection operator to constrain the results in the Wasserstein space. By carefully choosing the tangent point, we are able to derive fast empirical methods, exploiting a constrained B-spline approximation. As a byproduct of our approach, we are also able to derive faster routines for previous work on PCA for distributions. By means of simulation studies, we compare our approaches to previously proposed methods, showing that our projected PCA has similar performance for a fraction of the computational cost and that the projected regression is extremely flexible even under misspecification. Several theoretical properties of the models are investigated, and asymptotic consistency is proven. Two real world applications to Covid-19 mortality in the US and wind speed forecasting are discussed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766901500001,0
J,"Puchkin, N; Spokoiny, V",,,,"Puchkin, Nikita; Spokoiny, Vladimir",,,Structure-Adaptive Manifold Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a problem of manifold estimation from noisy observations. Many manifold learning procedures locally approximate a manifold by a weighted average over a small neighborhood. However, in the presence of large noise, the assigned weights become so corrupted that the averaged estimate shows very poor performance. We suggest a structure-adaptive procedure, which simultaneously reconstructs a smooth manifold and estimates projections of the point cloud onto this manifold. The proposed approach iteratively refines the weights on each step, using the structural information obtained at previous steps. After several iterations, we obtain nearly oracle weights, so that the final estimates are nearly efficient even in the presence of relatively large noise. In our theoretical study, we establish tight lower and upper bounds proving asymptotic optimality of the method for manifold estimation under the Hausdorlf loss, provided that the noise degrades to zero fast enough.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,62,,,,,,,,,,,,,,,,WOS:000752391300001,0
J,"Savitsky, TD; Williams, MR; Hu, JC",,,,"Savitsky, Terrance D.; Williams, Matthew R.; Hu, Jingchen",,,Bayesian Pseudo Posterior Mechanism under Asymptotic Differential Privacy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a Bayesian pseudo posterior mechanism to generate record-level synthetic databases equipped with an (epsilon, pi) - probabilistic differential privacy (pDP) guarantee, where 7r denotes the probability that any observed database exceeds epsilon. The pseudo pos-terior mechanism employs a data record-indexed, risk-based weight vector with weight values is an element of [0, 1] that surgically downweight the likelihood contributions for high-risk records for model estimation and the generation of record-level synthetic data for public release. The pseudo posterior synthesizer constructs a weight for each datum record by using the Lipschitz bound for that record under a log-pseudo likelihood utility function that general-izes the exponential mechanism (EM) used to construct a formally private data generating mechanism. By selecting weights to remove likelihood contributions with non-finite log-likelihood values, we guarantee a finite local privacy guarantee for our pseudo posterior mechanism at every sample size. Our results may be applied to any synthesizing model envisioned by the data disseminator in a computationally tractable way that only involves estimation of a pseudo posterior distribution for parameters, theta, unlike recent approaches that use naturally-bounded utility functions implemented through the EM. We specify con-ditions that guarantee the asymptotic contraction of pi to 0 over the space of databases, such that the form of the guarantee provided by our method is asymptotic. We illustrate our pseudo posterior mechanism on the sensitive family income variable from the Consumer Expenditure Surveys database published by the U.S. Bureau of Labor Statistics. We show that utility is better preserved in the synthetic data for our pseudo posterior mechanism as compared to the EM, both estimated using the same non-private synthesizer, due to our use of targeted downweighting.",,,,,,"Williams, Matthew/0000-0001-8894-1240",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000754635500001,0
J,"Cauchois, M; Gupta, S; Duchi, JC",,,,"Cauchois, Maxime; Gupta, Suyash; Duchi, John C.",,,Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop conformal prediction methods for constructing valid predictive confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods-which give marginal validity (coverage) guarantees-provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples. By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically consistent) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels. Our methods can be bolted on top of any classification model-neural network, random forest, boosted tree-to guarantee its validity. We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.",,,,,,"Cauchois, Maxime/0000-0002-0323-019X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656399500001,0
J,"Issartel, Y",,,,"Issartel, Yann",,,On the Estimation of Network Complexity: Dimension of Graphons,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Network complexity has been studied for over half a century and has found a wide range of applications. Many methods have been developed to characterize and estimate the complexity of networks. However, there has been little research with statistical guarantees. In this paper, we develop a statistical theory of graph complexity in a general model of random graphs, the so-called graphon model. Given a graphon, we endow the latent space of the nodes with the neighborhood distance. Our complexity index is then based on the covering number and the Minkowksi dimension of this metric space. Although the latent space is not identifiable, these indices turn out to be identifiable. This notion of complexity has simple interpretations on popular examples: it matches the number of communities in stochastic block models; the dimension of the Euclidean space in random geometric graphs; the regularity of the link function in Holder graphons. From a single observation of the graph, we construct an estimator of the neighborhooddistance and show universal non-asymptotic bounds for its risk, matching minimax lower bounds. Based on this estimated distance, we compute the corresponding covering number and Minkowski dimension and we provide optimal non-asymptotic error bounds for these two plug-in estimators.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,22,,,,,,,,,,,,,,,WOS:000706863300001,0
J,"Metel, MR; Takeda, A",,,,"Metel, Michael R.; Takeda, Akiko",,,Stochastic Proximal Methods for Non-Smooth Non-Convex Constrained Sparse Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper focuses on stochastic proximal gradient methods for optimizing a smooth non-convex loss function with a non-smooth non-convex regularizer and convex constraints. To the best of our knowledge we present the first non-asymptotic convergence bounds for this class of problem. We present two simple stochastic proximal gradient algorithms, for general stochastic and finite-sum optimization problems. In a numerical experiment we compare our algorithms with the current state-of-the-art deterministic algorithm and find our algorithms to exhibit superior convergence.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,115,,,,,,,,,,,,,,,WOS:000663169500001,0
J,"Miller, JW",,,,"Miller, Jeffrey W.",,,"Asymptotic Normality, Concentration, and Coverage of Generalized Posteriors",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Generalized likelihoods are commonly used to obtain consistent estimators with attractive computational and robustness properties. Formally, any generalized likelihood can be used to define a generalized posterior distribution, but an arbitrarily defined posterior cannot be expected to appropriately quantify uncertainty in any meaningful sense. In this article, we provide sufficient conditions under which generalized posteriors exhibit concentration, asymptotic normality (Bernstein-von Mises), an asymptotically correct Laplace approximation, and asymptotically correct frequentist coverage. We apply our results in detail to generalized posteriors for a wide array of generalized likelihoods, including pseudolikelihoods in general, the Gaussian Markov random field pseudolikelihood, the fully observed Boltzmann machine pseudolikelihood, the Ising model pseudolikelihood, the Cox proportional hazards partial likelihood, and a median-based likelihood for robust inference of location. Further, we show how our results can be used to easily establish the asymptotics of standard posteriors for exponential families and generalized linear models. We make no assumption of model correctness so that our results apply with or without misspecification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687124900001,0
J,"Mou, WL; Ma, YA; Wainwright, MJ; Bartlett, PL; Jordan, MI",,,,"Mou, Wenlong; Ma, Yi-An; Wainwright, Martin J.; Bartlett, Peter L.; Jordan, Michael, I",,,High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a Markov chain Monte Carlo (MCMC) algorithm based on third-order Langevin dynamics for sampling from distributions with smooth, log-concave densities. The higher-order dynamics allow for more flexible discretization schemes, and we develop a specific method that combines splitting with more accurate integration. For a broad class of d-dimensional distributions arising from generalized linear models, we prove that the resulting third-order algorithm produces samples from a distribution that is at most epsilon > 0 in Wasserstein distance from the target distribution in O(d(1/4)/epsilon(1/2)) steps. This result requires only Lipschitz conditions on the gradient. For general strongly convex potentials with alpha-th order smoothness, we prove that the mixing time scales as O(d(1/4)/epsilon(1/2)+d(1/2)/epsilon(1/(alpha-1))).",,,,,,"Ma, Yi-An/0000-0001-6074-6638",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500042,0
J,"Nikolakakis, KE; Kalogerias, DS; Sarwate, AD",,,,"Nikolakakis, Konstantinos E.; Kalogerias, Dionysios S.; Sarwate, Anand D.",,,Predictive Learning on Hidden Tree-Structured Ising Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide high-probability sample complexity guarantees for exact structure recovery and accurate predictive learning using noise-corrupted samples from an acyclic (tree-shaped) graphical model. The hidden variables follow a tree-structured Ising model distribution, whereas the observable variables are generated by a binary symmetric channel taking the hidden variables as its input (flipping each bit independently with some constant probability q is an element of [0, 1/2)). In the absence of noise, predictive learning on Ising models was recently studied by Bresler and Karzand (2020); this paper quantifies how noise in the hidden model impacts the tasks of structure recovery and marginal distribution estimation by proving upper and lower bounds on the sample complexity. Our results generalize state-of-the-art bounds reported in prior work, and they exactly recover the noiseless case (q = 0). In fact, for any tree with p vertices and probability of incorrect recovery delta > 0, the sufficient number of samples remains logarithmic as in the noiseless case, i.e., O(log(p/(delta)), while the dependence on q is O(1/(1 - 2q)(4)), for both aforementioned tasks. We also present a new equivalent of Isserlis' Theorem for sign-valued tree-structured distributions, yielding a new low-complexity algorithm for higher-order moment estimation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656358200001,0
J,"Papadogeorgou, G; Zhang, ZW; Dunson, DB",,,,"Papadogeorgou, Georgia; Zhang, Zhengwu; Dunson, David B.",,,Soft Tensor Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Statistical methods relating tensor predictors to scalar outcomes in a regression model generally vectorize the tensor predictor and estimate the coefficients of its entries employing some form of regularization, use summaries of the tensor covariate, or use a low dimensional approximation of the coefficient tensor. However, low rank approximations of the coefficient tensor can suffer if the true rank is not small. We propose a tensor regression framework which assumes a soft version of the parallel factors (PARAFAC) approximation. In contrast to classic PARAFAC where each entry of the coefficient tensor is the sum of products of rowspecific contributions across the tensor modes, the soft tensor regression (Softer) framework allows the row-specific contributions to vary around an overall mean. We follow a Bayesian approach to inference, and show that softening the PARAFAC increases model flexibility, leads to improved estimation of coefficient tensors, more accurate identification of important predictor entries, and more precise predictions, even for a low approximation rank. From a theoretical perspective, we show that employing Softer leads to a weakly consistent posterior distribution of the coefficient tensor, irrespective of the true or approximation tensor rank, a result that is not true when employing the classic PARAFAC for tensor regression. In the context of our motivating application, we adapt Softer to symmetric and semi-symmetric tensor predictors and analyze the relationship between brain network characteristics and human traits.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,35754924,,,,,WOS:000706885600001,0
J,"Zhu, JL; Wu, QT; Zhang, MC; Zheng, RJ; Li, KQ",,,,"Zhu, Junlong; Wu, Qingtao; Zhang, Mingchuan; Zheng, Ruijuan; Li, Keqin",,,Projection-free Decentralized Online Learning for Submodular Maximization over Time-Varying Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers a decentralized online submodular maximization problem over time-varying networks, where each agent only utilizes its own information and the received information from its neighbors. To address the problem, we propose a decentralized Meta-Frank-Wolfe online learning method in the adversarial online setting by using local communication and local computation. Moreover, we show that an expected regret bound of O(root T) is achieved with (1-1/e) approximation guarantee, where T is a time horizon. In addition, we also propose a decentralized one-shot Frank-Wolfe online learning method in the stochastic online setting. Furthermore, we also show that an expected regret bound O(T-2/3) is obtained with (1 - 1/e) approximation guarantee. Finally, we confirm the theoretical results via various experiments on different datasets.",,,,,,"Zhang, Mingchuan/0000-0002-2523-1089",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656351500001,0
J,"Bloem-Reddy, B; Teh, YW",,,,"Bloem-Reddy, Benjamin; Teh, Yee Whye",,,Probabilistic Symmetries and Invariant Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,90,,,,,,,,,,,,,,,WOS:000545026800001,0
J,"Chen, SX; Dobriban, E; Lee, JH",,,,"Chen, Shuxiao; Dobriban, Edgar; Lee, Jane H.",,,A Group-Theoretic Framework for Data Augmentation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,245,,,,,,,,,,,,,,,WOS:000608916600001,0
J,"Gao, C; Yao, Y; Zhu, WZ",,,,"Gao, Chao; Yao, Yuan; Zhu, Weizhi",,,Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Robust covariance matrix estimation is a fundamental task in statistics. The recent discovery on the connection between robust estimation and generative adversarial nets (GANs) by Gao et al. (2019) suggests that it is possible to compute depth-like robust estimators using similar techniques that optimize GANs. In this paper, we introduce a general learning via classification framework based on the notion of proper scoring rules. This framework allows us to understand both matrix depth function, a technique of rateoptimal robust estimation, and various GANs through the lens of variational approximations of f -divergences induced by proper scoring rules. We then propose a new class of robust covariance matrix estimators in this framework by carefully constructing discriminators with appropriate neural network structures. These estimators are proved to achieve the minimax rate of covariance matrix estimation under Huber's contamination model. The results are also extended to robust scatter estimation for elliptical distributions. Our numerical results demonstrate the good performance of the proposed procedures under various settings against competitors in the literature. Keywords: robust statistics, neural networks, minimax",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,160,,,,,,,,,,,,,,,WOS:000570149600001,0
J,"Wang, Z; Liang, YB; Ji, PS",,,,"Wang, Zhe; Liang, Yingbin; Ji, Pengsheng",,,Spectral Algorithms for Community Detection in Directed Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Community detection in large social networks is affected by degree heterogeneity of nodes. The D-SCORE algorithm for directed networks was introduced to reduce this effect by taking the element-wise ratios of the singular vectors of the adjacency matrix before clustering. Meaningful results were obtained for the statistician citation network, but rigorous analysis on its performance was missing. First, this paper establishes theoretical guarantee for this algorithm and its variants for the directed degree-corrected block model (Directed-DCBM). Second, this paper provides significant improvements for the original D-SCORE algorithms by attaching the nodes outside of the community cores using the information of the original network instead of the singular vectors.",,,,,"Âì≤, Áéã/GYJ-1551-2022","Liang, Yingbin/0000-0002-8635-2992",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,153,,,,,,,,,,,,,,,WOS:000570115000001,0
J,"Xu, GG; Wang, M; Bian, JZ; Huang, H; Burch, TR; Andrade, SC; Zhang, JF; Guan, YT",,,,"Xu, Ganggang; Wang, Ming; Bian, Jiangze; Huang, Hui; Burch, Timothy R.; Andrade, Sandro C.; Zhang, Jingfei; Guan, Yongtao",,,Semi-parametric Learning of Structured Temporal Point Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a general framework of using multi-level log-Gaussian Cox process to model repeatedly observed point processes with complex structures; such type of data have become increasingly available in various areas including medical research, social sciences, economics and finance due to technological advances. A novel nonparametric approach is developed to efficiently and consistently estimate the covariance functions of the latent Gaussian processes at all levels. To predict the functional principal component scores, we propose a consistent estimation procedure by maximizing the conditional likelihood of super-positions of point processes. We further extend our procedure to the bivariate point process case in which potential correlations between the processes can be assessed. Asymptotic properties of the proposed estimators are investigated, and the effectiveness of our procedures is illustrated through a simulation study and an application to a stock trading dataset.",,,,,"Zhang, Jing/HII-4294-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,192,,,,,,,,,,,,,,,WOS:000590000100001,0
J,"Zhou, C; Wang, XF; Guo, JH",,,,"Zhou, Can; Wang, Xiaofei; Guo, Jianhua",,,Learning Mixed Latent Tree Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Latent structural learning has attracted more attention in recent years. But most related works only focuses on pure continuous or pure discrete data. In this paper, we consider mixed latent tree models for mixed data mining. We address the latent structural learning and parameter estimation for those mixed models. For structural learning, we propose a consistent bottom-up algorithm, and give a finite sample bound guarantee for the exact structural recovery. For parameter estimation, we suggest a moment estimator by exploiting matrix decomposition, and prove asymptotic normality of the estimator. Experiments on the simulated and real data support that our method is valid for mining the hierarchical structure and latent information.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,249,,,,,,,,,,,,,,,WOS:000608919200001,0
J,"Kossaifi, J; Panagakis, Y; Anandkumar, A; Pantic, M",,,,"Kossaifi, Jean; Panagakis, Yannis; Anandkumar, Anima; Pantic, Maja",,,TensorLy: Tensor Learning in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly",,,,,"Panagakis, Yannis/AAZ-8090-2020; Kossaifi, Jean/AAW-8519-2021","Kossaifi, Jean/0000-0002-4445-3429; Panagakis, Ioannis/0000-0003-0153-5210",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,26,,,,,,,,,,,,,,,WOS:000458669100001,0
J,"Loukas, A",,,,"Loukas, Andreas",,,Graph Reduction with Spectral and Cut Guarantees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Can one reduce the size of a graph without significantly altering its basic properties? The graph reduction problem is hereby approached from the perspective of restricted spectral approximation, a modification of the spectral similarity measure used for graph sparsification. This choice is motivated by the observation that restricted approximation carries strong spectral and cut guarantees, and that it implies approximation results for unsupervised learning problems relying on spectral embeddings. The article then focuses on coarsening|-the most common type of graph reduction. Sufficient conditions are derived for a small graph to approximate a larger one in the sense of restricted approximation. These findings give rise to algorithms that, compared to both standard and advanced graph reduction methods, find coarse graphs of improved quality, often by a large margin, without sacrificing speed.",,,,,,"Loukas, Andreas/0000-0003-4866-1599",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,116,,,,,,,,,,,,,,,WOS:000476625100001,0
J,"Zhang, RY; Sojoudi, S; Lavaei, J",,,,"Zhang, Richard Y.; Sojoudi, Somayeh; Lavaei, Javad",,,Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonconvex matrix recovery is known to contain no spurious local minima under a restricted isometry property (RIP) with a sufficiently small RIP constant delta. If delta is too large, however, then counterexamples containing spurious local minima are known to exist. In this paper, we introduce a proof technique that is capable of establishing sharp thresholds on delta to guarantee the inexistence of spurious local minima. Using the technique, we prove that in the case of a rank-1 ground truth, an RIP constant of delta < 1/2 is both necessary and sufficient for exact recovery from any arbitrary initial point (such as a random point). We also prove a local recovery result: given an initial point x(0) satisfying f(x(0)) <= (1 - delta)(2) f(0), any descent algorithm that converges to second-order optimality guarantees exact recovery. Keywords: matrix factorization, nonconvex optimization, Restricted Isometry Property, matrix sensing, spurious local minima",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,114,,,,,,,,,,,,,,,WOS:000476624800001,0
J,"Zhou, ZX; Amini, AA",,,,"Zhou, Zhixin; Amini, Arash A.",,,Analysis of spectral clustering algorithms for community detection: the general bipartite setting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider spectral clustering algorithms for community detection under a general bipartite stochastic block model (SBM). A modern spectral clustering algorithm consists of three steps: (1) regularization of an appropriate adjacency or Laplacian matrix (2) a form of spectral truncation and (3) a k-means type algorithm in the reduced spectral domain. We focus on the adjacency-based spectral clustering and for the first step, propose a new data-driven regularization that can restore the concentration of the adjacency matrix even for the sparse networks. This result is based on recent work on regularization of random binary matrices, but avoids using unknown population level parameters, and instead estimates the necessary quantities from the data. We also propose and study a novel variation of the spectral truncation step and show how this variation changes the nature of the misclassification rate in a general SBM. We then show how the consistency results can be extended to models beyond SBMs, such as inhomogeneous random graph models with approximate clusters, including a graphon clustering problem, as well as general sub-Gaussian biclustering. A theme of the paper is providing a better understanding of the analysis of spectral methods for community detection and establishing consistency results, under fairly general clustering models and for a wide regime of degree growths, including sparse cases where the average expected degree grows arbitrarily slowly.",,,,,"Zhou, Zhixin/GSD-7016-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,47,,,,,,,,,,,,,,,WOS:000463321300001,0
J,"Borkar, VS; Dwaracherla, VR; Sahasrabudhe, N",,,,"Borkar, Vivek S.; Dwaracherla, Vikranth R.; Sahasrabudhe, Neeraja",,,Gradient Estimation with Simultaneous Perturbation and Compressive Sensing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a scheme for finding a good estimator for the gradient of a function on a high-dimensional space with few function evaluations, for applications where function evaluations are expensive and the function under consideration is not sensitive in all coordinates locally, making its gradient almost sparse. Exploiting the latter aspect, our method combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. We theoretically justify its computational advantages and illustrate them empirically by numerical experiments. In particular, applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,161,,,,,,,,,,,,,,,WOS:000433254000001,0
J,"Sosic, A; Zoubir, AM; Rueckert, E; Peters, J; Koeppl, H",,,,"Sosic, Adrian; Zoubir, Abdelhak M.; Rueckert, Elmar; Peters, Jan; Koeppl, Heinz",,,Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.",,,,,"Peters, Jan/P-6027-2019; Zoubir, Abdelhak M/AAW-6349-2021; Peters, Jan R/D-5068-2009","Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091; Rueckert, Elmar/0000-0003-1221-8253; Sosic, Adrian/0000-0003-2845-6635",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,69,,,,,,,,,,,,,,,WOS:000452055300001,0
J,"Srivastava, S; Li, C; Dunson, DB",,,,"Srivastava, Sanvesh; Li, Cheng; Dunson, David B.",,,Scalable Bayes via Barycenter in Wasserstein Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Divide-and-conquer based methods for Bayesian inference provide a general approach for tractable posterior inference when the sample size is large. These methods divide the data into smaller subsets, sample from the posterior distribution of parameters in parallel on all the subsets, and combine posterior samples from all the subsets to approximate the full data posterior distribution. The smaller size of any subset compared to the full data implies that posterior sampling on any subset is computationally more efficient than sampling from the true posterior distribution. Since the combination step takes negligible time relative to sampling, posterior computations can be scaled to massive data by dividing the full data into sufficiently large number of data subsets. One such approach relies on the geometry of posterior distributions estimated across different subsets and combines them through their barycenter in a Wasserstein space of probability measures. We provide theoretical guarantees on the accuracy of approximation that are valid in many applications. We show that the geometric method approximates the full data posterior distribution better than its competitors across diverse simulations and reproduces known results when applied to a movie ratings database.",,,,,,"Li, Cheng/0000-0001-7522-7072",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,8,,,,,,,,,,,,,,,WOS:000443223200001,0
J,"Caron, F; Neiswanger, W; Wood, F; Doucet, A; Davy, M",,,,"Caron, Francois; Neiswanger, Willie; Wood, Frank; Doucet, Arnaud; Davy, Manuel",,,Generalized Polya Urn for Time-Varying Pitman-Yor Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This article introduces a class of first-order stationary time-varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized Polya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,27,,,,,,,,,,,,,,,WOS:000399842400001,0
J,"Chang, XY; Lin, SB; Zhou, DX",,,,"Chang, Xiangyu; Lin, Shao-Bo; Zhou, Ding-Xuan",,,Distributed Semi-supervised Learning with Kernel Ridge Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper provides error analysis for distributed semi-supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) to data subsets that are distributively stored on multiple servers to produce individual output functions, and then takes a weighted average of the individual output functions as a final estimator. Using a novel error decomposition which divides the generalization error of DSKRR into the approximation error, sample error and distributed error, we find that the sample error and distributed error reflect the power and limitation of DSKRR, compared with KRR processing the whole data. Thus a small distributed error provides a large range of the number of data subsets to guarantee a small generalization error. Our results show that unlabeled data play important roles in reducing the distributed error and enlarging the number of data subsets in DSKRR. Our analysis also applies to the case when the regression function is out of the reproducing kernel Hilbert space. Numerical experiments including toy simulations and a music-prediction task are employed to demonstrate our theoretical statements and show the power of unlabeled data in distributed learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,46,,,,,,,,,,,,,,,WOS:000405964100001,0
J,"Fenn, S; Moscato, P",,,,"Fenn, Shannon; Moscato, Pablo",,,Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the effect of introducing a curriculum of targets when training Boolean models on supervised Multi Label Classification (MLC) problems. In particular, we consider how to order targets in the absence of prior knowledge, and how such a curriculum may be enforced when using meta-heuristics to train discrete non-linear models.& para;& para;We show that hierarchical dependencies between targets can be exploited by enforcing an appropriate curriculum using hierarchical loss functions. On several multi-output circuit-inference problems with known target difficulties, Feedforward Boolean Networks (FBNs) trained with such a loss function achieve significantly lower out-of-sample error, up to 10% in some cases. This improvement increases as the loss places more emphasis on target order and is strongly correlated with an easy-to-hard curricula. We also demonstrate the same improvements on three real-world models and two Gene Regulatory Network (GRN) inference problems.& para;& para;We posit a simple a-priori method for identifying an appropriate target order and estimating the strength of target relationships in Boolean MLCs. These methods use intrinsic dimension as a proxy for target difficulty, which is estimated using optimal solutions to a combinatorial optimisation problem known as the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same generalisation gains can be achieved without providing any knowledge of target difficulty.",,,,,"fenn, shannon/S-2744-2019","fenn, shannon/0000-0002-6543-0306; Moscato, Pablo/0000-0003-2570-5966",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,114,,,,,,,,,,,,,,,WOS:000424539600001,0
J,"Gao, C; Ma, ZM; Zhang, AY; Zhou, HH",,,,"Gao, Chao; Ma, Zongming; Zhang, Anderson Y.; Zhou, Harrison H.",,,Achieving Optimal Misclassification Proportion in Stochastic Block Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Community detection is a fundamental statistical problem in network data analysis. In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a refinement stage motivated by penalized local maximum likelihood estimation. This stage can take a wide range of weakly consistent community detection procedures as its initializer, to which it applies and outputs a community assignment that achieves optimal misclassification proportion with high probability. The theoretical property is confirmed by simulated examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,45,60,,,,,,,,,,,,,,,WOS:000405991600001,0
J,"Ghosdastidar, D; Dukkipati, A",,,,"Ghosdastidar, Debarghya; Dukkipati, Ambedkar",,,Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs-a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy-an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes.",,,,,,"Ghoshdastidar, Debarghya/0000-0003-0202-7007",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,50,,,,,,,,,,,,,,,WOS:000405980100001,0
J,"Jagabathula, S; Subramaniam, L; Venkataraman, A",,,,"Jagabathula, Srikanth; Subramaniam, Lakshminarayanan; Venkataraman, Ashwin",,,Identifying Unreliable and Adversarial Workers in Crowdsourced Labeling Tasks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst-case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,93,,,,,,,,,,,,,,,WOS:000412484100001,0
J,"Lim, SH; Chen, YD; Xu, H",,,,"Lim, Shiau Hong; Chen, Yudong; Xu, Huan",,,Clustering from General Pairwise Observations with Applications to Time-varying Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a general framework for graph clustering and bi-clustering where we are given a general observation ( called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,49,,,,,,,,,,,,,,,WOS:000405980000001,0
J,"Sun, H; Craig, BA; Zhang, LS",,,,"Sun, Hui; Craig, Bruce A.; Zhang, Lingsong",,,Angle-based Multicategory Distance-weighted SVM,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MDWSVM) classification method that is motivated from the binary distance-weighted support vector machine (DWSVM) classification method. The new method has the merits of both support vector machine (SVM) and distance-weighted discrimination (DWD) but also alleviates both the data piling issue of SVM and the imbalanced data issue of DWD. Theoretical and numerical studies demonstrate the advantages of MDWSVM method over existing angle-based methods.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,85,,,,,,,,,,,,,,,WOS:000412063600001,0
J,"Liu, J; Zhu, XJ",,,,"Liu, Ji; Zhu, Xiaojin",,,The Teaching Dimension of Linear Learners,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,162,,,,,,,,,,,,,,,WOS:000391668400001,0
J,"Menon, AK; Williamson, RC",,,,"Menon, Aditya Krishna; Williamson, Robert C.",,,Bipartite Ranking: a Risk-Theoretic Perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a systematic study of the bipartite ranking problem, with the aim of explicating its connections to the class-probability estimation problem. Our study focuses on the properties of the statistical risk for bipartite ranking with general losses, which is closely related to a generalised notion of the area under the ROC curve: we establish alternate representations of this risk, relate the Bayes-optimal risk to a class of probability divergences, and characterise the set of Bayes-optimal scorers for the risk. We further study properties of a generalised class of bipartite risks, based on the p-norm push of Rudin (2009). Our analysis is based on the rich framework of proper losses, which are the central tool in the study of class-probability estimation. We show how this analytic tool makes transparent the generalisations of several existing results, such as the equivalence of the minimisers for four seemingly disparate risks from bipartite ranking and class-probability estimation. A novel practical implication of our analysis is the design of new families of losses for scenarios where accuracy at the head of ranked list is paramount, with comparable empirical performance to the p-norm push.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,195,,,,,,,,,,,,,,,WOS:000391827100001,0
J,"Uria, B; Cote, MA; Gregor, K; Murray, I; Larochelle, H",,,,"Uria, Benigno; Cote, Marc-Alexandre; Gregor, Karol; Murray, Iain; Larochelle, Hugo",,,Neural Autoregressive Distribution Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,205,,,,,,,,,,,,,,,WOS:000391831700001,0
J,"Zuluaga, M; Krause, A; Puschel, M",,,,"Zuluaga, Marcela; Krause, Andreas; Pueschel, Markus",,,epsilon-PAL: An Active Learning Approach to the Multi-Objective Optimization Problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many fields one encounters the challenge of identifying out of a pool of possible designs those that simultaneously optimize multiple objectives. In many applications an exhaustive search for the Pareto-optimal set is infeasible. To address this challenge, we propose the is an element of-ParetoActive Learning (is an element of-PAL) algorithm which adaptively samples the design space to predict a set of Pareto-optimal solutions that cover the true Pareto front of the design space with some granularity regulated by a parameter is an element of. Key features of is an element of-PAL include (1) modeling the objectives as draws from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on is an element of-PAL ' s sampling cost required to achieve a desired accuracy. Further, we perform an experimental evaluation on three real-world data sets that demonstrate is an element of-PAL ' s e ff ectiveness; in comparison to the state-of-the-art active learning algorithm PAL, is an element of-PAL reduces the amount of computations and the number of samples from the design space required to meet the user's desired level of accuracy. In addition, we show that is an element of-PAL improves signi fi cantly over a state-of-the-art multi-objective optimization method, saving in most cases 30% to 70% evaluations to achieve the same accuracy.",,,,,,"Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,104,,,,,,,,,,,,,,,WOS:000391543700001,0
J,"Janzing, D; Scholkopf, B",,,,"Janzing, Dominik; Schoelkopf, Bernhard",,,Semi-Supervised Interpolation in an Anticausal Learning Scenario,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"According to a recently stated 'independence postulate', the distribution P-cause contains no information about the conditional P-effect vertical bar cause while P-effect may contain information about P-cause vertical bar effect. Since semi-supervised learning (SSL) attempts to exploit information from P-X to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f (X) as recently assayed in Information-Geometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of P-X and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated - which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1923,1948,,,,,,,,,,,,,,,,WOS:000369887300009,0
J,"Garcia, J; Fernandez, F",,,,"Garcia, Javier; Fernandez, Fernando",,,A Comprehensive Survey on Safe Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Safe Reinforcement Learning can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted finite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.",,,,,"Fernandez, Fernando/H-7902-2015; Fern√°ndez, Fernando/K-1681-2019; Garc√≠a, Javier/A-3607-2019","Fernandez, Fernando/0000-0003-3801-6801; Fern√°ndez, Fernando/0000-0003-3801-6801; Garc√≠a, Javier/0000-0002-5638-5240",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1437,1480,,,,,,,,,,,,,,,,WOS:000369887100004,0
J,"Feldman, S; Gupta, MR; Frigyik, BA",,,,"Feldman, Sergey; Gupta, Maya R.; Frigyik, Bela A.",,,Revisiting Stein's Paradox: Multi-Task Averaging,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a multi-task learning approach to jointly estimate the means of multiple independent distributions from samples. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the individual task's sample averages. We derive the optimal amount of regularization for the two task case for the minimum risk estimator and a minimax estimator, and show that the optimal amount of regularization can be practically estimated without cross-validation. We extend the practical estimators to an arbitrary number of tasks. Simulations and real data experiments demonstrate the advantage of the proposed MTA estimators over standard averaging and James-Stein estimation.",,,,,"Frigyik, Andrew B/H-9400-2014","Frigyik, Andrew B/0000-0002-4220-4680",,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3441,3482,,,,,,,,,,,,,,,,WOS:000353126200004,0
J,"Couprie, C; Farabet, C; Najman, L; LeCun, Y",,,,"Couprie, Camille; Farabet, Clement; Najman, Laurent; LeCun, Yann",,,Convolutional Nets and Watershed Cuts for Real-Time Semantic Labeling of RGBD Videos,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.",,,,,"Math et Info, Direction Math/C-1462-2013; Najman, Laurent/AAB-4212-2020","Najman, Laurent/0000-0002-6190-0235",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3489,3511,,,,,,,,,,,,,,,,WOS:000344638800022,0
J,"Jylanki, P; Nummenmaa, A; Vehtari, A",,,,"Jylanki, Pasi; Nummenmaa, Aapo; Vehtari, Aki",,,Expectation Propagation for Neural Networks with Sparsity-Promoting Priors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel approach for nonlinear regression using a two-layer neural network (NN) model structure with sparsity-favoring hierarchical priors on the network weights. We present an expectation propagation (EP) approach for approximate integration over the posterior distribution of the weights, the hierarchical scale parameters of the priors, and the residual scale. Using a factorized posterior approximation we derive a computationally efficient algorithm, whose complexity scales similarly to an ensemble of independent sparse linear models. The approach enables flexible definition of weight priors with different sparseness properties such as independent Laplace priors with a common scale parameter or Gaussian automatic relevance determination (ARD) priors with different relevance parameters for all inputs. The approach can be extended beyond standard activation functions and NN model structures to form flexible nonlinear predictors from multiple sparse linear models. The effects of the hierarchical priors and the predictive performance of the algorithm are assessed using both simulated and real-world data. Comparisons are made to two alternative models with ARD priors: a Gaussian process with a NN covariance function and marginal maximum a posteriori estimates of the relevance parameters, and a NN with Markov chain Monte Carlo integration over all the unknown model parameters.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2014,15,,,,,,1849,1901,,,,,,,,,,,,,,,,WOS:000344638100007,0
J,"Cuong, NV; Ye, N; Lee, WS; Chieu, HL",,,,"Nguyen Viet Cuong; Ye, Nan; Lee, Wee Sun; Chieu, Hai Leong",,,Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dependencies among neighboring labels in a sequence are important sources of information for sequence labeling and segmentation. However, only first-order dependencies, which are dependencies between adjacent labels or segments, are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account. In this paper, we give efficient inference algorithms to handle high-order dependencies between labels or segments in conditional random fields, under the assumption that the number of distinct label patterns used in the features is small. This leads to efficient learning algorithms for these conditional random fields. We show experimentally that exploiting high-order dependencies can lead to substantial performance improvements for some problems, and we discuss conditions under which high-order features can be effective.",,,,,,"Ye, Nan/0000-0001-5971-9202",,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,981,1009,,,,,,,,,,,,,,,,WOS:000335458100005,0
J,"Fukumizu, K; Song, L; Gretton, A",,,,"Fukumizu, Kenji; Song, Le; Gretton, Arthur",,,Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A kernel method for realizing Bayes' rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes' rule are presented, including Bayesian computation without likelihood and filtering with a nonparametric state-space model.",,,,,,"Gretton, Arthur/0000-0003-3169-7624; Fukumizu, Kenji/0000-0002-3488-2625",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3753,3783,,,,,,,,,,,,,,,,WOS:000335457100009,0
J,"Hannah, LA; Dunson, DB",,,,"Hannah, Lauren A.; Dunson, David B.",,,Multivariate Convex Regression with Adaptive Partitioning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3261,3294,,,,,,,,,,,,,,,,WOS:000329786900002,0
J,"Brakel, P; Stroobandt, D; Schrauwen, B",,,,"Brakel, Philemon; Stroobandt, Dirk; Schrauwen, Benjamin",,,Training Energy-Based Models for Time-Series Imputation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Imputing missing values in high dimensional time-series is a difficult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difficulties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-field iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artificial and two real-world data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2771,2797,,,,,,,,,,,,,,,,WOS:000327007400009,0
J,"Ly, DL; Lipson, H",,,,"Ly, Daniel L.; Lipson, Hod",,,Learning Symbolic Representations of Hybrid Dynamical Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air traffic controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms-clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inequalities that describe binary classification boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3585,3618,,,,,,,,,,,,,,,,WOS:000314529000005,0
J,"Genovese, CR; Perone-Pacifico, M; Verdinelli, I; Wasserman, L",,,,"Genovese, Christopher R.; Perone-Pacifico, Marco; Verdinelli, Isabella; Wasserman, Larry",,,Minimax Manifold Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We find the minimax rate of convergence in Hausdorff distance for estimating a manifold M of dimension d embedded in R-D given a noisy sample from the manifold. Under certain conditions, we show that the optimal rate of convergence is n(-2/(2+d)). Thus, the minimax rate depends only on the dimension of the manifold, not on the dimension of the space in which M is embedded.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1263,1291,,,,,,,,,,,,,,,,WOS:000305456600001,0
J,"Piccolo, SR; Frey, LJ",,,,"Piccolo, Stephen R.; Frey, Lewis J.",,,ML-Flex: A Flexible Toolbox for Performing Classification Analyses In Parallel,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by a need to classify high-dimensional, heterogeneous data from the bioinformatics domain, we developed ML-Flex, a machine-learning toolbox that enables users to perform two-class and multi-class classification analyses in a systematic yet flexible manner. ML-Flex was written in Java but is capable of interfacing with third-party packages written in other programming languages. It can handle multiple input-data formats and supports a variety of customizations. ML-Flex provides implementations of various validation strategies, which can be executed in parallel across multiple computing cores, processors, and nodes. Additionally, ML-Flex supports aggregating evidence across multiple algorithms and data sets via ensemble learning. This open-source software package is freely available from http://mlflex.sourceforge.net.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,555,559,,,,,,,,,,,,,,,,WOS:000303772100003,0
J,"Zhang, HZ; Xu, YS; Zhang, QH",,,,"Zhang, Haizhang; Xu, Yuesheng; Zhang, Qinghui",,,Refinement of Operator-valued Reproducing Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper studies the construction of a refinement kernel for a given operator-valued reproducing kernel such that the vector-valued reproducing kernel Hilbert space of the refinement kernel contains that of the given kernel as a subspace. The study is motivated from the need of updating the current operator-valued reproducing kernel in multi-task learning when underfitting or overfitting occurs. Numerical simulations confirm that the established refinement kernel method is able to meet this need. Various characterizations are provided based on feature maps and vector-valued integral representations of operator-valued reproducing kernels. Concrete examples of refining translation invariant and finite Hilbert-Schmidt operator-valued reproducing kernels are provided. Other examples include refinement of Hessian of scalar-valued translation-invariant kernels and transformation kernels. Existence and properties of operator-valued reproducing kernels preserved during the refinement process are also investigated.,,,,,"Zhang, Haizhang/GPX-1222-2022; Xu, yue/HGE-1737-2022","Zhang, Haizhang/0000-0002-8241-3145; ",,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,91,136,,,,,,,,,,,,,,,,WOS:000303045100004,0
J,"Ertekin, S; Rudin, C",,,,"Ertekin, Seyda; Rudin, Cynthia",,,On Equivalence Relationships Between Classification and Ranking Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We demonstrate that there are machine learning algorithms that can achieve success for two separate tasks simultaneously, namely the tasks of classification and bipartite ranking. This means that advantages gained from solving one task can be carried over to the other task, such as the ability to obtain conditional density estimates, and an order-of-magnitude reduction in computational time for training the algorithm. It also means that some algorithms are robust to the choice of evaluation metric used; they can theoretically perform well when performance is measured either by a misclassification error or by a statistic of the ROC curve (such as the area under the curve). Specifically, we provide such an equivalence relationship between a generalization of Freund et al.'s RankBoost algorithm, called the P-Norm Push, and a particular cost-sensitive classification algorithm that generalizes AdaBoost, which we call P-Classification. We discuss and validate the potential benefits of this equivalence relationship, and perform controlled experiments to understand P-Classification's empirical performance. There is no established equivalence relationship for logistic regression and its ranking counterpart, so we introduce a logistic-regression-style algorithm that aims in between classification and ranking, and has promising experimental performance with respect to both tasks.",,,,,"Ertekin, Seyda/N-9066-2013; Ertekin, Seyda/AAP-5301-2021",,,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2905,2929,,,,,,,,,,,,,,,,WOS:000298103200007,0
J,"Jenatton, R; Mairal, J; Obozinski, G; Bach, F",,,,"Jenatton, Rodolphe; Mairal, Julien; Obozinski, Guillaume; Bach, Francis",,,Proximal Methods for Hierarchical Sparse Coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difficult to optimize, and in this paper, we propose efficient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the l(1)-norm. Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications: first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally self-organize in a prespecified arborescent structure, leading to better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2297,2334,,,,,,,,,,,,,,,,WOS:000293757900007,0
J,"Ganchev, K; Graca, J; Gillenwater, J; Taskar, B",,,,"Ganchev, Kuzman; Graca, Joao; Gillenwater, Jennifer; Taskar, Ben",,,Posterior Regularization for Structured Latent Variable Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present posterior regularization, a probabilistic framework for structured, weakly supervised learning. Our framework efficiently incorporates indirect supervision via constraints on posterior distributions of probabilistic models with latent variables. Posterior regularization separates model complexity from the complexity of structural constraints it is desired to satisfy. By directly imposing decomposable regularization on the posterior moments of latent variables during learning, we retain the computational efficiency of the unconstrained model while ensuring desired constraints hold in expectation. We present an efficient algorithm for learning with posterior regularization and illustrate its versatility on a diverse set of structural constraints such as bijectivity, symmetry and group sparsity in several large scale experiments, including multi-view learning, cross-lingual dependency grammar induction, unsupervised part-of-speech induction, and bitext word alignment.(1)",,,,,"Graca, Joao/E-6329-2011",,,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,2001,2049,,,,,,,,,,,,,,,,WOS:000282523000003,0
J,"Ryabko, D",,,,"Ryabko, Daniil",,,On Finding Predictors for Arbitrary Families of Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem is sequence prediction in the following setting. A sequence x(1),...,x(n),... of discrete-valued observations is generated according to some unknown probabilistic law (measure) mu. After observing each outcome, it is required to give the conditional probabilities of the next observation. The measure mu belongs to an arbitrary but known class C of stochastic process measures. We are interested in predictors rho whose conditional probabilities converge (in some sense) to the true mu-conditional probabilities, if any mu is an element of C is chosen to generate the sequence. The contribution of this work is in characterizing the families C for which such predictors exist, and in providing a specific and simple form in which to look for a solution. We show that if any predictor works, then there exists a Bayesian predictor, whose prior is discrete, and which works too. We also find several sufficient and necessary conditions for the existence of a predictor, in terms of topological characterizations of the family C, as well as in terms of local behaviour of the measures in C, which in some cases lead to procedures for constructing such predictors. It should be emphasized that the framework is completely general: the stochastic processes considered are not required to be i.i.d., stationary, or to belong to any parametric or countable family.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,581,602,,10.1145/1756006.1756023,0,,,,,,,,,,,,,WOS:000277186500005,0
J,"Ding, YF; Simonoff, JS",,,,"Ding, Yufeng; Simonoff, Jeffrey S.",,,An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There are many different methods used by classification tree algorithms when missing data occur in the predictors, but few studies have been done comparing their appropriateness and performance. This paper provides both analytic and Monte Carlo evidence regarding the effectiveness of six popular missing data methods for classification trees applied to binary response data. We show that in the context of classification trees, the relationship between the missingness and the dependent variable, as well as the existence or non-existence of missing values in the testing data, are the most helpful criteria to distinguish different missing data methods. In particular, separate class is clearly the best method to use when the testing set has missing values and the missingness is related to the response variable. A real data set related to modeling bankruptcy of a firm is then analyzed. The paper concludes with discussion of adaptation of these results to logistic regression, and other potential generalizations.",,,,,"Simonoff, Jeffrey S/A-6765-2008; Ding, yu/GWV-1732-2022",,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,131,170,,,,,,,,,,,,,,,,WOS:000277186400006,0
J,"Shahbaba, B; Neal, R",,,,"Shahbaba, Babak; Neal, Radford",,,Nonlinear Models Using Dirichlet Process Mixtures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new nonlinear model for classification, in which we model the joint distribution of response variable, y, and covariates, x, non-parametrically using Dirichlet process mixtures. We keep the relationship between y and x linear within each component of the mixture. The overall relationship becomes nonlinear if the mixture contains more than one component, with different regression coefficients. We use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit (MNL) models, decision trees, and support vector machines. We also evaluate our approach on two classification problems: identifying the folding class of protein sequences and detecting Parkinson's disease. Our model can sometimes improve predictive accuracy. Moreover, by grouping observations into sub-populations (i.e., mixture components), our model can sometimes provide insight into hidden structure in the data.",,,,,,"Shahbaba, Babak/0000-0002-8102-1609",,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1829,1850,,,,,,,,,,,,,,,,WOS:000270825200003,0
J,"White, H; Chalak, K",,,,"White, Halbert; Chalak, Karim",,,"Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Judea Pearl's Causal Model is a rich framework that provides deep insight into the nature of causal relations. As yet, however, the Pearl Causal Model (PCM) has had a lesser impact on economics or econometrics than on other disciplines. This may be due in part to the fact that the PCM is not as well suited to analyzing structures that exhibit features of central interest to economists and econometricians: optimization, equilibrium, and learning. We offer the settable systems framework as an extension of the PCM that permits causal discourse in systems embodying optimization, equilibrium, and learning. Because these are common features of physical, natural, or social systems, our framework may prove generally useful for machine learning. Important features distinguishing the settable system framework from the PCM are its countable dimensionality and the use of partitioning and partition-specific response functions to accommodate the behavior of optimizing and interacting agents and to eliminate the requirement of a unique fixed point for the system. Refinements of the PCM include the settable systems treatment of attributes, the causal role of exogenous variables, and the dual role of variables as causes and responses. A series of closely related machine learning examples and examples from game theory and machine learning with feedback demonstrates some limitations of the PCM and motivates the distinguishing features of settable systems.",,,,,,"Chalak, Karim/0000-0002-6875-172X",,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1759,1799,,,,,,,,,,,,,,,,WOS:000270825200001,0
J,"King, DE",,,,"King, Davis E.",,,Dlib-ml: A Machine Learning Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1755,1758,,,,,,,,,,,,,,,,WOS:000270825000016,0
J,"Yehezkel, R; Lerner, B",,,,"Yehezkel, Raanan; Lerner, Boaz",,,Bayesian Network Structure Learning by Recursive Autonomy Identification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose the recursive autonomy identification (RAI) algorithm for constraint-based (CB) Bayesian network structure learning. The RAI algorithm learns the structure by sequential application of conditional independence (CI) tests, edge direction and structure decomposition into autonomous sub-structures. The sequence of operations is performed recursively for each autonomous substructure while simultaneously increasing the order of the CI test. While other CB algorithms d-separate structures and then direct the resulted undirected graph, the RAI algorithm combines the two processes from the outset and along the procedure. By this means and due to structure decomposition, learning a structure using RAI requires a smaller number of CI tests of high orders. This reduces the complexity and run-time of the algorithm and increases the accuracy by diminishing the curse-of-dimensionality. When the RAI algorithm learned structures from databases representing synthetic problems, known networks and natural problems, it demonstrated superiority with respect to computational complexity, run-time, structural correctness and classification accuracy over the PC, Three Phase Dependency Analysis, Optimal Reinsertion, greedy search, Greedy Equivalence Search, Sparse Candidate, and Max-Min Hill-Climbing algorithms.",,,,,"LERNER, BOAZ/AAJ-4064-2020",,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1527,1570,,,,,,,,,,,,,,,,WOS:000270825000009,0
J,"Shpitser, I; Pearl, J",,,,"Shpitser, Ilya; Pearl, Judea",,,Complete Identification Methods for the Causal Hierarchy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple parallel worlds and resulting from simultaneous, possibly conflicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Specifically, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy.",,,,,,"Shpitser, Ilya/0000-0003-2571-7326",,,,,,,,,,,,,1532-4435,,,,,SEP,2008,9,,,,,,1941,1979,,,,,,,,,,,,,,,,WOS:000262637100001,0
J,"Zhu, J; Nie, ZQ; Zhang, B; Wen, JR",,,,"Zhu, Jun; Nie, Zaiqing; Zhang, Bo; Wen, Ji-Rong",,,Dynamic Hierarchical Markov Random Fields for integrated web data extraction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies-attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and define a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model's parameters and to find the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve significant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by fixed-structured hierarchical models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1583,1614,,,,,,,,,,,,,,,,WOS:000258646800010,0
J,"Shafer, G; Vovk, V",,,,"Shafer, Glenn; Vovk, Vladimir",,,A tutorial on conformal prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability e, together with a method that makes a prediction (y) over bar of a label y, it produces a set of labels, typically containing (y) over bar, that also contains y with probability 1-epsilon. Conformal prediction can be applied to any method for producing (y) over bar: a nearest-neighbor method, a support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 - epsilon of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).",,,,,,"Vovk, Vladimir/0000-0003-2602-6877",,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,371,421,,,,,,,,,,,,,,,,WOS:000256642000002,0
J,"Ye, JP",,,,"Ye, Jieping",,,Comments on the complete characterization of a family of solutions to a generalized Fisher criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justifies its practical use.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,517,519,,,,,,,,,,,,,,,,WOS:000256642000006,0
J,"Kolter, JZ; Maloof, MA",,,,"Kolter, J. Zico; Maloof, Marcus A.",,,Dynamic weighted majority: An ensemble method for drifting concepts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority ( DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation consisting of five experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like-we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, fixed-size ensemble of experts.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2007,8,,,,,,2755,2790,,,,,,,,,,,,,,,,WOS:000252745100004,0
J,"Koppel, M; Schler, J; Bonchek-Dokow, E",,,,"Koppel, Moshe; Schler, Jonathan; Bonchek-Dokow, Elisheva",,,Measuring differentiability: Unmasking pseudonymous authors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the depth of difference between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process.",,,,,"Bonchek Dokow, Elisheva/GQP-2080-2022; bonchek, elisheva/GQP-1967-2022","Bonchek Dokow, Elisheva/0000-0002-0173-0502; ",,,,,,,,,,,,,1532-4435,,,,,JUN,2007,8,,,,,,1261,1276,,,,,,,,,,,,,,,,WOS:000248351800002,0
J,"Chan, PK; Lippmann, RP",,,,"Chan, Philip K.; Lippmann, Richard P.",,,Machine learning for computer security,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The prevalent use of computers and internet has enhanced the quality of life for many people, but it has also attracted undesired attempts to undermine these systems. This special topic contains several research studies on how machine learning algorithms can help improve the security of computer systems.",,,,,"Chan, Philip K./ABD-7633-2021; Lippmann, Richard P/G-3832-2018","Chan, Philip K./0000-0002-3878-4205; Lippmann, Richard P/0000-0003-0904-0984",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2669,2672,,,,,,,,,,,,,,,,WOS:000245390800006,0
J,"Porta, JM; Vlassis, N; Spaan, MTJ; Poupart, P",,,,"Porta, Josep M.; Vlassis, Nikos; Spaan, Matthijs T. J.; Poupart, Pascal",,,Point-based value iteration for continuous POMDPs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) defined on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally defined on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the PERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend PERSEUS to deal with continuous action and observation sets by designing effective sampling approaches.",,,,,"Porta, Josep M/L-5069-2014","Porta, Josep M/0000-0002-5056-1717",,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2329,2367,,,,,,,,,,,,,,,,WOS:000245390700003,0
J,"Kampke, T",,,,"Kaempke, Thomas",,,Distance patterns in structural similarity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Similarity of edge labeled graphs is considered in the sense of minimum squared distance between corresponding values. Vertex correspondences are established by isomorphisms if both graphs are of equal size and by subisomorphisms if one graph has fewer vertices than the other. Best fit isomorphisms and subisomorphisms amount to solutions of quadratic assignment problems and are computed exactly as well as approximately by minimum cost flow, linear assignment relaxations and related graph algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2065,2086,,,,,,,,,,,,,,,,WOS:000245390500005,0
J,"Meinshausen, N",,,,"Meinshausen, Nicolai",,,Quantile regression forests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,983,999,,,,,,,,,,,,,,,,WOS:000245388400004,0
J,"Watanabe, K; Watanabe, S",,,,"Watanabe, K; Watanabe, S",,,Stochastic complexities of Gaussian mixtures in variational Bayesian approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,IEEE Conference on Cybernetics and Intelligent Systems,"DEC 01-03, 2004","Singapore, SINGAPORE",IEEE,,,,"Bayesian learning has been widely used and proved to be effective in many data modeling problems. However, computations involved in it require huge costs and generally cannot be performed exactly. The variational Bayesian approach, proposed as an approximation of Bayesian learning, has provided computational tractability and good generalization performance in many applications. The properties and capabilities of variational Bayesian learning itself have not been clarified yet. It is still unknown how good approximation the variational Bayesian approach can achieve. In this paper, we discuss variational Bayesian learning of Gaussian mixture models and derive upper and lower bounds of variational stochastic complexities. The variational stochastic complexity, which corresponds to the minimum variational free energy and a lower bound of the Bayesian evidence, not only becomes important in addressing the model selection problem, but also enables us to discuss the accuracy of the variational Bayesian approach as an approximation of true Bayesian learning.",,,,,"Watanabe, Sumio/C-3880-2015; Watanabe, Sumio/M-7370-2019","Watanabe, Sumio/0000-0001-8341-5639; Watanabe, Kazuho/0000-0001-6357-5141",,,,,,,,,,,,,1532-4435,,,,,APR,2006,7,,,,,,625,643,,,,,,,,,,,,,,,,WOS:000237359100003,0
J,"Wu, MR; Scholkopf, B; Bakir, G",,,,"Wu, MR; Scholkopf, B; Bakir, G",,,A direct method for building sparse kernel learning algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many kernel learning algorithms, including support vector machines, result in a kernel machine, such as a kernel classifier, whose key component is a weight vector in a feature space implicitly introduced by a positive definite kernel function. This weight vector is usually obtained by solving a convex optimization problem. Based on this fact we present a direct method to build sparse kernel learning algorithms by adding one more constraint to the original convex optimization problem, such that the sparseness of the resulting kernel machine is explicitly controlled while at the same time performance is kept as high as possible. A gradient based approach is provided to solve this modified optimization problem. Applying this method to the support vectom machine results in a concrete algorithm for building sparse large margin classifiers. These classifiers essentially find a discriminating subspace that can be spanned by a small number of vectors, and in this subspace, the different classes of data are linearly well separated. Experimental results over several classification benchmarks demonstrate the effectiveness of our approach.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925",,,,,,,,,,,,,1532-4435,,,,,APR,2006,7,,,,,,603,624,,,,,,,,,,,,,,,,WOS:000237359100002,0
J,"Quinonero-Candela, JQ; Rasmussen, CE",,,,"Quinonero-Candela, JQ; Rasmussen, CE",,,A unifying view of sparse approximate Gaussian process regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.",,,,,,"Rasmussen, Carl Edward/0000-0001-8899-7850",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,1939,1959,,,,,,,,,,,,,,,,WOS:000236331100003,0
J,"Jang, GJ; Lee, TW",,,,"Jang, GJ; Lee, TW",,,A maximum likelihood approach to single-channel source separation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new technique for achieving blind signal separation when given only a single channel recording. The main concept is based on exploiting a priori sets of time-domain basis functions learned by independent component analysis (ICA) to the separation of mixed source signals observed in a single channel. The inherent time structure of sound sources is reflected in the ICA basis functions, which encode the sources in a statistically efficient manner. We derive a learning algorithm using a maximum likelihood approach given the observed single channel data and sets of basis functions. For each time point we infer the source parameters and their contribution factors. This inference is possible due to prior knowledge of the basis functions and the associated coefficient densities. A flexible model for density estimation allows accurate modeling of the observation and our experimental results exhibit a high level of separation performance for simulated mixtures as well as real environment recordings employing mixtures of two different sources.",,,,,"Jang, Gil-Jin/E-9360-2010",,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1365,1392,,,,,,,,,,,,,,,,WOS:000224808300010,0
J,"Rosset, S; Ji, Z; Hastie, T",,,,"Rosset, S; Ji, Z; Hastie, T",,,Boosting as a regularized path to a maximum margin classifier,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we study boosting methods from a new perspective. We build on recent work by Efron et al. to show that boosting approximately (and in some cases exactly) minimizes its loss criterion with an l(1) constraint on the coefficient vector. This helps understand the success of boosting with early stopping as regularized fitting of the loss criterion. For the two most commonly used criteria (exponential and binomial log-likelihood), we further show that as the constraint is relaxed-or equivalently as the boosting iterations proceed - the solution converges (in the separable case) to an '' l(1)-optimal '' separating hyper- plane. We prove that this l(1)-optimal separating hyper- plane has the property of maximizing the minimal l(1)-margin of the training data, as defined in the boosting literature. An interesting fundamental similarity between boosting and kernel support vector machines emerges, as both can be described as methods for regularized optimization in high-dimensional predictor space, using a computational trick to make the calculation practical, and converging to margin-maximizing solutions. While this statement describes SVMs exactly, it applies to boosting only approximately.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,941,973,,,,,,,,,,,,,,,,WOS:000236328000004,0
J,"Cicchello, O; Kremer, SC",,,,"Cicchello, O; Kremer, SC",,,Inducing grammars from sparse data sets: A survey of algorithms and results,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper provides a comprehensive survey of the field of grammar induction applied to randomly generated languages using sparse example sets.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,603,632,,10.1162/153244304773936063,0,,,,,,,,,,,,,WOS:000221345700008,0
J,"Basak, J; Sudarshan, A; Trivedi, D; Santhanam, MS",,,,"Basak, J; Sudarshan, A; Trivedi, D; Santhanam, MS",,,Weather data mining using independent component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we apply the independent component analysis technique for mining spatio-temporal data. The technique has been applied to mine for patterns in weather data using the North Atlantic Oscillation (NAO) as a specific example. We find that the strongest independent components match the observed synoptic weather patterns corresponding to the NAO. We also validate our results by matching the independent component activities with the NAO index.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2004,5,,,,,,239,253,,,,,,,,,,,,,,,,WOS:000236327200002,0
J,"Eshragh, A; Roosta, F; Nazari, A; Mahoney, MW",,,,"Eshragh, Ali; Roosta, Fred; Nazari, Asef; Mahoney, Michael W.",,,LSAR: Efficient Leverage Score Sampling Algorithm for the Analysis of Big Time Series Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We apply methods from randomized numerical linear algebra (RandNLA) to develop improved algorithms for the analysis of large-scale time series data. We first develop a new fast algorithm to estimate the leverage scores of an autoregressive (AR) model in big data regimes. We show that the accuracy of approximations lies within (1 + O (epsilon)) of the true leverage scores with high probability. These theoretical results are subsequently exploited to develop an efficient algorithm, called LSAR, for fitting an appropriate AR model to big time series data. Our proposed algorithm is guaranteed, with high probability, to find the maximum likelihood estimates of the parameters of the underlying true AR model and has a worst case running time that significantly improves those of the state-of-the-art alternatives in big data regimes. Empirical results on large-scale synthetic as well as real data highly support the theoretical results and reveal the efficacy of this new approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,36,,,,,,,,,,,,,,,,WOS:000752314700001,0
J,"Banerjee, T; Mukherjee, G; Paul, D",,,,"Banerjee, Trambak; Mukherjee, Gourab; Paul, Debashis",,,Improved Shrinkage Prediction under a Spiked Covariance Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a novel shrinkage rule for prediction in a high-dimensional non-exchangeable hierarchical Gaussian model with an unknown spiked covariance structure. We propose a family of priors for the mean parameter, governed by a power hyper-parameter, which encompasses independent to highly dependent scenarios. Corresponding to popular loss functions such as quadratic, generalized absolute, and Linex losses, these prior models induce a wide class of shrinkage predictors that involve quadratic forms of smooth functions of the unknown covariance. By using uniformly consistent estimators of these quadratic forms, we propose an efficient procedure for evaluating these predictors which outperforms factor model based direct plug-in approaches. We further improve our predictors by considering possible reduction in their variability through a novel coordinate-wise shrinkage policy that only uses covariance level information and can be adaptively tuned using the sample eigen structure. Finally, we extend our disaggregate model based methodology to prediction in aggregate models. We propose an easy-to-implement functional substitution method for predicting linearly aggregated targets and establish asymptotic optimality of our proposed procedure. We present simulation experiments as well as real data examples illustrating the efficacy of the proposed method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687139200001,0
J,"Bartlett, PL; Long, PM",,,,"Bartlett, Peter L.; Long, Philip M.",,,Failures of Model-dependent Generalization Bounds for Least-norm Interpolation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider bounds on the generalization performance of the least-norm linear regressor, in the over-parameterized regime where it can interpolate the data. We describe a sense in which any generalization bound of a type that is commonly proved in statistical learning theory must sometimes be very loose when applied to analyze the least-norm interpolant. In particular, for a variety of natural joint distributions on training examples, any valid generalization bound that depends only on the output of the learning algorithm, the number of training examples, and the confidence parameter, and that satisfies a mild condition (substantially weaker than monotonicity in sample size), must sometimes be very loose-it can be bounded below by a constant when the true excess risk goes to zero.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706442600001,0
J,"d'Aspremont, A; Cucuringu, M; Tyagi, H",,,,"d'Aspremont, Alexandre; Cucuringu, Mihai; Tyagi, Hemant",,,Ranking and synchronization from pairwise measurements via SVD,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a measurement graph G = (V, E) and an unknown signal r is an element of R-n, we investigate algorithms for recovering r from pairwise measurements of the form r(i) - r(j); {i, j} is an element of E. This problem arises in a variety of applications, such as ranking teams in sports data and time synchronization of distributed networks. Framed in the context of ranking, the task is to recover the ranking of n teams (induced by r) given a small subset of noisy pairwise rank offsets. We propose a simple SVD-based algorithmic pipeline for both the problem of time synchronization and ranking. We provide a detailed theoretical analysis in terms of robustness against both sampling sparsity and noise perturbations with outliers, using results from matrix perturbation and random matrix theory. Our theoretical findings are complemented by a detailed set of numerical experiments on both synthetic and real data, showcasing the competitiveness of our proposed algorithms with other state-of-the-art methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500019,0
J,"Huang, ZF; Lin, XM; Zhang, WJ; Zhang, Y",,,,"Huang, Zengfeng; Lin, Xuemin; Zhang, Wenjie; Zhang, Ying",,,"Communication-Efficient Distributed Covariance Sketch, with Application to Distributed PCA",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A sketch of a large data set captures vital properties of the original data while typically occupying much less space. In this paper, we consider the problem of computing a sketch of a massive data matrix A is an element of R-nxd that is distributed across s machines. Our goal is to output a matrix B is an element of R-lxd which is significantly smaller than but still approximates A well in terms of covariance error, i.e., parallel to A(T)A - (BB)-B-T parallel to(2). Such a matrix B is called a covariance sketch of A. We are mainly focused on minimizing the communication cost, which is arguably the most valuable resource in distributed computations. We show that there is a nontrivial gap between deterministic and randomized communication complexity for computing a covariance sketch. More specifically, we first prove an almost tight deterministic communication lower bound, then provide a new randomized algorithm with communication cost smaller than the deterministic lower bound. Based on a well-known connection between covariance sketch and approximate principle component analysis, we obtain better communication bounds for the distributed PCA problem. Moreover, we also give an improved distributed PCA algorithm for sparse input matrices, which uses our distributed sketching algorithm as a key building block.",,,,,,"Zhang, Ying/0000-0002-2674-1638",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656397400001,0
J,"Kubkowski, M; Mielniczuk, J; Teisseyre, P",,,,"Kubkowski, Mariusz; Mielniczuk, Jan; Teisseyre, Pawel",,,How to Gain on Power: Novel Conditional Independence Tests Based on Short Expansion of Conditional Mutual Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional independence tests play a crucial role in many machine learning procedures such as feature selection, causal discovery, and structure learning of dependence networks. They are used in most of the existing algorithms for Markov Blanket discovery such as Grow-Shrink or Incremental Association Markov Blanket. One of the most frequently used tests for categorical variables is based on the conditional mutual information (CMI) and its asymptotic distribution. However, it is known that the power of such test dramatically decreases when the size of the conditioning set grows, i.e. the test fails to detect true significant variables, when the set of already selected variables is large. To overcome this drawback for discrete data, we propose to replace the conditional mutual information by Short Expansion of Conditional Mutual Information (called SECMI), obtained by truncating the Mobius representation of CMI. We prove that the distribution of SECMI converges to either a normal distribution or to a distribution of some quadratic form in normal random variables. This property is crucial for the construction of a novel test of conditional independence which uses one of these distributions, chosen in a data dependent way, as a reference under the null hypothesis. The proposed methods have significantly larger power for discrete data than the standard asymptotic tests of conditional independence based on CMI while retaining control of the probability of type I error.",,,,,,"Teisseyre, Pawel/0000-0002-4296-9819; Kubkowski, Mariusz/0000-0002-1453-5589",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656362700001,0
J,"Lei, YW; Hu, T; Tang, K",,,,"Lei, Yunwen; Hu, Ting; Tang, Ke",,,Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic gradient descent (SGD) has become the method of choice to tackle large-scale datasets due to its low computational cost and good practical performance. Learning rate analysis, either capacity-independent or capacity-dependent, provides a unifying viewpoint to study the computational and statistical properties of SGD, as well as the implicit regularization by tuning the number of passes. Existing capacity-independent learning rates require a nontrivial bounded subgradient assumption and a smoothness assumption to be optimal. Furthermore, existing capacity-dependent learning rates are only established for the specific least squares loss with a special structure. In this paper, we provide both optimal capacity-independent and capacity-dependent learning rates for SGD with general convex loss functions. Our results require neither bounded subgradient assumptions nor smoothness assumptions, and are stated with high probability. We achieve this improvement by a refined estimate on the norm of SGD iterates based on a careful martingale analysis and concentration inequalities on empirical processes.",,,,,"Lei, Yunwen/V-2782-2018","Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500025,0
J,"Chen, X; Du, SS; Tong, XT",,,,"Chen, Xi; Du, Simon S.; Tong, Xin T.",,,On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. (2017) presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in Zhang et al. (2017) is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds. In this paper, using intuitions from stochastic differential equations, we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. (2017) and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects. Under suitable conditions, we show that the hitting time of SGLD to first-order stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000020,0
J,"Hashimoto, Y; Ishikawa, I; Ikeda, M; Matsuo, Y; Kawahara, Y",,,,"Hashimoto, Yuka; Ishikawa, Isao; Ikeda, Masahiro; Matsuo, Yoichi; Kawahara, Yoshinobu",,,Krylov Subspace Method for Nonlinear Dynamical Systems with Random Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Operator-theoretic analysis of nonlinear dynamical systems has attracted much attention in a variety of engineering and scientific fields, endowed with practical estimation methods using data such as dynamic mode decomposition. In this paper, we address a lifted representation of nonlinear dynamical systems with random noise based on transfer operators, and develop a novel Krylov subspace method for estimating the operators using finite data, with consideration of the unboundedness of operators. For this purpose, we first consider Perron-Frobenius operators with kernel-mean embeddings for such systems. We then extend the Arnoldi method, which is the most classical type of Kryov subspace methods, so that it can be applied to the current case. Meanwhile, the Arnoldi method requires the assumption that the operator is bounded, which is not necessarily satisfied for transfer operators on nonlinear systems. We accordingly develop the shift-invert Arnoldi method for Perron-Frobenius operators to avoid this problem. Also, we describe an approach of evaluating predictive accuracy by estimated operators on the basis of the maximum mean discrepancy, which is applicable, for example, to anomaly detection in complex systems. The empirical performance of our methods is investigated using synthetic and real-world healthcare data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,172,,,,,,,,,,,,,,,WOS:000570205400001,0
J,"Kallus, N; Uehara, M",,,,"Kallus, Nathan; Uehara, Masatoshi",,,Double Reinforcement Learning for Efficient Off-Policy Evaluation in Markov Decision Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"ff-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of q-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent. We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,167,,,,,,,,,,,,,,,WOS:000570153800001,0
J,"Li, P; He, N; Milenkovic, O",,,,"Li, Pan; He, Niao; Milenkovic, Olgica",,,Quadratic Decomposable Submodular Function Minimization: Theory and Practice,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization (QDSFM), which allows to model a number of learning tasks on graphs and hypergraphs. The problem exhibits close ties to decomposable submodular function minimization (DSFM) yet is much more challenging to solve. We approach the problem via a new dual strategy and formulate an objective that can be optimized through a number of double-loop algorithms. The outer-loop uses either random coordinate descent (RCD) or alternative projection (AP) methods, for both of which we prove linear convergence rates. The inner-loop computes projections onto cones generated by base polytopes of the submodular functions via the modified min-norm-point or Frank-Wolfe algorithms. We also describe two new applications of QDSFM: hypergraph-adapted PageRank and semi-supervised learning. The proposed hypergraph-based PageRank algorithm can be used for local hypergraph partitioning and comes with provable performance guarantees. For hypergraph-adapted semi-supervised learning, we provide numerical experiments demonstrating the efficiency of our QDSFM solvers and their significant improvements on prediction accuracy when compared to state-of-the-art methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,106,,,,,,,,,,,,,,,WOS:000546626800001,0
J,"Mohamed, S; Rosca, M; Figurnov, M; Mnih, A",,,,"Mohamed, Shakir; Rosca, Mihaela; Figurnov, Michael; Mnih, Andriy",,,Monte Carlo Gradient Estimation in Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies-the pathwise, score function, and measure valued gradient estimators-exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,132,,,,,,,,,,,,,,,WOS:000556298200001,0
J,"Vandaele, R; Saeys, Y; De Bie, T",,,,"Vandaele, Robin; Saeys, Yvan; De Bie, Tijl",,,Mining Topological Structure in Graphs through Forest Representations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of inferring simplified topological substructures-which we term backbones-in metric and non-metric graphs. Intuitively, these are subgraphs with 'few' nodes, multifurcations, and cycles, that model the topology of the original graph well. We present a multistep procedure for inferring these backbones. First, we encode local (geometric) information of each vertex in the original graph by means of the boundary coefficient (BC) to identify 'core' nodes in the graph. Next, we construct a forest representation of the graph, termed an f-pine, that connects every node of the graph to a local 'core' node. The final backbone is then inferred from the f-pine through CLOF (Constrained Leaves Optimal subForest), a novel graph optimization problem we introduce in this paper. On a theoretical level, we show that CLOF is NP-hard for general graphs. However, we prove that CLOF can be efficiently solved for forest graphs, a surprising fact given that CLOF induces a nontrivial monotone submodular set function maximization problem on tree graphs. This result is the basis of our method for mining backbones in graphs through forest representation. We qualitatively and quantitatively confirm the applicability, effectiveness, and scalability of our method for discovering backbones in a variety of graph-structured data, such as social networks, earthquake locations scattered across the Earth, and high-dimensional cell trajectory data.",,,,,,"Vandaele, Robin/0000-0001-8484-3002",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,215,,,,,,,,,,,,,,,WOS:000590025400001,0
J,"Vipond, O",,,,"Vipond, Oliver",,,Multiparameter Persistence Landscapes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An important problem in the field of Topological Data Analysis is defining topological summaries which can be combined with traditional data analytic tools. In recent work Bubenik introduced the persistence landscape, a stable representation of persistence diagrams amenable to statistical analysis and machine learning tools. In this paper we generalise the persistence landscape to multiparameter persistence modules providing a stable representation of the rank invariant. We show that multiparameter landscapes are stable with respect to the interleaving distance and persistence weighted Wasserstein distance, and that the collection of multiparameter landscapes faithfully represents the rank invariant. Finally we provide example calculations and statistical tests to demonstrate a range of potential applications and how one can interpret the landscapes associated to a multiparameter module.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000013,0
J,"Campbell, T; Broderick, T",,,,"Campbell, Trevor; Broderick, Tamara",,,Automated Scalable Bayesian Inference via Hilbert Coresets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern data sets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the data set itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms-one based on importance sampling, and one based on the Frank-Wolfe algorithm-along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic data sets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,15,,,,,,,,,,,,,,,WOS:000458666500001,0
J,"Hill, SM; Oates, CJ; Blythe, DA; Mukherjee, S",,,,"Hill, Steven M.; Oates, Chris J.; Blythe, Duncan A.; Mukherjee, Sach",,,Causal Learning via Manifold Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper frames causal structure estimation as a machine learning task. The idea is to treat indicators of causal relationships between variables as 'labels' and to exploit available data on the variables of interest to provide features for the labelling task. Background scientific knowledge or any available interventional data provide labels on some causal relationships and the remainder are treated as unlabelled. To illustrate the key ideas, we develop a distance-based approach (based on bivariate histograms) within a manifold regularization framework. We present empirical results on three different biological data sets (including examples where causal effects can be verified by experimental intervention), that together demonstrate the efficacy and general nature of the approach as well as its simplicity from a user's point of view.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,127,,,,,,,,,,31992961,,,,,WOS:000487068900011,0
J,"Ruggieri, S",,,,"Ruggieri, Salvatore",,,Complete Search for Feature Selection in Decision Trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We design an exact enumeration procedure of the subsets of features that lead to all and only the distinct decision trees built by a greedy top-down decision tree induction algorithm. The procedure stores, in the worst case, a number of trees linear in the number of features. By exploiting a further pruning of the search space, we design a complete procedure for finding delta-acceptable feature subsets, which depart by at most delta from the best estimated error over any feature subset. Feature subsets with the best estimated error are called best feature subsets. Our results apply to any error estimator function, but experiments are mainly conducted under the wrapper model, in which the misclassification error over a search set is used as an estimator. The approach is also adapted to the design of a computational optimization of the sequential backward elimination heuristic, extending its applicability to large dimensional datasets. The procedures of this paper are implemented in a multi-core data parallel C++ system. We investigate experimentally the properties and limitations of the procedures on a collection of 20 benchmark datasets, showing that oversearching increases both overfitting and instability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,104,,,,,,,,,,,,,,,WOS:000476622800001,0
J,"Sondhi, A; Shojaie, A",,,,"Sondhi, Arjun; Shojaie, Ali",,,The Reduced PC-Algorithm: Improved Causal Structure Learning in Large Random Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the task of estimating a high-dimensional directed acyclic graph, given observations from a linear structural equation model with arbitrary noise distribution. By exploiting properties of common random graphs, we develop a new algorithm that requires conditioning only on small sets of variables. The proposed algorithm, which is essentially a modified version of the PC-Algorithm, offers significant gains in both computational complexity and estimation accuracy. In particular, it results in more efficient and accurate estimation in large networks containing hub nodes, which are common in biological systems. We prove the consistency of the proposed algorithm, and show that it also requires a less stringent faithfulness assumption than the PC-Algorithm. Simulations in low and high-dimensional settings are used to illustrate these findings. An application to gene expression data suggests that the proposed algorithm can identify a greater number of clinically relevant genes than current methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,164,,,,,,,,,,,,,,,WOS:000506403100004,0
J,"Achab, M; Bacry, E; Gaiffas, S; Mastromatteo, I; Muzy, JF",,,,"Achab, Massil; Bacry, Emmanuel; Gaiffas, Stephane; Mastromatteo, Iacopo; Muzy, Jean-Francois",,,Uncovering Causality from Multivariate Hawkes Integrated Cumulants,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,192,,,,,,,,,,,,,,,WOS:000435453100001,0
J,"Barbero, A; Sra, S",,,,"Barbero, Alvaro; Sra, Suvrit",,,Modular Proximal Optimization for Multidimensional Total-Variation Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study TV regularization, a widely used technique for eliciting structured sparsity. In particular, we propose efficient algorithms for computing prox-operators for l(p)-norm TV. The most important among these is l(1)-norm TV, for whose prox-operator we present a new geometric analysis which unveils a hitherto unknown connection to taut-string methods. This connection turns out to be remarkably useful as it shows how our geometry guided implementation results in efficient weighted and unweighted 1D-TV solvers, surpassing state-of-the-art methods. Our 1D-TV solvers provide the backbone for building more complex (two or higher-dimensional) TV solvers within a modular proximal optimization approach. We review the literature for an array of methods exploiting this strategy, and illustrate the benefits of our modular design through extensive suite of experiments on (i) image denoising, (ii) image deconvolution, (iii) four variants of fused-lasso, and (iv) video denoising. To underscore our claims and permit easy reproducibility, we provide all the reviewed and our new TV solvers in an easy to use multi-threaded C++, Matlab and Python library.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,56,,,,,,,,,,,,,,,WOS:000452042700001,0
J,"Carriere, M; Michel, B; Oudot, S",,,,"Carriere, Mathieu; Michel, Bertrand; Oudot, Steve",,,Statistical Analysis and Parameter Selection for Mapper,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we study the question of the statistical convergence of the 1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that the Mapper is an optimal estimator of the Reeb graph, which gives, as a byproduct, a method to automatically tune its parameters and compute confidence regions on its topological features, such as its loops and flares. This allows to circumvent the issue of testing a large grid of parameters and keeping the most stable ones in the brute-force setting, which is widely used in visualization, clustering and feature selection with the Mapper.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,12,,,,,,,,,,,,,,,WOS:000443225500001,0
J,"Gao, C",,,,"Gao, Chao",,,Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider goodness-of-fit tests with i.i.d. samples generated from a categorical distribution (p(1), ...,p(k)). For a given (q(1),...,q(k)), we test the null hypothesis whether p(j) = q(pi)(j) for some label permutation pi. The uncertainty of label permutation implies that the null hypothesis is composite instead of being singular. In this paper, we construct a testing procedure using statistics that are de fined as indefinite integrals of some symmetric polynomials. This method is aimed directly at the invariance of the problem, and avoids the need of matching the unknown labels. The asymptotic distribution of the testing statistic is shown to be chi-squared, and its power is proved to be nearly optimal under a local alternative hypothesis. Various degenerate structures of the null hypothesis are carefully analyzed in the paper. A two-sample version of the test is also studied.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,50,,,,,,,,,,,,,,,,WOS:000444403200001,0
J,"Khetan, A; Oh, S",,,,"Khetan, Ashish; Oh, Sewoong",,,Generalized Rank-Breaking: Computational and Statistical Tradeoffs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of rank aggregation, for the Plackett-Luce model, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. Further, the proposed generalized rank-breaking algorithm involves set-wise comparisons as opposed to traditional pairwise comparisons. The maximum likelihood estimate of pairwise comparisons is computed efficiently using the celebrated minorization maximization algorithm (Hunter, 2004). To compute the pseudo-maximum likelihood estimate of the set-wise comparisons, we provide a generalization of the minorization maximization algorithm and give guarantees on its convergence.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444401000001,0
J,"Kontorovich, A; Sabato, S; Urner, R",,,,"Kontorovich, Aryeh; Sabato, Sivan; Urner, Ruth",,,Active Nearest-Neighbor Learning in Metric Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARM AN N), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARM AN N is significantly lower than that of any passive learner with similar error guarantees. M AR M AN N is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.",,,,,"Kontorovich, Aryeh/X-9225-2019; Kontorovich, Aryeh/AAB-4744-2020; Sabato, Sivan/U-4730-2017","Kontorovich, Aryeh/0000-0001-8038-8671; Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,195,,,,,,,,,,,,,,,WOS:000435453900001,0
J,"Lei, YW; Shi, L; Guo, ZC",,,,"Lei, Yunwen; Shi, Lei; Guo, Zheng-Chu",,,Convergence of Unregularized Online Learning Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high-probability convergence rate for the last iterate of online gradient descent algorithms in the general convex setting. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one-step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm.",,,,,"Lei, Yunwen/V-2782-2018; Shi, Lei/P-1989-2018","Shi, Lei/0000-0002-9512-5273; Lei, Yunwen/0000-0002-5383-467X",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,171,,,,,,,,,,,,,,,WOS:000435442100001,0
J,"Morningstar, A; Melko, RG",,,,"Morningstar, Alan; Melko, Roger G.",,,Deep Learning the Ising Model Near Criticality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality.",,,,,,"Morningstar, Alan/0000-0002-2398-1804",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000433255100001,0
J,"Negahban, S; Oh, S; Thekumparampil, KK; Xu, JM",,,,"Negahban, Sahand; Oh, Sewoong; Thekumparampil, Kiran K.; Xu, Jiaming",,,Learning from Comparisons and Choices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When tracking user-specific online activities, each user's preference is revealed in the form of choices and comparisons. For example, a user's purchase history is a record of her choices, i.e. which item was chosen among a subset of offerings. A user's preferences can be observed either explicitly as in movie ratings or implicitly as in viewing times of news articles. Given such individualized ordinal data in the form of comparisons and choices, we address the problem of collaboratively learning representations of the users and the items. The learned features can be used to predict a user's preference of an unseen item to be used in recommendation systems. This also allows one to compute similarities among users and items to be used for categorization and search. Motivated by the empirical successes of the MultiNomial Logit (MNL) model in marketing and transportation, and also more recent successes in word embedding and crowdsourced image embedding, we pose this problem as learning the MNL model parameters that best explain the data. We propose a convex relaxation for learning the MNL model, and show that it is minimax optimal up to a logarithmic factor by comparing its performance to a fundamental lower bound. This characterizes the minimax sample complexity of the problem, and proves that the proposed estimator cannot be improved upon other than by a logarithmic factor. Further, the analysis identifies how the accuracy depends on the topology of sampling via the spectrum of the sampling graph. This provides a guideline for designing surveys when one can choose which items are to be compared. This is accompanied by numerical simulations on synthetic and real data sets, con firming our theoretical predictions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444406400001,0
J,"Raissi, M",,,,"Raissi, Maziar",,,Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schrodinger, and Navier-Stokes equations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,25,,,,,,,,,,,,,,,WOS:000443230100001,0
J,"Zarezade, A; De, A; Upadhyay, U; Rabiee, HR; Gomez-Rodriguez, M",,,,"Zarezade, Ali; De, Abir; Upadhyay, Utkarsh; Rabiee, Hamid R.; Gomez-Rodriguez, Manuel",,,Steering Social Activity: A Stochastic Optimal Control Point Of View,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"User engagement in online social networking depends critically on the level of social activity in the corresponding platform the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art.",,,,,"Rabiee, Hamid R./L-7866-2017; Rodriguez, Manuel Gomez/AAB-5005-2021","Rabiee, Hamid R./0000-0002-9835-4493; Gomez Rodriguez, Manuel/0000-0003-3930-1161",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,205,,,,,,,,,,,,,,,WOS:000435627700001,0
J,"Du, N; Liang, YY; Balcan, MF; Gomez-Rodriguez, M; Zha, HY; Song, L",,,,"Du, Nan; Liang, Yingyu; Balcan, Maria-Florina; Gomez-Rodriguez, Manuel; Zha, Hongyuan; Song, Le",,,Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of a matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence in a network (|V| nodes, |epsilon| edges) to an accuracy of epsilon with n=O(1/epsilon(2)) randomizations and (O) over tilde (n|epsilon|+n|V|) computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor k(a)/(2+2k) of the optimal when k(a) out of the k knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of-the-art in terms of effectiveness and scalability.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021","Gomez Rodriguez, Manuel/0000-0003-3930-1161",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,2,,,,,,,,,,,,,,,WOS:000397017600001,0
J,"Gao, ZY; Ries, C; Simon, HU; Zilles, S",,,,"Gao, Ziyuan; Ries, Christoph; Simon, Hans U.; Zilles, Sandra",,,Preference-based Teaching,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new model of teaching named preference-based teaching and a corresponding complexity parameter-the preference-based teaching dimension (PBTD)-representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well-known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over N-0 (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD.",,,,,,"Simon, Hans/0000-0002-1587-0944",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,32,,,,,,,,,,,,,,,,WOS:000399842900001,0
J,"Mandt, S; Hoffman, MD; Blei, DM",,,,"Mandt, Stephan; Hoffman, Matthew D.; Blei, David M.",,,Stochastic Gradient Descent as Approximate Bayesian Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4) We analyze stochastic-gradient MCMC algorithms. For Stochastic-Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,134,,,,,,,,,,,,,,,WOS:000424546600001,0
J,"Matthews, AGD; van der Wilk, M; Nickson, T; Fujii, K; Boukouvalas, A; Leon-Villagra, P; Ghahramani, Z; Hensman, J",,,,"Matthews, Alexander G. de G.; van der Wilk, Mark; Nickson, Tom; Fujii, Keisuke; Boukouvalas, Alexis; Leon-Villagra, Pablo; Ghahramani, Zoubin; Hensman, James",,,GPflow: A Gaussian Process Library using TensorFlow,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. 1 The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.",,,,,,"fujii, keisuke/0000-0003-0390-9984",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,6,40,,,,,,,,,,,,,,,WOS:000400521000001,0
J,"Arias-Castro, E; Mason, D; Pelletier, B",,,,"Arias-Castro, Ery; Mason, David; Pelletier, Bruno",,,On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating the gradient lines of a density, which can be used to cluster points sampled from that density, for example via the mean-shift algorithm of Fukunaga and Hostetler (1975). We prove general convergence bounds that we then specialize to kernel density estimation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,43,,,,,,,,,,,,,,,WOS:000391483300001,0
J,"Ho, QR; Yin, JM; Xing, EP",,,,"Ho, Qirong; Yin, Junming; Xing, Eric P.",,,Latent Space Inference of Internet-Scale Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The rise of Internet-scale networks, such as web graphs and social media with hundreds of millions to billions of nodes, presents new scientific opportunities, such as overlapping community detection to discover the structure of the Internet, or to analyze trends in online social behavior. However, many existing probabilistic network models are difficult or impossible to deploy at these massive scales. We propose a scalable approach for modeling and inferring latent spaces in Internet-scale networks, with an eye towards overlapping community detection as a key application. By applying a succinct representation of networks as a bag of triangular motifs, developing a parsimonious statistical model, deriving an efficient stochastic variational inference algorithm, and implementing it as a distributed cluster program via the Petuum parameter server system, we demonstrate overlapping community detection on real networks with up to 100 million nodes and 1000 communities on 5 machines in under 40 hours. Compared to other state-of-the-art probabilistic network approaches, our method is several orders of magnitude faster, with competitive or improved accuracy at overlapping community detection.",,,,,"Ho, Qirong/AAB-8152-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,78,,,,,,,,,,,,,,,WOS:000391524900001,0
J,"Khaleghi, A; Ryabko, D; Mary, J; Preux, P",,,,"Khaleghi, Azadesh; Ryabko, Daniil; Mary, Jeremie; Preux, Philippe",,,Consistent Algorithms for Clustering Time Series,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.",,,,,"PREUX, Pierre-Marie/B-8393-2014; Preux, Pierre-Marie/N-3538-2019","PREUX, Pierre-Marie/0000-0002-2171-2977; Preux, Pierre-Marie/0000-0002-2171-2977; Khaleghi, Azadeh/0000-0001-8643-5416",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,1,,,,,,,,,,,,,,,WOS:000391462800001,0
J,"Romano, S; Vinh, NX; Bailey, J; Verspoor, K",,,,"Romano, Simone; Nguyen Xuan Vinh; Bailey, James; Verspoor, Karin",,,Adjusting for Chance Clustering Comparison Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Adjusted for chance measures are widely used to compare partitions/ clusterings of the same data set. In particular, the Adjusted Rand Index (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI) based on Shannon information theory are very popular in the clustering community. Nonetheless it is an open problem as to what are the best application scenarios for each measure and guidelines in the literature for their usage are sparse, with the result that users often resort to using both. Generalized Information Theoretic (IT) measures based on the Tsallis entropy have been shown to link pair-counting and Shannon IT measures. In this paper, we aim to bridge the gap between adjustment of measures based on pair-counting and measures based on information theory. We solve the key technical challenge of analytically computing the expected value and variance of generalized IT measures. This allows us to propose adjustments of generalized IT measures, which reduce to well known adjusted clustering comparison measures as special cases. Using the theory of generalized IT measures, we are able to propose the following guidelines for using ARI and AMI as external validation indices: ARI should be used when the reference clustering has large equal sized clusters; AMI should be used when the reference clustering is unbalanced and there exist small clusters.",,,,,"Verspoor, Karin/G-6034-2016","Verspoor, Karin/0000-0002-8661-1544",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,134,,,,,,,,,,,,,,,WOS:000391657800001,0
J,"Yang, L; Lv, SG; Wang, JH",,,,"Yang, Lei; Lv, Shaogao; Wang, Junhui",,,Model-free Variable Selection in Reproducing Kernel Hilbert Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Variable selection is popular in high-dimensional data analysis to identify the truly informative variables. Many variable selection methods have been developed under various model assumptions. Whereas success has been widely reported in literature, their performances largely depend on validity of the assumed models, such as the linear or additive models. This article introduces a model-free variable selection method via learning the gradient functions. The idea is based on the equivalence between whether a variable is informative and whether its corresponding gradient function is substantially non-zero. The proposed variable selection method is then formulated in a framework of learning gradients in a flexible reproducing kernel Hilbert space. The key advantage of the proposed method is that it requires no explicit model assumption and allows for general variable effects. Its asymptotic estimation and selection consistencies are studied, which establish the convergence rate of the estimated sparse gradients and assure that the truly informative variables are correctly identified in probability. The effectiveness of the proposed method is also supported by a variety of simulated examples and two real-life examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,82,,,,,,,,,,,,,,,WOS:000391526300001,0
J,"Yang, ZR; Corander, J; Oja, E",,,,"Yang, Zhirong; Corander, Jukka; Oja, Erkki",,,Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Cluster analysis by nonnegative low-rank approximations has experienced a remarkable progress in the past decade. However, the majority of such approximation approaches are still restricted to nonnegative matrix factorization (NMF) and suffer from the following two drawbacks: 1) they are unable to produce balanced partitions for large-scale manifold data which are common in real-world clustering tasks; 2) most existing NMF-type clustering methods cannot automatically determine the number of clusters. We propose a new low-rank learning method to address these two problems, which is beyond matrix factorization. Our method approximately decomposes a sparse input similarity in a normalized way and its objective can be used to learn both cluster assignments and the number of clusters. For efficient optimization, we use a relaxed formulation based on Data-Cluster-Data random walk, which is also shown to be equivalent to low-rank factorization of the doubly-stochastically normalized cluster incidence matrix. The probabilistic cluster assignments can thus be learned with a multiplicative majorization-minimization algorithm. Experimental results show that the new method is more accurate both in terms of clustering large-scale manifold data sets and of selecting the number of clusters.",,,,,"Yang, Zhirong/E-8312-2012","Yang, Zhirong/0000-0001-8412-5684",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,187,,,,,,,,,,,,,,,WOS:000391825500001,0
J,"Zhao, J; Sun, S",,,,"Zhao, Jing; Sun, Shiliang",,,Variational Dependent Multi-output Gaussian Process Dynamical Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely different from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a flexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the variational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be flexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traffic flow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the effectiveness of our model, and encouraging results are observed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,36,121,,,,,,,,,,,,,,,WOS:000391654500001,0
J,"Nakajima, S; Tomioka, R; Sugiyama, M; Babacan, SD",,,,"Nakajima, Shinichi; Tomioka, Ryota; Sugiyama, Masashi; Babacan, S. Derin",,,Condition for Perfect Dimensionality Recovery by Variational Bayesian PCA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Having shown its good performance in many applications, variational Bayesian (VB) learning is known to be one of the best tractable approximations to Bayesian learning. However, its performance was not well understood theoretically. In this paper, we clarify the behavior of VB learning in probabilistic PCA (or fully-observed matrix factorization). More specifically, we establish a necessary and sufficient condition for perfect dimensionality (or rank) recovery in the large-scale limit when the matrix size goes to infinity. Our result theoretically guarantees the performance of VB-PCA. At the same time, it also reveals the conservative nature of VB learning it offers a low false positive rate at the expense of low sensitivity. By contrasting with an alternative dimensionality selection method, we characterize VB learning in PCA. In our analysis, we obtain bounds of the noise variance estimator, and a new and simple analytic-form solution for the other parameters, which themselves are useful for implementation of VB-PCA.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3757,3811,,,,,,,,,,,,,,,,WOS:000369888000043,0
J,"Tibshirani, RJ",,,,"Tibshirani, Ryan J.",,,A General Framework for Fast Stagewise Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount epsilon) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size epsilon goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an l(1) norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). Even when they do not match their l(1)-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the l(1) norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising,and more.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2543,2588,,,,,,,,,,,,,,,,WOS:000369888000007,0
J,"Scherrer, B; Ghavamzadeh, M; Gabillon, V; Lesner, B; Geist, M",,,,"Scherrer, Bruno; Ghavamzadeh, Mohammad; Gabillon, Victor; Lesner, Boris; Geist, Matthieu",,,Approximate Modified Policy Iteration and its Application to the Game of Tetris,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1629,1676,,,,,,,,,,,,,,,,WOS:000369887100011,0
J,"Weninger, F; Bergmann, J; Schuller, B",,,,"Weninger, Felix; Bergmann, Johannes; Schuller, Bjoern",,,Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we introduce CURRENNT, an open-source parallel implementation of deep recurrent neural networks (RNNs) supporting graphics processing units (GPUs) through NVIDIA's Computed Unified Device Architecture (CUDA). CURRENNT supports uni- and bidirectional RNNs with Long Short-Term Memory (LSTM) memory cells which overcome the vanishing gradient problem. To our knowledge, CURRENNT is the first publicly available parallel implementation of deep LSTM-RNNs. Benchmarks are given on a noisy speech recognition task from the 2013 2nd CHiME Speech Separation and Recognition Challenge, where LSTM-RNNs have been shown to deliver best performance. In the result, double digit speedups in bidirectional LSTM training are achieved with respect to a reference single-threaded CPU implementation. CURRENNT is available under the GNU General Public License from http://sourceforge.net/p/currennt.",,,,,,"Schuller, Bjorn/0000-0002-6478-8699",,,,,,,,,,,,,1532-4435,,,,,MAR,2015,16,,,,,,547,551,,,,,,,,,,,,,,,,WOS:000369886000007,0
J,"Tan, KM; London, P; Mohan, K; Lee, SI; Fazel, M; Witten, D",,,,"Tan, Kean Ming; London, Palma; Mohan, Karthik; Lee, Su-In; Fazel, Maryam; Witten, Daniela",,,Learning Graphical Models With Hubs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning a high-dimensional graphical model in which there are a few hub nodes that are densely-connected to many other nodes. Many authors have studied the use of an l(1) penalty in order to learn a sparse graph in the high-dimensional setting. However, the l(1) penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely-used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3297,3331,,,,,,,,,,,25620891,,,,,WOS:000344638800015,0
J,"Gentile, C; Orabona, F",,,,"Gentile, Claudio; Orabona, Francesco",,,On Multilabel Classification and Ranking with Bandit Feedback,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T-1/2 log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on diverse real-world multilabel data sets, often obtaining comparable performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2451,2487,,,,,,,,,,,,,,,,WOS:000344638400003,0
J,"Wand, MP",,,,"Wand, Matt P.",,,Fully Simplified Multivariate Normal Updates in Non-Conjugate Variational Message Passing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fully simplified expressions for Multivariate Normal updates in non-conjugate variational message passing approximate inference schemes are obtained. The simplicity of these expressions means that the updates can be achieved very efficiently. Since the Multivariate Normal family is the most common for approximating the joint posterior density function of a continuous parameter vector, these fully simplified updates are of great practical benefit.",,,,,"Liu, Xuanyong/J-2609-2018; Cao, Huiliang/V-1999-2017; Wand, Matt P/F-9413-2012","Cao, Huiliang/0000-0002-5209-4497; Wand, Matt P/0000-0003-2555-896X",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1351,1369,,,,,,,,,,,,,,,,WOS:000338420000005,0
J,"Zhang, YC; Duchi, JC; Wainwright, MJ",,,,"Zhang, Yuchen; Duchi, John C.; Wainwright, Martin J.",,,Communication-Efficient Algorithms for Statistical Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We analyze two communication-efficient algorithms for distributed optimization in statistical settings involving large-scale data sets. The first algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error (MSE) that decays as O(N-1 + (N/m)(-2)). Whenever m <= root N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O(N-1 + (N/m)(-3)), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O(N-1 + (N/m)(-3/2)), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efficiently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N approximate to 2.4 x 10(8) samples and d approximate to 740,000 covariates.",,,,,"Zhang, Yuchen/GYI-8858-2022","Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3321,3363,,,,,,,,,,,,,,,,WOS:000329786900004,0
J,"Gerber, S; Whitaker, R",,,,"Gerber, Samuel; Whitaker, Ross",,,Regularization-Free Principal Curve Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Principal curves and manifolds provide a framework to formulate manifold learning within a statistical context. Principal curves define the notion of a curve passing through the middle of a distribution. While the intuition is clear, the formal definition leads to some technical and practical difficulties. In particular, principal curves are saddle points of the mean-squared projection distance, which poses severe challenges for estimation and model selection. This paper demonstrates that the difficulties in model selection associated with the saddle point property of principal curves are intrinsically tied to the minimization of the mean-squared projection distance. We introduce a new objective function, facilitated through a modification of the principal curve estimation approach, for which all critical points are principal curves and minima. Thus, the new formulation removes the fundamental issue for model selection in principal curve estimation. A gradient-descent-based estimator demonstrates the effectiveness of the new formulation for controlling model complexity on numerical experiments with synthetic and real data.",,,,,"Gerber, Samuel/AAK-1278-2020","Gerber, Samuel/0000-0001-9521-3145",,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1285,1302,,,,,,,,,,,,,,,,WOS:000320709300003,0
J,"Zhao, MJ; Edakunni, N; Pocock, A; Brown, G",,,,"Zhao, Ming-Jie; Edakunni, Narayanan; Pocock, Adam; Brown, Gavin",,,"Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fano's inequality lower bounds the probability of transmission error through a communication channel. Applied to classification problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general definition of conditional entropy (including Shannon's as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano's result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classifier. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate-we derive similar optimal thresholds for F-score and BER.",,,,,,"Brown, Gavin/0000-0003-2261-9018",,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,1033,1090,,,,,,,,,,,,,,,,WOS:000318590500010,0
J,"Huang, J; Zhang, CH",,,,"Huang, Jian; Zhang, Cun-Hui",,,Estimation and Selection via Absolute Penalized Convex Minimization And Its Multistage Adaptive Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The l(1)-penalized method, or the Lasso, has emerged as an important tool for the analysis of large data sets. Many important results have been obtained for the Lasso in linear regression which have led to a deeper understanding of high-dimensional statistical problems. In this article, we consider a class of weighted l(1)-penalized estimators for convex loss functions of a general form, including the generalized linear models. We study the estimation, prediction, selection and sparsity properties of the weighted l(1)-penalized estimator in sparse, high-dimensional settings where the number of predictors p can be much larger than the sample size n. Adaptive Lasso is considered as a special case. A multistage method is developed to approximate concave regularized estimation by applying an adaptive Lasso recursively. We provide prediction and estimation oracle inequalities for single-and multi-stage estimators, a general selection consistency theorem, and an upper bound for the dimension of the Lasso estimator. Important models including the linear regression, logistic regression and log-linear models are used throughout to illustrate the applications of the general results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1839,1864,,,,,,,,,,,24348100,,,,,WOS:000307020700004,0
J,"Qin, ZW; Goldfarb, D",,,,"Qin, Zhiwei (Tony); Goldfarb, Donald",,,Structured Sparsity via Alternating Direction Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a class of sparse learning problems in high dimensional feature space regularized by a structured sparsity-inducing norm that incorporates prior knowledge of the group structure of the features. Such problems often pose a considerable challenge to optimization algorithms due to the non-smoothness and non-separability of the regularization term. In this paper, we focus on two commonly adopted sparsity-inducing regularization terms, the overlapping Group Lasso penalty l(1)/l(2)-norm and the l(1)/l(infinity)-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of regularization and their variants can be efficiently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting technique and prove that the accelerated versions of these algorithms require O(1/root epsilon) iterations to obtain an epsilon-optimal solution. We compare the performance of these algorithms against that of the alternating direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them to two real-world problems to compare the relative merits of the two norms.",,,,,"Qin, Zhiwei/F-3201-2014","Qin, Zhiwei/0000-0001-5383-4816",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1435,1468,,,,,,,,,,,,,,,,WOS:000305456600006,0
J,"Mairal, J; Jenatton, R; Obozinski, G; Bach, F",,,,"Mairal, Julien; Jenatton, Rodolphe; Obozinski, Guillaume; Bach, Francis",,,Convex and Network Flow Optimization for Structured Sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a class of learning problems regularized by a structured sparsity-inducing norm defined as the sum of l(2)- or l(infinity)-norms over groups of variables. Whereas much effort has been put in developing fast optimization techniques when the groups are disjoint or embedded in a hierarchy, we address here the case of general overlapping groups. To this end, we present two different strategies: On the one hand, we show that the proximal operator associated with a sum of l(infinity)-norms can be computed exactly in polynomial time by solving a quadratic min-cost flow problem, allowing the use of accelerated proximal gradient methods. On the other hand, we use proximal splitting techniques, and address an equivalent formulation with non-overlapping groups, but in higher dimension and with additional constraints. We propose efficient and scalable algorithms exploiting these two strategies, which are significantly faster than alternative approaches. We illustrate these methods with several problems such as CUR matrix factorization, multi-task learning of tree-structured dictionaries, background subtraction in video sequences, image denoising with wavelets, and topographic dictionary learning of natural image patches.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2681,2720,,,,,,,,,,,,,,,,WOS:000298102900005,0
J,"Kolar, M; Lafferty, J; Wasserman, L",,,,"Kolar, Mladen; Lafferty, John; Wasserman, Larry",,,Union Support Recovery in Multi-task Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We sharply characterize the performance of different penalization schemes for the problem of selecting the relevant variables in the multi-task setting. Previous work focuses on the regression problem where conditions on the design matrix complicate the analysis. A clearer and simpler picture emerges by studying the Normal means model. This model, often used in the field of statistics, is a simplified model that provides a laboratory for studying complex procedures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2415,2435,,,,,,,,,,,,,,,,WOS:000293757900012,0
J,"Ye, F; Zhang, CH",,,,"Ye, Fei; Zhang, Cun-Hui",,,Rate Minimaxity of the Lasso and Dantzig Selector for the l(q) Loss in l(r) Balls,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the estimation of regression coefficients in a high-dimensional linear model. For regression coefficients in l(r) balls, we provide lower bounds for the minimax l(q) risk and minimax quantiles of the l(q) loss for all design matrices. Under an l(0) sparsity condition on a target coefficient vector, we sharpen and unify existing oracle inequalities for the Lasso and Dantzig selector. We derive oracle inequalities for target coefficient vectors with many small elements and smaller threshold levels than the universal threshold. These oracle inequalities provide sufficient conditions on the design matrix for the rate minimaxity of the Lasso and Dantzig selector for the l(q) risk and loss in l(r) balls, 0 <= r <= 1 <= q <= infinity. By allowing q = infinity, our risk bounds imply the variable selection consistency of threshold Lasso and Dantzig selectors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3519,3540,,,,,,,,,,,,,,,,WOS:000286637200008,0
J,"Hothorn, T; Buhlmann, P; Kneib, T; Schmid, M; Hofner, B",,,,"Hothorn, Torsten; Buehlmann, Peter; Kneib, Thomas; Schmid, Matthias; Hofner, Benjamin",,,Model-based Boosting 2.0,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe version 2.0 of the R add-on package mboost. The package implements boosting for optimizing general risk functions using component-wise (penalized) least squares estimates or regression trees as base-learners for fitting generalized linear, additive and interaction models to potentially high-dimensional data.",,,,,"Hofner, Benjamin/AGN-7505-2022; Hothorn, Torsten/A-3639-2010; B√ºhlmann, Peter/A-2107-2013","Hofner, Benjamin/0000-0003-2810-3186; Hothorn, Torsten/0000-0001-8301-0471; B√ºhlmann, Peter/0000-0002-1782-6015; Kneib, Thomas/0000-0003-3390-0972; Schmid, Matthias/0000-0002-0788-0317",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2109,2113,,,,,,,,,,,,,,,,WOS:000282523300001,0
J,"Hu, T; Zhou, DX",,,,"Hu, Ting; Zhou, Ding-Xuan",,,Online Learning with Samples Drawn from Non-identical Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a Holder space. For regression with least square or insensitive loss, learning rates are given in both the RKHS norm and the L-2 norm. For classification with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassification error.",,,,,"Zhou, Ding-Xuan/B-3160-2013","Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2873,2898,,,,,,,,,,,,,,,,WOS:000273877300005,0
J,"Syed, Z; Indyk, P; Guttag, J",,,,"Syed, Zeeshan; Indyk, Piotr; Guttag, John",,,Learning Approximate Sequential Patterns for Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we first estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess significance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classification. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identified approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistics improved the running time of the search algorithm by an order of magnitude without any noticeable effect on accuracy. These results suggest that our methods may allow for an unsupervised approach to efficiently learn interesting dissimilarities between positive and negative examples that may have a functional role.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1913,1936,,,,,,,,,,,,,,,,WOS:000270825200006,0
J,"Abeel, T; Van de Peer, Y; Saeys, Y",,,,"Abeel, Thomas; Van de Peer, Yves; Saeys, Yvan",,,Java-ML: A Machine Learning Library,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/under the GNU GPL license.",,,,,"Van de Peer, Yves/D-4388-2009; Saeys, Yvan/C-1311-2009; Van de Peer, Yves/AAE-7666-2019","Van de Peer, Yves/0000-0003-4327-3730; Saeys, Yvan/0000-0002-0415-1506; Van de Peer, Yves/0000-0003-4327-3730; Abeel, Thomas/0000-0002-7205-7431",,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,931,934,,,,,,,,,,,,,,,,WOS:000270824600005,0
J,"Mannor, S; Tsitsiklis, JN; Yu, JY",,,,"Mannor, Shie; Tsitsiklis, John N.; Yu, Jia Yuan",,,Online Learning with Sample Path Constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study online learning where a decision maker interacts with Nature with the objective of maximizing her long-term average reward subject to some sample path average constraints. We define the reward-in-hindsight as the highest reward the decision maker could have achieved, while satisfying the constraints, had she known Nature's choices in advance. We show that in general the reward-in-hindsight is not attainable. The convex hull of the reward-in-hindsight function is, however, attainable. For the important case of a single constraint, the convex hull turns out to be the highest attainable function. Using a calibrated forecasting rule, we provide an explicit strategy that attains this convex hull. We also measure the performance of heuristic methods based on non-calibrated forecasters in experiments involving a CPU power management problem.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,569,590,,,,,,,,,,,,,,,,WOS:000270824500002,0
J,"Takacs, G; Pilaszy, I; Nemeth, B; Tikk, D",,,,"Takacs, Gabor; Pilaszy, Istvan; Nemeth, Bottyan; Tikk, Domonkos",,,Scalable Collaborative Filtering Approaches for Large Recommender Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netflix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efficiently. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets.",,,,,"Tak√°cs, G√°bor/A-6518-2012; Tikk, Domonkos/A-1107-2008","Takacs, Gabor/0000-0002-2525-7449",,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,623,656,,,,,,,,,,,,,,,,WOS:000270824500004,0
J,"Krause, A; McMahan, HB; Guestrin, C; Gupta, A",,,,"Krause, Andreas; McMahan, H. Brendan; Guestrin, Carlos; Gupta, Anupam",,,Robust Submodular Observation Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, one has to actively select among a set of expensive observations before making an informed decision. For example, in environmental monitoring, we want to select locations to measure in order to most effectively predict spatial phenomena. Often, we want to select observations which are robust against a number of possible objective functions. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the submodular Saturation algorithm, a simple and efficient algorithm with strong theoretical approximation guarantees for cases where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efficient algorithms. We show how our algorithm can be extended to handle complex cost functions (incorporating non-unit observation cost or communication and path costs). We also show how the algorithm can be used to near-optimally trade off expected-case (e. g., the Mean Square Prediction Error in Gaussian Process regression) and worst- case (e. g., maximum predictive variance) performance. We show that many important machine learning problems fit our robust submodular observation selection formalism, and provide extensive empirical evaluation on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms.",,,,,"Krause, Andreas/A-5888-2008","Krause, Andreas/0000-0001-7260-9673",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2761,2801,,,,,,,,,,,,,,,,WOS:000263240700006,0
J,"Bach, FR",,,,"Bach, Francis R.",,,Consistency of the group Lasso and multiple kernel learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the least-square regression problem with regularization by a block l(1)-norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the l(1)-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1179,1225,,,,,,,,,,,,,,,,WOS:000258646300008,0
J,"Bax, E; Callejas, A",,,,"Bax, Eric; Callejas, Augusto",,,An error bound based on a worst likely assignment,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,This paper introduces a new PAC transductive error bound for classification. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,859,891,,,,,,,,,,,,,,,,WOS:000258645300002,0
J,"Loog, M",,,,"Loog, Marco",,,A complete characterization of a family of solutions to a generalized fisher criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, Ye (2005) suggested yet another optimization criterion for discriminant analysis and proposed a characterization of the family of solutions to this objective. The characterization, however, merely describes a part of the full solution set, that is, it is not complete and therefore not at all a characterization. This correspondence first gives the correct characterization and afterwards compares it to Ye's.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,2121,2123,,,,,,,,,,,,,,,,WOS:000252744600005,0
J,"Li, J; Ray, S; Lindsay, BG",,,,"Li, Jia; Ray, Surajit; Lindsay, Bruce G.",,,A nonparametric statistical approach to clustering via mode identification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A new clustering approach based on mode identification is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efficiently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model fitting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Specifically, a pairwise separability measure for clusters is defined using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difficulty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/similar to jiali/hmac.",,,,,"Ray, Surajit/H-4633-2019","Ray, Surajit/0000-0003-3965-8136",,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1687,1723,,,,,,,,,,,,,,,,WOS:000252744400001,0
J,"Gadat, S; Younes, L",,,,"Gadat, Sebastien; Younes, Laurent",,,A stochastic algorithm for feature selection in pattern recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efficiency criterion, on the basis of specified classification or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efficient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of fit criterion for classifiers based on variable randomly chosen according to P. We then generate classifiers from the optimal distribution of weights learned on the training set. The method is first tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classification and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination.",,,,,"Younes, E. Laurent/A-3349-2010","Younes, Laurent/0000-0003-2017-9565",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,509,547,,,,,,,,,,,,,,,,WOS:000247002700005,0
J,"Alaiz-Rodriguez, R; Guerrero-Curieses, A; Cid-Sueiro, J",,,,"Alaiz-Rodriguez, Rocio; Guerrero-Curieses, Alicia; Cid-Sueiro, Jesus",,,Minimax regret classifier for imprecise class distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The design of a minimum risk classifier based on data usually stems from the stationarity assumption that the conditions during training and test are the same: the misclassification costs assumed during training must be in agreement with real costs, and the same statistical process must have generated both training and test data. Unfortunately, in real world applications, these assumptions may not hold. This paper deals with the problem of training a classifier when prior probabilities cannot be reliably induced from training data. Some strategies based on optimizing the worst possible case ( conventional minimax) have been proposed previously in the literature, but they may achieve a robust classification at the expense of a severe performance degradation. In this paper we propose a minimax regret ( minimax deviation) approach, that seeks to minimize the maximum deviation from the performance of the optimal risk classifier. A neural-based minimax regret classifier for general multi-class decision problems is presented. Experimental results show its robustness and the advantages in relation to other approaches.",,,,,"Guerrero-Curieses, Alicia/F-3280-2016; Cid-Sueiro, Jesus/B-8773-2008","Guerrero-Curieses, Alicia/0000-0001-7403-165X; ALAIZ-RODRIGUEZ, ROCIO/0000-0003-4164-5887; Cid-Sueiro, Jesus/0000-0002-5243-5992",,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,103,130,,,,,,,,,,,,,,,,WOS:000247002500004,0
J,"Centeno, TP; Lawrence, ND",,,,"Centeno, TP; Lawrence, ND",,,Optimising kernel parameters and regularisation coefficients for non-linear discriminant analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we consider a novel Bayesian interpretation of Fisher's discriminant analysis. We relate Rayleigh's coefficient to a noise model that minimises a cost based on the most probable class centres and that abandons the `regression to the labels' assumption used by other algorithms. Optimisation of the noise model yields a direction of discrimination equivalent to Fisher's discriminant, and with the incorporation of a prior we can apply Bayes' rule to infer the posterior distribution of the direction of discrimination. Nonetheless, we argue that an additional constraining distribution has to be included if sensible results are to be obtained. Going further, with the use of a Gaussian process prior we show the equivalence of our model to a regularised kernel Fisher's discriminant. A key advantage of our approach is the facility to determine kernel parameters and the regularisation coefficient through the optimisation of the marginal log-likelihood of the data. An added bonus of the new formulation is that it enables us to link the regularisation coefficient with the generalisation error.",,,,,,"Lawrence, Neil/0000-0001-9258-1030",,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,455,491,,,,,,,,,,,,,,,,WOS:000236331700010,0
J,"Bongard, J; Lipson, H",,,,"Bongard, J; Lipson, H",,,Active coevolutionary learning of deterministic finite automata,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes an active learning approach to the problem of grammatical inference, specifically the inference of deterministic finite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to grammatical inference in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for grammatical inference, evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase is due to the fact that the EDSM algorithm only works well for DFAs with specific balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this finding we propose a more general method for generating DFAs to be used in the development of future grammatical inference algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2005,6,,,,,,1651,1678,,,,,,,,,,,,,,,,WOS:000236330500001,0
J,"Rousu, J; Shawe-Taylor, J",,,,"Rousu, J; Shawe-Taylor, J",,,Efficient computation of gapped substring kernels on large alphabets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a sparse dynamic programming algorithm that, given two strings s and t, a gap penalty l, and an integer p, computes the value of the gap-weighted length-p subsequences kernel. The algorithm works in time O(p vertical bar M vertical bar log vertical bar t vertical bar), where M = {(i,j)vertical bar s(i) = t(j)} is the set of matches of characters in the two sequences. The algorithm is easily adapted to handle bounded length subsequences and different gap-penalty schemes, including penalizing by the total length of gaps and the number of gaps as well as incorporating character-specific match/gap penalties. The new algorithm is empirically evaluated against a full dynamic programming approach and a trie-based algorithm both on synthetic and newswire article data. Based on the experiments, the full dynamic programming approach is the fastest on short strings, and on long strings if the alphabet is small. On large alphabets, the new sparse dynamic programming algorithm is the most efficient. On medium-sized alphabets the trie-based approach is best if the maximum number of allowed gaps is strongly restricted.",,,,,"Rousu, Juho/E-8195-2012","Rousu, Juho/0000-0002-0705-4314; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1323,1344,,,,,,,,,,,,,,,,WOS:000236330100003,0
J,"Tsang, IW; Kwok, JT; Cheung, PM",,,,"Tsang, IW; Kwok, JT; Cheung, PM",,,Core vector machines: Fast SVM training on very large data sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard SVM training has O(m(3)) time and O(m(2)) space complexities, where m is the training set size. It is thus computationally infeasible on very large data sets. By observing that practical SVM implementations only approximate the optimal solution by an iterative strategy, we scale up kernel methods by exploiting such approximateness in this paper. We first show that many kernel methods can be equivalently formulated as minimum enclosing ball (MEB) problems in computational geometry. Then, by adopting an efficient approximate MEB algorithm, we obtain provably approximately optimal solutions with the idea of core sets. Our proposed Core Vector Machine (CVM) algorithm can be used with nonlinear kernels and has a time complexity that is linear in m and a space complexity that is independent of m. Experiments on large toy and real-world data sets demonstrate that the CVM is as accurate as existing SVM implementations, but is much faster and can handle much larger data sets than existing scale-up methods. For example, CVM with the Gaussian kernel produces superior results on the KDDCUP-99 intrusion detection data, which has about five million training patterns, in only 1.4 seconds on a 3.2GHz Pentium-4 PC.",,,,,"Tsang, Ivor/E-8653-2011","Tsang, Ivor/0000-0003-2211-8176; Tsang, Ivor/0000-0001-8095-4637",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,363,392,,,,,,,,,,,,,,,,WOS:000236329600001,0
J,"Hoyer, PO",,,,"Hoyer, PO",,,Non-negative matrix factorization with sparseness constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of 'sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.",,,,,"Patanaik, Amiya/G-4336-2010",,,,,,,,,,,,,,1532-4435,,,,,NOV,2004,5,,,,,,1457,1469,,,,,,,,,,,,,,,,WOS:000236328400003,0
J,"Learned-Miller, EG; Fisher, JW",,,,"Learned-Miller, EG; Fisher, JW",,,ICA using spacings estimates of entropy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new algorithm for the independent components analysis (ICA) problem based on an efficient entropy estimator. Like many previous methods, this algorithm directly minimizes the measure of departure from independence according to the estimated Kullback-Leibler divergence between the joint distribution and the product of the marginal distributions. We pair this approach with efficient entropy estimators from the statistics literature. In particular, the entropy estimator we use is consistent and exhibits rapid convergence. The algorithm based on this estimator is simple, computationally efficient, intuitively appealing, and outperforms other well known algorithms. In addition, the estimator's relative insensitivity to outliers translates into superior performance by our ICA algorithm on outlier tests. We present favorable comparisons to the Kernel ICA, FAST-ICA, JADE, and extended Infomax algorithms in extensive simulations. We also provide public domain source code for our algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1271,1295,,,,,,,,,,,,,,,,WOS:000224808300006,0
J,"Barnard, K; Duygulu, P; Forsyth, D; de Freitas, N; Blei, DM; Jordan, MI",,,,"Barnard, K; Duygulu, P; Forsyth, D; de Freitas, N; Blei, DM; Jordan, MI",,,Matching words and pictures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Machine Learning Methods for Text and Images,2001,"VANCOUVER, CANADA",,,,,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et at.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.",,,,,"Duygulu, Pinar/N-2707-2013; Jordan, Michael I/C-5253-2013","Duygulu, Pinar/0000-0002-6420-2838; Barnard, Kobus/0000-0002-8568-9518",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1107,1135,,10.1162/153244303322533214,0,,,,,,,,,,,,,WOS:000186002400005,0
J,"Kandola, J; Hofmann, T; Poggio, T; Shawe-Taylor, J",,,,"Kandola, J; Hofmann, T; Poggio, T; Shawe-Taylor, J",,,Introduction to the special issue on machine learning methods for text and images,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,"Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2003,3,6,,,,,1023,1024,,10.1162/153244303322533179,0,,,,,,,,,,,,,WOS:000186002400001,0
J,"Cannon, A; Ettinger, JM; Hush, D; Scovel, C",,,,"Cannon, A; Ettinger, JM; Hush, D; Scovel, C",,,Machine learning with data dependent hypothesis classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We extend the VC theory of statistical learning to data dependent spaces of classifiers. This theory can be viewed as a decomposition of classifier design into two components; the first component is a restriction to a data dependent hypothesis class and the second is empirical risk minimization within that class. We define a measure of complexity for data dependent hypothesis classes and provide data dependent versions of bounds on error deviance and estimation error. We also provide a structural risk minimization procedure over data dependent hierarchies and prove consistency. We use this theory to provide a framework for studying the trade-offs between performance and computational complexity in classifier design. As a consequence we obtain a new family of classifiers with dimension independent performance bounds and efficient learning procedures.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2002,2,3,,,,,335,358,,10.1162/153244302760200650,0,,,,,,,,,,,,,WOS:000178101500002,0
J,"Mangasaian, OL; Musicant, DR",,,,"Mangasaian, OL; Musicant, DR",,,Lagrangian support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An implicit Lagrangian for the dual of a simple reformulation of the standard quadratic program of a linear support vector machine is proposed. This leads to the minimization of an unconstrained differentiable convex function in a space of dimensionality equal to the number of classified points. This problem is solvable by an extremely simple linearly convergent Lagrangian support vector machine (LSVM) algorithm. LSVM requires the inversion at the outset of a single matrix of the order of the much smaller dimensionality of the original input space phis one. The full algorithm is given in this paper in 11 lines of MATLAB code without any special optimization tools such as linear or quadratic programming solvers. This LSVM code can be used as is to solve classification problems with millions of points. For example, 2 million points in 10 dimensional input space were classified by a linear surface in 82 minutes on a Pentium III 500 MHz notebook with 384 megabytes of memory (and additional swap space), and in 7 minutes on a 250 MHz UltraSPARC 11 processor with 2 gigabytes of memory. Other standard classification test problems were also solved. Nonlinear kernel classification can also be solved by LSVM. Although it does not scale up to very large problems, it can handle any positive semidefinite kernel and is guaranteed to converge. A short MATLAB code is also given for nonlinear kernels and tested on a number of problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2001,1,3,,,,,161,177,,10.1162/15324430152748218,0,,,,,,,,,,,,,WOS:000173336900001,0
J,"D'Eramo, C; Tateo, D; Bonarini, A; Restelli, M; Peters, J",,,,"D'Eramo, Carlo; Tateo, Davide; Bonarini, Andrea; Restelli, Marcello; Peters, Jan",,,MushroomRL: Simplifying Reinforcement Learning Research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"MushroomRL is an open-source Python library developed to simplify the process of implementing and running Reinforcement Learning (RL) experiments. Compared to other available libraries, MushroomRL has been created with the purpose of providing a comprehensive and flexible framework to minimize the effort in implementing and testing novel RL methodologies. The architecture of MushroomRL is built in such a way that every component of a typical RL experiment is already provided, and most of the time users can only focus on the implementation of their own algorithms. MushroomRL is accompanied by a benchmarking suite collecting experimental results of state-of-the-art deep RL algorithms, and allowing to benchmark new ones. The result is a library from which RL researchers can significantly benefit in the critical phase of the empirical analysis of their works. MushroomRL stable code, tutorials, and documentation can be found at https://github.com/MushroomRL/mushroom-r1.",,,,,"Peters, Jan/P-6027-2019","Peters, Jan/0000-0002-5266-8091; Restelli, Marcello/0000-0002-6322-1076; D'Eramo, Carlo/0000-0003-2712-118X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687300200001,0
J,"Klaise, J; Van Looveren, A; Vacanti, G; Coca, A",,,,"Klaise, Janis; Van Looveren, Arnaud; Vacanti, Giovanni; Coca, Alexandru",,,Alibi Explain: Algorithms for Explaining Machine Learning Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce Alibi Explain, an open-source Python library for explaining predictions of machine learning models (https://github.com/SeldonIO/alibi). The library features state-of-the-art explainability algorithms for classification and regression models. The algorithms cover both the model-agnostic (black-box) and model-sp ecific (white-box) setting, cater for multiple data types (tabular, text, images) and explanation scope (local and global explanations). The library exposes a unified API enabling users to work with explanations in a consistent way. Alibi adheres to best development practices featuring extensive testing of code correctness and algorithm convergence in a continuous integration environment. The library comes with extensive documentation of both usage and theoretical background of methods, and a suite of worked end-to-end use cases. Alibi aims to be a productionready toolkit with integrations into machine learning deployment platforms such as Seldon Core and KFServing, and distributed explanation capabilities using Ray.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687114000001,0
J,"Zeng, JS; Lin, SB; Yao, Y; Zhou, DX",,,,"Zeng, Jinshan; Lin, Shao-Bo; Yao, Yuan; Zhou, Ding-Xuan",,,On ADMM in Deep Learning: Convergence and Saturation-Avoidance,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we develop an alternating direction method of multipliers (ADMM) for deep neural networks training with sigmoid-type activation functions (called sigmoid-ADMM pair), mainly motivated by the gradient-free nature of ADMM in avoiding the saturation of sigmoid-type activations and the advantages of deep neural networks with sigmoid-type activations (called deep sigmoid nets) over their rectified linear unit (ReLU) counterparts (called deep ReLU nets) in terms of approximation. In particular, we prove that the approximation capability of deep sigmoid nets is not worse than that of deep ReLU nets by showing that ReLU activation function can be well approximated by deep sigmoid nets with two hidden layers and finitely many free parameters but not vice-verse. We also establish the global convergence of the proposed ADMM for the nonlinearly constrained formulation of the deep sigmoid nets training from arbitrary initial points to a KarushKuhn-Tucker (KKT) point at a rate of order O(1/k). Besides sigmoid activation, such a convergence theorem holds for a general class of smooth activations. Compared with the widely used stochastic gradient descent (SGD) algorithm for the deep ReLU nets training (called ReLU-SGD pair), the proposed sigmoid-ADMM pair is practically stable with respect to the algorithmic hyperparameters including the learning rate, initial schemes and the pro-processing of the input data. Moreover, we find that to approximate and learn simple but important functions the proposed sigmoid-ADMM pair numerically outperforms the ReLU-SGD pair.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000706836700001,0
J,"Alatur, P; Levy, KY; Krause, A",,,,"Alatur, Pragnya; Levy, Kfir Y.; Krause, Andreas",,,Multi-Player Bandits: The Adversarial Case,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength fluctuations. In this work, we design the first multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).",,,,,,"Krause, Andreas/0000-0001-7260-9673; Levy, Kfir Yehuda/0000-0003-1236-2626",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000029,0
J,"Alexandrov, A; Benidis, K; Bohlke-Schneider, M; Flunkert, V; Gasthaus, J; Januschowski, T; Maddix, DC; Rangapuram, S; Salinas, D; Schulz, J; Stella, L; Turkmen, AC; Wang, YY",,,,"Alexandrov, Alexander; Benidis, Konstantinos; Bohlke-Schneider, Michael; Flunkert, Valentin; Gasthaus, Jan; Januschowski, Tim; Maddix, Danielle C.; Rangapuram, Syama; Salinas, David; Schulz, Jasper; Stella, Lorenzo; Tuerkmen, Ali Caner; Wang, Yuyang",,,GluonTS: Probabilistic and Neural Time Series Modeling in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,116,,,,,,,,,,,,,,,WOS:000546631300001,0
J,"Arik, SO; Pfister, T",,,,"Arik, Sercan O.; Pfister, Tomas",,,ProtoAttend: Attention-Based Prototypical Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. Protoattend yields superior results in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a sample-based interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All these can be achieved with minimal additional test time and a practically viable training time computational cost.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,210,,,,,,,,,,,,,,,WOS:000590012200001,0
J,"Bertsimas, D; Paskov, I",,,,"Bertsimas, Dimitris; Paskov, Ivan",,,Stable Regression: On the Power of Optimization over Randomization in Training Regression Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate and ultimately suggest remediation to the widely held belief that the best way to train regression models is via random assignment of our data to training and validation sets. In particular, we show that taking a robust optimization approach, and optimally selecting such training and validation sets, leads to models that not only perform significantly better than their randomly constructed counterparts in terms of prediction error, but more importantly, are considerably more stable in the sense that the standard deviation of the resulting predictions, as well as of the model coefficients, is greatly reduced. Moreover, we show that this optimization approach to training is far more effective at recovering the true support of a given data set, i.e., correctly identifying important features while simultaneously excluding spurious ones. We further compare the robust optimization approach to cross validation and find that optimization continues to have a performance edge albeit smaller. Finally, we show that this optimization approach to training is equivalent to building models that are robust to all subpopulations in the data, and thus in particular are robust to the hardest subpopulation, which leads to interesting domain specific interpretations through the use of optimal classification trees. The proposed robust optimization algorithm is efficient and scales training to essentially any desired size.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,230,,,,,,,,,,,,,,,WOS:000605749600001,0
J,"Blondel, M; Martins, AFT; Niculae, V",,,,"Blondel, Mathieu; Martins, Andre F. T.; Niculae, Vlad",,,Learning with Fenchel-Young Losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Over the past decades, numerous loss functions have been been proposed for a variety of supervised learning tasks, including regression, classification, ranking, and more generally structured prediction. Understanding the core principles and theoretical properties underpinning these losses is key to choose the right loss for the right problem, as well as to create new losses which combine their strengths. In this paper, we introduce Fenchel-Young losses, a generic way to construct a convex loss function for a regularized prediction function. We provide an in-depth study of their properties in a very broad setting, covering all the aforementioned supervised learning tasks, and revealing new connections between sparsity, generalized entropies, and separation margins. We show that Fenchel-Young losses unify many well-known loss functions and allow to create useful new ones easily. Finally, we derive efficient predictive and training algorithms, making Fenchel-Young losses appealing both in theory and practice.",,,,,,"Torres Martins, Andre Filipe/0000-0001-8282-625X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000010,0
J,"Cai, ZR; Li, RZ; Zhu, LP",,,,"Cai, Zhanrui; Li, Runze; Zhu, Liping",,,Online Sufficient Dimension Reduction Through Sliced Inverse Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sliced inverse regression is an effective paradigm that achieves the goal of dimension reduction through replacing high dimensional covariates with a small number of linear combinations. It does not impose parametric assumptions on the dependence structure. More importantly, such a reduction of dimension is sufficient in that it does not cause loss of information. In this paper, we adapt the stationary sliced inverse regression to cope with the rapidly changing environments. We propose to implement sliced inverse regression in an online fashion. This online learner consists of two steps. In the first step we construct an online estimate for the kernel matrix; in the second step we propose two online algorithms, one is motivated by the perturbation method and the other is originated from the gradient descent optimization, to perform online singular value decomposition. The theoretical properties of this online learner are established. We demonstrate the numerical performance of this online learner through simulations and real world applications. All numerical studies confirm that this online learner performs as well as the batch learner.",,,,,"Li, Runze/C-5444-2013; Li, Runze/ABF-1320-2020; Li, Runze/HCH-8063-2022","Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202; Li, Runze/0000-0002-0154-2202; Cai, Zhanrui/0000-0003-3359-9446",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300010,0
J,"Martin, R; Tang, YQ",,,,"Martin, Ryan; Tang, Yiqi",,,Empirical Priors for Prediction in Sparse High-dimensional Linear Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein-von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,144,,,,,,,,,,,,,,,WOS:000558805500001,0
J,"Niazadeh, R; Roughgarden, T; Wang, JR",,,,"Niazadeh, Rad; Roughgarden, Tim; Wang, Joshua R.",,,Optimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular Maximization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we study the fundamental problems of maximizing a continuous non-monotone submodular function over the hypercube, both with and without coordinate-wise concavity. This family of optimization problems has several applications in machine learning, economics, and communication systems. Our main result is the first 1/2-approximation algorithm for continuous submodular function maximization; this approximation factor of 1/2 is the best possible for algorithms that only query the objective function at polynomially many points. For the special case of DR-submodular maximization, i.e. when the submodular function is also coordinate-wise concave along all coordinates, we provide a different 1/2-approximation algorithm that runs in quasi-linear time. Both these results improve upon prior work (Bian et al., 2017a,b; Soma and Yoshida, 2017). Our first algorithm uses novel ideas such as reducing the guaranteed approximation problem to analyzing a zero-sum game for each coordinate, and incorporates the geometry of this zero-sum game to fix the value at this coordinate. Our second algorithm exploits coordinate-wise concavity to identify a monotone equilibrium condition sufficient for getting the required approximation guarantee, and hunts for the equilibrium point using binary search. We further run experiments to verify the performance of our proposed algorithms in related machine learning applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,125,,,,,,,,,,,,,,,WOS:000556293700001,0
J,"Pei, YR; Manukian, H; Di Ventra, M",,,,"Pei, Yan Ru; Manukian, Haik; Di Ventra, Massimiliano",,,Generating Weighted MAX-2-SAT Instances with Frustrated Loops: an RBM Case Study,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many optimization problems can be cast into the maximum satisfiability (MAX-SAT) form, and many solvers have been developed for tackling such problems. To evaluate a MAX-SAT solver, it is convenient to generate hard MAX-SAT instances with known solutions. Here, we propose a method of generating weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by the quantum annealing community. We extend the algorithm for instances of general bipartite couplings, with the associated optimization problem being the minimization of the restricted Boltzmann machine (RBM) energy over the nodal values, which is useful for effectively pre-training the RBM. The hardness of the generated instances can be tuned through a central parameter known as the frustration index. Two versions of the algorithm are presented: the random- and structured-loop algorithms. For the random-loop algorithm, we provide a thorough theoretical and empirical analysis on its mathematical properties from the perspective of frustration, and observe empirically a double phase transition behavior in the hardness scaling behavior driven by the frustration index. For the structured-loop algorithm, we show that it offers an improvement in hardness over the random-loop algorithm in the regime of high loop density, with the variation of hardness tunable through the concentration of frustrated weights.",,,,,"Di Ventra, Massimiliano/E-1667-2011; Pei, Yan Ru/R-7675-2017","Di Ventra, Massimiliano/0000-0001-9416-189X; Pei, Yan Ru/0000-0002-6999-3691",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,159,,,,,,,,,,,,,,,WOS:000570149200001,0
J,"Wen, ZY; Liu, HF; Shi, JS; Li, QB; He, BS; Chen, J",,,,"Wen, Zeyi; Liu, Hanfeng; Shi, Jiashuai; Li, Qinbin; He, Bingsheng; Chen, Jian",,,ThunderGBM: Fast GBDTs and Random Forests on GPUs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gradient Boosting Decision Trees (GBDTs) and Random Forests (RFs) have been used in many real-world applications. They are often a standard recipe for building state-of-the-art solutions to machine learning and data mining problems. However, training and prediction are very expensive computationally for large and high dimensional problems. This article presents an efficient and open source software toolkit called ThunderGBM which exploits the high-performance Graphics Processing Units (GPUs) for GBDTs and RFs. ThunderGBM supports classification, regression and ranking, and can run on single or multiple GPUs of a machine. Our experimental results show that ThunderGBM outperforms the existing libraries while producing similar models, and can handle high dimensional problems where existing GPU-based libraries fail.",,,,,,"Li, Qinbin/0000-0002-6539-6443; He, Bingsheng/0000-0001-8618-4581; Wen, Zeyi/0000-0003-3370-6053",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,108,,,,,,,,,,,,,,,WOS:000546627900001,0
J,"Williamson, SA; Zhang, MMY; Damien, P",,,,"Williamson, Sinead A.; Zhang, Michael Minyi; Damien, Paul",,,A New Class of Time Dependent Latent Factor Models with Applications,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process - a probability measure on the space of random, unbounded binary matrices - finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000002,0
J,"Chen, YC; Ye, YY; Wang, MD",,,,"Chen, Yichen; Ye, Yinyu; Wang, Mengdi",,,Approximation Hardness for A Class of Sparse Optimization Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider three typical optimization problems with a convex loss function and a nonconvex sparse penalty or constraint. For the sparse penalized problem, we prove that fi nding an O (n(c1)d(c2))-optimal solution to an n x d problem is strongly NP-hard for any c(1), c(2) epsilon [0; 1) such that c(1) + c(2) < 1. For two constrained versions of the sparse optimization problem, we show that it is intractable to approximately compute a solution path associated with increasing values of some tuning parameter. The hardness results apply to a broad class of loss functions and sparse penalties. They suggest that one cannot even approximately solve these three problems in polynomial time, unless P = NP.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,38,,,,,,,,,,,,,,,WOS:000464271900001,0
J,"Shi, CC; Lu, WB; Song, R",,,,"Shi, Chengchun; Lu, Wenbin; Song, Rui",,,Determining the Number of Latent Factors in Statistical Multi-Relational Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Statistical relational learning is primarily concerned with learning and inferring relationships between entities in large-scale knowledge graphs. Nickel et al. (2011) proposed a RESCAL tensor factorization model for statistical relational learning, which achieves better or at least comparable results on common benchmark data sets when compared to other state-of-the-art methods. Given a positive integer s, RESCAL computes an s-dimensional latent vector for each entity. The latent factors can be further used for solving relational learning tasks, such as collective classification, collective entity resolution and link-based clustering. The focus of this paper is to determine the number of latent factors in the RESCAL model. Due to the structure of the RESCAL model, its log-likelihood function is not concave. As a result, the corresponding maximum likelihood estimators (MLEs) may not be consistent. Nonetheless, we design a specific pseudometric, prove the consistency of the MLEs under this pseudometric and establish its rate of convergence. Based on these results, we propose a general class of information criteria and prove their model selection consistencies when the number of relations is either bounded or diverges at a proper rate of the number of entities. Simulations and real data examples show that our proposed information criteria have good finite sample properties.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,23,,,,,,,,,,31983896,,,,,WOS:000458668200001,0
J,"Teixeira, LV; Assuncao, RM; Loschi, RH",,,,"Teixeira, Leonardo V.; Assuncao, Renato M.; Loschi, Rosangela H.",,,Bayesian Space-Time Partitioning by Sampling and Pruning Spanning Trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A typical problem in spatial data analysis is regionalization or spatially constrained clustering, which consists of aggregating small geographical areas into larger regions. A major challenge when partitioning a map is the huge number of possible partitions that compose the search space. This is compounded if we are partitioning spatio-temporal data rather than purely spatial data. We introduce a spatio-temporal product partition model that deals with the regionalization problem in a probabilistic way. Random spanning trees are used as a tool to tackle the problem of searching the space of possible partitions making feasible this exploration. Based on this framework, we propose an efficient Gibbs sampler algorithm to sample from the posterior distribution of the parameters, specially the random partition. The proposed Gibbs sampler scheme carries out a random walk on the space of the spanning trees and the partitions induced by deleting tree edges. In the purely spatial situation, we compare our proposed model with other state-of-art regionalization techniques to partition maps using simulated and real social and health data. To illustrate how the temporal component is handled by the algorithm and to show how the spatial clusters vary along the time we presented an application using human development index data. The analysis shows that our proposed model is better than state-of-art alternatives. Another appealing feature of the method is that the prior distribution for the partition is interpretable with a trivial coin flipping mechanism allowing its easy elicitation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,85,,,,,,,,,,,,,,,WOS:000470906600001,0
J,"Zhou, DR; Xu, P; Gu, QQ",,,,"Zhou, Dongruo; Xu, Pan; Gu, Quanquan",,,Stochastic Variance-Reduced Cubic Regularization Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex optimization. At the core of SVRC is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specifically designed for cubic regularization method. For a nonconvex function with n component functions, we show that our algorithm is guaranteed to converge to an (epsilon, root epsilon)-approximate local minimum within (O) over tilde (n(4/5)/epsilon(3/2))(1) second-order oracle calls, which outperforms the state-of-the-art cubic regularization algorithms including subsampled cubic regularization. To further reduce the sample complexity of Hessian matrix computation in cubic regularization based methods, we also propose a sample efficient stochastic variance-reduced cubic regularization (Lite-SVRC) algorithm for finding the local minimum more efficiently. Lite-SVRC converges to an (epsilon, root epsilon)-approximate local minimum within (O) over tilde (n(2/3)/epsilon(3/2)) Hessian sample complexity, which is faster than all existing cubic regularization based methods. Numerical experiments with different nonconvex optimization problems conducted on real datasets validate our theoretical results for both SVRC and Lite-SVRC.",,,,,"Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022; Zhou, Dongruo/GYJ-3503-2022","Xu, Pan/0000-0002-2559-8622; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,134,,,,,,,,,,,,,,,WOS:000487068900018,0
J,"Athreya, A; Fishkind, DE; Tang, M; Priebe, CE; Park, Y; Vogelstein, JT; Levin, K; Lyzinski, V; Qin, YC; Sussman, DL",,,,"Athreya, Avanti; Fishkind, Donniell E.; Minh Tang; Priebe, Carey E.; Park, Youngser; Vogelstein, Joshua T.; Levin, Keith; Lyzinski, Vince; Qin, Yichen; Sussman, Daniel L.",,,Statistical Inference on Random Dot Product Graphs: a Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference.",,,,,"Vogelstein, Joshua/AAG-5489-2019","Vogelstein, Joshua/0000-0003-2487-6237; Park, Youngser/0000-0002-3978-5533; Tang, Minh/0000-0003-1420-7187",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000438191400001,0
J,"Chow, Y; Ghavamzadeh, M; Janson, L; Pavone, M",,,,"Chow, Yinlam; Ghavamzadeh, Mohammad; Janson, Lucas; Pavone, Marco",,,Risk-Constrained Reinforcement Learning with Percentile Risk Criteria,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.",,,,,,"Pavone, Marco/0000-0002-0206-4337",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,167,,,,,,,,,,,,,,,WOS:000433256300001,0
J,"Mucke, N; Blanchard, G",,,,"Muecke, Nicole; Blanchard, Gilles",,,Parallelizing Spectrally Regularized Kernel Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an reproducing kernel Hilbert space (RKHS) framework. The data set of size n is partitioned into m = O (n(alpha)), alpha < 1/2, disjoint subsamples. On each subsample, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, L-2-boosting and spectral cut-off) is applied. The regression function f is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if m grows sufficiently slowly (corresponding to an upper bound for alpha) as n -> infinity, depending on the smoothness assumptions on f and the intrinsic dimensionality. In spirit, the analysis relies on a classical bias/stochastic error analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444402000001,0
J,"Schreiber, J",,,,"Schreiber, Jacob",,,pomegranate: Fast and Flexible Probabilistic Modeling in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with-or outperform-other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code. The code is available at https://github.com/jmschrei/pomegranate",,,,,"Schreiber, Jacob/GWC-3164-2022",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,164,,,,,,,,,,,,,,,WOS:000433255500001,0
J,"Yu, FX; Bhaskara, A; Kumar, S; Gong, Y; Chang, SF",,,,"Yu, Felix X.; Bhaskara, Aditya; Kumar, Sanjiv; Gong, Yunchao; Chang, Shih-Fu",,,On Binary Embedding using Circulant Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining k-bit binary codes from d-dimensional data, our method improves the time complexity from O(dk) to O(dlog d), and the space complexity from O (dk) to O (d).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,2018,,,,,,,,,,,,,,,WOS:000431707800001,0
J,"Yuan, XT; Li, P; Zhang, T",,,,"Yuan, Xiao-Tong; Li, Ping; Zhang, Tong",,,Gradient Hard Thresholding Pursuit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this article, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard-thresholding step with or without debiasing. We analyze the parameter estimation and sparsity recovery performance of the proposed method. Extensive numerical results confirm our theoretical predictions and demonstrate the superiority of our method to the state-of-the-art greedy selection methods in sparse linear regression, sparse logistic regression and sparse precision matrix estimation problems.(1)",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,166,,,,,,,,,,,,,,,WOS:000433255900001,0
J,"Farajtabar, M; Wang, YC; Gomez-Rodriguez, M; Li, S; Zha, HY; Song, L",,,,"Farajtabar, Mehrdad; Wang, Yichen; Gomez-Rodriguez, Manuel; Li, Shuang; Zha, Hongyuan; Song, Le",,,COEVOLVE : A Joint Point Process Model for Information Diffusion and Network Evolution,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, Coevolve, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks such as Twitter. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021","Gomez Rodriguez, Manuel/0000-0003-3930-1161",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,49,,,,,,,,,,,,,,,WOS:000405961700001,0
J,"Kumar, KSS; Bach, F",,,,"Kumar, K. S. Sesh; Bach, Francis",,,Active-set Methods for Submodular Minimization Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the submodular function minimization (SFM) and the quadratic minimization problems regularized by the Lovasz extension of the submodular function. These optimization problems are intimately related; for example, min-cut problems and total variation denoising problems, where the cut function is submodular and its Lovasz extension is given by the associated total variation. When a quadratic loss is regularized by the total variation of a cut function, it thus becomes a total variation denoising problem and we use the same terminology in this paper for general submodular functions. We propose a new active-set algorithm for total variation denoising with the assumption of an oracle that solves the corresponding SFM problem. This can be seen as local descent algorithm over ordered partitions with explicit convergence guarantees. It is more flexible than the existing algorithms with the ability for warm-restarts using the solution of a closely related problem. Further, we also consider the case when a submodular function can be decomposed into the sum of two submodular functions F-1 and F-2 and assume SFM oracles for these two functions. We propose a new active-set algorithm for total variation denoising (and hence SFM by thresholding the solution at zero). This algorithm also performs local descent over ordered partitions and its ability to warm start considerably improves the performance of the algorithm. In the experiments, we compare the performance of the proposed algorithms with state-of-the-art algorithms, showing that it reduces the calls to SFM oracles.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000424546200001,0
J,"Chen, YD; Xu, JM",,,,"Chen, Yudong; Xu, Jiaming",,,Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider two closely related problems: planted clustering and submatrix localization. In the planted clustering problem, a random graph is generated based on an underlying cluster structure of the nodes; the task is to recover these clusters given the graph. The submatrix localization problem concerns locating hidden submatrices with elevated means inside a large real-valued random matrix. Of particular interest is the setting where the number of clusters/submatrices is allowed to grow unbounded with the problem size. These formulations cover several classical models such as planted clique, planted densest subgraph, planted partition, planted coloring, and the stochastic block model, which are widely used for studying community detection, graph clustering and bi-clustering. For both problems, we show that the space of the model parameters (cluster/submatrix size, edge probabilities and the mean of the submatrices) can be partitioned into four disjoint regions corresponding to decreasing statistical and computational complexities: (1) the impossible regime, where all algorithms fail; (2) the hard regime, where the computationally expensive Maximum Likelihood Estimator (MLE) succeeds; (3) the easy regime, where the polynomial-time convexified MLE succeeds; (4) the simple regime, where a local counting/thresholding procedure succeeds. Moreover, we show that each of these algorithms provably fails in the harder regimes. Our results establish the minimax recovery limits, which are tight up to universal constants and hold even with a growing number of clusters/submatrices, and provide order-wise stronger performance guarantees for polynomial-time algorithms than previously known. Our study demonstrates the tradeoffs between statistical and computational considerations, and suggests that the minimax limits may not be achievable by polynomial-time algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,27,,,,,,,,,,,,,,,WOS:000391477100001,0
J,"Giscard, PL; Choo, Z; Thwaite, SJ; Jaksch, D",,,,"Giscard, P. -L.; Choo, Z.; Thwaite, S. J.; Jaksch, D.",,,Exact Inference on Gaussian Graphical Models of Arbitrary Topology using Path-Sums,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present the path-sum formulation for exact statistical inference of marginals on Gaussian graphical models of arbitrary topology. The path-sum formulation gives the covariance between each pair of variables as a branched continued fraction of finite depth and breadth. Our method originates from the closed-form resummation of in finite families of terms of the walk-sum representation of the covariance matrix. We prove that the path-sum formulation always exists for models whose covariance matrix is positive de finite: i.e. it is valid for both walk-summable and non-walk-summable graphical models of arbitrary topology. We show that for graphical models on trees the path-sum formulation is equivalent to Gaussian belief propagation. We also recover, as a corollary, an existing result that uses determinants to calculate the covariance matrix. We show that the path-sum formulation formulation is valid for arbitrary partitions of the inverse covariance matrix. We give detailed examples demonstrating our results.",,,,,"Jaksch, Dieter/F-1964-2010",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,71,,,,,,,,,,,,,,,WOS:000391523500001,0
J,"Gonen, A; Rosenbaum, D; Eldar, YC; Shalev-Shwartz, S",,,,"Gonen, Alon; Rosenbaum, Dan; Eldar, Yonina C.; Shalev-Shwartz, Shai",,,Subspace Learning with Partial Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of subspace learning is to find a k-dimensional subspace of R-d, such that the expected squared distance between instance vectors and the subspace is as small as possible. In this paper we study subspace learning in a partial information setting, in which the learner can only observe r <= d attributes from each instance vector. We propose several efficient algorithms for this task, and analyze their sample complexity.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,52,,,,,,,,,,,,,,,WOS:000391487100001,0
J,"Josse, J; Wager, S",,,,"Josse, Julie; Wager, Stefan",,,Bootstrap-Based Regularization for Low-Rank Matrix Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a flexible framework for low-rank matrix estimation that allows us to transform noise models into regularization schemes via a simple bootstrap algorithm. Effectively, our procedure seeks an autoencoding basis for the observed matrix that is stable with respect to the specified noise model; we call the resulting procedure a stable autoencoder. In the simplest case, with an isotropic noise model, our method is equivalent to a classical singular value shrinkage estimator. For non-isotropic noise models-e.g., Poisson noise-the method does not reduce to singular value shrinkage, and instead yields new estimators that perform well in experiments. Moreover, by iterating our stable autoencoding scheme, we can automatically generate low-rank estimates without specifying the target rank as a tuning parameter.",,,,,,"josse, julie/0000-0001-9547-891X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,124,,,,,,,,,,,,,,,WOS:000391654800001,0
J,"Kaufmann, E; Cappe, O; Garivier, A",,,,"Kaufmann, Emilie; Cappe, Olivier; Garivier, Aurelien",,,On the Complexity of Best-Arm Identification in Multi-Armed Bandit Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The stochastic multi-armed bandit model is a simple abstraction that has proven useful in many different contexts in statistics and machine learning. Whereas the achievable limit in terms of regret minimization is now well known, our aim is to contribute to a better understanding of the performance in terms of identifying the m best arms. We introduce generic notions of complexity for the two dominant frameworks considered in the literature: fixed-budget and fixed-confidence settings. In the fixed-confidence setting, we provide the first known distribution-dependent lower bound on the complexity that involves information-theoretic quantities and holds when m >= 1 under general assumptions. In the specific case of two armed-bandits, we derive refined lower bounds in both the fixed-confidence and fixed-budget settings, along with matching algorithms for Gaussian and Bernoulli bandit models. These results show in particular that the complexity of the fixed-budget setting may be smaller than the complexity of the fixed-confidence setting, contradicting the familiar behavior observed when testing fully specified alternatives. In addition, we also provide improved sequential stopping rules that have guaranteed error probabilities and shorter average running times. The proofs rely on two technical results that are of independent interest : a deviation lemma for self-normalized sums (Lemma 7) and a novel change of measure inequality for bandit models (Lemma 1).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,1,,,,,,,,,,,,,,,WOS:000391461800001,0
J,"Wainberg, M; Alipanahi, B; Frey, BJ",,,,"Wainberg, Michael; Alipanahi, Babak; Frey, Brendan J.",,,Are Random Forests Truly the Best Classifiers?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The JMLR study Do we need hundreds of classifiers to solve real world classification problems? benchmarks 179 classifiers in 17 families on 121 data sets from the UCI repository and claims that the random forest is clearly the best family of classifier. In this response, we show that the study's results are biased by the lack of a held-out test set and the exclusion of trials with errors. Further, the study's own statistical tests indicate that random forests do not have significantly higher percent accuracy than support vector machines and neural networks, calling into question the conclusion that random forests are the best classifiers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,110,,,,,,,,,,,,,,,WOS:000391547500001,0
J,"Wang, YX; Sharpnack, J; Smola, AJ; Tibshirani, RJ",,,,"Wang, Yu-Xiang; Sharpnack, James; Smola, Alexander J.; Tibshirani, Ryan J.",,,Trend Filtering on Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a family of adaptive estimators on graphs, based on penalizing the '1 norm of discrete graph differences. This generalizes the idea of trend filtering (Kim et al., 2009; Tibshirani, 2014), used for univariate nonparametric regression, to graphs. Analogous to the univariate case, graph trend filtering exhibits a level of local adaptivity unmatched by the usual l(2)-based graph smoothers. It is also defined by a convex minimization problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We demonstrate the merits of graph trend filtering through both examples and theory.",,,,,,"Wang, Yu-Xiang/0000-0002-6403-212X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,105,,,,,,,,,,28066157,,,,,WOS:000391543800001,0
J,"Zhou, MY; Cong, YL; Chen, B",,,,"Zhou, Mingyuan; Cong, Yulai; Chen, Bo",,,Augmentable Gamma Belief Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,44,163,,,,,,,,,,,,,,,WOS:000391668700001,0
J,"Statnikov, A; Ma, SS; Henaff, M; Lytkin, N; Efstathiadis, E; Peskin, ER; Aliferis, CF",,,,"Statnikov, Alexander; Ma, Sisi; Henaff, Mikael; Lytkin, Nikita; Efstathiadis, Efstratios; Peskin, Eric R.; Aliferis, Constantin F.",,,Ultra-Scalable and Efficient Methods for Hybrid Observational and Experimental Local Causal Pathway Discovery,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Discovery of causal relations from data is a fundamental objective of several scientific disciplines. Most causal discovery algorithms that use observational data can infer causality only up to a statistical equivalency class, thus leaving many causal relations undetermined. In general, complete identification of causal relations requires experimentation to augment discoveries from observational data. This has led to the recent development of several methods for active learning of causal networks that utilize both observational and experimental data in order to discover causal networks. In this work, we focus on the problem of discovering local causal pathways that contain only direct causes and direct effects of the target variable of interest and propose new discovery methods that aim to minimize the number of required experiments, relax common sufficient discovery assumptions in order to increase discovery accuracy, and scale to high-dimensional data with thousands of variables. We conduct a comprehensive evaluation of new and existing methods with data of dimensionality up to 1,000,000 variables. We use both artificially simulated networks and in-silico gene transcriptional networks that model the characteristics of real gene expression data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3219,3267,,,,,,,,,,,,,,,,WOS:000369888000029,0
J,"Lyzinski, V; Fishkind, DE; Priebe, CE",,,,"Lyzinski, Vince; Fishkind, Donniell E.; Priebe, Carey E.",,,Seeded Graph Matching for Correlated Erdos-Renyi Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph matching is an important problem in machine learning and pattern recognition. Herein, we present theoretical and practical results on the consistency of graph matching for estimating a latent alignment function between the vertex sets of two graphs, as well as subsequent algorithmic implications when the latent alignment is partially observed. In the correlated Erdos-Renyi graph setting, we prove that graph matching provides a strongly consistent estimate of the latent alignment in the presence of even modest correlation. We then investigate a tractable, restricted-focus version of graph matching, which is only concerned with adjacency involving vertices in a partial observation of the latent alignment; we prove that a logarithmic number of vertices whose alignment is known is sufficient for this restricted-focus version of graph matching to yield a strongly consistent estimate of the latent alignment of the remaining vertices. We show how Frank-Wolfe methodology for approximate graph matching, when there is a partially observed latent alignment, inherently incorporates this restricted-focus graph matching. Lastly, we illustrate the relationship between seeded graph matching and restricted-focus graph matching by means of an illuminating example from human connectomics.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3513,3540,,,,,,,,,,,,,,,,WOS:000353126200006,0
J,"Waegeman, W; Dembczynski, K; Jachnik, A; Cheng, WW; Hullermeier, E",,,,"Waegeman, Willem; Dembczynski, Krzysztof; Jachnik, Arkadiusz; Cheng, Weiwei; Huellermeier, Eyke",,,On the Bayes-Optimality of F-Measure Maximizers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The F-measure, which has originally been introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this measure is a statistically and computationally challenging problem, since no closed-form solution exists. Adopting a decision-theoretic perspective, this article provides a formal and experimental analysis of different approaches for maximizing the F-measure. We start with a Bayes-risk analysis of related loss functions, such as Hamming loss and subset zero-one loss, showing that optimizing such losses as a surrogate of the F-measure leads to a high worst-case regret. Subsequently, we perform a similar type of analysis for F-measure maximizing algorithms, showing that such algorithms are approximate, while relying on additional assumptions regarding the statistical distribution of the binary response variables. Furthermore, we present a new algorithm which is not only computationally efficient but also Bayes-optimal, regardless of the underlying distribution. To this end, the algorithm requires only a quadratic (with respect to the number of binary responses) number of parameters of the joint distribution. We illustrate the practical performance of all analyzed methods by means of experiments with multi-label classification problems.",,,,,"Dembczy≈Ñski, Krzysztof/L-9197-2014","Dembczy≈Ñski, Krzysztof/0000-0001-7477-6758",,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3333,3388,,,,,,,,,,,,,,,,WOS:000353126200001,0
J,"Nguyen-Dinh, LV; Calatroni, A; Troster, G",,,,"Nguyen-Dinh, Long-Van; Calatroni, Alberto; Troester, Gerhard",,,Robust Online Gesture Recognition with Crowdsourced Annotations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Crowdsourcing is a promising way to reduce the effort of collecting annotations for training gesture recognition systems. Crowdsourced annotations suffer from noise such as mislabeling, or inaccurate identification of start and end time of gesture instances. In this paper we present SegmentedLCSS and WarpingLCSS, two template-matching methods offering robustness when trained with noisy crowdsourced annotations to spot gestures from wearable motion sensors. The methods quantize signals into strings of characters and then apply variations of the longest common subsequence algorithm (LCSS) to spot gestures. We compare the noise robustness of our methods against baselines which use dynamic time warping (DTW) and support vector machines (SVM). The experiments are performed on data sets with various gesture classes (10-17 classes) recorded from accelerometers on arms, with both real and synthetic crowdsourced annotations. WarpingLCSS has similar or better performance than baselines in absence of noisy annotations. In presence of 60% mislabeled instances, WarpingLCSS outperformed SVM by 22% Fl-score and outperformed DTWbased methods by 36% Fl-score on average. SegmentedLCSS yields similar performance as WarpingLCSS, however it performs one order of magnitude slower. Additionally, we show to use our methods to filter out the noise in the crowdsourced annotation before training a traditional classifier. The filtering increases the performance of SVM by 20% Fl-score and of DTW-based methods by 8% Fl-score on average in the noisy real crowdsourced annotations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3187,3220,,,,,,,,,,,,,,,,WOS:000344638800012,0
J,"Slivkins, A",,,,"Slivkins, Aleksandrs",,,Contextual Bandits with Similarity Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a multi-armed bandit (MAB) problem, an online algorithm makes a sequence of choices. In each round it chooses from a time-invariant set of alternatives and receives the payoff associated with this alternative. While the case of small strategy sets is by now well-understood, a lot of recent work has focused on MAB problems with exponentially or infinitely large strategy sets, where one needs to assume extra structure in order to make the problem tractable. In particular, recent literature considered information on similarity between arms. We consider similarity information in the setting of contextual bandits, a natural extension of the basic MAB problem where before each round an algorithm is given the context a hint about the payoffs in this round. Contextual bandits are directly motivated by placing advertisements on web pages, one of the crucial problems in sponsored search. A particularly simple way to represent similarity information in the contextual bandit setting is via a similarity distance between the context-arm pairs which bounds from above the difference between the respective expected payoffs. Prior work on contextual bandits with similarity uses uniform partitions of the similarity space, so that each context-arm pair is approximated by the closest pair in the partition. Algorithms based on uniform partitions disregard the structure of the payoffs and the context arrivals, which is potentially wasteful. We present algorithms that are based on adaptive partitions, and take advantage of benign payoffs and context arrivals without sacrificing the worst-case performance. The central idea is to maintain a finer partition in high-payoff regions of the similarity space and in popular regions of the context space. Our results apply to several other settings, e.g., MAB with constrained temporal change (Slivkins and Upfal, 2008) and sleeping bandits (Kleinberg et al., 2008a).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2014,15,,,,,,2533,2568,,,,,,,,,,,,,,,,WOS:000344638400006,0
J,"Huang, X; Shi, L; Suykens, JAK",,,,"Huang, Xiaolin; Shi, Lei; Suykens, Johan A. K.",,,Ramp Loss Linear Programming Support Vector Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The ramp loss is a robust but non-convex loss for classification. Compared with other non-convex losses, a local minimum of the ramp loss can be effectively found. The effectiveness of local search comes from the piecewise linearity of the ramp loss. Motivated by the fact that the l(1)-penalty is piecewise linear as well, the l(1)-penalty is applied for the ramp loss, resulting in a ramp loss linear programming support vector machine (rampLPSVM). The proposed ramp-LPSVM is a piecewise linear minimization problem and the related optimization techniques are applicable. Moreover, the l1-penalty can enhance the sparsity. In this paper, the corresponding misclassification error and convergence behavior are discussed. Generally, the ramp loss is a truncated hinge loss. Therefore ramp-LPSVM possesses some similar properties as hinge loss SVMs. A local minimization algorithm and a global search strategy are discussed. The good optimization capability of the proposed algorithms makes ramp-LPSVM perform well in numerical experiments: the result of rampLPSVM is more robust than that of hinge SVMs and is sparser than that of ramp-SVM, which consists of the l(1)-Ilye-penalty and the ramp loss.",,,,,"Shi, Lei/P-1989-2018; Suykens, Johan A.K./C-9781-2014","Shi, Lei/0000-0002-9512-5273; Suykens, Johan A.K./0000-0002-8846-6352; Huang, Xiaolin/0000-0003-4285-6520",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2185,2211,,,,,,,,,,,,,,,,WOS:000344638300010,0
J,"Srivastava, N; Hinton, G; Krizhevsky, A; Sutskever, I; Salakhutdinov, R",,,,"Srivastava, Nitish; Hinton, Geoffrey; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan",,,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,1929,1958,,,,,,,,,,,,,,,,WOS:000344638300002,0
J,"Dubout, C; Fleuret, F",,,,"Dubout, Charles; Fleuret, Francois",,,Adaptive Sampling for Large Scale Boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classical boosting algorithms, such as AdaBoost, build a strong classifier without concern for the computational cost. Some applications, in particular in computer vision, may involve millions of training examples and very large feature spaces. In such contexts, the training time of off-the-shelf boosting algorithms may become prohibitive. Several methods exist to accelerate training, typically either by sampling the features or the examples used to training the weak learners. Even if some of these methods provide a guaranteed speed improvement, they offer no insurance of being more efficient than any other, given the same amount of time. The contributions of this paper are twofold: (1) a strategy to better deal with the increasingly common case where features come from multiple sources (for example, color, shape, texture, etc., in the case of images) and therefore can be partitioned into meaningful subsets; (2) new algorithms which balance at every boosting iteration the number of weak learners and the number of training examples to look at in order to maximize the expected loss reduction. Experiments in image classification and object recognition on four standard computer vision data sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1431,1453,,,,,,,,,,,,,,,,WOS:000338420000007,0
J,"Janzamin, M; Anandkumar, A",,,,"Janzamin, Majid; Anandkumar, Animashree",,,High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains. We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. We characterize sufficient conditions for identifiability of the two models, viz., Markov and independence models. We propose an efficient decomposition method based on a modification of the popular l(1)-penalized maximum-likelihood estimator (l(1)-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = Omega(d(2) log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1549,1591,,,,,,,,,,,,,,,,WOS:000338420000012,0
J,"Wade, S; Dunson, DB; Petrone, S; Trippa, L",,,,"Wade, Sara; Dunson, David B.; Petrone, Sonia; Trippa, Lorenzo",,,Improving Prediction from Dirichlet Process Mixtures via Enrichment,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Flexible covariate-dependent density estimation can be achieved by modelling the joint density of the response and covariates as a Dirichlet process mixture. An appealing aspect of this approach is that computations are relatively easy. In this paper, we examine the predictive performance of these models with an increasing number of covariates. Even for a moderate number of covariates, we find that the likelihood for x tends to dominate the posterior of the latent random partition, degrading the predictive performance of the model. To overcome this, we suggest using a different nonparametric prior, namely an enriched Dirichlet process. Our proposal maintains a simple allocation rule, so that computations remain relatively simple. Advantages are shown through both predictive equations and examples, including an application to diagnosis Alzheimer's disease.",,,,,,"Wade, Sara/0000-0002-6547-5555",,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1041,1071,,,,,,,,,,,,,,,,WOS:000335458100007,0
J,"Zhang, T; Lerman, G",,,,"Zhang, Teng; Lerman, Gilad",,,A Novel M-Estimator for Robust PCA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate robust inverse sample covariance by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy.",,,,,,"Lerman, Gilad/0000-0003-4624-3115",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,749,808,,,,,,,,,,,,,,,,WOS:000335457700012,0
J,"Harris, N; Drton, M",,,,"Harris, Naftali; Drton, Mathias",,,PC Algorithm for Nonparanormal Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the 'Rank PC' algorithm works as well as the 'Pearson PC' algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations.",,,,,,"Drton, Mathias/0000-0001-5614-3025",,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3365,3383,,,,,,,,,,,,,,,,WOS:000329786900005,0
J,"Martinez, A; Du, SC",,,,"Martinez, Aleix; Du, Shichuan",,,A Model of the Perception of Facial Expressions of Emotion by Humans: Research Overview and Perspectives,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In cognitive science and neuroscience, there have been two leading models describing how humans perceive and classify facial expressions of emotion-the continuous and the categorical model. The continuous model defines each facial expression of emotion as a feature vector in a face space. This model explains, for example, how expressions of emotion can be seen at different intensities. In contrast, the categorical model consists of C classifiers, each tuned to a specific emotion category. This model explains, among other findings, why the images in a morphing sequence between a happy and a surprise face are perceived as either happy or surprise but not something in between. While the continuous model has a more difficult time justifying this latter finding, the categorical model is not as good when it comes to explaining how expressions are recognized at different intensities or modes. Most importantly, both models have problems explaining how one can recognize combinations of emotion categories such as happily surprised versus angrily surprised versus surprise. To resolve these issues, in the past several years, we have worked on a revised model that justifies the results reported in the cognitive science and neuroscience literature. This model consists of C distinct continuous spaces. Multiple (compound) emotion categories can be recognized by linearly combining these C face spaces. The dimensions of these spaces are shown to be mostly configural. According to this model, the major task for the classification of facial expressions of emotion is precise, detailed detection of facial landmarks rather than recognition. We provide an overview of the literature justifying the model, show how the resulting model can be employed to build algorithms for the recognition of facial expression of emotion, and propose research directions in machine learning and computer vision researchers to keep pushing the state of the art in these areas. We also discuss how the model can aid in studies of human perception, social interactions and disorders.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1589,1608,,,,,,,,,,,23950695,,,,,WOS:000305456600008,0
J,"Park, C; Huang, JZ; Ding, Y",,,,"Park, Chiwoo; Huang, Jianhua Z.; Ding, Yu",,,GPLP: A Local and Parallel Computation Toolbox for Gaussian Process Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the Getting-started style documentation for the local and parallel computation toolbox for Gaussian process regression (GPLP), an open source software package written in Matlab (but also compatible with Octave). The working environment and the usage of the software package will be presented in this paper.",,,,,"Park, Chiwoo/ABA-4876-2021","Park, Chiwoo/0000-0002-2463-8901",,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,775,779,,,,,,,,,,,,,,,,WOS:000303772100010,0
J,"Gutmann, MU; Hyvarinen, A",,,,"Gutmann, Michael U.; Hyvarinen, Aapo",,,"Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,307,361,,,,,,,,,,,,,,,,WOS:000303046000004,0
J,"Bigot, J; Biscay, RJ; Loubes, JM; Muniz-Alvarez, L",,,,"Bigot, Jeremie; Biscay, Rolando J.; Loubes, Jean-Michel; Muniz-Alvarez, Lilian",,,Group Lasso Estimation of High-dimensional Covariance Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the Group Lasso estimator of the covariance matrix of a stochastic process corrupted by an additive noise. We propose to estimate the covariance matrix in a high-dimensional setting under the assumption that the process has a sparse representation in a large dictionary of basis functions. Using a matrix regression model, we propose a new methodology for high-dimensional covariance matrix estimation based on empirical contrast regularization by a group Lasso penalty. Using such a penalty, the method selects a sparse set of basis functions in the dictionary used to approximate the process, leading to an approximation of the covariance matrix into a low dimensional space. Consistency of the estimator is studied in Frobenius and operator norms and an application to sparse PCA is proposed.",,,,,"Loubes, Jean-Michel/B-7663-2019","Loubes, Jean-Michel/0000-0002-1252-2960",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3187,3225,,,,,,,,,,,,,,,,WOS:000298103700005,0
J,"Wu, JX; Tan, WC; Rehg, JM",,,,"Wu, Jianxin; Tan, Wei-Chian; Rehg, James M.",,,Efficient and Effective Visual Codebook Generation Using Additive Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Common visual codebook generation methods used in a bag of visual words model, for example, k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that with histogram features, the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks. In this paper, we demonstrate that HIK can be used in an unsupervised manner to significantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as the standard k-means. The HIK codebooks have consistently higher recognition accuracy over k-means codebooks by 2-4% in several benchmark object and scene recognition data sets. The algorithm is also generalized to arbitrary additive kernels. Its speed is thousands of times faster than a naive implementation of the kernel k-means algorithm. In addition, we propose a one-class SVM formulation to create more effective visual code words. Finally, we show that the standard k-median clustering method can be used for visual codebook generation and can act as a compromise between the HIK / additive kernel and the k-means approaches.",,,,,"Wu, Jianxin/A-3700-2011; Rehg, James/AAM-6888-2020; Wu, Jianxin/B-8539-2012","Rehg, James/0000-0003-1793-5462; ",,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3097,3118,,,,,,,,,,,,,,,,WOS:000298103700002,0
J,"Van Belle, V; Pelckmans, K; Suykens, JAK; Van Huffel, S",,,,"Van Belle, Vanya; Pelckmans, Kristiaan; Suykens, Johan A. K.; Van Huffel, Sabine",,,Learning Transformation Models for Ranking and Survival Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP. The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the 'margin' is for Support Vector Machines for classification. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O(n) constraints and O(n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O(n(2)) constraints or unknowns. We specify the MINLIP method for three different cases: the first one concerns the preference learning problem. Secondly it is specified how to adapt the method to ordinal regression with a finite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets.",,,,,"Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan A.K./C-9781-2014","Suykens, Johan A.K./0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,819,862,,,,,,,,,,,,,,,,WOS:000289635000004,0
J,"Mazumder, R; Hastie, T; Tibshirani, R",,,,"Mazumder, Rahul; Hastie, Trevor; Tibshirani, Robert",,,Spectral Regularization Algorithms for Learning Large Incomplete Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We use convex relaxation techniques to provide a sequence of regularized low-rank solutions for large-scale matrix completion problems. Using the nuclear norm as a regularizer, we provide a simple and very efficient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm. Our algorithm SOFT-IMPUTE iteratively replaces the missing elements with those obtained from a soft-thresholded SVD. With warm starts this allows us to efficiently compute an entire regularization path of solutions on a grid of values of the regularization parameter. The computationally intensive part of our algorithm is in computing a low-rank SVD of a dense matrix. Exploiting the problem structure, we show that the task can be performed with a complexity of order linear in the matrix dimensions. Our semidefinite-programming algorithm is readily scalable to large matrices; for example SOFT-IMPUTE takes a few hours to compute low-rank approximations of a 10(6) x 10(6) incomplete matrix with 107 observed entries, and fits a rank-95 approximation to the full Netflix training set in 3.3 hours. Our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state-of-the-art techniques.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2287,2322,,,,,,,,,,,21552465,,,,,WOS:000282523300010,0
J,"Ralaivola, L; Szafranski, M; Stempfel, G",,,,"Ralaivola, Liva; Szafranski, Marie; Stempfel, Guillaume",,,Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and Stationary beta-Mixing Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"PAC-Bayes bounds are among the most accurate generalization bounds for classifiers learned from independently and identically distributed (IID) data, and it is particularly so for margin classifiers: there have been recent contributions showing how practical these bounds can be either to perform model selection (Ambroladze et al., 2007) or even to directly guide the learning of linear classifiers (Germain et al., 2009). However, there are many practical situations where the training data show some dependencies and where the traditional IID assumption does not hold. Stating generalization bounds for such frameworks is therefore of the utmost interest, both from theoretical and practical standpoints. In this work, we propose the first-to the best of our knowledge-PAC-Bayes generalization bounds for classifiers trained on data exhibiting interdependencies. The approach undertaken to establish our results is based on the decomposition of a so-called dependency graph that encodes the dependencies within the data, in sets of independent data, thanks to graph fractional covers. Our bounds are very general, since being able to find an upper bound on the fractional chromatic number of the dependency graph is sufficient to get new PAC-Bayes bounds for specific settings. We show how our results can be used to derive bounds for ranking statistics (such as AUC) and classifiers trained on data distributed according to a stationary beta-mixing process. In the way, we show how our approach seamlessly allows us to deal with U-processes. As a side note, we also provide a PAC-Bayes generalization bound for classifiers learned on data from stationary phi-mixing distributions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2010,11,,,,,,1927,1956,,,,,,,,,,,,,,,,WOS:000282523000001,0
J,"Lazaro-Gredilla, M; Quinonero-Candela, J; Rasmussen, CE; Figueiras-Vidal, AR",,,,"Lazaro-Gredilla, Miguel; Quinonero-Candela, Joaquin; Rasmussen, Carl Edward; Figueiras-Vidal, Anibal R.",,,Sparse Spectrum Gaussian Process Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.",,,,,"Figueiras-Vidal, Anibal R./AAA-1995-2019","FIGUEIRAS, ANIBAL RAMON/0000-0001-7068-9884; Rasmussen, Carl Edward/0000-0001-8899-7850",,,,,,,,,,,,,1532-4435,,,,,JUN,2010,11,,,,,,1865,1881,,,,,,,,,,,,,,,,WOS:000282522400004,0
J,"Leskovec, J; Chakrabarti, D; Kleinberg, J; Faloutsos, C; Ghahramani, Z",,,,"Leskovec, Jure; Chakrabarti, Deepayan; Kleinberg, Jon; Faloutsos, Christos; Ghahramani, Zoubin",,,Kronecker Graphs: An Approach to Modeling Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"How can we generate realistic networks? In addition, how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties? Real networks exhibit a long list of surprising properties: Heavy tails for the in-and out-degree distribution, heavy tails for the eigenvalues and eigenvectors, small diameters, and densification and shrinking diameters over time. Current network models and generators either fail to match several of the above properties, are complicated to analyze mathematically, or both. Here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties. Our main idea here is to use a non-standard matrix operation, the Kronecker product, to generate graphs which we refer to as Kronecker graphs. First, we show that Kronecker graphs naturally obey common network properties. In fact, we rigorously prove that they do so. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KRONFIT, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super-exponential time. In contrast, KRONFIT takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on a wide range of large real and synthetic networks show that KRONFIT finds accurate parameters that very well mimic the properties of target networks. In fact, using just four parameters we can accurately model several aspects of global network structure. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null-models, anonymization, extrapolations, and graph summarization.",,,,,,"Faloutsos, Christos/0000-0003-2996-9790; Chakrabarti, Deepayan/0000-0002-3863-4928; Leskovec, Jure/0000-0002-5411-923X",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,985,1042,,,,,,,,,,,,,,,,WOS:000277186500021,0
J,"Mordohai, P; Medioni, G",,,,"Mordohai, Philippos; Medioni, Gerard",,,"Dimensionality Estimation, Manifold Learning and Function Approximation using Tensor Voting",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We address instance-based learning from a perceptual organization standpoint and present methods for dimensionality estimation, manifold learning and function approximation. Under our approach, manifolds in high-dimensional spaces are inferred by estimating geometric relationships among the input instances. Unlike conventional manifold learning, we do not perform dimensionality reduction, but instead perform all operations in the original input space. For this purpose we employ a novel formulation of tensor voting, which allows an N-D implementation. Tensor voting is a perceptual organization framework that has mostly been applied to computer vision problems. Analyzing the estimated local structure at the inputs, we are able to obtain reliable dimensionality estimates at each instance, instead of a global estimate for the entire data set. Moreover, these local dimensionality and structure estimates enable us to measure geodesic distances and perform nonlinear interpolation for data sets with varying density, outliers, perturbation and intersections, that cannot be handled by state-of-the-art methods. Quantitative results on the estimation of local manifold structure using ground truth data are presented. In addition, we compare our approach with several leading methods for manifold learning at the task of measuring geodesic distances. Finally, we show competitive function approximation results on real data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,411,450,,,,,,,,,,,,,,,,WOS:000277186400012,0
J,"del Coz, JJ; Diez, J; Bahamonde, A",,,,"Jose del Coz, Juan; Diez, Jorge; Bahamonde, Antonio",,,Learning Nondeterministic Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nondeterministic classifiers are defined as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classifiers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classifiers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Naive Bayes. The data sets considered comprise both UCI multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classifiers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions.",,,,,"D√≠ez, Jorge/G-7806-2015; del Coz, Juan/K-7952-2014","D√≠ez, Jorge/0000-0002-1314-2441; del Coz, Juan/0000-0002-4288-3839",,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2273,2293,,,,,,,,,,,,,,,,WOS:000272346400004,0
J,"Lin, SW; Sturmfels, B; Xu, ZQ",,,,"Lin, Shaowei; Sturmfels, Bernd; Xu, Zhiqiang",,,Marginal Likelihood Integrals for Mixtures of Independence Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inference in Bayesian statistics involves the evaluation of marginal likelihood integrals. We present algebraic algorithms for computing such integrals exactly for discrete data of small sample size. Our methods apply to both uniform priors and Dirichlet priors. The underlying statistical models are mixtures of independent distributions, or, in geometric language, secant varieties of Segre-Veronese varieties.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1611,1631,,,,,,,,,,,,,,,,WOS:000270825000011,0
J,"Klanke, S; Vijayakumar, S; Schaal, S",,,,"Klanke, Stefan; Vijayakumar, Sethu; Schaal, Stefan",,,A library for locally weighted projection regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,623,626,,,,,,,,,,,,,,,,WOS:000256642100003,0
J,"Lebanon, G; Mao, Y; Dillon, J",,,,"Lebanon, Guy; Mao, Yi; Dillon, Joshua",,,The locally weighted bag of words framework for document representation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2405,2441,,,,,,,,,,,,,,,,WOS:000252744800007,0
J,"Cardoso, JS; da Costa, JFP",,,,"Cardoso, Jaime S.; da Costa, Joaquim F. Pinto",,,Learning to classify ordinal data: The data replication method,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Classification of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm specifically intended for classification problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classifier are also provided. An experimental study with artificial and real data sets, including an application to gene expression analysis, verifies the usefulness of the proposed approach.",,,,,"Da Costa, Joaquim F/B-6720-2011; Cardoso, Jaime S/I-3286-2013","Da Costa, Joaquim F/0000-0002-3991-2715; Cardoso, Jaime S/0000-0002-3760-2473",,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1393,1429,,,,,,,,,,,,,,,,WOS:000249353700002,0
J,"Kalisch, M; Buhlmann, P",,,,"Kalisch, Markus; Buehlmann, Peter",,,Estimating high-dimensional directed acyclic graphs with the PC-algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the PC-algorithm ( Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph ( DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes ( variables), and it has the attractive property to automatically achieve high computational efficiency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O ( n(a)) for any 0 < a < infinity. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,613,636,,,,,,,,,,,,,,,,WOS:000247002700008,0
J,"Keerthi, SS; Chapelle, O; DeCoste, D",,,,"Keerthi, S. Sathiya; Chapelle, Olivier; DeCoste, Dennis",,,Building support vector machines with reduced classifier complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines (SVMs), though accurate, are not preferred in applications requiring great classification speed, due to the number of support vectors being large. To overcome this problem we devise a primal method with the following properties: (1) it decouples the idea of basis functions from the concept of support vectors; (2) it greedily finds a set of kernel basis functions of a specified maximum size (d(max)) to approximate the SVM primal cost function well; (3) it is efficient and roughly scales as O(nd(max)(2)) where n is the number of training examples; and, (4) the number of basis functions it requires to achieve an accuracy close to the SVM accuracy is usually far less than the number of SVM support vectors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1493,1515,,,,,,,,,,,,,,,,WOS:000245388800014,0
J,"Schmitt, M; Martignon, L",,,,"Schmitt, M; Martignon, L",,,On the complexity of learning lexicographic strategies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-the-best searches for a sufficiently good ordering of cues (or features) in a task where objects are to be compared lexicographically. We investigate the computational complexity of finding optimal cue permutations for lexicographic strategies and prove that the problem is NP-complete. It follows that no efficient (that is, polynomial-time) algorithm computes optimal solutions, unless P = NP. We further analyze the complexity of approximating optimal cue permutations for lexicographic strategies. We show that there is no efficient algorithm that approximates the optimum to within any constant factor, unless P = NP. The results have implications for the complexity of learning lexicographic strategies from examples. They show that learning them in polynomial time within the model of agnostic probably approximately correct (PAC) learning is impossible, unless RP = NP. We further consider greedy approaches for building lexicographic strategies and determine upper and lower bounds for the performance ratio of simple algorithms. Moreover, we present a greedy algorithm that performs provably better than take-the-best. Tight bounds on the sample complexity for learning lexicographic strategies are also given in this article.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,55,83,,,,,,,,,,,,,,,,WOS:000236331400003,0
J,"Fan, RE; Chen, PH; Lin, CJ",,,,"Fan, RE; Chen, PH; Lin, CJ",,,Working set selection using second order information for training support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Working set selection is an important step in decomposition methods for training support vector machines (SVMs). This paper develops a new technique for working set selection in SMO-type decomposition methods. It uses second order information to achieve fast convergence. Theoretical properties such as linear convergence are established. Experiments demonstrate that the proposed method is faster than existing selection methods using first order information.,,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,1889,1918,,,,,,,,,,,,,,,,WOS:000236331100001,0
J,"Fleuret, F",,,,"Fleuret, F",,,Fast binary feature selection with conditional mutual information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose in this paper a very fast feature selection technique based on conditional mutual information. By picking features which maximize their mutual information with the class to predict conditional to any feature already picked, it ensures the selection of features which are both individually informative and two-by-two weakly dependant. We show that this feature selection method outperforms other classical algorithms, and that a naive Bayesian classifier built with features selected that way achieves error rates similar to those of state-of-the-art methods such as boosting or SVMs. The implementation we propose selects 50 features among 40,000, based on a training set of 500 examples in a tenth of a second on a standard 1Ghz PC.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2004,5,,,,,,1531,1555,,,,,,,,,,,,,,,,WOS:000236328400005,0
J,"Dash, D; Cooper, GF",,,,"Dash, D; Cooper, GF",,,Model averaging for prediction with discrete Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper(1) we consider the problem of performing Bayesian model-averaging over a class of discrete Bayesian network structures consistent with a partial ordering and with bounded in-degree k. We show that for N nodes this class contains in the worst-case at least Omega(((N/2)(k))(N/2))distinct network structures, and yet model averaging over these structures can be performed using O(((N)(k)) (.) N) operations. Furthermore we show that there exists a single Bayesian network that defines a joint distribution over the variables that is equivalent to model averaging over these structures. Although constructing this network is computationally prohibitive, we show that it can be approximated by a tractable network, allowing approximate model-averaged probability calculations to be performed in O( N) time. Our result also leads to an exact and linear-time solution to the problem of averaging over the 2(N) possible feature sets in a naive Bayes model, providing an exact Bayesian solution to the troublesome feature-selection problem for naive Bayes classifiers. We demonstrate the utility of these techniques in the context of supervised classification, showing empirically that model averaging consistently beats other generative Bayesian-network-based models, even when the generating model is not guaranteed to be a member of the class being averaged over. We characterize the performance over several parameters on simulated and real-world data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2004,5,,,,,,1177,1203,,,,,,,,,,,,,,,,WOS:000236328100005,0
J,"Freund, Y; Iyer, R; Schapire, RE; Singer, Y",,,,"Freund, Y; Iyer, R; Schapire, RE; Singer, Y",,,An efficient boosting algorithm for combining preferences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the collaborative-filtering problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,933,969,,10.1162/1532443041827916,0,,,,,,,,,,,,,WOS:000231002600002,0
J,"Malzahn, D; Opper, M",,,,"Malzahn, D; Opper, M",,,An approximate analytical approach to resampling averages,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Using a novel reformulation, we develop a framework to compute approximate resampling data averages analytically. The method avoids multiple retraining of statistical models on the samples. Our approach uses a combination of the replica trick of statistical physics and the TAP approach for approximate Bayesian inference. We demonstrate our approach on regression with Gaussian processes. A comparison with averages obtained by Monte-Carlo sampling shows that our method achieves good accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,1151,1173,,10.1162/1532443041827899,0,,,,,,,,,,,,,WOS:000231002600008,0
J,"Dy, JG; Brodley, CE",,,,"Dy, JG; Brodley, CE",,,Feature selection for unsupervised learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,845,889,,,,,,,,,,,,,,,,WOS:000236328000001,0
J,"Baldi, P; Pollastri, G",,,,"Baldi, P; Pollastri, G",,,The principled design of large-scale recursive neural network architectures-DAG-RNNs and the protein structure prediction problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a general methodology for the design of large-scale recursive neural network architectures (DAG-RNNs) which comprises three fundamental steps: (1) representation of a given domain using suitable directed acyclic graphs (DAGs) to connect visible and hidden node variables; (2) parameterization of the relationship between each variable and its parent variables by feedforward neural networks; and (3) application of weight-sharing within appropriate subsets of DAG connections to capture stationarity and control model complexity. Here we use these principles to derive several specific classes of DAG-RNN architectures based on lattices, trees, and other structured graphs. These architectures can process a wide range of data structures with variable sizes and dimensions. While the overall resulting models remain probabilistic, the internal deterministic dynamics allows efficient propagation of information, as well as training by gradient descent, in order to tackle large-scale problems. These methods are used here to derive state-of-the-art predictors for protein structural features such as secondary structure (1D) and both fine- and coarse-grained contact maps (2D). Extensions, relationships to graphical models, and implications for the design of neural architectures are briefly discussed. The protein prediction servers are available over the Web at: www.igb.uci.edu/tools.htm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,575,602,,10.1162/153244304773936054,0,,,,,,,,,,,,,WOS:000221345700007,0
J,"Castelo, R; Kocka, T",,,,"Castelo, R; Kocka, T",,,On inclusion-driven learning of Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Two or more Bayesian network structures are Markov equivalent when the corresponding acyclic digraphs encode the same set of conditional independencies. Therefore, the search space of Bayesian network structures may be organized in equivalence classes, where each of them represents a different set of conditional independencies. The collection of sets of conditional independencies obeys a partial order, the so-called inclusion order. This paper discusses in depth the role that the inclusion order plays in learning the structure of Bayesian networks. In particular, this role involves the way a learning algorithm traverses the search space. We introduce a condition for traversal operators, the inclusion boundary condition, which, when it is satisfied, guarantees that the search strategy can avoid local maxima. This is proved under the assumptions that the data is sampled from a probability distribution which is faithful to an acyclic digraph, and the length of the sample is unbounded. The previous discussion leads to the design of a new traversal operator and two new learning algorithms in the context of heuristic search and the Markov Chain Monte Carlo method. We carry out a set of experiments with synthetic and real-world data that show empirically the benefit of striving for the inclusion order when learning Bayesian networks from data.",,,,,"Gasull, Martina/J-4076-2019; Castelo, Robert/A-4679-2010","Castelo, Robert/0000-0003-2229-4508",,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,527,574,,10.1162/153244304773936045,0,,,,,,,,,,,,,WOS:000221345700006,0
J,"Srinivasan, A; King, RD; Bain, ME",,,,"Srinivasan, A; King, RD; Bain, ME",,,An empirical study of the use of relevance information in inductive logic programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Knowledge-Data Fusion,"JUN, 2000","Stanford Univ, Stanford, CA",,Stanford Univ,,,"Inductive Logic Programming (ILP) systems construct models for data using domain-specific background information. When using these systems, it is typically assumed that sufficient human expertise is at hand to rule out irrelevant background information. Such irrelevant information can, and typically does, hinder an ILP system's search for good models. Here, we provide evidence that if expertise is available that can provide a partial-ordering on sets of background predicates in terms of relevance to the analysis task, then this can be used to good effect by an ILP system. In particular, using data from biochemical domains, we investigate an incremental strategy of including sets of predicates in decreasing order of relevance. Results obtained suggest that: (a) the incremental approach identifies, in substantially less time, a model that is comparable in predictive accuracy to that obtained with all background information in place; and (b) the incremental approach using the relevance ordering performs better than one that does not (that is, one that adds sets of predicates randomly). For a practitioner concerned with use of ILP, the implication of these findings are twofold: (1) when not all background information can be used at once (either due to limitations of the ILP system, or the nature of the domain) expert assessment of the relevance of background predicates can assist substantially in the construction of good models; and (2) good first-cut results can be obtained quickly by a simple exclusion of information known to be less relevant.",,,,,,"King, Ross/0000-0001-7208-4387",,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,369,383,,10.1162/153244304773633861,0,,,,,,,,,,,,,WOS:000221043900006,0
J,"Lavrac, N; Kavsek, B; Flach, P; Todorovski, L",,,,"Lavrac, N; Kavsek, B; Flach, P; Todorovski, L",,,Subgroup discovery with CN2-SD,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates how to adapt standard classification rule learning approaches to subgroup discovery. The goal of subgroup discovery is to find rules describing subsets of the population that are sufficiently large and statistically unusual. The paper presents a subgroup discovery algorithm, CN2-SD, developed by modifying parts of the CN2 classification rule learner: its covering algorithm, search heuristic, probabilistic classification of instances, and evaluation measures. Experimental evaluation of CN2-SD on 23 UCI data sets shows substantial reduction of the number of induced rules, increased rule coverage and rule significance, as well as slight improvements in terms of the area under ROC curve, when compared with the CN2 algorithm. Application of CN2-SD to a large traffic accident data set confirms these findings.",,,,,,"Kavsek, Branko/0000-0002-9092-1633; Flach, Peter/0000-0001-6857-5810",,,,,,,,,,,,,1532-4435,,,,,FEB,2004,5,,,,,,153,188,,,,,,,,,,,,,,,,WOS:000236327000002,0
J,"Furnkranz, J",,,,"Furnkranz, J",,,Round robin classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we discuss round robin classification (aka pairwise classification), a technique for handling multi-class problems with binary classifiers by learning one classifier for each pair of classes. We present an empirical evaluation of the method, implemented as a wrapper around the Ripper rule learning algorithm, on 20 multi-class datasets from the UCI database repository. Our results show that the technique is very likely to improve Ripper's classification accuracy without having a high risk of decreasing it. More importantly, we give a general theoretical analysis of the complexity of the approach and show that its run-time complexity is below that of the commonly used one-against-all technique. These theoretical results are not restricted to rule learning but are also of interest to other communities where pairwise classification has recently received some attention. Furthermore, we investigate its properties as a general ensemble technique and show that round robin classification with C5.0 may improve C5.0's performance on multi-class problems. However, this improvement does not reach the performance increase of boosting, and a combination of boosting and round robin classification does not produce any gain over conventional boosting. Finally, we show that the performance of round robin classification can be further improved by a straight-forward integration with bagging.",,,,,"F√ºrnkranz, Johannes/AAH-2585-2019","Furnkranz, Johannes/0000-0002-1207-0159",,,,,,,,,,,,,1532-4435,,,,,MAR,2002,2,4,,,,,721,747,,10.1162/153244302320884605,0,,,,,,,,,,,,,WOS:000179542800008,0
J,"Rosipal, R; Trejo, LJ",,,,"Rosipal, R; Trejo, LJ",,,Kernel partial least squares regression in Reproducing Kernel Hilbert Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"A family of regularized least squares regression models in a Reproducing Kernel Hilbert Space is extended by the kernel partial least squares (PLS) regression model. Similar to principal components regression (PCR), PLS is a method based on the projection of input (explanatory) variables to the latent variables (components). However, in contrast to PCR, PLS creates the components by modeling the relationship between input and output variables while maintaining most of the information in the input variables. PLS is useful in situations where the number of explanatory variables exceeds the number of observations and/or a high level of multicollinearity among those variables is assumed. Motivated by this fact we will provide a kernel PLS algorithm for construction of nonlinear regression models in possibly high-dimensional feature spaces. We give the theoretical description of the kernel PLS algorithm and we experimentally compare the algorithm with the existing kernel PCR and kernel ridge regression techniques. We will demonstrate that on the data sets employed kernel PLS achieves the same results as kernel PCR but uses significantly fewer, qualitatively different components.",,,,,"Rosipal, Roman/B-8060-2008",,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,97,123,,10.1162/15324430260185556,0,,,,,,,,,,,,,WOS:000176055300002,0
J,"Allwein, EL; Schapire, RE; Singer, Y",,,,"Allwein, EL; Schapire, RE; Singer, Y",,,Reducing multiclass to binary: A unifying approach for margin classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2000,1,2,,,,,113,141,,10.1162/15324430152733133,0,,,,,,,,,,,,,WOS:000173336800002,0
J,"Le, TM; Clarke, B",,,,"Le, Tri M.; Clarke, Bertrand",,,Model Averaging Is Asymptotically Better Than Model Selection For Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We compare the performance of six model average predictors-Mallows' model averaging, stacking, Bayes model averaging, bagging, random forests, and boosting-to the components used to form them. In all six cases we identify conditions under which the model average predictor is consistent for its intended limit and performs as well or better than any of its components asymptotically. This is well known empirically, especially for complex problems, although theoretical results do not seem to have been formally established. We have focused our attention on the regression context since that is where model averaging techniques differ most often from current practice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,53,,,,,,,,,,,,,,,,WOS:000752260700001,0
J,"Zhao, H; Gordon, GJ",,,,"Zhao, Han; Gordon, Geoffrey J.",,,Inherent Tradeoffs in Learning Fair Representations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy is not entirely clear, even for the basic paradigm of classification problems. In this paper, we characterize an inherent tradeoff between statistical parity and accuracy in the classification setting by providing a lower bound on the sum of group-wise errors of any fair classifiers. Our impossibility theorem could be interpreted as a certain uncertainty principle in fairness: if the base rates differ among groups, then any fair classifier satisfying statistical parity has to incur a large error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair classifiers, from the perspective of learning fair representations. To show that our lower bound is tight, assuming oracle access to Bayes (potentially unfair) classifiers, we also construct an algorithm that returns a randomized classifier which is both optimal (in terms of accuracy) and fair. Interestingly, when the protected attribute can take more than two values, an extension of this lower bound does not admit an analytic solution. Nevertheless, in this case, we show that the lower bound can be efficiently computed by solving a linear program, which we term as the TV-Barycenter problem, a barycenter problem under the TV-distance. On the upside, we prove that if the group-wise Bayes optimal classifiers are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, we also conduct experiments on real-world datasets to confirm our theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,26,,,,,,,,,,,,,,,,WOS:000752282900001,0
J,"Celisse, A; Wahl, M",,,,"Celisse, Alain; Wahl, Martin",,,Analyzing the discrepancy principle for kernelized spectral filter learning algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the construction of early stopping rules in the nonparametric regression problem where iterative learning algorithms are used and the optimal iteration number is unknown. More precisely, we study the discrepancy principle, as well as modifications based on smoothed residuals, for kernelized spectral filter learning algorithms including Tikhonov regularization and gradient descent. Our main theoretical bounds are oracle inequalities established for the empirical estimation error (fixed design), and for the prediction error (random design). From these finite-sample bounds it follows that the classical discrepancy principle is statistically adaptive for slow rates occurring in the hard learning scenario, while the smoothed discrepancy principles are adaptive over ranges of faster rates (resp. higher smoothness parameters). Our approach relies on deviation inequalities for the stopping rules in the fixed design setting, combined with change-of-norm arguments to deal with the random design setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656389300001,0
J,"Kovachki, NB; Stuart, AM",,,,"Kovachki, Nikola B.; Stuart, Andrew M.",,,Continuous Time Analysis of Momentum Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gradient descent-based optimization methods underpin the parameter training of neural networks, and hence comprise a significant component in the impressive test results found in a number of applications. Introducing stochasticity is key to their success in practical problems, and there is some understanding of the role of stochastic gradient descent in this context. Momentum modifications of gradient descent such as Polyak's Heavy Ball method (HB) and Nesterov's method of accelerated gradients (NAG), are also widely adopted. In this work our focus is on understanding the role of momentum in the training of neural networks, concentrating on the common situation in which the momentum contribution is fixed at each step of the algorithm. To expose the ideas simply we work in the deterministic setting. Our approach is to derive continuous time approximations of the discrete algorithms; these continuous time approximations provide insights into the mechanisms at play within the discrete algorithms. We prove three such approximations. Firstly we show that standard implementations of fixed momentum methods approximate a time-rescaled gradient descent flow, asymptotically as the learning rate shrinks to zero; this result does not distinguish momentum methods from pure gradient descent, in the limit of vanishing learning rate. We then proceed to prove two results aimed at understanding the observed practical advantages of fixed momentum methods over gradient descent, when implemented in the non-asymptotic regime with fixed small, but non-zero, learning rate. We achieve this by proving approximations to continuous time limits in which the small but fixed learning rate appears as a parameter; this is known as the method of modified equations in the numerical analysis literature, recently rediscovered as the high resolution ODE approximation in the machine learning context. In our second result we show that the momentum method is approximated by a continuous time gradient flow, with an additional momentum-dependent second order time-derivative correction, proportional to the learning rate; this may be used to explain the stabilizing effect of momentum algorithms in their transient phase. Furthermore in a third result we show that the momentum methods admit an exponentially attractive invariant manifold on which the dynamics reduces, approximately, to a gradient flow with respect to a modified loss function, equal to the original loss function plus a small perturbation proportional to the learning rate; this small correction provides convexification of the loss function and encodes additional robustness present in momentum methods, beyond the transient phase.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500017,0
J,"Luckett, DJ; Laber, EB; Kim, S; Kosorok, MR",,,,"Luckett, Daniel J.; Laber, Eric B.; Kim, Siyeon; Kosorok, Michael R.",,,Estimation and Optimization of Composite Outcomes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is tremendous interest in precision medicine as a means to improve patient outcomes by tailoring treatment to individual characteristics. An individualized treatment rule formalizes precision medicine as a map from patient information to a recommended treatment. A treatment rule is defined to be optimal if it maximizes the mean of a scalar outcome in a population of interest, e.g., symptom reduction. However, clinical and intervention scientists often seek to balance multiple and possibly competing outcomes, e.g., symptom reduction and the risk of an adverse event. One approach to precision medicine in this setting is to elicit a composite outcome which balances all competing outcomes; unfortunately, eliciting a composite outcome directly from patients is difficult without a high-quality instrument, and an expert-derived composite outcome may not account for heterogeneity in patient preferences. We propose a new paradigm for the study of precision medicine using observational data that relies solely on the assumption that clinicians are approximately (i.e., imperfectly) making decisions to maximize individual patient utility. Estimated composite outcomes are subsequently used to construct an estimator of an individualized treatment rule which maximizes the mean of patient-specific composite outcomes. The estimated composite outcomes and estimated optimal individualized treatment rule provide new insights into patient preference heterogeneity, clinician behavior, and the value of precision medicine in a given domain. We derive inference procedures for the proposed estimators under mild conditions and demonstrate their finite sample performance through a suite of simulation experiments and an illustrative application to data from a study of bipolar depression.",,,,,"Kosorok, Michael/ABB-7427-2021",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,34733120,,,,,WOS:000700312500001,0
J,"Luo, YT; Raskutti, G; Yuan, M; Zhang, AR",,,,"Luo, Yuetian; Raskutti, Garvesh; Yuan, Ming; Zhang, Anru R.",,,A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we develop novel perturbation bounds for the higher-order orthogo nal iteration (HOOI) (De Lathauwer et al., 2000a). Under mild regularity conditions, we establish blockwise tensor perturbation bounds for HOOI with guarantees for both tensor reconstruction in Hilbert-Schmidt norm parallel to(T) over cap - T parallel to(HS) and mode-k singular subspace estimation in Schatten-q norm parallel to sin Theta((U) over cap (k),U-k)parallel to(q) for any q >= 1. We show the upper bounds of mode-k singular subspace estimation are unilateral and converge linearly to a quantity characterized by blockwise errors of the perturbation and signal strength. For the tensor reconstruction error bound, we express the bound through a simple quantity xi, which depends only on perturbation and the multilinear rank of the underlying signal. Rate matching deterministic lower bound for tensor reconstruction, which demonstrates the optimality of HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI with only a single iteration) is also optimal in terms of tensor reconstruction and can be used to lower the computational cost. The perturbation results are also extended to the case that only partial modes of T have low-rank structure. We support our theoretical results by extensive numerical studies. Finally, we apply the novel perturbation bounds of HOOI on two applications, tensor denoising and tensor co-clustering, from machine learning and statistics, which demonstrates the superiority of the new perturbation results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,48,,,,,,,,,,,,,,,,WOS:000700304100001,0
J,"Montiel, J; Halford, M; Mastelini, SM; Bolmier, G; Sourty, R; Vaysse, R; Zouitine, A; Gomes, HM; Read, J; Abdessalem, T; Bifet, A",,,,"Montiel, Jacob; Halford, Max; Mastelini, Saulo Martiello; Bolmier, Geoffrey; Sourty, Raphael; Vaysse, Robin; Zouitine, Adil; Gomes, Heitor Murilo; Read, Jesse; Abdessalem, Talel; Bifet, Albert",,,River: machine learning for streaming data in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of two popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.",,,,,,"Gomes, Heitor Murilo/0000-0002-5276-637X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,110,,,,,,,,,,,,,,,WOS:000663165500001,0
J,"Zhang, SQ; Zhang, ZY; Zhou, ZH",,,,"Zhang, Shao-Qun; Zhang, Zhao-Yu; Zhou, Zhi-Hua",,,Bifurcation Spiking Neural Network,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Spiking neural networks (SNNs) have attracted much attention due to their great potential for modeling time-dependent signals. The performance of SNNs depends not only on picking an apposite architecture and searching optimal connection weights as well as conventional deep neural networks, but also on the careful tuning of many hyper-parameters within fundamental spiking neural models. However, so far, there has been less systematic work on analyzing SNNs' dynamical characteristics, especially ones relative to these internal hyper-parameters, which leads to whether SNNs are adequate for modeling actual data relies on fortune. In this work, we provide a theoretical framework for investigating spiking neural models from the perspective of dynamical systems. As a result, we point out that the LIF model with control rate hyper-parameters is a bifurcation dynamical system. This point explains why the performance of SNNs is so sensitive to the setting of control rate hyper-parameters, leading to a recommendation that diverse and adaptive eigenvalues are beneficial to improve the performance of SNNs. Inspired by this insight, we develop the Bifurcation Spiking Neural Network (BSNN) with supervised implementation, and theoretically show that BSNN is an adaptive dynamical system. Experiments validate the effectiveness of BSNN on several benchmark data sets, showing that BSNN achieves superior performance to existing SNNs and is robust to the setting of control rates.",,,,,"Zhang, Shao-Qun/ADS-6463-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,21,,,,,,,,,,,,,,,,WOS:000726629200001,0
J,"Cucuringu, M; Tyagi, H",,,,"Cucuringu, Mihai; Tyagi, Hemant",,,Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider an unknown smooth function f : [0, 1](d) -> R, and assume we are given n noisy mod 1 samples of f, i.e., y(i) = (f (x(i))+eta(i)) mod 1, for x(i) is an element of [0, 1] (d), where eta(i) denotes the noise. Given the samples (x(i), y(i))(i=)(n)(1) , our goal is to recover smooth, robust estimates of the clean samples f (x(i)) mod 1. We formulate a natural approach for solving this problem, which works with angular embeddings of the noisy mod 1 samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem - a quadratically constrained quadratic program (QCQP) - where the variables are constrained to lie on the unit circle. Our proposed approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, as well as random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our proposed approach via extensive numerical simulations on synthetic data, along with a simple least-squares based solution for the unwrapping stage, that recovers the original samples of f (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo 1 samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semi-definite programming relaxation of our formulation. For the two-dimensional version of the problem, which has applications in synthetic aperture radar interferometry (InSAR), we are able to solve instances of real-world data with a million sample points in under 10 seconds, on a personal laptop.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000007,0
J,"Kocak, T; Munos, R; Kveton, B; Agrawal, S; Valko, M",,,,"Kocak, Tomas; Munos, Remi; Kveton, Branislav; Agrawal, Shipra; Valko, Michal",,,Spectral bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Smooth functions on graphs have wide applications in manifold and semi-supervised learning. In this work, we study a bandit problem where the payoffs of arms are smooth on a graph. This framework is suitable for solving online learning problems that involve graphs, such as content-based recommendation. In this problem, each item we can recommend is a node of an undirected graph and its expected rating is similar to the one of its neighbors. The goal is to recommend items that have high expected ratings. We aim for the algorithms where the cumulative regret with respect to the optimal policy would not scale poorly with the number of nodes. In particular, we introduce the notion of an effective dimension, which is small in real-world graphs, and propose three algorithms for solving our problem that scale linearly and sublinearly in this dimension. Our experiments on content recommendation problem show that a good estimator of user preferences for thousands of items can be learned from just tens of node evaluations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,218,,,,,,,,,,,,,,,WOS:000605738800001,0
J,"Rontsis, N; Osborne, MA; Goulart, PJ",,,,"Rontsis, Nikitas; Osborne, Michael A.; Goulart, Paul J.",,,Distributionally Ambiguous Optimization for Batch Bayesian Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel, theoretically-grounded, acquisition function for Batch Bayesian Optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function, which requires evaluation of a Gaussian expectation over a multivariate piecewise affine function. Our bound is computed instead by evaluating the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches, including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly - even on large batch sizes - as the solution of a tractable convex optimization problem. Our suggested acquisition function can also be optimized efficiently, since first and second derivative information can be calculated inexpensively as by-products of the acquisition function calculation itself. We derive various novel theorems that ground our work theoretically and we demonstrate superior performance via simple motivating examples, benchmark functions and real-world problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,149,,,,,,,,,,,,,,,WOS:000570112600001,0
J,"Tenzer, Y; Moscovich, A; Dorn, MF; Nadler, B; Spiegelman, C",,,,"Tenzer, Yaniv; Moscovich, Amit; Dorn, Mary Frances; Nadler, Boaz; Spiegelman, Clifford",,,Beyond Trees: Classification with Sparse Pairwise Dependencies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Several classification methods assume that the underlying distributions follow tree-structured graphical models. Indeed, trees capture statistical dependencies between pairs of variables, which may be crucial to attaining low classification errors. In this setting, the optimal classifier is linear in the log-transformed univariate and bivariate densities that correspond to the tree edges. In practice, observed data may not be well approximated by trees. Yet, motivated by the importance of pairwise dependencies for accurate classification, here we propose to approximate the optimal decision boundary by a sparse linear combination of the univariate and bivariate log-transformed densities. Our proposed approach is semi-parametric in nature: we non-parametrically estimate the univariate and bivariate densities, remove pairs of variables that are nearly independent using the Hilbert-Schmidt independence criterion, and finally construct a linear SVM using the retained log-transformed densities. We demonstrate on synthetic and real data sets, that our classifier, named SLB (sparse log-bivariate density), is competitive with other popular classification methods.",,,,,"Moscovich, Amit/Y-4085-2018","Moscovich, Amit/0000-0002-1289-8052; Nadler, Boaz/0000-0002-9777-4576",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,189,,,,,,,,,,,,,,,WOS:000589998200001,0
J,"Weinshall, D; Amir, D",,,,"Weinshall, Daphna; Amir, Dan",,,"Theory of Curriculum Learning, with Convex Loss Functions",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score - the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.",,,,,,"Amir, Dan/0000-0002-3518-5346",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,222,,,,,,,,,,,,,,,WOS:000605742600001,0
J,"Gelbhart, R; El-Yaniv, R",,,,"Gelbhart, Roei; El-Yaniv, Ran",,,"The Relationship Between Agnostic Selective Classification, Active Learning and the Disagreement Coefficient",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A selective classifier (f, g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A fast rejection rate is achieved if the rejection mass is bounded from above by (O) over tilde (1/m) where m is the number of labeled examples used to train the classifier (and (O) over tilde hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke's disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke's disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke's disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,33,,,,,,,,,,,,,,,WOS:000463316900001,0
J,"Huang, ZF",,,,"Huang, Zengfeng",,,Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a large matrix A is an element of R-n x d, we consider the problem of computing a sketch matrix B is an element of R-l x d which is significantly smaller than but still well approximates A. We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error parallel to A(T) A - B-T B parallel to(2). The popular Frequent Directions algorithm of Liberty (2013) and its variants achieve optimal space-error tradeoffs. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of matrix multiplication up to lower order terms. Specifically, we provide new space-optimal algorithms with faster running times and also show that the running times of our algorithms can be improved if and only if the state-of-the-art running time of matrix multiplication can be improved significantly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,56,,,,,,,,,,,,,,,WOS:000463325000001,0
J,"Abbe, E",,,,"Abbe, Emmanuel",,,Community Detection and Stochastic Block Models: Recent Developments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,177,,,,,,,,,,,,,,,WOS:000435443300001,0
J,"Bertsimas, D; Pawlowski, C; Zhuo, YD",,,,"Bertsimas, Dimitris; Pawlowski, Colin; Zhuo, Ying Daisy",,,From Predictive Methods to Missing Data Imputation: An Optimization Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including K-nearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt . impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt . impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, K-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3% against the best cross-validated benchmark method. Moreover, opt. impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt . impute single imputations with 50% data missing, the average out-of-sample R-2 is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1% in the classification tasks, compared to 0.315 and 84.4% for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt . impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,196,,,,,,,,,,,,,,,WOS:000435454100001,0
J,"Liu, XQ; Liu, XS",,,,"Liu, Xu-Qing; Liu, Xin-Sheng",,,Markov Blanket and Markov Boundary of Multiple Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Markov blanket (Mb) and Markov boundary (MB) are two key concepts in Bayesian networks (BNs). In this paper, we study the problem of Mb and MB for multiple variables. First, we show that Mb possesses the additivity property under the local intersection assumption, that is, an Mb of multiple targets can be constructed by simply taking the union of Mbs of the individual targets and removing the targets themselves. MB is also proven to have additivity under the local intersection assumption. Second, we analyze the cases of violating additivity of Mb and MB and then put forward the notions of Markov blanket supplementary (MbS) and Markov boundary supplementary (MBS). The properties of MbS and MBS are studied in detail. Third, we build two MB discovery algorithms and prove their correctness under the local composition assumption. We also discuss the ways of practically doing conditional independence tests and analyze the complexities of the algorithms. Finally, we make a benchmarking study based on six synthetic BNs and then apply MB discovery to multi-class prediction based on a real data set. The experimental results reveal our algorithms have higher accuracies and lower complexities than existing algorithms.",,,,,"Liu, Xu-Qing/D-7007-2011","Liu, Xu-Qing/0000-0002-6007-6993",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,50,43,,,,,,,,,,,,,,,WOS:000448366800001,0
J,"Shah, NB; Tabibian, B; Muandet, K; Guyon, I; von Luxburg, U",,,,"Shah, Nihar B.; Tabibian, Behzad; Muandet, Krikamol; Guyon, Isabelle; von Luxburg, Ulrike",,,Design and Analysis of the NIPS 2016 Review Process,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Neural Information Processing Systems (NIPS) is a top-tier annual conference in machine learning. The 2016 edition of the conference comprised more than 2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents a growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and over 100% in terms of attendees as compared to the previous year. The massive scale as well as rapid growth of the conference calls for a thorough quality assessment of the peer-review process and novel means of improvement. In this paper, we analyze several aspects of the data collected during the review process, including an experiment investigating the efficacy of collecting ordinal rankings from reviewers. We make a number of key observations, provide suggestions that may be useful for subsequent conferences, and discuss open problems towards the goal of improving peer review.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,34,,,,,,,,,,,,,,,,WOS:000448370300001,0
J,"Tepper, M; Sengupta, AM; Chklovskii, D",,,,"Tepper, Mariano; Sengupta, Anirvan M.; Chklovskii, Dmitri",,,Clustering is semidefinitely not that hard: Nonnegative SDP for manifold disentangling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In solving hard computational problems, semidefinite program (SDP) relaxations often play an important role because they come with a guarantee of optimality. Here, we focus on a popular semidefinite relaxation of K-means clustering which yields the same solution as the non-convex original formulation for well segregated datasets. We report an unexpected finding: when data contains (greater than zero-dimensional) manifolds, the SDP solution captures such geometrical structures. Unlike traditional manifold embedding techniques, our approach does not rely on manually defining a kernel but rather enforces locality via a nonnegativity constraint. We thus call our approach NOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive understanding of its manifold learning capabilities, we develop a theoretical analysis of NOMAD on idealized datasets. While NOMAD is convex and the globally optimal solution can be found by generic SDP solvers with polynomial time complexity, they are too slow for modern datasets. To address this problem, we analyze a non-convex heuristic and present a new, convex and yet efficient, algorithm, based on the conditional gradient method. Our results render NOMAD a versatile, understandable, and powerful tool for manifold learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,82,,,,,,,,,,,,,,,WOS:000454480700001,0
J,"Zheng, C; Achanta, R; Benjamini, Y",,,,"Zheng, Charles; Achanta, Rakesh; Benjamini, Yuval",,,Extrapolating Expected Accuracies for Large Multi-Class Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The difficulty of multi-class classification generally increases with the number of classes. Using data for a small set of the classes, can we predict how well the classifier scales as the number of classes increases ? We propose a framework for studying this question, assuming that classes in both sets are sampled from the same population and that the classifier is based on independently learned scoring functions. Under this framework, we can express the classification accuracy on a set of k classes as the (k-1)st moment of a discriminability function; the discriminability function itself does not depend on k. We leverage this result to develop a non-parametric regression estimator for the discriminability function, which can extrapolate accuracy results to larger unobserved sets. We also formalize an alternative approach that extrapolates accuracy separately for each class, and identify tradeoffs between the two methods. We show that both methods can accurately predict classifier performance on label sets up to ten times the size of the original set, both in simulations as well as in realistic face recognition or character recognition tasks.",,,,,,"Zheng, Charles/0000-0003-3427-0845",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,65,,,,,,,,,,,,,,,WOS:000452053600001,0
J,"Zhou, Y; Liang, YB; Yu, YL; Dai, W; Xing, EP",,,,"Zhou, Yi; Liang, Yingbin; Yu, Yaoliang; Dai, Wei; Xing, Eric P.",,,Distributed Proximal Gradient Algorithm for Partially Asynchronous Computer Clusters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile distributed algorithm has become vital for the success of many large-scale machine learning applications. In this work we propose m-PAPG, an implementation of the flexible proximal gradient algorithm in model parallel systems equipped with the partially asynchronous communication protocol. The worker machines communicate asynchronously with a controlled staleness bound s and operate at different frequencies. We characterize various convergence properties of m-PAPG: 1) Under a general non-smooth and non-convex setting, we prove that every limit point of the sequence generated by m-PAPG is a critical point of the objective function; 2) Under an error bound condition of convex objective functions, we prove that the optimality gap decays linearly for every s steps; 3) Under the Kurdyka-Lojasiewicz inequality and a sufficient decrease assumption , we prove that the sequences generated by m-PAPG converge to the same critical point, provided that a proximal Lipschitz condition is satisfied.",,,,,,"Liang, Yingbin/0000-0002-8635-2992",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,19,,,,,,,,,,,,,,,WOS:000443227800001,0
J,"Mahajan, D; Keerthi, SS; Sundararajan, S",,,,"Mahajan, Dhruv; Keerthi, S. Sathiya; Sundararajan, S.",,,A distributed block coordinate descent method for training l(1) regularized linear classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Distributed training of l(1) regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore systems where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for l(1) regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,35,,,,,,,,,,,,,,,,WOS:000412483500001,0
J,"Narayanan, H; Rakhlin, A",,,,"Narayanan, Hariharan; Rakhlin, Alexander",,,Efficient Sampling from Time-Varying Log-Concave Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a computationally efficient random walk on a convex body which rapidly mixes with respect to a fixed log-concave distribution and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,112,,,,,,,,,,,,,,,WOS:000424539300001,0
J,"Dogan, U; Glasmachers, T; Igel, C",,,,"Dogan, Urun; Glasmachers, Tobias; Igel, Christian",,,A Unified View on Multi-class Support Vector Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A unified view on multi-class support vector machines (SVMs) is presented, covering most prominent variants including the one-vs-all approach and the algorithms proposed by Weston & Watkins, Crammer & Singer, Lee, Lin, & Wahba, and Liu & Yuan. The unification leads to a template for the quadratic training problems and new multi-class SVM formulations. Within our framework, we provide a comparative analysis of the various notions of multi-class margin and margin-based loss. In particular, we demonstrate limitations of the loss function considered, for instance, in the Crammer & Singer machine. We analyze Fisher consistency of multi-class loss functions and universal consistency of the various machines. On the one hand, we give examples of SVMs that are, in a particular hyperparameter regime, universally consistent without being based on a Fisher consistent loss. These include the canonical extension of SVMs to multiple classes as proposed by Weston & Watkins and Vapnik as well as the one-vs-all approach. On the other hand, it is demonstrated that machines based on Fisher consistent loss functions can fail to identify proper decision boundaries in low-dimensional feature spaces. We compared the performance of nine different multi-class SVMs in a thorough empirical study. Our results suggest to use the Weston & Watkins SVM, which can be trained comparatively fast and gives good accuracies on benchmark functions. If training time is a major concern, the one-vs-all approach is the method of choice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,45,,,,,,,,,,,,,,,WOS:000391484600001,0
J,"Elisha, O; Dekel, S",,,,"Elisha, Oren; Dekel, Shai",,,"Wavelet decompositions of Random Forests - smoothness analysis, sparse approximation and applications",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce, in the setting of machine learning, a generalization of wavelet analysis which is a popular approach to low dimensional structured signal analysis. The wavelet decomposition of a Random Forest provides a sparse approximation of any regression or classification high dimensional function at various levels of detail, with a concrete ordering of the Random Forest nodes: from 'significant' elements to nodes capturing only 'insignificant' noise. Motivated by function space theory, we use the wavelet decomposition to compute numerically a 'weak-type' smoothness index that captures the complexity of the underlying function. As we show through extensive experimentation, this sparse representation facilitates a variety of applications such as improved regression for difficult datasets, a novel approach to feature importance, resilience to noisy or irrelevant features, compression of ensembles, etc.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,198,,,,,,,,,,,,,,,WOS:000391828100001,0
J,"Minh, HQ; Bazzani, L; Murino, V",,,,"Ha Quang Minh; Bazzani, Loris; Murino, Vittorio",,,A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) framework for the problem of learning an unknown functional dependency between a structured input space and a structured output space. Our formulation encompasses both Vector-valued Manifold Regularization and Co-regularized Multi-view Learning, providing in particular a unifying framework linking these two important learning approaches. In the case of the least square loss function, we provide a closed form solution, which is obtained by solving a system of linear equations. In the case of Support Vector Machine (SVM) classi fi cation, our formulation generalizes in particular both the binary Laplacian SVM to the multi-class, multi-view settings and the multi-class Simplex Cone SVM to the semisupervised, multi-view settings. The solution is obtained by solving a single quadratic optimization problem, as in standard SVM, via the Sequential Minimal Optimization (SMO) approach. Empirical results obtained on the task of object recognition, using several challenging data sets, demonstrate the competitiveness of our algorithms compared with other state-of-the-art methods.",,,,,,"HA QUANG, Minh/0000-0003-3926-8875",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,25,,,,,,,,,,,,,,,WOS:000391475900001,0
J,"Khetan, A; Oh, S",,,,"Khetan, Ashish; Oh, Sewoong",,,Data-driven Rank Breaking for Efficient Rank Aggregation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Rank aggregation systems collect ordinal preferences from individuals to produce a global ranking that represents the social preference. Rank-breaking is a common practice to reduce the computational complexity of learning the global ranking. The individual preferences are broken into pairwise comparisons and applied to efficient algorithms tailored for independent paired comparisons. However, due to the ignored dependencies in the data, naive rank-breaking approaches can result in inconsistent estimates. The key idea to produce accurate and consistent estimates is to treat the pairwise comparisons unequally, depending on the topology of the collected data. In this paper, we provide the optimal rank-breaking estimator, which not only achieves consistency but also achieves the best error bound. This allows us to characterize the fundamental tradeoff between accuracy and complexity. Further, the analysis identifies how the accuracy depends on the spectral gap of a corresponding comparison graph.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,193,,,,,,,,,,,,,,,WOS:000391826800001,0
J,"Lapuschkin, S; Binder, A; Montavon, G; Muller, KR; Samek, W",,,,"Lapuschkin, Sebastian; Binder, Alexander; Montavon, Gregoire; Mueller, Klaus-Robert; Samek, Wojciech",,,The LRP Toolbox for Artificial Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's. mat files and the. npy format for numpy or plain text.",,,,,"Mueller, Klaus-Robert/Y-3547-2019; Samek, Wojciech/AAZ-2165-2021; Samek, Wojciech/AAZ-2156-2021","Mueller, Klaus-Robert/0000-0002-3861-7685; Samek, Wojciech/0000-0002-6283-3265; Samek, Wojciech/0000-0002-6283-3265; Binder, Alexander/0000-0001-9605-6209",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,114,,,,,,,,,,,,,,,WOS:000391550900001,0
J,"Meng, XR; Bradley, J; Yavuz, B; Sparks, E; Venkataraman, S; Liu, D; Freeman, J; Tsai, D; Amde, M; Owen, S; Xin, D; Xin, R; Franklin, MJ; Zadeh, R; Zaharia, M; Talwalkar, A",,,,"Meng, Xiangrui; Bradley, Joseph; Yavuz, Burak; Sparks, Evan; Venkataraman, Shivaram; Liu, Davies; Freeman, Jeremy; Tsai, D. B.; Amde, Manish; Owen, Sean; Xin, Doris; Xin, Reynold; Franklin, Michael J.; Zadeh, Reza; Zaharia, Matei; Talwalkar, Ameet",,,MLlib: Machine Learning in Apache Spark,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides effcient functionality fo wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,34,,,,,,,,,,,,,,,WOS:000391480800001,0
J,"Mentch, L; Hooker, G",,,,"Mentch, Lucas; Hooker, Giles",,,Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work develops formal statistical inference procedures for predictions generated by supervised learning ensembles. Ensemble methods based on bootstrapping, such as bagging and random forests, have improved the predictive accuracy of individual trees, but fail to provide a framework in which distributional results can be easily determined. Instead of aggregating full bootstrap samples, we consider predicting by averaging over trees built on subsamples of the training set and demonstrate that the resulting estimator takes the form of a U-statistic. As such, predictions for individual feature vectors are asymptotically normal, allowing for con fi dence intervals to accompany predictions. In practice, a subset of subsamples is used for computational speed; here our estimators take the form of incomplete U-statistics and equivalent results are derived. We further demonstrate that this setup provides a framework for testing the signi fi cance of features. Moreover, the internal estimation method we develop allows us to estimate the variance parameters and perform these inference procedures at no additional computational cost. Simulations and illustrations on a real data set are provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,26,,,,,,,,,,,,,,,WOS:000391476700001,0
J,"Moghaddass, R; Rudin, C; Madigan, D",,,,"Moghaddass, Ramin; Rudin, Cynthia; Madigan, David",,,The Factorized Self-Controlled Case Series Method: An Approach for Estimating the Effects of Many Drugs on Many Outcomes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We provide a hierarchical Bayesian model for estimating the effects of transient drug exposures on a collection of health outcomes, where the effects of all drugs on all outcomes are estimated simultaneously. The method possesses properties that allow it to handle important challenges of dealing with large-scale longitudinal observational databases. In particular, this model is a generalization of the self-controlled case series (SCCS) method, meaning that certain patient specific baseline rates never need to be estimated. Further, this model is formulated with layers of latent factors, which substantially reduces the number of parameters and helps with interpretability by illuminating latent classes of drugs and outcomes. We believe our work is the first to consider multivariate SCCS (in the sense of multiple outcomes) and is the first to couple latent factor analysis with SCCS. We demonstrate the approach by estimating the effects of various time-sensitive insulin treatments for diabetes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,185,,,,,,,,,,,,,,,WOS:000391825100001,0
J,"Mooij, JM; Peters, J; Janzing, D; Zscheischler, J; Scholkopf, B",,,,"Mooij, Joris M.; Peters, Jonas; Janzing, Dominik; Zscheischler, Jakob; Schoelkopf, Bernhard",,,Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The discovery of causal relationships from purely observational data is a fundamental problem in science. The most elementary form of such a causal discovery problem is to decide whether X causes Y or, alternatively, Y causes X, given joint observations of two variables X; Y. An example is to decide whether altitude causes temperature, or vice versa, given only joint measurements of both variables. Even under the simplifying assumptions of no confounding, no feedback loops, and no selection bias, such bivariate causal discovery problems are challenging. Nevertheless, several approaches for addressing those problems have been proposed in recent years. We review two families of such methods: methods based on Additive Noise Models (ANMs) and Information Geometric Causal Inference (IGCI). We present the benchmark CAUSEEFFECTPAIRS that consists of data for 100 different cause-effect pairs selected from 37 data sets from various domains (e.g., meteorology, biology, medicine, engineering, economy, etc.) and motivate our decisions regarding the ground truth causal directions of all pairs. We evaluate the performance of several bivariate causal discovery methods on these real-world benchmark data and in addition on artificially simulated data. Our empirical results on real-world data indicate that certain methods are indeed able to distinguish cause from effect using only purely observational data, although more benchmark data would be needed to obtain statistically significant conclusions. One of the best performing methods overall is the method based on Additive Noise Models that has originally been proposed by Hoyer et al. (2009), which obtains an accuracy of 63 +/- 10 % and an AUC of 0.74 +/- 0.05 on the real-world benchmark. As the main theoretical contribution of this work we prove the consistency of that method.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013; Zscheischler, Jakob/ABE-7324-2021","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Zscheischler, Jakob/0000-0001-6045-1629; Peters, Jonas/0000-0002-1487-7511",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,32,,,,,,,,,,,,,,,WOS:000391480200001,0
J,"Reshef, YA; Reshef, DN; Finucane, HK; Sabeti, PC; Mitzenmacher, M",,,,"Reshef, Yakir A.; Reshef, David N.; Finucane, Hilary K.; Sabeti, Pardis C.; Mitzenmacher, Michael",,,Measuring Dependence Powerfully and Equitably,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a high-dimensional data set, we often wish to find the strongest relationships within it. A common strategy is to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. This strategy works well if the statistic used (a) has good power to detect non-trivial relationships, and (b) is equitable, meaning that for some measure of noise it assigns similar scores to equally noisy relationships regardless of relationship type (e.g., linear, exponential, periodic). In this paper, we define and theoretically characterize two new statistics that together yield an efficient approach for obtaining both power and equitability. To do this, we first introduce a new population measure of dependence and show three equivalent ways that it can be viewed, including as a canonical smoothing of mutual information. We then introduce an efficiently computable consistent estimator of our population measure of dependence, and we empirically establish its equitability on a large class of noisy functional relationships. This new statistic has better bias/variance properties and better runtime complexity than a previous heuristic approach. Next, we derive a second, related statistic whose computation is a trivial side-product of our algorithm and whose goal is powerful independence testing rather than equitability. We prove that this statistic yields a consistent independence test and show in simulations that the test has good power against independence. Taken together, our results suggest that these two statistics are a valuable pair of tools for exploratory data analysis.",,,,,"Sabeti, Pardis/GQH-8610-2022",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,63,211,,,,,,,,,,,,,,,WOS:000391835000001,0
J,"Anandkumar, A; Hsu, D; Janzamin, M; Kakade, S",,,,"Anandkumar, Animashree; Hsu, Daniel; Janzamin, Majid; Kakade, Sham",,,When Are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of higher order expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition.",,,,,,"Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2643,2694,,,,,,,,,,,,,,,,WOS:000369888000011,0
J,"Ravanbakhsh, S; Greiner, R",,,,"Ravanbakhsh, Siamak; Greiner, Russell",,,Perturbed Message Passing for Constraint Satisfaction Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Belief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (w.r.t. the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes.",,,,,"Greiner, Russell/AAQ-4502-2020","Greiner, Russell/0000-0001-8327-934X",,,,,,,,,,,,,1532-4435,,,,,JUL,2015,16,,,,,,1249,1274,,,,,,,,,,,,,,,,WOS:000369886800001,0
J,"Colombo, D; Maathuis, MH",,,,"Colombo, Diego; Maathuis, Marloes H.",,,Order-Independent Constraint-Based Causal Structure Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2014,15,,,,,,3741,3782,,,,,,,,,,,,,,,,WOS:000353126200014,0
J,"Hsieh, CJ; Sustik, MA; Dhillon, IS; Ravikumar, P",,,,"Hsieh, Cho-Jui; Sustik, Matyas A.; Dhillon, Inderjit S.; Ravikumar, Pradeep",,,QUIC: Quadratic Approximation for Sparse Inverse Covariance Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The l(1)-regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a Gaussian Markov Random Field, from very limited samples. We propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program. In contrast to recent state-of-the-art methods that largely use first order gradient information, our algorithm is based on Newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse Gaussian MLE problem. We show that our method is superlinearly convergent, and present experimental results using synthetic and real-world application data that demonstrate the considerable improvements in performance of our method when compared to previous methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,2911,2947,,,,,,,,,,,,,,,,WOS:000344638800003,0
J,"Gillis, N; Luce, R",,,,"Gillis, Nicolas; Luce, Robert",,,Robust Near-Separable Nonnegative Matrix Factorization Using Linear Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonnegative matrix factorization (NMF) has been shown recently to be tractable under the separability assumption, under which all the columns of the input data matrix belong to the convex cone generated by only a few of these columns. Bittorf, Recht, Re and Tropp ('Factoring nonnegative matrices with linear programs', NIPS 2012) proposed a linear programming (LP) model, referred to as Hottopixx, which is robust under any small perturbation of the input matrix. However, Hottopixx has two important drawbacks: (i) the input matrix has to be normalized, and (ii) the factorization rank has to be known in advance. In this paper, we generalize Hottopixx in order to resolve these two drawbacks, that is, we propose a new LP model which does not require normalization and detects the factorization rank automatically. Moreover, the new LP model is more flexible, significantly more tolerant to noise, and can easily be adapted to handle outliers and other noise models. Finally, we show on several synthetic data sets that it outperforms Hottopixx while competing favorably with two state-of-the-art methods.",,,,,,"Gillis, Nicolas/0000-0001-6423-6897; Luce, Robert/0000-0001-6960-9656",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1249,1280,,,,,,,,,,,,,,,,WOS:000338420000002,0
J,"Sprekeler, H; Zito, T; Wiskott, L",,,,"Sprekeler, Henning; Zito, Tiziano; Wiskott, Laurenz",,,An Extension of Slow Feature Analysis for Nonlinear Blind Source Separation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We present and test an extension of slow feature analysis as a novel approach to nonlinear blind source separation. The algorithm relies on temporal correlations and iteratively reconstructs a set of statistically independent sources from arbitrary nonlinear instantaneous mixtures. Simulations show that it is able to invert a complicated nonlinear mixture of two audio signals with a high reliability. The algorithm is based on a mathematical analysis of slow feature analysis for the case of input data that are generated from statistically independent sources.,,,,,"Wiskott, Laurenz/P-7715-2017","Wiskott, Laurenz/0000-0001-6237-740X; Sprekeler, Henning/0000-0003-0690-3553; Zito, Tiziano/0000-0002-7199-7456",,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,921,947,,,,,,,,,,,,,,,,WOS:000335458100003,0
J,"Djuric, N; Lan, L; Vucetic, S; Wang, Z",,,,"Djuric, Nemanja; Lan, Liang; Vucetic, Slobodan; Wang, Zhuang",,,BudgetedSVM: A Toolbox for Scalable SVM Approximations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present BudgetedSVM, an open-source C++ toolbox comprising highly-optimized implementations of recently proposed algorithms for scalable training of Support Vector Machine (SVM) approximators: Adaptive Multi-hyperplane Machines, Low-rank Linearization SVM, and Budgeted Stochastic Gradient Descent. BudgetedSVM trains models with accuracy comparable to LibSVM in time comparable to LibLinear, solving non-linear problems with millions of high-dimensional examples within minutes on a regular computer. We provide command-line and Matlab interfaces to BudgetedSVM, an efficient API for handling large-scale, high-dimensional data sets, as well as detailed documentation to help developers use and further extend the toolbox.",,,,,"Lan, Liang/V-8242-2019","Lan, Liang/0000-0002-0427-977X",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3813,3817,,,,,,,,,,,,,,,,WOS:000335457100011,0
J,"Hernandez-Lobato, D; Hernandez-Lobato, JM; Dupont, P",,,,"Hernandez-Lobato, Daniel; Miguel Hernandez-Lobato, Jose; Dupont, Pierre",,,Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.",,,,,"Hern√°ndez-Lobato, Daniel/E-8337-2012; Hernandez-Lobato, Jose Miguel/F-2056-2016","Hern√°ndez-Lobato, Daniel/0000-0001-5845-437X; Hernandez-Lobato, Jose Miguel/0000-0001-7610-949X",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1891,1945,,,,,,,,,,,,,,,,WOS:000323367000008,0
J,"Zaidi, NA; Cerquides, J; Carman, MJ; Webb, GI",,,,"Zaidi, Nayyar A.; Cerquides, Jesus; Carman, Mark J.; Webb, Geoffrey I.",,,Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.",,,,,"Carman, Mark/U-7997-2017; Webb, Geoffrey/R-9967-2017; Cerquides Bueno, Jesus/H-5405-2015","Carman, Mark/0000-0001-6575-9737; Webb, Geoffrey/0000-0001-9963-5169; Cerquides Bueno, Jesus/0000-0002-3752-644X; Zaidi, Nayyar/0000-0003-4024-2517",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,1947,1988,,,,,,,,,,,,,,,,WOS:000323367000009,0
J,"Zhang, C; Tao, DC",,,,"Zhang, Chao; Tao, Dacheng",,,Risk Bounds of Learning Processes for Levy Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Levy processes refer to a class of stochastic processes, for example, Poisson processes and Brownian motions, and play an important role in stochastic processes and machine learning. Therefore, it is essential to study risk bounds of the learning process for time-dependent samples drawn from a Levy process (or briefly called learning process for Levy process). It is noteworthy that samples in this learning process are not independently and identically distributed (i.i.d.). Therefore, results in traditional statistical learning theory are not applicable (or at least cannot be applied directly), because they are obtained under the sample-i.i.d. assumption. In this paper, we study risk bounds of the learning process for time-dependent samples drawn from a Levy process, and then analyze the asymptotical behavior of the learning process. In particular, we first develop the deviation inequalities and the symmetrization inequality for the learning process. By using the resultant inequalities, we then obtain the risk bounds based on the covering number. Finally, based on the resulting risk bounds, we study the asymptotic convergence and the rate of convergence of the learning process for Levy process. Meanwhile, we also give a comparison to the related results under the sample-i.i.d. assumption.",,,,,"zhang, chi/GRX-3610-2022",,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,351,376,,,,,,,,,,,,,,,,WOS:000315981900003,0
J,"Sabato, S; Tishby, N",,,,"Sabato, Sivan; Tishby, Naftali",,,Multi-Instance Learning with Any Hypothesis Class,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classification rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to specific settings or applications. In this work we provide a unified theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a specific application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efficient PAC-learning for MIL can be generated from any efficient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size.",,,,,"Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,OCT,2012,13,,,,,,2999,3039,,,,,,,,,,,,,,,,WOS:000313200000007,0
J,"Snoek, J; Adams, RP; Larochelle, H",,,,"Snoek, Jasper; Adams, Ryan P.; Larochelle, Hugo",,,Nonparametric Guidance of Autoencoder Representations using Label Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While unsupervised learning has long been useful for density modeling, exploratory data analysis and visualization, it has become increasingly important for discovering features that will later be used for discriminative tasks. Discriminative algorithms often work best with highly-informative features; remarkably, such features can often be learned without the labels. One particularly effective way to perform such unsupervised learning has been to use autoencoder neural networks, which find latent representations that are constrained but nevertheless informative for reconstruction. However, pure unsupervised learning with autoencoders can find representations that may or may not be useful for the ultimate discriminative task. It is a continuing challenge to guide the training of an autoencoder so that it finds features which will be useful for predicting labels. Similarly, we often have a priori information regarding what statistical variation will be irrelevant to the ultimate discriminative task, and we would like to be able to use this for guidance as well. Although a typical strategy would be to include a parametric discriminative model as part of the autoencoder training, here we propose a nonparametric approach that uses a Gaussian process to guide the representation. By using a nonparametric model, we can ensure that a useful discriminative function exists for a given set of features, without explicitly instantiating it. We demonstrate the superiority of this guidance mechanism on four data sets, including a real- world application to rehabilitation research. We also show how our proposed approach can learn to explicitly ignore statistically significant covariate information that is label-irrelevant, by evaluating on the small NORB image recognition problem in which pose and lighting labels are available.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2567,2588,,,,,,,,,,,,,,,,WOS:000309580600003,0
J,"Nickisch, H",,,,"Nickisch, Hannes",,,glm-ie: Generalised Linear Models Inference & Estimation Toolbox,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The glm-ie toolbox contains functionality for estimation and inference in generalised linear models over continuous-valued variables. Besides a variety of penalised least squares solvers for estimation, it offers inference based on (convex) variational bounds, on expectation propagation and on factorial mean field. Scalable and efficient inference in fully-connected undirected graphical models or Markov random fields with Gaussian and non-Gaussian potentials is achieved by casting all the computations as matrix vector multiplications. We provide a wide choice of penalty functions for estimation, potential functions for inference and matrix classes with lazy evaluation for convenient modelling. We designed the glm-ie package to be simple, generic and easily expansible. Most of the code is written in Matlab including some MEX files to be fully compatible to both Matlab 7.x and GNU Octave 3.3.x. Large scale probabilistic classification as well as sparse linear modelling can be performed in a common algorithmical framework by the glm-ie toolkit.",,,,,"Nickisch, Hannes/I-7049-2017","Nickisch, Hannes/0000-0003-1604-6647",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1699,1703,,,,,,,,,,,,,,,,WOS:000305456600012,0
J,"Frank, M; Streich, AP; Basin, D; Buhmann, JM",,,,"Frank, Mario; Streich, Andreas P.; Basin, David; Buhmann, Joachim M.",,,Multi-Assignment Clustering for Boolean Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a probabilistic model for clustering Boolean data where an object can be simultaneously assigned to multiple clusters. By explicitly modeling the underlying generative process that combines the individual source emissions, highly structured data are expressed with substantially fewer clusters compared to single-assignment clustering. As a consequence, such a model provides robust parameter estimators even when the number of samples is low. We extend the model with different noise processes and demonstrate that maximum-likelihood estimation with multiple assignments consistently infers source parameters more accurately than single-assignment clustering. Our model is primarily motivated by the task of role mining for role-based access control, where users of a system are assigned one or more roles. In experiments with real-world access-control data, our model exhibits better generalization performance than state-of-the-art approaches.",,,,,"Buhmann, Joachim/AAU-4760-2020",,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,459,489,,,,,,,,,,,,,,,,WOS:000303046000008,0
J,"Nakajima, S; Sugiyama, M",,,,"Nakajima, Shinichi; Sugiyama, Masashi",,,Theoretical Analysis of Bayesian Matrix Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, variational Bayesian (VB) techniques have been applied to probabilistic matrix factorization and shown to perform very well in experiments. In this paper, we theoretically elucidate properties of the VB matrix factorization (VBMF) method. Through finite-sample analysis of the VBMF estimator, we show that two types of shrinkage factors exist in the VBMF estimator: the positive-part James-Stein (PJS) shrinkage and the trace-norm shrinkage, both acting on each singular component separately for producing low-rank solutions. The trace-norm shrinkage is simply induced by non-flat prior information, similarly to the maximum a posteriori (MAP) approach. Thus, no trace-norm shrinkage remains when priors are non-informative. On the other hand, we show a counter-intuitive fact that the PJS shrinkage factor is kept activated even with flat priors. This is shown to be induced by the non-identifiability of the matrix factorization model, that is, the mapping between the target matrix and factorized matrices is not one-to-one. We call this model-induced regularization. We further extend our analysis to empirical Bayes scenarios where hyperparameters are also learned based on the VB free energy. Throughout the paper, we assume no missing entry in the observed matrix, and therefore collaborative filtering is out of scope.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2583,2648,,,,,,,,,,,,,,,,WOS:000298102900003,0
J,"Collobert, R; Weston, J; Bottou, L; Karlen, M; Kavukcuoglu, K; Kuksa, P",,,,"Collobert, Ronan; Weston, Jason; Bottou, Leon; Karlen, Michael; Kavukcuoglu, Koray; Kuksa, Pavel",,,Natural Language Processing (Almost) from Scratch,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2011,12,,,,,,2493,2537,,,,,,,,,,,,,,,,WOS:000298102200003,0
J,"Ryabko, D",,,,"Ryabko, Daniil",,,On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A sequence x(1),...,x(n),... of discrete-valued observations is generated according to some unknown probabilistic law (measure) mu. After observing each outcome, one is required to give conditional probabilities of the next observation. The realizable case is when the measure mu belongs to an arbitrary but known class C of process measures. The non-realizable case is when mu is completely arbitrary, but the prediction performance is measured with respect to a given set C of process measures. We are interested in the relations between these problems and between their solutions, as well as in characterizing the cases when a solution exists and finding these solutions. We show that if the quality of prediction is measured using the total variation distance, then these problems coincide, while if it is measured using the expected average KL divergence, then they are different. For some of the formalizations we also show that when a solution exists it can be obtained as a Bayes mixture over a countable subset of C. We also obtain several characterization of those sets C for which solutions to the considered problems exist. As an illustration to the general results obtained, we show that a solution to the non-realizable case of the sequence prediction problem exists for the set of all finite-memory processes, but does not exist for the set of all stationary processes. It should be emphasized that the framework is completely general: the processes measures considered are not required to be i.i.d., mixing, stationary, or to belong to any parametric family.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2161,2180,,,,,,,,,,,,,,,,WOS:000293757900002,0
J,"Hahsler, M; Chelluboina, S; Hornik, K; Buchta, C",,,,"Hahsler, Michael; Chelluboina, Sudheer; Hornik, Kurt; Buchta, Christian",,,The arules R-Package Ecosystem: Analyzing Interesting Patterns from Large Transaction Data Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes the ecosystem of R add-on packages developed around the infrastructure provided by the package arules. The packages provide comprehensive functionality for analyzing interesting patterns including frequent itemsets, association rules, frequent sequences and for building applications like associative classification. After discussing the ecosystem's design we illustrate the ease of mining and visualizing rules with a short example.",,,,,"Hornik, Kurt/S-8548-2017","Hornik, Kurt/0000-0003-4198-9911; Hahsler, Michael/0000-0003-2716-1405",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,2021,2025,,,,,,,,,,,,,,,,WOS:000293757200008,0
J,"Yuan, M",,,,"Yuan, Ming",,,High Dimensional Inverse Covariance Matrix Estimation via Linear Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the problem of estimating a high dimensional inverse covariance matrix that can be well approximated by sparse matrices. Taking advantage of the connection between multivariate linear regression and entries of the inverse covariance matrix, we propose an estimating procedure that can effectively exploit such sparsity. The proposed method can be computed using linear programming and therefore has the potential to be used in very high dimensional problems. Oracle inequalities are established for the estimation error in terms of several operator norms, showing that the method is adaptive to different types of sparsity of the problem.",,,,,"Yuan, Ming/J-6153-2019","Yuan, Ming/0000-0002-4415-8606",,,,,,,,,,,,,1532-4435,,,,,AUG,2010,11,,,,,,2261,2286,,,,,,,,,,,,,,,,WOS:000282523300009,0
J,"Spirtes, P",,,,"Spirtes, Peter",,,Introduction to Causal Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to find a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classification and prediction problems.",,,,,"Spirtes, Peter/GYD-5724-2022",,,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1643,1662,,,,,,,,,,,,,,,,WOS:000282522000003,0
J,"Verbancsics, P; Stanley, KO",,,,"Verbancsics, Phillip; Stanley, Kenneth O.",,,Evolving Static Representations for Task Transfer,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An important goal for machine learning is to transfer knowledge between tasks. For example, learning to play RoboCup Keepaway should contribute to learning the full game of RoboCup soccer. Previous approaches to transfer in Keepaway have focused on transforming the original representation to fit the new task. In contrast, this paper explores the idea that transfer is most effective if the representation is designed to be the same even across different tasks. To demonstrate this point, a bird's eye view (BEV) representation is introduced that can represent different tasks on the same two-dimensional map. For example, both the 3 vs. 2 and 4 vs. 3 Keepaway tasks can be represented on the same BEV. Yet the problem is that a raw two-dimensional map is high-dimensional and unstructured. This paper shows how this problem is addressed naturally by an idea from evolutionary computation called indirect encoding, which compresses the representation by exploiting its geometry. The result is that the BEV learns a Keepaway policy that transfers without further learning or manipulation. It also facilitates transferring knowledge learned in a different domain, Knight Joust, into Keepaway. Finally, the indirect encoding of the BEV means that its geometry can be changed without altering the solution. Thus static representations facilitate several kinds of transfer.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1737,1769,,,,,,,,,,,,,,,,WOS:000282522000007,0
J,"Huang, FL; Hsieh, CJ; Chang, KW; Lin, CJ",,,,"Huang, Fang-Lan; Hsieh, Cho-Jui; Chang, Kai-Wei; Lin, Chih-Jen",,,Iterative Scaling and Coordinate Descent Methods for Maximum Entropy Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum entropy (Maxent) is useful in natural language processing and many other areas. Iterative scaling (IS) methods are one of the most popular approaches to solve Maxent. With many variants of IS methods, it is difficult to understand them and see the differences. In this paper, we create a general and unified framework for iterative scaling methods. This framework also connects iterative scaling and coordinate descent methods. We prove general convergence results for IS methods and analyze their computational complexity. Based on the proposed framework, we extend a coordinate descent method for linear SVM to Maxent. Results show that it is faster than existing iterative scaling methods.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,815,848,,,,,,,,,,,,,,,,WOS:000277186500015,0
J,"Madani, O; Connor, M; Greiner, W",,,,"Madani, Omid; Connor, Michael; Greiner, Wiley",,,Learning When Concepts Abound,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many learning tasks, such as large-scale text categorization and word prediction, can benefit from efficient training and classification when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classification using perceptrons and support vector machines. We find that index learning is highly advantageous for space and time efficiency, at both training and classification times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classification take O(dl log(dl)).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2571,2613,,,,,,,,,,,,,,,,WOS:000272346600006,0
J,"Xiang, DH; Zhou, DX",,,,"Xiang, Dao-Hong; Zhou, Ding-Xuan",,,Classification with Gaussians and Convex Loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers binary classification algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassification error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis.",,,,,"Zhou, Ding-Xuan/B-3160-2013","Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1447,1468,,,,,,,,,,,,,,,,WOS:000270825000005,0
J,"Martins, AFT; Smith, NA; Xing, EP; Aguiar, PMQ; Figueiredo, MAT",,,,"Martins, Andre F. T.; Smith, Noah A.; Xing, Eric P.; Aguiar, Pedro M. Q.; Figueiredo, Mario A. T.",,,Nonextensive Information Theoretic Kernels on Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Positive definite kernels on probability measures have been recently applied to classification problems involving text, images, and other types of structured data. Some of these kernels are related to classic information theoretic quantities, such as (Shannon's) mutual information and the Jensen-Shannon (JS) divergence. Meanwhile, there have been recent advances in nonextensive generalizations of Shannon's information theory. This paper bridges these two trends by introducing nonextensive information theoretic kernels on probability measures, based on new JS-type divergences. These new divergences result from extending the the two building blocks of the classical JS divergence: convexity and Shannon's entropy. The notion of convexity is extended to the wider concept of q-convexity, for which we prove a Jensen q-inequality. Based on this inequality, we introduce Jensen-Tsallis (JT) q-differences, a nonextensive generalization of the JS divergence, and define a k-th order JT q-difference between stochastic processes. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, JS, and linear kernels as particular cases. Nonextensive string kernels are also defined that generalize the p-spectrum kernel. We illustrate the performance of these kernels on text categorization tasks, in which documents are modeled both as bags of words and as sequences of characters.",,,,,"Figueiredo, Mario/C-5428-2008","Figueiredo, Mario/0000-0002-0970-7745",,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,935,975,,,,,,,,,,,,,,,,WOS:000270824600006,0
J,"Langford, J; Li, LH; Zhang, T",,,,"Langford, John; Li, Lihong; Zhang, Tong",,,Sparse Online Learning via Truncated Gradient,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous-a parameter controls the rate of sparsification from no sparsification to total sparsification. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L-1-regularization method in the batch setting. We prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and find for data sets with large numbers of features, substantial sparsity is discoverable.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,777,801,,,,,,,,,,,,,,,,WOS:000270824500010,0
J,"Ma, ZM; Xie, XC; Geng, Z",,,,"Ma, Zongming; Xie, Xianchao; Geng, Zhi",,,Structural Learning of Chain Graphs via Decomposition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2847,2880,,,,,,,,,,,19759856,,,,,WOS:000263240700008,0
J,"Christmann, A; Van Messem, A",,,,"Christmann, Andreas; Van Messem, Arnout",,,Bouligand derivatives and robustness of support vector machines for regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in infinite dimensional Hilbert spaces. Leading examples are the support vector machine based on the e-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand influence function (BIF) a modification of F. R. Hampel's influence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel's influence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on influence functions.",,,,,"Christmann, Andreas/R-3542-2019; Van Messem, Arnout/B-4814-2015","Christmann, Andreas/0000-0002-8408-3549; Van Messem, Arnout/0000-0001-8545-7437",,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,915,936,,,,,,,,,,,,,,,,WOS:000258645300004,0
J,"Banerjee, O; El Ghaoui, L; d'Aspremont, A",,,,"Banerjee, Onureena; El Ghaoui, Laurent; d'Aspremont, Alexandre",,,Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added l(1)-norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our first algorithm uses block coordinate descent, and can be interpreted as recursive l(1)-norm penalized regression. Our second algorithm, based on Nesterov's first order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,485,516,,,,,,,,,,,,,,,,WOS:000256642000005,0
J,"Szabo, Z; Poczos, B; Lorincz, A",,,,"Szabo, Zoltan; Poczos, Barnabas; Lorincz, Andras",,,Undercomplete blind subspace deconvolution,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated 'high dimensional' ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efficiently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods.",,,,,"Lorincz, Andras/H-4125-2012","Lorincz, Andras/0000-0002-1280-3447",,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1063,1095,,,,,,,,,,,,,,,,WOS:000248351700006,0
J,"Tibshirani, R; Hastie, T",,,,"Tibshirani, Robert; Hastie, Trevor",,,Margin trees for high-dimensional classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a method for the classification of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classifier at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the margin tree to the closely related all-pairs ( one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We find that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,637,652,,,,,,,,,,,,,,,,WOS:000247002700009,0
J,"Singliar, T; Hauskrecht, M",,,,"Singliar, Tomas; Hauskrecht, Milos",,,Noisy-OR component analysis and its application to link analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a new component analysis framework, the Noisy-Or Component Analyzer (NOCA), that targets high-dimensional binary data. NOCA is a probabilistic latent variable model that assumes the expression of observed high-dimensional binary data is driven by a small number of hidden binary sources combined via noisy-or units. The component analysis procedure is equivalent to learning of NOCA parameters. Since the classical EM formulation of the NOCA learning problem is intractable, we develop its variational approximation. We test the NOCA framework on two problems: (1) a synthetic image-decomposition problem and (2) a co-citation data analysis problem for thousands of CiteSeer documents. We demonstrate good performance of the new model on both problems. In addition, we contrast the model to two mixture-based latent-factor models: the probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA). Differing assumptions underlying these models cause them to discover different types of structure in co-citation data, thus illustrating the benefit of NOCA in building our understanding of high-dimensional data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2189,2213,,,,,,,,,,,,,,,,WOS:000245390500009,0
J,"Roverato, A; Studeny, M",,,,"Roverato, Alberto; Studeny, Milan",,,A graphical representation of equivalence classes of AMP chain graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper deals with chain graph models under alternative AMP interpretation. A new representative of an AMP Markov equivalence class, called the largest deflagged graph, is proposed. The representative is based on revealed internal structure of the AMP Markov equivalence class. More specifically, the AMP Markov equivalence class decomposes into finer strong equivalence classes and there exists a distinguished strong equivalence class among those forming the AMP Markov equivalence class. The largest deflagged graph is the largest chain graph in that distinguished strong equivalence class. A composed graphical procedure to get the largest deflagged graph on the basis of any AMP Markov equivalent chain graph is presented. In general, the largest deflagged graph differs from the AMP essential graph, which is another representative of the AMP Markov equivalence class.",,,,,"Studeny, Milan/H-2719-2014; Roverato, Alberto/M-1372-2014","Studeny, Milan/0000-0001-6038-629X; ",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1045,1078,,,,,,,,,,,,,,,,WOS:000245388400007,0
J,"Sugiyama, M",,,,"Sugiyama, M",,,Active learning in approximately linear regression based on conditional expectation of generalization error,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly specified. In many practical situations, however, this assumption may not be fulfilled. Recently, active learning methods using importance-weighted least-squares learning have been proposed, which are shown to be robust against misspecification of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE ( Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be fine-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,JAN,2006,7,,,,,,141,166,,,,,,,,,,,,,,,,WOS:000236331400006,0
J,"Aiolli, F; Sperduti, A",,,,"Aiolli, F; Sperduti, A",,,Multiclass classification with multi-prototype support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Winner-take-all multiclass classifiers are built on the top of a set of prototypes each representing one of the available classes. A pattern is then classified with the label associated to the most 'similar' prototype. Recent proposal of SVM extensions to multiclass can be considered instances of the same strategy with one prototype per class. The multi-prototype SVM proposed in this paper extends multiclass SVM to multiple prototypes per class. It allows to combine several vectors in a principled way to obtain large margin decision functions. For this problem, we give a compact constrained quadratic formulation and we propose a greedy optimization algorithm able to find locally optimal solutions for the non convex objective function. This algorithm proceeds by reducing the overall problem into a series of simpler convex problems. For the solution of these reduced problems an efficient optimization algorithm is proposed. A number of pattern selection strategies are then discussed to speed-up the optimization process. In addition, given the combinatorial nature of the overall problem, stochastic search strategies are suggested to escape from local minima which are not globally optimal. Finally, we report experiments on a number of datasets. The performance obtained using few simple linear prototypes is comparable to that obtained by state-of-the-art kernel-based methods but with a significant reduction (of one or two orders) in response time.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2005,6,,,,,,817,850,,,,,,,,,,,,,,,,WOS:000236329700004,0
J,"Greensmith, E; Bartlett, PL; Baxter, J",,,,"Greensmith, E; Bartlett, PL; Baxter, J",,,Variance reduction techniques for gradient estimates in reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.",,,,,,"Bartlett, Peter/0000-0002-8760-3140",,,,,,,,,,,,,1532-4435,,,,,NOV,2004,5,,,,,,1471,1530,,,,,,,,,,,,,,,,WOS:000236328400004,0
J,"Basalyga, G; Rattray, M",,,,"Basalyga, G; Rattray, M",,,Statistical dynamics of on-line independent component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The learning dynamics of on-line independent component analysis is analysed in the limit of large data dimension. We study a simple Hebbian learning algorithm that can be used to separate out a small number of non-Gaussian components from a high-dimensional data set. The de-mixing matrix parameters are confined to a Stiefel manifold of tall, orthogonal matrices and we introduce a natural gradient variant of the algorithm which is appropriate to learning on this manifold. For large input dimension the parameter trajectory of both algorithms passes through a sequence of unstable fixed points, each described by a diffusion process in a polynomial potential. Choosing the learning rate too large increases the escape time from each of these fixed points, effectively trapping the learning in a sub-optimal state. In order to avoid these trapping states a very low learning rate must be chosen during the learning transient, resulting in learning time-scales of O(N-2) or O(N-3) iterations where N is the data dimension. Escape from each sub-optimal state results in a sequence of symmetry breaking events as the algorithm learns each source in turn. This is in marked contrast to the learning dynamics displayed by related on-line learning algorithms for multilayer neural networks and principal component analysis. Although the natural gradient variant of the algorithm has nice asymptotic convergence properties, it has an equivalent transient dynamics to the standard Hebbian algorithm.",,,,,"Rattray, Magnus/B-4393-2009","Rattray, Magnus/0000-0001-8196-5565",,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1393,1410,,,,,,,,,,,,,,,,WOS:000224808300011,0
J,"Bounkong, S; Toch, B; Saad, D; Lowe, D",,,,"Bounkong, S; Toch, B; Saad, D; Lowe, D",,,ICA for watermarking digital images,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a domain-independent ICA-based approach to watermarking. This approach can be used on images, music or video to embed either a robust or fragile watermark. In the case of robust watermarking, the method shows high information rate and robustness against malicious and nonmalicious attacks, while keeping a low induced distortion. The fragile watermarking scheme, on the other hand, shows high sensitivity to tampering attempts while keeping the requirement for high information rate and low distortion. The improved performance is achieved by employing a set of statistically independent sources (the independent components) as the feature space and principled statistical decoding methods. The performance of the suggested method is compared to other state of the art approaches. The paper focuses on applying the method to digitized images although the same approach can be used for other media, such as music or video.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1471,1496,,,,,,,,,,,,,,,,WOS:000224808300014,0
J,"Kisilev, P; Zibulevsky, M; Zeevi, YY",,,,"Kisilev, P; Zibulevsky, M; Zeevi, YY",,,A multiscale framework for blind separation of linearly mixed signals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of blind separation of unknown source signals or images from a given set of their linear mixtures. It was discovered recently that exploiting the sparsity of sources and their mixtures, once they are projected onto a proper space of sparse representation, improves the quality of separation. In this study we take advantage of the properties of multiscale transforms, such as wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We then study how the separation error is affected by the sparsity of decomposition coefficients, and by the misfit between the probabilistic model of these coefficients and their actual distribution. Our error estimator, based on the Taylor expansion of the quasi-ML function, is used in selection of the best subsets of coefficients and utilized, in turn, in further separation. The performance of the algorithm is evaluated by using noise-free and noisy data. Experiments with simulated signals, musical sounds and images, demonstrate significant improvement of separation quality over previously reported results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1339,1363,,,,,,,,,,,,,,,,WOS:000224808300009,0
J,"Claveau, V; Sebillot, P; Fabre, U; Bouillon, P",,,,"Claveau, V; Sebillot, P; Fabre, U; Bouillon, P",,,Learning semantic lexicons from a part-of-speech and semantically tagged corpus using inductive logic programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,10th International Conference on Inductive Logic Programming (ILP2000),"JUL, 2000","London, ENGLAND",,,,,"This paper describes an inductive logic programming learning method designed to acquire from a corpus specific Noun-Verb (N-V) pairs-relevant in information retrieval applications to perform index expansion-in order to build up semantic lexicons based on Pustejovsky's generative lexicon (GL) principles (Pustejovsky, 1995). In one of the components of this lexical model, called the qualia structure, words are described in terms of semantic roles. For example, the telic role indicates the purpose or function of an item (cut for knife), the agentive role its creation mode (build for house), etc. The qualia structure of a noun is mainly made up of verbal associations, encoding relational information. The learning method enables us to automatically extract, from a morpho-syntactically and semantically tagged corpus, N-V pairs whose elements are linked by one of the semantic relations defined in the qualia structure in GL. It also infers rules explaining what in the surrounding context distinguishes such pairs from others also found in sentences of the corpus but which are not relevant. Stress is put here on the learning efficiency that is required to be able to deal with all the available contextual information, and to produce linguistically meaningful rules.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,493,525,,10.1162/153244304773936036,0,,,,,,,,,,,,,WOS:000221345700005,0
J,"Lewis, DD; Yang, YM; Rose, TG; Li, F",,,,"Lewis, DD; Yang, YM; Rose, TG; Li, F",,,RCV1: A new benchmark collection for text categorization research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reuters Corpus Volume I ( RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2004,5,,,,,,361,397,,,,,,,,,,,,,,,,WOS:000236327400003,0
J,"Califf, ME; Mooney, RJ",,,,"Califf, ME; Mooney, RJ",,,Bottom-up relational learning of pattern matching rules for information extraction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2004,4,2,,,,,177,210,,10.1162/153244304322972685,0,,,,,,,,,,,,,WOS:000221043600003,0
J,"Ling, CX; Zhang, HJ",,,,"Ling, CX; Zhang, HJ",,,The representational power of discrete Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MASSACHUSETTS",,,,,"One of the most important fundamental properties of Bayesian networks is the representational power, reflecting what kind of functions they can or cannot represent. In this paper, we establish an association between the structural complexity of Bayesian networks and their representational power. We use the maximum number of nodes' parents as the measure for the structural complexity of Bayesian networks, and the maximum XOR contained in a target function as the measure for the function complexity. A representational upper bound is established and proved. Roughly speaking, discrete Bayesian networks with each node having at most k parents cannot represent any function containing (k + 1)-XORs. Our theoretical results help us to gain a deeper understanding on the capacities and limitations of Bayesian networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,709,721,,10.1162/jmlr.2003.3.4-5.709,0,,,,,,,,,,,,,WOS:000184926200005,0
J,"Marx, Z; Dagan, I; Buhmann, JM; Shamir, E",,,,"Marx, Z; Dagan, I; Buhmann, JM; Shamir, E",,,Coupled clustering: A method for detecting structural correspondence,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"This paper proposes a new paradigm and a computational framework for revealing equivalencies (analogies) between sub-structures of distinct composite systems that are initially represented by unstructured data sets. For this purpose, we introduce and investigate a variant of traditional data clustering, termed coupled clustering, which outputs a configuration of corresponding subsets of two such representative sets. We apply our method to synthetic as well as textual data. Its achievements in detecting topical correspondences between textual corpora are evaluated through comparison to performance of human experts.",,,,,"Buhmann, Joachim/AAU-4760-2020",,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,747,780,,10.1162/jmlr.2003.3.4-5.747,0,,,,,,,,,,,,,WOS:000184926200007,0
J,"Crammer, K; Singer, Y",,,,"Crammer, K; Singer, Y",,,On the algorithmic implementation of multiclass kernel-based vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,265,292,,10.1162/15324430260185628,0,,,,,,,,,,,,,WOS:000176055300009,0
J,"Paren, A; Berrada, L; Poudel, RPK; Kumar, MP",,,,"Paren, Alasdair; Berrada, Leonard; Poudel, Rudra P. K.; Kumar, M. Pawan",,,A Stochastic Bundle Method for Interpolating Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel method for training deep neural networks that are capable of interpolation, that is, driving the empirical loss to zero. At each iteration, our method constructs a stochastic approximation of the learning objective. The approximation, known as a bundle, is a pointwise maximum of linear functions. Our bundle contains a constant function that lower bounds the empirical loss. This enables us to compute an automatic adaptive learning rate, thereby providing an accurate solution. In addition, our bundle includes linear approximations computed at the current iterate and other linear estimates of the DNN parameters. The use of these additional approximations makes our method significantly more robust to its hyperparameters. Based on its desirable empirical properties, we term our method Bundle Optimisation for Robust and Accurate Training (BORAT). In order to operationalise BORAT, we design a novel algorithm for optimising the bundle approximation efficiently at each iteration. We establish the theoretical convergence of BORAT in both convex and non-convex settings. Using standard publicly available data sets, we provide a thorough comparison of BORAT to other single hyperparameter optimisation algorithms. Our experiments demonstrate BORAT matches the state-of-the-art generalisation performance for these methods and is the most robust.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,57,,,,,,,,,,,,,,,,WOS:000752366500001,0
J,"Subramanian, J; Sinha, A; Seraj, R; Mahajan, A",,,,"Subramanian, Jayakumar; Sinha, Amit; Seraj, Raihan; Mahajan, Aditya",,,Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a theoretical framework for approximate planning and learning in partially observed systems. Our framework is based on the fundamental notion of information state. We provide two definitions of information state i) a function of history which is sufficient to compute the expected reward and predict its next value; ii) a function of the history which can be recursively updated and is sufficient to compute the expected reward and predict the next observation. An information state always leads to a dynamic programming decomposition. Our key result is to show that if a function of the history (called approximate information state (AIS)) approximately satisfies the properties of the information state, then there is a corresponding approximate dynamic program. We show that the policy computed using this is approximately optimal with bounded loss of optimality. We show that several approximations in state, observation and action spaces in literature can be viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A salient feature of AIS is that it can be learnt from data. We present AIS based multi-time scale policy gradient algorithms and detailed numerical experiments with low, moderate and high dimensional environments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,83,,,,,,,,,,,,,,,,WOS:000752278200001,0
J,"Davis, D; Drusvyatskiy, D; Xiao, L; Zhang, JY",,,,"Davis, Damek; Drusvyatskiy, Dmitriy; Xiao, Lin; Zhang, Junyu",,,From Low Probability to High Confidence in Stochastic Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard results in stochastic convex optimization bound the number of samples that an algorithm needs to generate a point with small function value in expectation. More nuanced high probability guarantees are rare, and typically either rely on light-tail noise assumptions or exhibit worse sample complexity. In this work, we show that a wide class of stochastic optimization algorithms for strongly convex problems can be augmented with high confidence bounds at an overhead cost that is only logarithmic in the confidence level and polylogarithmic in the condition number. The procedure we propose, called proxBoost, is elementary and builds on two well-known ingredients: robust distance estimation and the proximal point method. We discuss consequences for both streaming (online) algorithms and offline algorithms based on empirical risk minimization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500049,0
J,"Gourdeau, P; Kanade, V; Kwiatkowska, M; Worrell, J",,,,"Gourdeau, Pascale; Kanade, Varun; Kwiatkowska, Marta; Worrell, James",,,On the Hardness of Robust Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of adversarially robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show, moreover, that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb omega(log n) input bits. However, we also show that if the adversary is restricted to perturbing O(log n) bits, then one can robustly learn the class of 1-decision lists (which subsumes monotone conjunctions) with respect to the class of log-Lipschitz distributions. We then extend this result to show learnability of 2-decision lists and monotone k-decision lists in the same distributional and adversarial setting. Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on a more restricted model of learning, such as the statistical query model, nor on any hardness assumption other than the existence of an (average-case) hard learning problem in the PAC framework; this allows us to have a clean proof of the reduction, and the assumption is no stronger than assumptions that are used to build cryptographic primitives.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000726667000001,0
J,"Hao, ML; Qu, LQ; Kong, DH; Sun, LQ; Zhu, HT",,,,"Hao, Meiling; Qu, Lianqiang; Kong, Dehan; Sun, Liuquan; Zhu, Hongtu",,,Optimal Minimax Variable Selection for Large-Scale Matrix Linear Regression Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale matrix linear regression models with high-dimensional responses and high dimensional variables have been widely employed in various large-scale biomedical studies. In this article, we propose an optimal minimax variable selection approach for the matrix linear regression model when the dimensions of both the response matrix and predictors diverge at the exponential rate of the sample size. We develop an iterative hard-thresholding algorithm for fast computation and establish an optimal minimax theory for the parameter estimates. The finite sample performance of the method is examined via extensive simulation studies and a real data application from the Alzheimer's Disease Neuroimaging Initiative study is provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687116300001,0
J,"Janizek, JD; Sturmfels, P; Lee, SI",,,,"Janizek, Joseph D.; Sturmfels, Pascal; Lee, Su-In",,,Explaining Explanations: Axiomatic Feature Interactions for Deep Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain the features that are important to a model's prediction on a given input. However, for many tasks, simply identifying significant features may be insufficient for understanding model behavior. The interactions between features within the model may better explain not only the model, but why certain features outrank others in importance. In this work, we present Integrated Hessians, an extension of Integrated Gradients (Sundararajan et al., 2017) that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods, and unlike them, is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,104,,,,,,,,,,,,,,,WOS:000663159500001,0
J,"Kratsios, A; Hyndman, C",,,,"Kratsios, Anastasis; Hyndman, Cody",,,NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Effective feature representation is key to the predictive performance of any algorithm. This paper introduces a meta-procedure, called Non-Euclidean Upgrading (NEU), which learns feature maps that are expressive enough to embed the universal approximation property (UAP) into most model classes while only outputting feature maps that preserve any model class's UAP. We show that NEU can learn any feature map with these two properties if that feature map is asymptotically deformable into the identity. We also find that the feature-representations learned by NEU are always submanifolds of the feature space. NEU's properties are derived from a new deep neural model that is universal amongst all orientation-preserving homeomorphisms on the input space. We derive qualitative and quantitative approximation guarantees for this architecture. We quantify the number of parameters required for this new architecture to memorize any set of input-output pairs while simultaneously fixing every point of the input space lying outside some compact set, and we quantify the size of this set as a function of our model's depth. Moreover, we show that deep feed-forward networks with most commonly used activation functions typically do not have all these properties. NEU's performance is evaluated against competing machine learning methods on various regression and dimension reduction tasks both with financial and simulated data.",,,,,,"Kratsios, Anastasis/0000-0001-6791-3371",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,92,,,,,,,,,,,,,,,WOS:000663146800001,0
J,"Levin, KD; Roosta, F; Tang, M; Mahoney, MW; Priebe, CE",,,,"Levin, Keith D.; Roosta, Fred; Tang, Minh; Mahoney, Michael W.; Priebe, Carey E.",,,Limit theorems for out-of-sample extensions of the adjacency and Laplacian spectral embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph embeddings, a class of dimensionality reduction techniques designed for relational data, have proven useful in exploring and modeling network structure. Most dimensionality reduction methods allow out-of-sample extensions, by which an embedding can be applied to observations not present in the training set. Applied to graphs, the out-of-sample extension problem concerns how to compute the embedding of a vertex that is added to the graph after an embedding has already been computed. In this paper, we consider the out-of-sample extension problem for two graph embedding procedures: the adjacency spectral embedding and the Laplacian spectral embedding. In both cases, we prove that when the underlying graph is generated according to a latent space model called the random dot product graph, which includes the popular stochastic block model as a special case, an out-of-sample extension based on a least-squares objective obeys a central limit theorem. In addition, we prove a concentration inequality for the out-of-sample extension of the adjacency spectral embedding based on a maximum-likelihood objective. Our results also yield a convenient framework in which to analyze trade-offs between estimation accuracy and computational expenses, which we explore briefly. Finally, we explore the performance of these out-of-sample extensions as applied to both simulated and real-world data. We observe significant computational savings with minimal losses to the quality of the learned embeddings, in keeping with our theoretical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,59,,,,,,,,,,,,,,,,WOS:000712030700001,0
J,"Ye, HS; Zhang, T",,,,"Ye, Haishan; Zhang, Tong",,,DeEPCA: Decentralized Exact PCA with Linear Convergence Rate,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Due to the rapid growth of smart agents such as weakly connected computational nodes and sensors, developing decentralized algorithms that can perform computations on local agents becomes a major research direction. This paper considers the problem of decentralized principal components analysis (PCA), which is a statistical method widely used for data analysis. We introduce a technique called subspace tracking to reduce the communication cost, and apply it to power iterations. This leads to a decentralized PCA algorithm called DeEPCA, which has a convergence rate similar to that of the centralized PCA, while achieving the best communication complexity among existing decentralized PCA algorithms. DeEPCA is the first decentralized PCA algorithm with the number of communication rounds for each power iteration independent of target precision. Compared to existing algorithms, the proposed method is easier to tune in practice, with an improved overall communication cost. Our experiments validate the advantages of DeEPCA empirically.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706875700001,0
J,"Binette, O; Pati, D; Dunson, DB",,,,"Binette, Olivier; Pati, Debdeep; Dunson, David B.",,,Bayesian Closed Surface Fitting Through Tensor Products,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Closed surfaces provide a useful model for 3-d shapes, with the data typically consisting of a cloud of points in R-3. The existing literature on closed surface modeling focuses on frequentist point estimation methods that join surface patches along the edges, with surface patches created via Bezier surfaces or tensor products of B-splines. However, the resulting surfaces are not smooth along the edges and the geometric constraints required to join the surface patches lead to computational drawbacks. In this article, we develop a Bayesian model for closed surfaces based on tensor products of a cyclic basis resulting in infinitely smooth surface realizations. We impose sparsity on the control points through a double-shrinkage prior. Theoretical properties of the support of our proposed prior are studied and it is shown that the posterior achieves the optimal rate of convergence under reasonable assumptions on the prior. The proposed approach is illustrated with some examples.",,,,,,"Binette, Olivier/0000-0001-6009-5206",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,26,119,,,,,,,,,,,,,,,WOS:000556188600001,0
J,"Bonald, T; de Lara, N; Lutz, Q; Charpentier, B",,,,"Bonald, Thomas; de Lara, Nathan; Lutz, Quentin; Charpentier, Bertrand",,,Scikit-network: Graph Analysis in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online(1).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,185,,,,,,,,,,,,,,,WOS:000570236600001,0
J,"Hocking, TD; Rigaill, G; Fearnhead, P; Bourque, G",,,,"Hocking, Toby Dylan; Rigaill, Guillem; Fearnhead, Paul; Bourque, Guillaume",,,Constrained Dynamic Programming and Supervised Penalty Learning Algorithms for Peak Detection in Genomic Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Peak detection in genomic data involves segmenting counts of DNA sequence reads aligned to different locations of a chromosome. The goal is to detect peaks with higher counts, and filter out background noise with lower counts. Most existing algorithms for this problem are unsupervised heuristics tailored to patterns in specific data types. We propose a supervised framework for this problem, using optimal changepoint detection models with learned penalty functions. We propose the first dynamic programming algorithm that is guaranteed to compute the optimal solution to changepoint detection problems with constraints between adjacent segment mean parameters. Implementing this algorithm requires the choice of penalty parameter that determines the number of segments that are estimated. We show how the supervised learning ideas of Rigaill et al. (2013) can be used to choose this penalty. We compare the resulting implementation of our algorithm to several baselines in a benchmark of labeled ChIP-seq data sets with two different patterns (broad H3K36me3 data and sharp H3K4me3 data). Whereas baseline unsupervised methods only provide accurate peak detection for a single pattern, our supervised method achieves state-of-the-art accuracy in all data sets. The log-linear timings of our proposed dynamic programming algorithm make it scalable to the large genomic data sets that are now common. Our implementation is available in the PeakSegOptimal R package on CRAN.",,,,,"Bourque, Guillaume/AFR-2927-2022; Fearnhead, Paul/A-5938-2015","Bourque, Guillaume/0000-0002-3933-9656; Rigaill, Guillem/0000-0002-7176-7511; Fearnhead, Paul/0000-0002-9386-2341",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,87,,,,,,,,,,,,,,,WOS:000545026500001,0
J,"Samanta, B; De, A; Jana, G; Gomez, V; Chattaraj, PK; Ganguly, N; Gomez-Rodriguez, M",,,,"Samanta, Bidisha; De, Abir; Jana, Gourhari; Gomez, Vicenc; Chattaraj, Pratim Kumar; Ganguly, Niloy; Gomez-Rodriguez, Manuel",,,NEVAE: A Deep Generative Model for Molecular Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep generative models have been praised for their ability to learn smooth latent representations of images, text, and audio, which can then be used to generate new, plausible data. Motivated by these success stories, there has been a surge of interest in developing deep generative models for automated molecule design. However, these models face several difficulties due to the unique characteristics of molecular graphs-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes' labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given any arbitrary molecule, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning.",,,,,"Jana, Gourhari/AAW-7361-2020; G√≥mez, Vicen√ß/D-1984-2009; Rodriguez, Manuel Gomez/AAB-5005-2021","Jana, Gourhari/0000-0001-7864-8489; G√≥mez, Vicen√ß/0000-0001-5146-7645; Gomez Rodriguez, Manuel/0000-0003-3930-1161; Ganguly, Niloy/0000-0002-3967-186X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,114,,,,,,,,,,,,,,,WOS:000546630500001,0
J,"Tino, P",,,,"Tino, Peter",,,Dynamical Systems as Temporal Feature Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input-driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000019,0
J,"Xu, YC; Balakrishnan, S; Singh, A; Dubrawski, A",,,,"Xu, Yichong; Balakrishnan, Sivaraman; Singh, Aarti; Dubrawski, Artur",,,Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In supervised learning, we typically leverage a fully labeled dataset to design methods for function estimation or prediction. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. In this paper, we consider a semi-supervised regression setting, where we obtain additional ordinal (or comparison) information for the unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in both nonparametric and linear regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively escaping the curse of dimensionality. We also present lower bounds, that establish fundamental limits for the task and show that our algorithms are optimal in a variety of settings. Finally, we present extensive experiments on new datasets that demonstrate the efficacy and practicality of our algorithms and investigate their robustness to various sources of noise and model misspecification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,162,,,,,,,,,,,,,,,WOS:000570150700001,0
J,"Dang, KD; Quiroz, M; Kohn, R; Tran, MN; Villani, M",,,,"Dang, Khue-Dung; Quiroz, Matias; Kohn, Robert; Minh-Ngoc Tran; Villani, Mattias",,,Hamiltonian Monte Carlo with Energy Conserving Subsampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large data sets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that use gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients.",,,,,"Villani, Mattias/AAM-7969-2020; Quiroz, Matias/AAE-4127-2020","Quiroz, Matias/0000-0002-4400-9184; Kohn, Robert/0000-0002-3733-1474",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,100,,,,,,,,,,,,,,,WOS:000476621700001,0
J,"Park, G; Park, S",,,,"Park, Gunwoong; Park, Sion",,,High-Dimensional Poisson Structural Equation Model Learning via l(1)-Regularized Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we develop a new approach to learning high-dimensional Poisson structural equation models from only observational data without strong assumptions such as faithfulness and a sparse moralized graph. A key component of our method is to decouple the ordering estimation or parent search where the problems can be efficiently addressed using l(1)-regularized regression and the moments relation. We show that sample size n = Omega(d(2) log(9) p) is sufficient for our polynomial time Moments Ratio Scoring (MRS) algorithm to recover the true directed graph, where p is the number of nodes and d is the maximum indegree. We verify through simulations that our algorithm is statistically consistent in the high-dimensional p > n setting, and performs well compared to state-of-the-art ODS, GES, and MMHC algorithms. We also demonstrate through multivariate real count data that our MRS algorithm is well-suited to estimating DAG models for multivariate count data in comparison to other methods used for discrete data.",,,,,"Park, Gunwoong/AAA-5853-2020",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,95,,,,,,,,,,,,,,,WOS:000470908700001,0
J,"Shang, ZF; Hao, BT; Cheng, G",,,,"Shang, Zuofeng; Hao, Botao; Cheng, Guang",,,Nonparametric Bayesian Aggregation for Massive Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a set of scalable Bayesian inference procedures for a general class of nonparametric regression models. Specifically, nonparametric Bayesian inferences are separately performed on each subset randomly split from a massive dataset, and then the obtained local results are aggregated into global counterparts. This aggregation step is explicit without involving any additional computation cost. By a careful partition, we show that our aggregated inference results obtain an oracle rule in the sense that they are equivalent to those obtained directly from the entire data (which are computationally prohibitive). For example, an aggregated credible ball achieves desirable credibility level and also frequentist coverage while possessing the same radius as the oracle ball.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,140,,,,,,,,,,,,,,,WOS:000491132200004,0
J,"Buccapatnam, S; Liu, F; Eryilmaz, A; Shroff, NB",,,,"Buccapatnam, Swapna; Liu, Fang; Eryilmaz, Atilla; Shroff, Ness B.",,,Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the stochastic multi-armed bandit (MAB) problem in the presence of side-observations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policies a randomized policy; and a policy based on the well-known upper confidence bound (UCB) policies-both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,18,,,,,,,,,,,,,,,WOS:000435629100001,0
J,"Derezinski, M; Warmuth, MK",,,,"Derezinski, Michal; Warmuth, Manfred K.",,,Reverse Iterative Volume Sampling for Linear Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the following basic machine learning task: Given a fixed set of input points in lk d for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the remaining hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension d many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all n responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithm for volume sampling which makes this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,39,23,,,,,,,,,,,,,,,WOS:000443228800001,0
J,"Durand, A; Maillard, OA; Pineau, J",,,,"Durand, Audrey; Maillard, Odalric-Ambrym; Pineau, Joelle",,,"Streaming kernel regression with provably adaptive mean, variance, and regularization",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of streaming kernel regression, when the observations arrive sequentially and the goal is to recover the underlying mean function, assumed to belong to an RKHS. The variance of the noise is not assumed to be known. In this context, we tackle the problem of tuning the regularization parameter adaptively at each time step, while maintaining tight confidence bounds estimates on the value of the mean function at each point. To this end, we first generalize existing results for finite-dimensional linear regression with fixed regularization and known variance to the kernel setup with a regularization parameter allowed to be a measurable function of past observations. Then, using appropriate self-normalized inequalities we build upper and lower bound estimates for the variance, leading to Bernstein-like concentration bounds. The latter is used in order to define the adaptive regularization. The bounds resulting from our technique are valid uniformly over all observation points and all time steps, and are compared against the literature with numerical experiments. Finally, the potential of these tools is illustrated by an application to kernelized bandits, where we revisit the Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of the novel adaptive kernel tuning strategy.",,,,,,"Salguero Tejada, Carlos/0000-0003-0930-9277",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,17,,,,,,,,,,,,,,,WOS:000443226600001,0
J,"Heusser, AC; Ziman, K; Owen, LLW; Manning, JR",,,,"Heusser, Andrew C.; Ziman, Kirsten; Owen, Lucy L. W.; Manning, Jeremy R.",,,HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dimensionality reduction algorithms have played a foundational role in facilitating the deep understanding of complex high-dimensional data. One particularly useful application of dimensionality reduction techniques is in data visualization. Low-dimensional visualizations can help practitioners understand where machine learning algorithms might leverage the geometric properties of a dataset to improve performance. Another challenge is to generalize insights across datasets [e.g. data from multiple modalities describing the same system (Haxby et al., 2011), artwork or photographs of similar content in different styles (Zhu et al., 2017), etc.]. Several recently developed techniques (e.g. Haxby et al., 2011; Chen et al., 2015) use the procrustean transformation (Schonemann, 1966) to align the geometries of two or more spaces so that data with different axes may be plotted in a common space. We propose that each of these techniques (dimensionality reduction, alignment, and visualization) applied in sequence should be cast as a single conceptual hyperplot operation for gaining geometric insights into high-dimensional data. Our Python toolbox enables this operation in a single (highly flexible) function call.",,,,,,"Ziman, Kirsten/0000-0002-8942-3362",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,152,,,,,,,,,,,,,,,WOS:000431708700001,0
J,"Park, C; Apley, D",,,,"Park, Chiwoo; Apley, Daniel",,,Patchwork Kriging for Large-scale Gaussian Process Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the prediction accuracy of existing local partitioned GP methods over regional boundaries. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches.",,,,,"Park, Chiwoo/ABA-4876-2021; Apley, Daniel/B-7454-2009","Park, Chiwoo/0000-0002-2463-8901; ",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,7,,,,,,,,,,,,,,,WOS:000443222900001,0
J,"Tsakiris, MC; Vidal, R",,,,"Tsakiris, Manolis C.; Vidal, Rene",,,Dual Principal Component Pursuit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative to the ambient dimension. Our approach works with a dual representation of the subspace and hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces whose dimension is close to the ambient dimension (subspaces of high relative dimension). We pose the problem of computing normal vectors to the inlier subspace as a non-convex l(1) minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution to DPCP is a vector in the orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem to a recursion of linear programs whose solutions are shown to converge in a finite number of steps to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the solutions to the recursion of linear programs converge to the global minimum of the non-convex DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the context of the three-view geometry problem in computer vision suggest that the proposed methods can be a useful or even superior alternative to traditional RANSAC-based approaches for computer vision and other applications.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,18,,,,,,,,,,,,,,,WOS:000443227200001,0
J,"Zhang, Q; Zhou, MY",,,,"Zhang, Quan; Zhou, Mingyuan",,,Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance.",,,,,"Zhou, Mingyuan/AAE-8717-2021",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000435627500001,0
J,"Zhang, T; Yang, Y",,,,"Zhang, Teng; Yang, Yi",,,Robust PCA by Manifold Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Robust PCA is a widely used statistical procedure to recover an underlying low-rank matrix with grossly corrupted observations. This work considers the problem of robust PCA as a nonconvex optimization problem on the manifold of low-rank matrices and proposes two algorithms based on manifold optimization. It is shown that, with a properly designed initialization, the proposed algorithms are guaranteed to converge to the underlying lowrank matrix linearly. Compared with a previous work based on the factorization of low-rank matrices Yi et al. (2016), the proposed algorithms reduce the dependence on the condition number of the underlying low-rank matrix theoretically. Simulations and real data examples con fi rm the competitive performance of our method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454480000001,0
J,"Kundu, A; Drineas, P; Magdon-Ismail, M",,,,"Kundu, Abhisek; Drineas, Petros; Magdon-Ismail, Malik",,,"Recovering PCA and Sparse PCA via Hybrid-(l(1),l(2)) Sparse Sampling of Data Elements",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares (l(2) sampling) and absolute values (l(1) sampling) of the entries. We prove that this hybrid algorithm (i) achieves a near-PCA reconstruction of the data, and (ii) recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-(l(1);l(2)) inherits the l(2)-ability to sample the important elements, as well as the regularization properties of l(1) sampling, and maintains strictly better quality than either l(1) or l(2) on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch",,,,,,"Drineas, Petros/0000-0003-1994-8670",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000412061200001,0
J,"Schiratti, JB; Allassonniere, S; Colliot, O; Durrleman, S",,,,"Schiratti, Jean-Baptiste; Allassonniere, Stephanie; Colliot, Olivier; Durrleman, Stanley",,,A Bayesian Mixed-Effects Model to Learn Trajectories of Changes from Repeated Manifold-Valued Observations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a generic Bayesian mixed-effects model to estimate the temporal progression of a biological phenomenon from observations obtained at multiple time points for a group of individuals. The progression is modeled by continuous trajectories in the space of measurements. Individual trajectories of progression result from spatiotemporal transformations of an average trajectory. These transformations allow for the quantification of changes in direction and pace at which the trajectories are followed. The framework of Riemannian geometry allows the model to be used with any kind of measurements with smooth constraints. A stochastic version of the Expectation-Maximization algorithm is used to produce maximum a posteriori estimates of the parameters. We evaluated our method using a series of neuropsychological test scores from patients with mild cognitive impairments, later diagnosed with Alzheimer's disease, and simulated evolutions of symmetric positive definite matrices. The data-driven model of impairment of cognitive functions illustrated the variability in the ordering and timing of the decline of these functions in the population. We showed that the estimated spatiotemporal transformations effectively put into correspondence significant events in the progression of individuals.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,133,,,,,,,,,,,,,,,WOS:000424546500001,0
J,"Sourati, J; Akcakaya, M; Leen, TK; Erdogmus, D; Dy, JG",,,,"Sourati, Jamshid; Akcakaya, Murat; Leen, Todd K.; Erdogmus, Deniz; Dy, Jennifer G.",,,Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,34,,,,,,,,,,,,,,,WOS:000400517500001,0
J,"Stucky, B; van de Geer, S",,,,"Stucky, Benjamin; van de Geer, Sara",,,Sharp Oracle Inequalities for Square Root Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector beta. Structured sparsity norms, as defined in Micchelli et al. (2010), are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al., 2011), the group square root LASSO (Bunea et al., 2014) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. 2015). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE.",,,,,,"Stucky, Benjamin/0000-0002-7425-2927",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,29,67,,,,,,,,,,,,,,,WOS:000412057600001,0
J,"Wainer, J; Cawley, G",,,,"Wainer, Jacques; Cawley, Gavin",,,Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often performed by minimising a resampling/cross-validation based model selection criterion, however there seems little practical guidance on the most suitable form of resampling. This paper presents the results of an extensive empirical evaluation of resampling procedures for SVM hyperparameter selection, designed to address this gap in the machine learning literature. We tested 15 different resampling procedures on 121 binary classification data sets in order to select the best SVM hyperparameters. We used three very different statistical procedures to analyse the results: the standard multi-classifier/multidata set procedure proposed by Demsar, the confidence intervals on the excess loss of each procedure in relation to 5-fold cross validation, and the Bayes factor analysis proposed by Barber. We conclude that a 2-fold procedure is appropriate to select the hyperparameters of an SVM for data sets for 1000 or more datapoints, while a 3-fold procedure is appropriate for smaller data sets.",,,,,"Wainer, Jacques/AAQ-6029-2021","Wainer, Jacques/0000-0001-5201-1244",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,15,,,,,,,,,,,,,,,WOS:000399838300001,0
J,"Yang, F; Balakrishnan, S; Wainwright, MJ",,,,"Yang, Fanny; Balakrishnan, Sivaraman; Wainwright, Martin J.",,,Statistical and Computational Guarantees for the Baum-Welch Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates and show geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,125,,,,,,,,,,,,,,,WOS:000424542300001,0
J,"de Montjoye, YA; Rocher, L; Pentland, A",,,,"de Montjoye, Yves-Alexandre; Rocher, Luc; Pentland, Alex 'Sandy'",,,bandicoot: a Python Toolbox for Mobile Phone Metadata,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"bandicoot is an open-source Python toolbox to extract more than 1442 features from standard mobile phone metadata. bandicoot makes it easy for machine learning researchers and practitioners to load mobile phone data, to analyze and visualize them, and to extract robust features which can be used for various classification and clustering tasks. Emphasis is put on ease of use, consistency, and documentation. bandicoot has no dependencies and is distributed under MIT license.",,,,,,"Rocher, Luc/0000-0002-9956-1187",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,175,,,,,,,,,,,,,,,WOS:000391679800001,0
J,"Odense, S; Edwards, R",,,,"Odense, Simon; Edwards, Roderick",,,Universal Approximation Results for the Temporal Restricted Boltzmann Machine and the Recurrent Temporal Restricted Boltzmann Machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Restricted Boltzmann Machine (RBM) has proved to be a powerful tool in machine learning, both on its own and as the building block for Deep Belief Networks (multi-layer generative graphical models). The RBM and Deep Belief Network have been shown to be universal approximators for probability distributions on binary vectors. In this paper we prove several similar universal approximation results for two variations of the Restricted Boltzmann Machine with time dependence, the Temporal Restricted Boltzmann Machine (TRBM) and the Recurrent Temporal Restricted Boltzmann Machine (RTRBM). We show that the TRBM is a universal approximator for Markov chains and generalize the theorem to sequences with longer time dependence. We then prove that the RTRBM is a universal approximator for stochastic processes with fi nite time dependence. We conclude with a discussion on e ffi ciency and how the constructions developed could explain some previous experimental results.",,,,,"Edwards, Roderick/N-7378-2014","Edwards, Roderick/0000-0002-8239-5987",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,21,158,,,,,,,,,,,,,,,WOS:000391667100001,0
J,"Park, C; Huang, JHZ",,,,"Park, Chiwoo; Huang, Jianhua Z.",,,Efficient Computation of Gaussian Process Regression for Large Spatial Data Sets by Patching Local Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper develops an efficient computational method for solving a Gaussian process (GP) regression for large spatial data sets using a collection of suitably defined local GP regressions. The conventional local GP approach first partitions a domain into multiple non-overlapping local regions, and then fits an independent GP regression for each local region using the training data belonging to the region. Two key issues with the local GP are (1) the prediction around the boundary of a local region is not as accurate as the prediction at interior of the local region, and (2) two local GP regressions for two neighboring local regions produce different predictions at the boundary of the two regions, creating undesirable discontinuity in the prediction. We address these issues by constraining the predictions of local GP regressions sharing a common boundary to satisfy the same boundary constraints, which in turn are estimated by the data. The boundary constrained local GP regressions are solved by a finite element method. Our approach shows competitive performance when compared with several state-of-the-art methods using two synthetic data sets and three real data sets.",,,,,"Park, Chiwoo/ABA-4876-2021","Park, Chiwoo/0000-0002-2463-8901",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,174,,,,,,,,,,,,,,,WOS:000391679200001,0
J,"Rohde, D; Wand, MP",,,,"Rohde, David; Wand, Matt P.",,,Semiparametric Mean Field Variational Bayes: General Principles and Numerical Issues,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We introduce the term semiparametric mean field variational Bayes to describe the relaxation of mean field variational Bayes in which some density functions in the product density restriction are pre-specified to be members of convenient parametric families. This notion has appeared in various guises in the mean field variational Bayes literature during its history and we endeavor to unify this important topic. We lay down a general framework and explain how previous relevant methodologies fall within this framework. A major contribution is elucidation of numerical issues that impact semiparametric mean field variational Bayes in practice.,,,,,"Wand, Matt P/F-9413-2012","Wand, Matt P/0000-0003-2555-896X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,172,,,,,,,,,,,,,,,WOS:000391676700001,0
J,"Zhang, SL; Jiang, H; Dai, LR",,,,"Zhang, Shiliang; Jiang, Hui; Dai, Lirong",,,Hybrid Orthogonal Projection and Estimation (HOPE): A New Framework to Learn Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a novel model for high-dimensional data, called the Hybrid Orth ogonal Projection and Estimation (HOPE) model, which combines a linear orthogonal projection and a finite mixture model under a unified generative modeling framework. The HOPE model itself can be learned unsupervised from unlabelled data based on the maximum likelihood estimation as well as discriminatively from labelled data. More interestingly, we have shown the proposed HOPE models are closely related to neural networks (NNs) in a sense that each hidden layer can be reformulated as a HOPE model. As a result, the HOPE framework can be used as a novel tool to probe why and how NNs work, more importantly, to learn NNs in either supervised or unsupervised ways. In this work, we have investigated the HOPE framework to learn NNs for several standard tasks, including image recognition on MNIST and speech recognition on TIMIT. Experimental results have shown that the HOPE framework yields significant performance gains over the current state-of-the- art methods in various types of NN learning problems, including unsupervised feature learning, supervised or semi-supervised learning.",,,,,"Zhang, Yuchen/GYI-8858-2022; Zhang, ShiLiang/AAA-4638-2020",,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,37,,,,,,,,,,,,,,,WOS:000391481600001,0
J,"Lin, JH; Zhou, DX",,,,"Lin, Junhong; Zhou, Ding-Xuan",,,Learning Theory of Randomized Kaczmarz Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A relaxed randomized Kaczmarz algorithm is investigated in a least squares regression setting by a learning theory approach. When the sampling values are accurate and the regression function (conditional means) is linear, such an algorithm has been well studied in the community of non-uniform sampling. In this paper, we are mainly interested in the different case of either noisy random measurements or a nonlinear regression function. In this case, we show that relaxation is needed. A necessary and sufficient condition on the sequence of relaxation parameters or step sizes for the convergence of the algorithm in expectation is presented. Moreover, polynomial rates of convergence, both in expectation and in probability, are provided explicitly. As a result, the almost sure convergence of the algorithm is proved by applying the Borel-Cantelli Lemma.",,,,,"Lin, Junhong/M-9045-2016; Zhou, Ding-Xuan/B-3160-2013","Lin, Junhong/0000-0002-4507-9424; Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3341,3365,,,,,,,,,,,,,,,,WOS:000369888000032,0
J,"Prasse, P; Sawade, C; Landwehr, N; Scheffer, T",,,,"Prasse, Paul; Sawade, Christoph; Landwehr, Niels; Scheffer, Tobias",,,Learning to Identify Concise Regular Expressions that Describe Email Campaigns,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a two-stage learning problem with structured output spaces and appropriate loss functions. We derive decoders and the resulting optimization problems which can be solved using standard cutting plane methods. We report on a case study conducted with an email service provider.",,,,,,"Prasse, Paul/0000-0003-1842-3645",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3687,3720,,,,,,,,,,,,,,,,WOS:000369888000041,0
J,"Feng, YL; Huang, XL; Shi, L; Yang, YN; Suykens, JAK",,,,"Feng, Yunlong; Huang, Xiaolin; Shi, Lei; Yang, Yuning; Suykens, Johan A. K.",,,Learning with the Maximum Correntropy Criterion Induced Losses for Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Within the statistical learning framework, this paper studies the regression model associated with the correntropy induced losses. The correntropy, as a similarity measure, has been frequently employed in signal processing and pattern recognition. Motivated by its empirical successes, this paper aims at presenting some theoretical understanding towards the maximum correntropy criterion in regression problems. Our focus in this paper is twofold: first, we are concerned with the connections between the regression model associated with the correntropy induced loss and the least squares regression model. Second, we study its convergence property. A learning theory analysis which is centered around the above two aspects is conducted. From our analysis, we see that the scale parameter in the loss function balances the convergence rates of the regression model and its robustness. We then make some efforts to sketch a general view on robust loss functions when being applied into the learning for regression problems. Numerical experiments are also implemented to verify the effectiveness of the model.",,,,,"Shi, Lei/P-1989-2018; Suykens, Johan A.K./C-9781-2014","Shi, Lei/0000-0002-9512-5273; Suykens, Johan A.K./0000-0002-8846-6352; Yang, Yuning/0000-0003-1805-4210; Huang, Xiaolin/0000-0003-4285-6520",,,,,,,,,,,,,1532-4435,,,,,MAY,2015,16,,,,,,993,1034,,,,,,,,,,,,,,,,WOS:000369886400002,0
J,"Wang, J; Wonka, P; Ye, JP",,,,"Wang, Jie; Wonka, Peter; Ye, Jieping",,,Lasso Screening Rules via Dual Polytope Projection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Lasso is a widely used regression technique to find sparse representations. When the dimension of the feature space and the number of samples are extremely large, solving the Lasso problem remains challenging. To improve the efficiency of solving large-scale Lasso problems, El Ghaoui and his colleagues have proposed the SAFE rules which are able to quickly identify the inactive predictors, i.e., predictors that have 0 components in the solution vector. Then, the inactive predictors or features can be removed from the optimization problem to reduce its scale. By transforming the standard Lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution. In this paper, we propose an efficient and effective screening rule via Dual Polytope Projections (DPP), which is mainly based on the uniqueness and nonexpansiveness of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope. Moreover, we show that our screening rule can be extended to identify inactive groups in group Lasso. To the best of our knowledge, there is currently no exact screening rule for group Lasso. We have evaluated our screening rule using synthetic and real data sets. Results show that our rule is more effective in identifying inactive predictors than existing state-of-the-art screening rules for Lasso.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2015,16,,,,,,1063,1101,,,,,,,,,,,,,,,,WOS:000369886400004,0
J,"Fernandez-Delgado, M; Cernadas, E; Barro, S; Amorim, D",,,,"Fernandez-Delgado, Manuel; Cernadas, Eva; Barro, Senen; Amorim, Dinani",,,Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We evaluate 179 classifiers arising from 17 families (discriminant analysis, Bayesian, neural networks, support vector machines, decision trees, rule-based classifiers, boosting, bagging, stacking, random forests and other ensembles, generalized linear models, nearest-neighbors, partial least squares and principal component regression, logistic and multinomial regression, multiple adaptive regression splines and other methods), implemented in Weka, R (with and without the caret package), C and Mat lab, including all the relevant classifiers available today. We use 121 data sets, which represent the whole UCI data base (excluding the large-scale problems) and other own real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection. The classifiers most likely to be the bests are the random forest (RF) versions, the best of which (implemented in R and accessed via caret) achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% of the data sets. However, the difference is not statistically significant with the second best, the SVM with Gaussian kernel implemented in C using LibSVM, which achieves 92.3% of the maximum accuracy. A few models are clearly better than the remaining ones: random forest, SVM with Gaussian and polynomial kernels, extreme learning machine with Gaussian kernel, C5.0 and avNNet (a committee of multi-layer perceptrons implemented in R with the caret package). The random forest is clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively).",,,,,"Cernadas, Eva/T-6391-2018; Fern√°ndez-Delgado, M./J-7699-2019; Barro, Senen/L-1462-2019","Cernadas, Eva/0000-0002-1562-2553; Fern√°ndez-Delgado, M./0000-0001-5483-9424; Barro, Senen/0000-0001-6035-540X",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3133,3181,,,,,,,,,,,,,,,,WOS:000344638800010,0
J,"Osting, B; Brune, C; Osher, SJ",,,,"Osting, Braxton; Brune, Christoph; Osher, Stanley J.",,,Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a graph where vertices represent alternatives and arcs represent pairwise comparison data, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with the pairwise comparisons. Our goal in this paper is to develop a method for collecting data for which the least squares estimator for the ranking problem has maximal Fisher information. Our approach, based on experimental design, is to view data collection as a bi-level optimization problem where the inner problem is the ranking problem and the outer problem is to identify data which maximizes the informativeness of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding multigraphs with large algebraic connectivity. This reduction of the data collection problem to graph-theoretic questions is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating data set and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking. As another application, we study the 2011-12 NCAA football schedule and propose schedules with the same number of games which are significantly more informative. Using spectral clustering methods to identify highly-connected communities within the division, we argue that the NCAA could improve its notoriously poor rankings by simply scheduling more out-of-conference games.",,,,,"Brune, Christoph/C-1700-2013","Brune, Christoph/0000-0003-0145-5069",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,2981,3012,,,,,,,,,,,,,,,,WOS:000344638800005,0
J,"Nishihara, R; Murray, I; Adams, RP",,,,"Nishihara, Robert; Murray, Iain; Adams, Ryan P.",,,Parallel MCMC with Generalized Elliptical Slice Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic models are conceptually powerful tools for finding structure in data, but their practical effectiveness is often limited by our ability to perform inference in them. Exact inference is frequently intractable, so approximate inference is often performed using Markov chain Monte Carlo (MCMC). To achieve the best possible results from MCMC, we want to efficiently simulate many steps of a rapidly mixing Markov chain which leaves the target distribution invariant. Of particular interest in this regard is how to take advantage of multi-core computing to speed up MCMC-based inference, both to improve mixing and to distribute the computational load. In this paper, we present a parallelizable Markov chain Monte Carlo algorithm for efficiently sampling from continuous probability distributions that can take advantage of hundreds of cores. This method shares information between parallel Markov chains to build a scale-location mixture of Gaussians approximation to the density function of the target distribution. We combine this approximation with a recently developed method known as elliptical slice sampling to create a Markov chain with no step-size parameters that can mix rapidly without requiring gradient or curvature computations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2087,2112,,,,,,,,,,,,,,,,WOS:000344638300007,0
J,"Aravkin, A; Burke, JV; Chiuso, A; Pillonetto, G",,,,"Aravkin, Aleksandr; Burke, James V.; Chiuso, Alessandro; Pillonetto, Gianluigi",,,Convex vs Non-Convex Estimators for Regression and Sparse Estimation: the Mean Squared Error Properties of ARD and GLasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a simple linear regression problem for grouped variables; we are interested in methods which jointly perform estimation and variable selection, that is, that automatically set to zero groups of variables in the regression vector. The Group Lasso (GLasso), a well known approach used to tackle this problem which is also a special case of Multiple Kernel Learning (MKL), boils down to solving convex optimization problems. On the other hand, a Bayesian approach commonly known as Sparse Bayesian Learning (SBL), a version of which is the well known Automatic Relevance Determination (ARD), lead to non-convex problems. In this paper we discuss the relation between ARD (and a penalized version which we call PARD) and Glasso, and study their asymptotic properties in terms of the Mean Squared Error in estimating the unknown parameter. The theoretical arguments developed here are independent of the correctness of the prior models and clarify the advantages of PARD over GLasso.",,,,,,"CHIUSO, ALESSANDRO/0000-0002-4410-6101",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,217,252,,,,,,,,,,,,,,,,WOS:000335457400007,0
J,"Claesen, M; De Smet, F; Suykens, JAK; De Moor, B",,,,"Claesen, Marc; De Smet, Frank; Suykens, Johan A. K.; De Moor, Bart",,,EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,EnsembleSVM is a free software package containing efficient routines to perform ensemble learning with support vector machine (SVM) base models. It currently offers ensemble methods based on binary SVM models. Our implementation avoids duplicate storage and evaluation of support vectors which are shared between constituent models. Experimental results show that using ensemble approaches can drastically reduce training complexity while maintaining high predictive accuracy. The EnsembleSVM software package is freely available online at http://esat.kuleuven.be/stadius/ensemblesvm.,,,,,"Suykens, Johan A.K./C-9781-2014; Claesen, Marc/A-1762-2015","Suykens, Johan A.K./0000-0002-8846-6352; Claesen, Marc/0000-0002-3551-7769",,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,141,145,,,,,,,,,,,,,,,,WOS:000335457400004,0
J,"Bohmer, W; Grunewalder, S; Shen, Y; Musial, M; Obermayer, K",,,,"Boehmer, Wendelin; Gruenewaelder, Steffen; Shen, Yun; Musial, Marek; Obermayer, Klaus",,,Construction of Approximation Spaces for Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance.",,,,,"Shen, Yun/AAA-3491-2020","Grunewalder, Steffen/0000-0002-4017-2048",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,2067,2118,,,,,,,,,,,,,,,,WOS:000323367000012,0
J,"Malgireddy, MR; Nwogu, I; Govindaraju, V",,,,"Malgireddy, Manavender R.; Nwogu, Ifeoma; Govindaraju, Venu",,,Language-Motivated Approaches to Action Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach.",,,,,"Nwogu, Ifeoma/GYV-0014-2022","Nwogu, Ifeoma/0000-0003-1414-6433",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,2189,2212,,,,,,,,,,,,,,,,WOS:000323367000015,0
J,"Johnson, MJ; Willsky, AS",,,,"Johnson, Matthew J.; Willsky, Alan S.",,,Bayesian Nonparametric Hidden Semi-Markov Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markov modeling, which has been developed mainly in the parametric non-Bayesian setting, to allow construction of highly interpretable models that admit natural prior information on state durations. In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,673,701,,,,,,,,,,,,,,,,WOS:000315981900012,0
J,"Salomon, A; Audibert, JY; El Alaoui, I",,,,"Salomon, Antoine; Audibert, Jean-Yves; El Alaoui, Issam",,,Lower Bounds and Selectivity of Weak-Consistent Policies in Stochastic Multi-Armed Bandit Problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistency, and exhibit a generalisation of the bound. We also study the existence of logarithmic bounds in general and in the case of Hannan consistency. Moreover, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment. To get these results, we study variants of popular Upper Confidence Bounds (UCB) policies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,187,207,,,,,,,,,,,,,,,,WOS:000314530200006,0
J,"Anandkumar, A; Tan, VYF; Huang, FR; Willsky, AS",,,,"Anandkumar, Animashree; Tan, Vincent Y. F.; Huang, Furong; Willsky, Alan S.",,,High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exists, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples n = Omega (J(min)(-2) log p), where p is the number of variables and J(min) is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2293,2337,,,,,,,,,,,,,,,,WOS:000308795200003,0
J,"Yuan, GX; Ho, CH; Lin, CJ",,,,"Yuan, Guo-Xun; Ho, Chia-Hua; Lin, Chih-Jen",,,An Improved GLMNET for L1-regularized Logistic Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recently, Yuan et al. (2010) conducted a comprehensive comparison on software for L1-regularized classification. They concluded that a carefully designed coordinate descent implementation CDN is the fastest among state-of-the-art solvers. In this paper, we point out that CDN is less competitive on loss functions that are expensive to compute. In particular, CDN for logistic regression is much slower than CDN for SVM because the logistic loss involves expensive exp/log operations. In optimization, Newton methods are known to have fewer iterations although each iteration costs more. Because solving the Newton sub-problem is independent of the loss calculation, this type of methods may surpass CDN under some circumstances. In L1-regularized classification, GLMNET by Friedman et al. is already a Newton-type method, but experiments in Yuan et al. (2010) indicated that the existing GLMNET implementation may face difficulties for some large-scale problems. In this paper, we propose an improved GLMNET to address some theoretical and implementation issues. In particular, as a Newton-type method, GLMNET achieves fast local convergence, but may fail to quickly obtain a useful solution. By a careful design to adjust the effort for each iteration, our method is efficient for both loosely or strictly solving the optimization problem. Experiments demonstrate that our improved GLMNET is more efficient than CDN for L1-regularized logistic regression.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1999,2030,,,,,,,,,,,,,,,,WOS:000307020700010,0
J,"Ailon, N",,,,"Ailon, Nir",,,An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a set V of n elements we wish to linearly order them given pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(epsilon(-6)nlog(5)n) preference labels for a regret of e times the optimal loss. As a function of n, this is asymptotically better than standard (non-adaptive) learning bounds achievable for the same problem. Our main result takes us a step closer toward settling an open problem posed by learning-to-rank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we analyze a typical test case in which a large margin linear relaxation is used for efficiently solving the simpler learning problems in our decomposition.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,137,164,,,,,,,,,,,,,,,,WOS:000303045100005,0
J,"Brown, G; Pocock, A; Zhao, MJ; Lujan, M",,,,"Brown, Gavin; Pocock, Adam; Zhao, Ming-Jie; Lujan, Mikel",,,Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: what are the implicit statistical assumptions of feature selection criteria based on mutual information?. To answer this, we adopt a different strategy than is usual in the feature selection literature-instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.",,,,,,"Brown, Gavin/0000-0003-2261-9018",,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,27,66,,,,,,,,,,,,,,,,WOS:000303045100002,0
J,"Ying, YM; Li, P",,,,"Ying, Yiming; Li, Peng",,,Distance Metric Learning with Eigenvalue Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efficient metric learning algorithms. Indeed, first-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their convergence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difficult and challenging face verification data set called Labeled Faces in the Wild (LFW).",,,,,"Ying, Yiming/AGD-7246-2022; Ying, Yiming/A-4196-2013","Ying, Yiming/0000-0001-7345-6672; Ying, Yiming/0000-0001-7345-6672",,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,1,26,,,,,,,,,,,,,,,,WOS:000303045100001,0
J,"Huang, JZ; Zhang, T; Metaxas, D",,,,"Huang, Junzhou; Zhang, Tong; Metaxas, Dimitris",,,Learning with Structured Sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates a learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures on the feature set, this concept generalizes the group sparsity idea that has become popular in recent years. A general theory is developed for learning with structured sparsity, based on the notion of coding complexity associated with the structure. It is shown that if the coding complexity of the target signal is small, then one can achieve improved performance by using coding complexity regularization methods, which generalize the standard sparse regularization. Moreover, a structured greedy algorithm is proposed to efficiently solve the structured sparsity problem. It is shown that the greedy algorithm approximately solves the coding complexity optimization problem under appropriate conditions. Experiments are included to demonstrate the advantage of structured sparsity over standard sparsity on some real applications.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,NOV,2011,12,,,,,,3371,3412,,,,,,,,,,,,,,,,WOS:000298103700010,0
J,"Gaiffas, S; Lecue, G",,,,"Gaiffas, Stephane; Lecue, Guillaume",,,Hyper-Sparse Optimal Aggregation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a finite set F of functions and a learning sample, the aim of an aggregation procedure is to have a risk as close as possible to risk of the best function in F. Up to now, optimal aggregation procedures are convex combinations of every elements of F. In this paper, we prove that optimal aggregation procedures combining only two functions in F exist. Such algorithms are of particular interest when F contains many irrelevant functions that should not appear in the aggregation procedure. Since selectors are suboptimal aggregation procedures, this proves that two is the minimal number of elements of F required for the construction of an optimal aggregation procedure in every situations. Then, we perform a numerical study for the problem of selection of the regularization parameters of the Lasso and the Elastic-net estimators. We compare on simulated examples our aggregation algorithms to aggregation with exponential weights, to Mallow's C(p) and to cross-validation selection procedures.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1813,1833,,,,,,,,,,,,,,,,WOS:000293757200001,0
J,"Kloft, M; Brefeld, U; Sonnenburg, S; Zien, A",,,,"Kloft, Marius; Brefeld, Ulf; Sonnenburg, Soeren; Zien, Alexander",,,l(p)-Norm Multiple Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown. Previous approaches to multiple kernel learning (MKL) promote sparse kernel combinations to support interpretability and scalability. Unfortunately, this l(1)-norm MKL is rarely observed to outperform trivial baselines in practical applications. To allow for robust kernel mixtures that generalize well, we extend MKL to arbitrary norms. We devise new insights on the connection between several existing MKL formulations and develop two efficient interleaved optimization strategies for arbitrary norms, that is l(p)-norms with p >= 1. This interleaved optimization is much faster than the commonly used wrapper approaches, as demonstrated on several data sets. A theoretical analysis and an experiment on controlled artificial data shed light on the appropriateness of sparse, non-sparse and l(infinity)-norm MKL in various scenarios. Importantly, empirical applications of l(p)-norm MKL to three real-world problems from computational biology show that non-sparse MKL achieves accuracies that surpass the state-of-the-art. Data sets, source code to reproduce the experiments, implementations of the algorithms, and further information are available at http://doc.ml.tu-berlin.de/nonsparse_mkl/.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,953,997,,,,,,,,,,,,,,,,WOS:000289635000007,0
J,"Dillon, JV; Lebanon, G",,,,"Dillon, Joshua V.; Lebanon, Guy",,,Stochastic Composite Likelihood,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. Each of the estimators resolve the computation-accuracy tradeoff differently, and taken together they span a continuous spectrum of computation-accuracy tradeoff resolutions. We prove the consistency of the estimators, provide formulas for their asymptotic variance, statistical robustness, and computational complexity. We discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators when the computational resources are insufficient. They also demonstrate that in some cases reduced computational complexity is associated with robustness thereby increasing statistical accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2597,2633,,,,,,,,,,,,,,,,WOS:000284040000002,0
J,"Carlsson, G; Memoli, F",,,,"Carlsson, Gunnar; Memoli, Facundo",,,"Characterization, Stability and Convergence of Hierarchical Clustering Methods",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1425,1470,,,,,,,,,,,,,,,,WOS:000282521500007,0
J,"Christoforou, C; Haralick, R; Sajda, P; Parra, LC",,,,"Christoforou, Christoforos; Haralick, Robert; Sajda, Paul; Parra, Lucas C.",,,Second-Order Bilinear Discriminant Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Traditional analysis methods for single-trial classification of electro-encephalography (EEG) focus on two types of paradigms: phase-locked methods, in which the amplitude of the signal is used as the feature for classification, that is, event related potentials; and second-order methods, in which the feature of interest is the power of the signal, that is, event related (de)synchronization. The process of deciding which paradigm to use is ad hoc and is driven by assumptions regarding the underlying neural generators. Here we propose a method that provides an unified framework for the analysis of EEG, combining first and second-order spatial and temporal features based on a bilinear model. Evaluation of the proposed method on simulated data shows that the technique outperforms state-of-the art techniques for single-trial classification for a broad range of signal-to-noise ratios. Evaluations on human EEG-including one benchmark data set from the Brain Computer Interface (BCI) competition-show statistically significant gains in classification accuracy, with a reduction in overall classification error from 26%-28% to 19%.",,,,,"Haralick, Robert/AAW-5151-2020","manickam, vijayabhama.M/0000-0001-9437-9477",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,665,685,,,,,,,,,,,,,,,,WOS:000277186500009,0
J,"Mairal, J; Bach, F; Ponce, J; Sapiro, G",,,,"Mairal, Julien; Bach, Francis; Ponce, Jean; Sapiro, Guillermo",,,Online Learning for Matrix Factorization and Sparse Coding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse coding-that is, modelling data vectors as sparse linear combinations of basis elements-is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.",,,,,"Mairal, Julien/AAL-5611-2021",,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,19,60,,,,,,,,,,,,,,,,WOS:000277186400002,0
J,"Bordes, A; Bottou, L; Gallinari, P",,,,"Bordes, Antoine; Bottou, Leon; Gallinari, Patrick",,,SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The SGD-QN algorithm is a stochastic gradient descent algorithm that makes careful use of second-order information and splits the parameter update into independently scheduled components. Thanks to this design, SGD-QN iterates nearly as fast as a first-order stochastic gradient descent but requires less iterations to achieve the same accuracy. This algorithm won the Wild Track of the first PASCAL Large Scale Learning Challenge (Sonnenburg et al., 2008).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1737,1754,,,,,,,,,,,,,,,,WOS:000270825000015,0
J,"Hausser, J; Strimmer, K",,,,"Hausser, Jean; Strimmer, Korbinian",,,"Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.",,,,,"Strimmer, Korbinian/C-1522-2009","Strimmer, Korbinian/0000-0001-7917-2056",,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1469,1484,,,,,,,,,,,,,,,,WOS:000270825000006,0
J,"Helmbold, DP; Warmuth, MK",,,,"Helmbold, David P.; Warmuth, Manfred K.",,,Learning Permutations with Exponential Weights,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We give an algorithm for the on-line learning of permutations. The algorithm maintains its uncertainty about the target permutation as a doubly stochastic weight matrix, and makes predictions using an efficient method for decomposing the weight matrix into a convex combination of permutations. The weight matrix is updated by multiplying the current matrix entries by exponential factors, and an iterative procedure is needed to restore double stochasticity. Even though the result of this procedure does not have a closed form, a new analysis approach allows us to prove an optimal (up to small constant factors) bound on the regret of our algorithm. This regret bound is significantly better than that of either Kalai and Vempala's more efficient Follow the Perturbed Leader algorithm or the computationally expensive method of explicitly representing each permutation as an expert.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2009,10,,,,,,1705,1736,,,,,,,,,,,,,,,,WOS:000270825000014,0
J,"Xu, YS; Zhang, HZ",,,,"Xu, Yuesheng; Zhang, Haizhang",,,Refinement of Reproducing Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We continue our recent study on constructing a refinement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the refinement kernel contains that with the original kernel as a subspace. To motivate this study, we first develop a refinement kernel method for learning, which gives an efficient algorithm for updating a learning predictor. Several characterizations of refinement kernels are then presented. It is shown that a nontrivial refinement kernel for a given kernel always exists if the input space has an infinite cardinal number. Refinement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided.",,,,,"Xu, yue/HGE-1737-2022; Zhang, Haizhang/GPX-1222-2022","Zhang, Haizhang/0000-0002-8241-3145",,,,,,,,,,,,,1532-4435,,,,,JAN,2009,10,,,,,,107,140,,,,,,,,,,,,,,,,WOS:000270824100004,0
J,"Garcia, S; Herrera, F",,,,"Garcia, Salvador; Herrera, Francisco",,,An Extension on Statistical Comparisons of Classifiers over Multiple Data Sets for all Pairwise Comparisons,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In a recently published paper in JMLR, Dem. sar (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n x n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures.",,,,,"Herrera, Francisco/K-9019-2017; Garc√≠a, Salvador/N-3624-2013; Herrera, Francisco/C-6856-2008","Garc√≠a, Salvador/0000-0003-4494-7565; Herrera, Francisco/0000-0002-7283-312X",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2677,2694,,,,,,,,,,,,,,,,WOS:000263240700002,0
J,"Amit, Y; Shalev-Shwartz, S; Singer, Y",,,,"Amit, Yonatan; Shalev-Shwartz, Shai; Singer, Yoram",,,Online learning of complex prediction problems using simultaneous projections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe and analyze an algorithmic framework for online classification where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online predictor by defining a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution of the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with synthetic data and with multiclass text categorization tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1399,1435,,,,,,,,,,,,,,,,WOS:000258646800005,0
J,"Jorgensen, Z; Zhou, Y; Inge, M",,,,"Jorgensen, Zach; Zhou, Yan; Inge, Meador",,,A multiple instance learning strategy for combating good word attacks on spam filters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Statistical spam filters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam filters by appending to spam messages sets of good words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classified as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classifier using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam filtering domain.",,,,,"Zhou, Yan/A-8988-2009",,,,,,,,,,,,,,1532-4435,,,,,JUN,2008,9,,,,,,1115,1146,,,,,,,,,,,,,,,,WOS:000258646300006,0
J,"Dekel, O; Long, PM; Singer, Y",,,,"Dekel, Ofer; Long, Philip M.; Singer, Yoram",,,Online learning of multiple tasks with a shared loss,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is defined by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Specifically, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a large-scale multiclass-multilabel text categorization problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2233,2264,,,,,,,,,,,,,,,,WOS:000252744800002,0
J,"Macskassy, SA; Provost, F",,,,"Macskassy, Sofus A.; Provost, Foster",,,Classification in networked data: A toolkit and a univariate case study,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classification in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classifiers comprise a local classifier, a relational classifier, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classification, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classification in machine learning benchmark data sets. The results demonstrate that very simple network-classification models perform quite well-well enough that they should be used regularly as baseline classifiers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes-that is, Gaussian-field classifiers, Hopfield networks, and relational-neighbor classifiers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,935,983,,,,,,,,,,,,,,,,WOS:000248351700002,0
J,"Sutton, C; McCallum, A; Rohanimanesh, K",,,,"Sutton, Charles; McCallum, Andrew; Rohanimanesh, Khashayar",,,Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields ( DCRFs), a generalization of linear-chain conditional random fields ( CRFs) in which each time slice contains a set of state variables and edges-a distributed state representation as in dynamic Bayesian networks ( DBNs)-and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization ( TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, finding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the final task directly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,693,723,,,,,,,,,,,,,,,,WOS:000247002700011,0
J,"Ekdahl, M; Koski, T",,,,"Ekdahl, Magnus; Koski, Timo",,,Bounds for the loss in probability of correct classification under model based approximation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many pattern recognition/classification problem the true class conditional model and class probabilities are approximated for reasons of reducing complexity and/or of statistical estimation. The approximated classifier is expected to have worse performance, here measured by the probability of correct classification. We present an analysis valid in general, and easily computable formulas for estimating the degradation in probability of correct classification when compared to the optimal classifier. An example of an approximation is the Naive Bayes classifier. We show that the performance of the Naive Bayes depends on the degree of functional dependence between the features and labels. We provide a sufficient condition for zero loss of performance, too.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2449,2480,,,,,,,,,,,,,,,,WOS:000245390700007,0
J,"Collobert, R; Sinz, F; Weston, J; Bottou, L",,,,"Collobert, Ronan; Sinz, Fabian; Weston, Jason; Bottou, Leon",,,Large scale transductive SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show how the concave-convex procedure can be applied to transductive SVMs, which traditionally require solving a combinatorial search problem. This provides for the first time a highly scalable algorithm in the nonlinear case. Detailed experiments verify the utility of our approach. Software is available at http://www.kyb.tuebingen.mpg.de/bs/people/fabee/transduction.html.",,,,,"Sinz, Fabian/E-6708-2010","Sinz, Fabian/0000-0002-1348-9736",,,,,,,,,,,,,1532-4435,,,,,AUG,2006,7,,,,,,1687,1712,,,,,,,,,,,,,,,,WOS:000245389200002,0
J,"Hastie, T; Rosset, S; Tibshirani, R; Zhu, J",,,,"Hastie, T; Rosset, S; Tibshirani, R; Zhu, J",,,The entire regularization path for the support vector machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1391,1415,,,,,,,,,,,,,,,,WOS:000236328300007,0
J,"Sarela, J; Vigario, R",,,,"Sarela, J; Vigario, R",,,Overlearning in marginal distribution-based ICA: Analysis and solutions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The present paper is written as a word of caution, with users of independent component analysis (ICA) in mind, to overlearning phenomena that are often observed. We consider two types of overlearning, typical to high-order statistics based ICA. These algorithms can be seen to maximise the negentropy of the source estimates. The first kind of overlearning results in the generation of spike-like signals, if there are not enough samples in the data or there is a considerable amount of noise present. It is argued that, if the data has power spectrum characterised by 1/f curve, we face a more severe problem, which cannot be solved inside the strict ICA model. This overlearning is better characterised by bumps instead of spikes. Both overlearning types are demonstrated in the case of artificial signals as well as magnetoencephalograms (MEG). Several methods are suggested to circumvent both types, either by making the estimation of the ICA model more robust or by including further modelling of the data.",,,,,"Vig√°rio, Ricardo/AAX-9822-2021; Vig√°rio, Ricardo/C-4641-2018","Vig√°rio, Ricardo/0000-0003-0950-6035; ",,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1447,1469,,,,,,,,,,,,,,,,WOS:000224808300013,0
J,"Chawla, NV; Hall, LO; Bowyer, KW; Kegelmeyer, WP",,,,"Chawla, NV; Hall, LO; Bowyer, KW; Kegelmeyer, WP",,,Learning ensembles from bites: A scalable and accurate approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bagging and boosting are two popular ensemble methods that typically achieve better accuracy than a single classifier. These techniques have limitations on massive data sets, because the size of the data set can be a bottleneck. Voting many classifiers built on small subsets of data (pasting small votes) is a promising approach for learning from massive data sets, one that can utilize the power of boosting and bagging. We propose a framework for building hundreds or thousands of such classifiers on small subsets of data in a distributed environment. Experiments show this approach is fast, accurate, and scalable.",,,,,"Chawla, Nitesh/F-2690-2016","Chawla, Nitesh/0000-0003-3932-5956",,,,,,,,,,,,,1532-4435,,,,,APR,2004,5,,,,,,421,451,,,,,,,,,,,,,,,,WOS:000236327400005,0
J,"Druzdzel, MJ; Diez, FJ",,,,"Druzdzel, MJ; Diez, FJ",,,Combining knowledge from different sources in causal Probabilistic models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Knowledge-Data Fusion,"JUN, 2000","Stanford Univ, Stanford, CA",,Stanford Univ,,,"Building probabilistic and decision-theoretic models requires a considerable knowledge engineering effort in which the most daunting task is obtaining the numerical parameters. Authors of Bayesian networks usually combine various sources of information, such as textbooks, statistical reports, databases, and expert judgement. In this paper, we demonstrate the risks of such a combination, even when this knowledge encompasses such seemingly population-independent characteristics as sensitivity and specificity of medical symptoms. We show that the criteria do not combine knowledge from different sources or use only data from the setting in which the model will be used are neither necessary nor sufficient to guarantee the correctness of the model. Instead, we offer graphical criteria for determining when knowledge from different sources can be safely combined into the general population model. We also offer a method for building subpopulation models. The analysis performed in this paper and the criteria we propose may be useful in such fields as knowledge engineering, epidemiology, machine learning, and statistical meta-analysis.",,,,,"Druzdzel, Marek J/AAR-9653-2020; Druzdzel, Marek/B-5534-2013; D√≠ez, Francisco Javier/A-9212-2008","Druzdzel, Marek J/0000-0002-7598-2286; Druzdzel, Marek/0000-0002-7598-2286; D√≠ez, Francisco Javier/0000-0001-9855-9248",,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,295,316,,10.1162/153244304773633834,0,,,,,,,,,,,,,WOS:000221043900003,0
J,"Bach, FR; Jordan, MI",,,,"Bach, FR; Jordan, MI",,,Kernel independent component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.",,,,,"Jordan, Michael I/C-5253-2013",,,,,,,,,,,,,,1532-4435,,,,,Jan-01,2003,3,1,,,,,1,48,,10.1162/153244303768966085,0,,,,,,,,,,,,,WOS:000181462700001,0
J,"Jonyer, I; Cook, DJ; Holder, LB",,,,"Jonyer, I; Cook, DJ; Holder, LB",,,Graph-based hierarchical conceptual clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hierarchical conceptual clustering has proven to be a useful, although under-explored, data mining technique. A graph-based representation of structural information combined with a substructure discovery technique has been shown to be successful in knowledge discovery. The SUBDUE substructure discovery system provides one such combination of approaches. This work presents SUBDUE and the development of its clustering functionalities. Several examples are used to illustrate the validity of the approach both in structured and unstructured domains, as well as to compare SUBDUE to the Cobweb clustering algorithm. We also develop a new metric for comparing structurally-defined clusterings. Results show that SUBDUE successfully discovers hierarchical clusterings in both structured and unstructured data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2001,2,1,,,,,19,43,,,,,,,,,,,,,,,,WOS:000173838200002,0
J,"Pekalska, E; Paclik, P; Duin, RPW",,,,"Pekalska, E; Paclik, P; Duin, RPW",,,A generalized kernel approach to dissimilarity-based classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"Usually, objects to be classified are represented by features. In this paper, we discuss an alternative object representation based on dissimilarity values. If such distances separate the classes well, the nearest neighbor method offers a good solution. However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suffers from its sensitivity to noisy examples. We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases. For classification purposes, two different ways of using generalized dissimilarity kernels are considered. In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there. In the second approach, classifiers are built directly on distance kernels. Both approaches are described theoretically and then compared using experiments with different dissimilarity measures and datasets including degraded data simulating the problem of missing values.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,175,211,,10.1162/15324430260185592,0,,,,,,,,,,,,,WOS:000176055300006,0
J,"Fasano, A; Durante, D",,,,"Fasano, Augusto; Durante, Daniele",,,A Class of Conjugate Priors for Multinomial Probit Models which Includes the Multivariate Normal One,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multinomial probit models are routinely-implemented representations for learning how the class probabilities of categorical response data change with p observed predictors. Although several frequentist methods have been developed for estimation, inference and classification within such a class of models, Bayesian inference is still lagging behind. This is due to the apparent absence of a tractable class of conjugate priors, that may facilitate posterior infer-ence on the multinomial probit coefficients. Such an issue has motivated increasing efforts toward the development of effective Markov chain Monte Carlo methods, but state-of-the -art solutions still face severe computational bottlenecks, especially in high dimensions. In this article, we show that the entire class of unified skew-normal (SUN) distributions is con-jugate to several multinomial probit models. Leveraging this result and the SUN properties, we improve upon state-of-the-art solutions for posterior inference and classification both in terms of closed-form results for several functionals of interest, and also by developing novel computational methods relying either on independent and identically distributed samples from the exact posterior or on scalable and accurate variational approximations based on blocked partially-factorized representations. As illustrated in simulations and in a gastroin-testinal lesions application, the magnitude of the improvements relative to current methods is particularly evident, in practice, when the focus is on high-dimensional studies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766899100001,0
J,"Nguyen, LH; Goulet, JA",,,,"Nguyen, Luong-Ha; Goulet, James-A",,,Analytically Tractable Hidden-States Inference in Bayesian Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With few exceptions, neural networks have been relying on backpropagation and gradient descent as the inference engine in order to learn the model parameters, because closed-form Bayesian inference for neural networks has been considered to be intractable. In this paper, we show how we can leverage the tractable approximate Gaussian inference's (TAGI) capabilities to infer hidden states, rather than only using it for inferring the network's parameters. One novel aspect is that it allows inferring hidden states through the imposition of constraints designed to achieve specific objectives, as illustrated through three examples: (1) the generation of adversarial-attack examples, (2) the usage of a neural network as a black-box optimization method, and (3) the application of inference on continuous-action reinforcement learning. In these three examples, the constrains are in (1), a target label chosen to fool a neural network, and in (2 & 3) the derivative of the network with respect to its input that is set to zero in order to infer the optimal input values that are either maximizing or minimizing it. These applications showcase how tasks that were previously reserved to gradient-based optimization approaches can now be approached with analytically tractable inference.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000766896700001,0
J,"Pan, YG; Tsang, IW; Chen, WJ; Niu, G; Sugiyama, M",,,,"Pan, Yuangang; Tsang, Ivor W.; Chen, Weijie; Niu, Gang; Sugiyama, Masashi",,,Fast and Robust Rank Aggregation against Model Misspecification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In rank aggregation (RA), a collection of preferences from different users are summarized into a total order under the assumption of homogeneity of users. Model misspecification in RA arises since the homogeneity assumption fails to be satisfied in the complex real-world situation. Existing robust RAs usually resort to an augmentation of the ranking model to account for additional noises, where the collected preferences can be treated as a noisy perturbation of idealized preferences. Since the majority of robust RAs rely on certain perturbation assumptions, they cannot generalize well to agnostic noise-corrupted preferences in the real world. In this paper, we propose CoarsenRank, which possesses robustness against model misspecification. Specifically, the properties of our CoarsenRank are summarized as follows: (1) CoarsenRank is designed for mild model misspecification, which assumes there exist the ideal preferences (consistent with model assumption) that locate in a neighborhood of the actual preferences. (2) CoarsenRank then performs regular RAs over a neighborhood of the preferences instead of the original data set directly. Therefore, CoarsenRank enjoys robustness against model misspecification within a neighborhood. (3) The neighborhood of the data set is defined via their empirical data distributions. Further, we put an exponential prior on the unknown size of the neighborhood, and derive a much-simplified posterior formula for CoarsenRank under particular divergence measures. (4) CoarsenRank is further instantiated to Coarsened Thurstone, Coarsened Bradly-Terry, and Coarsened Plackett-Luce with three popular probability ranking models. Meanwhile, tractable optimization strategies are introduced with regards to each instantiation respectively. In the end, we apply CoarsenRank on four real-world data sets. Experiments show that CoarsenRank is fast and robust, achieving consistent improvements over baseline methods.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743; Pan, Yuangang/0000-0002-7950-4900",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000752371200001,0
J,"Ayed, F; Battiston, M; Camerlenghi, F; Favaro, S",,,,"Ayed, Fadhel; Battiston, Marco; Camerlenghi, Federico; Favaro, Stefano",,,Consistent estimation of small masses in feature sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider an (observable) random sample of size n from an infinite population of individuals, each individual being endowed with a finite set of features from a collection of features (F-j)(j>1) with unknown probabilities (p(j))(j>1), i.e., p(j) is the probability that an individual displays feature F-j. Under this feature sampling framework, in recent years there has been a growing interest in estimating the sum of the probability masses p(j)'s of features observed with frequency r >= 0 in the sample, here denoted by M-n,M-r. This is the natural feature sampling counterpart of the classical problem of estimating small probabilities in the species sampling framework, where each individual is endowed with only one feature (or species). In this paper we study the problem of consistent estimation of the small mass M-n,M-r. We first show that there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass M-n,M-0. Then, we introduce an estimator of M-n,M-r and identify sufficient conditions under which the estimator is consistent. In particular, we propose a nonparametric estimator (M) over cap (n,r) of M-n,M-r which has the same analytic form of the celebrated Good-Turing estimator for small probabilities, with the sole difference that the two estimators have different ranges (supports). Then, we show that (M) over cap (n,r) is strongly consistent, in the multiplicative sense, under the assumption that (p(j))(j >= 1) has regularly varying heavy tails.",,,,,,"Battiston, Marco/0000-0003-0954-1244",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500006,0
J,"de Vilmarest, J; Wintenberger, O",,,,"de Vilmarest, Joseph; Wintenberger, Olivier",,,Stochastic Online Optimization using Kalman Recursion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the Extended Kalman Filter in constant dynamics, offering a bayesian perspective of stochastic optimization. For generalized linear models, we obtain high probability bounds on the cumulative excess risk in an unconstrained setting, under the assumption that the algorithm reaches a local phase. In order to avoid any projection step we propose a twophase analysis. First, for linear and logistic regressions, we prove that the algorithm enters a local phase where the estimate stays in a small region around the optimum. We provide explicit bounds with high probability on this convergence time, slightly modifying the Extended Kalman Filter in the logistic setting. Second, for generalized linear regressions, we provide a martingale analysis of the excess risk in the local phase, improving existing ones in bounded stochastic optimization. The algorithm appears as a parameter-free online procedure that optimally solves some unconstrained optimization problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,223,,,,,,,,,,,,,,,WOS:000706447200001,0
J,"Gomez-Uribe, CA; Karrer, B",,,,"Gomez-Uribe, Carlos A.; Karrer, Brian",,,The Decoupled Extended Kalman Filter for Dynamic Exponential-Family Factorization Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by the needs of online large-scale recommender systems, we specialize the decoupled extended Kalman filter (DEKF) to factorization models, including factorization machines, matrix and tensor factorization, and illustrate the effectiveness of the approach through numerical experiments on synthetic and on real-world data. Online learning of model parameters through the DEKF makes factorization models more broadly useful by (i) allowing for more flexible observations through the entire exponential family, (ii) modeling parameter drift, and (iii) producing parameter uncertainty estimates that can enable explore/exploit and other applications. We use a different parameter dynamics than the standard DEKF, allowing parameter drift while encouraging reasonable values. We also present an alternate derivation of the extended Kalman filter and DEKF that highlights the role of the Fisher information matrix in the EKF.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500005,0
J,"Le, CM",,,,"Le, Can M.",,,Edge Sampling Using Local Network Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Edge sampling is an important topic in network analysis. It provides a natural way to reduce network size while retaining desired features of the original network. Sampling methods that only use local information are common in practice as they do not require access to the entire network and can be easily parallelized. Despite promising empirical performances, most of these methods are derived from heuristic considerations and lack theoretical justification. In this paper, we study a simple and efficient edge sampling method that uses local network information. We show that when the local connectivity is sufficiently strong, the sampled network satisfies a strong spectral property. We measure the strength of local connectivity by a global parameter and relate it to more common network statistics such as the clustering coefficient and network curvature. Based on this result, we also provide sufficient conditions under which random networks and hypergraphs can be efficiently sampled.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000663144600001,0
J,"Li, Z; Ton, JF; Oglic, D; Sejdinovic, D",,,,"Li, Zhu; Ton, Jean-Francois; Oglic, Dino; Sejdinovic, Dino",,,Towards a Unified Analysis of Random Fourier Features,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the learning risk convergence rate is problem specific and expressed in terms of the regularization parameter and the number of effective degrees of freedom. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to ridge leverage scores and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency. Our empirical results illustrate the effectiveness of the proposed scheme relative to the standard random Fourier features method.",,,,,,"Sejdinovic, Dino/0000-0001-5547-9213",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,108,,,,,,,,,,,,,,,WOS:000663163600001,0
J,"Ma, SJ; Su, LJ; Zhang, YC",,,,"Ma, Shujie; Su, Liangjun; Zhang, Yichong",,,Determining the Number of Communities in Degree-corrected Stochastic Block Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose to estimate the number of communities in degree-corrected stochastic block models based on a pseudo likelihood ratio statistic. To this end, we introduce a method that combines spectral clustering with binary segmentation. This approach guarantees an upper bound for the pseudo likelihood ratio statistic when the model is over-fitted. We also derive its limiting distribution when the model is under-fitted. Based on these properties, we establish the consistency of our estimator for the true number of communities. Developing these theoretical properties require a mild condition on the average degrees - growing at a rate no slower than log (n), where n is the number of nodes. Our proposed method is further illustrated by simulation studies and analysis of real-world networks. The numerical results show that our approach has satisfactory performance when the network is semi-dense.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656371400001,0
J,"Mai, XY; Couillet, R",,,,"Mai, Xiaoyi; Couillet, Romain",,,Consistent Semi-Supervised Graph Regularization for High Dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Semi-supervised Laplacian regularization, a standard graph-based approach for learning from both labelled and unlabelled data, was recently demonstrated to have an insignificant high dimensional learning efficiency with respect to unlabelled data (Mai and Couillet, 2018), causing it to be outperformed by its unsupervised counterpart, spectral clustering, given sufficient unlabelled data. Following a detailed discussion on the origin of this inconsistency problem, a novel regularization approach involving centering operation is proposed as solution, supported by both theoretical analysis and empirical results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,94,,,,,,,,,,,,,,,WOS:000663148200001,0
J,"Moss, HB; Leslie, DS; Gonzalez, J; Rayson, P",,,,"Moss, Henry B.; Leslie, David S.; Gonzalez, Javier; Rayson, Paul",,,GIBBON: General-purpose Information-Based Bayesian OptimisatioN,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes a general-purpose extension of max-value entropy search, a popular approach for Bayesian Optimisation (BO). A novel approximation is proposed for the information gain - an information-theoretic quantity central to solving a range of BO problems, including noisy, multi-fidelity and batch optimisations across both continuous and highly-structured discrete spaces. Previously, these problems have been tackled separately within information-theoretic BO, each requiring a different sophisticated approximation scheme, except for batch BO, for which no computationally-lightweight information-theoretic approach has previously been proposed. GIB -BON (General-purpose Information-Based Bayesian OptimisatioN) provides a single principled framework suitable for all the above, out-performing existing approaches whilst incurring sub-stantially lower computational overheads. In addition, GIBBON does not require the problem's search space to be Euclidean and so is the first high-performance yet computationally light-weight acquisition function that supports batch BO over general highly structured input spaces like molec-ular search and gene design. Moreover, our principled derivation of GIBBON yields a natural interpretation of a popular batch BO heuristic based on determinantal point processes. Finally, we analyse GIBBON across a suite of synthetic benchmark tasks, a molecular search loop, and as part of a challenging batch multi-fidelity framework for problems with controllable experimental noise.",,,,,,"Rayson, Paul/0000-0002-1257-2191; Leslie, David Stuart/0000-0001-5253-7676",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,235,,,,,,,,,,,,,,,WOS:000706450500001,0
J,"van Veen, R; Biehl, M; De Vries, GJ",,,,"van Veen, Rick; Biehl, Michael; De Vries, Gert-Jan",,,sklvq: Scikit Learning Vector Quantization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The sklvq package is an open-source Python implementation of a set of learning vector quantization (LVQ) algorithms. In addition to providing the core functionality for the GLVQ, GMLVQ, and LGMLVQ algorithms, sklvq is distinctive by putting emphasis on its modular and customizable design. Not only resulting in a feature-rich implementation for users but enabling easy extensions of the algorithms for researchers. The theory behind this design is described in this paper. To facilitate adoptions and inspire future contributions, sklvq is publicly available on Github (under the BSD license) and can be installed through the Python package index (PyPI). Next to being well-covered by automated testing to ensure code quality, it is accompanied by detailed online documentation. The documentation covers usage examples and provides an in-depth API including theory and scientific references.",,,,,"Biehl, Michael/B-5105-2009","Biehl, Michael/0000-0001-5148-4568",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706867700001,0
J,"Fischer, S; Steinwart, I",,,,"Fischer, Simon; Steinwart, Ingo",,,Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning rates for least-squares regression are typically expressed in terms of L-2-norms. In this paper we extend these rates to norms stronger than the L-2-norm without requiring the regression function to be contained in the hypothesis space. In the special case of Sobolev reproducing kernel Hilbert spaces used as hypotheses spaces, these stronger norms coincide with fractional Sobolev norms between the used Sobolev space and L-2. As a consequence, not only the target function but also some of its derivatives can be estimated without changing the algorithm. From a technical point of view, we combine the well-known integral operator techniques with an embedding property, which so far has only been used in combination with empirical process arguments. This combination results in new finite sample bounds with respect to the stronger norms. From these finite sample bounds our rates easily follow. Finally, we prove the asymptotic optimality of our results in many cases. Keywords: statistical learning theory, regularized kernel methods, least-squares regression, interpolation norms, uniform convergence, learning rates",,,,,,"Fischer, Simon/0000-0003-4851-9648",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,205,,,,,,,,,,,,,,,WOS:000590008700001,0
J,"Kobak, D; Lomond, J; Sanchez, B",,,,"Kobak, Dmitry; Lomond, Jonathan; Sanchez, Benoit",,,The Optimal Ridge Penalty for Real-world High-dimensional Data Can Be Zero or Negative due to the Implicit Ridge Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A conventional wisdom in statistical learning is that large models require strong regularization to prevent overfitting. Here we show that this rule can be violated by linear regression in the underdetermined n << p situation under realistic conditions. Using simulations and real-life high-dimensional datasets, we demonstrate that an explicit positive ridge penalty can fail to provide any improvement over the minimum-norm least squares estimator. Moreover, the optimal value of ridge penalty in this situation can be negative. This happens when the high-variance directions in the predictor space can predict the response variable, which is often the case in the real-world high-dimensional data. In this regime, low-variance directions provide an implicit ridge regularization and can make any further positive ridge penalty detrimental. We prove that augmenting any linear model with random covariates and using minimum-norm estimator is asymptotically equivalent to adding the ridge penalty. We use a spiked covariance model as an analytically tractable example and prove that the optimal ridge penalty in this case is negative when n << p.",,,,,"Kobak, Dmitry/X-3189-2019","Kobak, Dmitry/0000-0002-5639-7209",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,169,,,,,,,,,,,,,,,WOS:000570155100001,0
J,"Malik, D; Pananjady, A; Bhatia, K; Khamaru, K; Bartlett, PL; Wainwright, MJ",,,,"Malik, Dhruv; Pananjady, Ashwin; Bhatia, Kush; Khamaru, Koulik; Bartlett, Peter L.; Wainwright, Martin J.",,,Derivative-Free Methods for Policy Optimization: Guarantees for Linear Quadratic Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study derivative-free methods for policy optimization over the class of linear policies. We focus on characterizing the convergence rate of these methods when applied to linear-quadratic systems, and study various settings of driving noise and reward feedback. Our main theoretical result provides an explicit bound on the sample or evaluation complexity: we show that these methods are guaranteed to converge to within any pre-specified tolerance of the optimal policy with a number of zero-order evaluations that is an explicit polynomial of the error tolerance, dimension, and curvature properties of the problem. Our analysis reveals some interesting differences between the settings of additive driving noise and random initialization, as well as the settings of one-point and two-point reward feedback. Our theory is corroborated by simulations of derivative-free methods in application to these systems. Along the way, we derive convergence rates for stochastic zero-order optimization algorithms when applied to a certain class of non-convex problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300021,0
J,"Ortelli, F; van de Geer, S",,,,"Ortelli, Francesco; van de Geer, Sara",,,Adaptive Rates for Total Variation Image Denoising,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the theoretical properties of image denoising via total variation penalized least-squares. We define the total vatiation in terms of the two-dimensional total discrete derivative of the image and show that it gives rise to denoised images that are piecewise constant on rectangular sets. We prove that, if the true image is piecewise constant on just a few rectangular sets, the denoised image converges to the true image at a parametric rate, up to a log factor. More generally, we show that the denoised image enjoys oracle properties, that is, it is almost as good as if some aspects of the true image were known. In other words, image denoising with total variation regularization leads to an adaptive reconstruction of the true image.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,247,,,,,,,,,,,,,,,WOS:000608918200001,0
J,"Pham, NH; Nguyen, LM; Phan, DT; Quoc, TD",,,,"Pham, Nhan H.; Nguyen, Lam M.; Phan, Dzung T.; Quoc Tran-Dinh",,,ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new stochastic first-order algorithmic framework to solve stochastic composite nonconvex optimization problems that covers both finite-sum and expectation settings. Our algorithms rely on the SARAH estimator introduced in Nguyen et al. (2017a) and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. The algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. They work with both constant and dynamic step-sizes, while allowing single sample and mini-batches. In all these cases, we prove that our algorithms can achieve the best-known complexity bounds in terms of stochastic first-order oracle. One key step of our methods is the new constant and dynamic step-sizes resulting in the desired complexity bounds while improving practical performance. Our constant step-size is much larger than existing methods including proximal SVRG scheme in the single sample case. We also specify our framework to the non-composite case that covers existing state-of-the-arts in terms of oracle complexity bounds. Our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. We test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known data sets.",,,,,"Pham, Nhan H/AAA-6202-2022; Tran-Dinh, Quoc/AAX-8950-2020","Pham, Nhan H/0000-0002-4490-8649; Tran-Dinh, Quoc/0000-0002-1077-2579",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,110,,,,,,,,,,,,,,,WOS:000546629000001,0
J,"Polsterl, S",,,,"Poelsterl, Sebastian",,,scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"scikit-survival is an open-source Python package for time-to-event analysis fully compatible with scikit-learn. It provides implementations of many popular machine learning techniques for time-to-event analysis, including penalized Cox model, Random Survival Forest, and Survival Support Vector Machine. In addition, the library includes tools to evaluate model performance on censored time-to-event data. The documentation contains installation instructions, interactive notebooks, and a full description of the API. scikit-survival is distributed under the GPL-3 license with the source code and detailed instructions available at https://github.com/sebp/scikit- survival",,,,,,"Polsterl, Sebastian/0000-0002-1607-7550",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,212,,,,,,,,,,,,,,,WOS:000590022700001,0
J,"Roy, A; Dunson, DB",,,,"Roy, Arkaprava; Dunson, David B.",,,Nonparametric graphical model for counts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Although multivariate count data are routinely collected in many application areas, there is surprisingly little work developing flexible models for characterizing their dependence structure. This is particularly true when interest focuses on inferring the conditional independence graph. In this article, we propose a new class of pairwise Markov random field-type models for the joint distribution of a multivariate count vector. By employing a novel type of transformation, we avoid restricting to non-negative dependence structures or inducing other restrictions through truncations. Taking a Bayesian approach to inference, we choose a Dirichlet process prior for the distribution of a random effect to induce great flexibility in the specification. An efficient Markov chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We prove various theoretical properties, including posterior consistency, and show that our COunt Nonparametric Graphical Analysis (CONGA) approach has good performance relative to competitors in simulation studies. The methods are motivated by an application to neuron spike count data in mice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,229,,,,,,,,,,33488299,,,,,WOS:000605746500001,0
J,"Durmus, A; Majewski, S; Miasojedow, B",,,,"Durmus, Alain; Majewski, Szymon; Miasojedow, Blazej",,,Analysis of Langevin Monte Carlo via Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we provide new insights on the Unadjusted Langevin Algorithm. We show that this method can be formulated as the first order optimization algorithm for an objective functional defined on the Wasserstein space of order 2. Using this interpretation and techniques borrowed from convex optimization, we give a non-asymptotic analysis of this method to sample from log-concave smooth target distribution on R-d. Based on this interpretation, we propose two new methods for sampling from a non-smooth target distribution. These new algorithms are natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD) algorithm, which is a popular extension of the Unadjusted Langevin Algorithm for largescale Bayesian inference. Using the optimization perspective, we provide non-asymptotic convergence analysis for the newly proposed methods.",,,,,,"Majewski, Szymon/0000-0001-7152-5777; Miasojedow, Blazej/0000-0002-3691-9372",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,73,,,,,,,,,,,,,,,WOS:000467895400001,0
J,"Dwivedi, R; Chen, Y; Wainwright, MJ; Yu, B",,,,"Dwivedi, Raaz; Chen, Yuansi; Wainwright, Martin J.; Yu, Bin",,,Log-concave sampling: Metropolis-Hastings algorithms are fast,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of sampling from a strongly log-concave density supported on Rd, and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most delta delta for a density with condition number K, we show that MALA requires O (kappa d log(1/delta)) steps from a warm start, as compared to the O(kappa(2)d/delta(2)) steps established in past work on ULA. We also demonstrate the gains of a modified version of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain O(kappa) mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms.",,,,,"Dwivedi, Raaz/AAZ-2028-2020",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,183,,,,,,,,,,,,,,,WOS:000506403100023,0
J,"Li, RL; Ye, XJ; Zhou, HM; Zha, HY",,,,"Li, Ruilin; Ye, Xiaojing; Zhou, Haomin; Zha, Hongyuan",,,Learning to Match via Inverse Optimal Transport,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a unified data-driven framework based on inverse optimal transport that can learn adaptive, nonlinear interaction cost function from noisy and incomplete empirical matching matrix and predict new matching in various matching contexts. We emphasize that the discrete optimal transport plays the role of a variational principle which gives rise to an optimization based framework for modeling the observed empirical matching data. Our formulation leads to a non-convex optimization problem which can be solved efficiently by an alternating optimization method. A key novel aspect of our formulation is the incorporation of marginal relaxation via regularized Wasserstein distance, significantly improving the robustness of the method in the face of noisy or missing empirical matching data. Our model falls into the category of prescriptive models, which not only predict potential future matching, but is also able to explain what leads to empirical matching and quantifies the impact of changes in matching factors. The proposed approach has wide applicability including predicting matching in online dating, labor market, college application and crowdsourcing. We back up our claims with numerical experiments on both synthetic data and real world data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,80,,,,,,,,,,,,,,,WOS:000467897100001,0
J,"Masood, MA; Doshi-Velez, F",,,,"Masood, Muhammad A.; Doshi-Velez, Finale",,,A Particle-Based Variational Approach to Bayesian Non-negative Matrix Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian Non-negative Matrix Factorization (BNMF) is a promising approach for understanding uncertainty and structure in matrix data. However, a large volume of applied work optimizes traditional non-Bayesian NMF objectives that fail to provide a principled understanding of the non-identifiability inherent in NMF-an issue ideally addressed by a Bayesian approach. Despite their suitability, current BNMF approaches have failed to gain popularity in an applied setting; they sacrifice flexibility in modeling for tractable computation, tend to get stuck in local modes, and can require many thousands of samples for meaningful uncertainty estimates. We address these issues through a particle-based variational approach to BNMF that only requires the joint likelihood to be differentiable for computational tractability, uses a novel transfer-based initialization technique to identify multiple modes in the posterior, and thus allows domain experts to inspect a small set of factorizations that faithfully represent the posterior. On several real datasets, we obtain better particle approximations to the BNMF posterior in less time than baselines and demonstrate the significant role that multimodality plays in NMF-related tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,90,,,,,,,,,,,,,,,WOS:000470907700001,0
J,"Xiao, L; Yu, AW; Lin, QH; Chen, WZ",,,,"Xiao, Lin; Yu, Adams Wei; Lin, Qihang; Chen, Weizhu",,,DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Machine learning with big data often involves large optimization models. For distributed optimization over a cluster of machines, frequent communication and synchronization of all model parameters (optimization variables) can be very costly. A promising solution is to use parameter servers to store different subsets of the model parameters, and update them asynchronously at different machines using local datasets. In this paper, we focus on distributed optimization of large linear models with convex loss functions, and propose a family of randomized primal-dual block coordinate algorithms that are especially suitable for asynchronous distributed implementation with parameter servers. In particular, we work with the saddle-point formulation of such problems which allows simultaneous data and model partitioning, and exploit its structure by doubly stochastic coordinate optimization with variance reduction (DSCOVR). Compared with other first-order distributed algorithms, we show that DSCOVR may require less amount of overall computation and communication, and less or no synchronization. We discuss the implementation details of the DSCOVR algorithms, and present numerical experiments on an industrial distributed computing system.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,43,,,,,,,,,,,,,,,WOS:000463320100001,0
J,"Csiba, D; Richtarik, P",,,,"Csiba, Dominik; Richtarik, Peter",,,Importance Sampling for Minibatches,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Minibatching is a very well studied and highly popular technique in supervised learning, used by practitioners due to its ability to accelerate training through better utilization of parallel processing power and reduction of stochastic variance. Another popular technique is importance sampling-a strategy for preferential sampling of more important examples also capable of accelerating the training process. However, despite considerable effort by the community in these areas, and due to the inherent technical difficulty of the problem, there is virtually no existing work combining the power of importance sampling with the strength of minibatching. In this paper we propose the first practical importance sampling for minibatches and give simple and rigorous complexity analysis of its performance. We illustrate on synthetic problems that for training data of certain properties, our sampling can lead to several orders of magnitude improvement in training time. We then test the new sampling on several popular data sets, and show that the improvement can reach an order of magnitude.",,,,,"Richtarik, Peter/O-5797-2018",,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444399400001,0
J,"Lehericy, L",,,,"Lehericy, Luc",,,State-by-state Minimax Adaptive Estimation for Nonparametric Hidden Markov Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce a new estimator for the emission densities of a nonparametric hidden Markov model. It is adaptive and minimax with respect to each state's regularity-as opposed to globally minimax estimators, which adapt to the worst regularity among the emission densities. Our method is based on Goldenshluger and Lepski's methodology. It is computationally efficient and only requires a family of preliminary estimators, without any restriction on the type of estimators considered. We present two such estimators that allow to reach minimax rates up to a logarithmic term: a spectral estimator and a least squares estimator. We show how to calibrate it in practice and assess its performance on simulations and on real data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,39,,,,,,,,,,,,,,,WOS:000444406100001,0
J,"Li, QX; Chen, L; Tai, C; Weinan, E",,,,"Li, Qianxiao; Chen, Long; Tai, Cheng; Weinan, E.",,,Maximum Principle Based Algorithms for Deep Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin's maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate per iteration, provided Hamiltonian maximization can be efficiently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,165,,,,,,,,,,,,,,,WOS:000433255700001,0
J,"Nogueira, S; Sechidis, K; Brown, G",,,,"Nogueira, Sarah; Sechidis, Konstantinos; Brown, Gavin",,,On the Stability of Feature Selection Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The stability of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is 'unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms.",,,,,,"Brown, Gavin/0000-0003-2261-9018",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,174,,,,,,,,,,,,,,,WOS:000435442600001,0
J,"Sasaki, H; Kanamori, T; Hyvarinen, A; Niu, G; Sugiyama, M",,,,"Sasaki, Hiroaki; Kanamori, Takafumi; Hyvarinen, Aapo; Niu, Gang; Sugiyama, Masashi",,,Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation is accurate estimation of the ratios of the first- and second-order density derivatives to the density. A naive approach takes a three-step approach of first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator, and division by the estimated density could significantly magnify the estimation error. To cope with these problems, we propose a novel estimator for the density-derivative-ratios. The proposed estimator does not involve density estimation, but rather directly approximates the ratios of density derivatives of any order. Moreover, we establish a convergence rate of the proposed estimator. Based on the proposed estimator, novel methods both for mode-seeking clustering and density ridge estimation are developed, and the respective convergence rates to the mode and ridge of the underlying density are also established. Finally, we experimentally demonstrate that the developed methods significantly outperform existing methods, particularly for relatively high-dimensional data.",,,,,"Sugiyama, Masashi/AEO-1176-2022; Sasaki, Hiroaki/GXG-5024-2022","Sugiyama, Masashi/0000-0001-6658-6743; Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,180,,,,,,,,,,,,,,,WOS:000435444300001,0
J,"Shah, RD; Meinshausen, N",,,,"Shah, Rajen D.; Meinshausen, Nicolai",,,On b-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale regression problems where both the number of variables, p, and the number of observations, n, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. b-bit min-wise hashing (Li and Konig, 2011; Li et al., 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models, we show that the average prediction error vanishes asymptotically as long as q parallel to beta*parallel to(2)(2)/n -> 0, where q is the average number of non-zero entries in each row of the design matrix and beta* is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors.",,,,,,"Shah, Rajen/0000-0001-9073-3782",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,178,,,,,,,,,,,,,,,WOS:000435443700001,0
J,"Kim, D; Swanson, BF; Hughes, MC; Sudderth, EB",,,,"Kim, Daeil; Swanson, Benjamin F.; Hughes, Michael C.; Sudderth, Erik B.",,,Refinery: An Open Source Topic Modeling Web Platform,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also re fine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms.",,,,,,"Sudderth, Erik/0000-0002-0595-9726",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,12,,,,,,,,,,,,,,,WOS:000399835900001,0
J,"Kotthoff, L; Thornton, C; Hoos, HH; Hutter, F; Leyton-Brown, K",,,,"Kotthoff, Lars; Thornton, Chris; Hoos, Holger H.; Hutter, Frank; Leyton-Brown, Kevin",,,Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm.",,,,,"Kotthoff, Lars/AFV-6526-2022; Hoos, Holger H/B-1461-2008","Hoos, Holger H/0000-0003-0629-0099; Kotthoff, Lars/0000-0003-4635-6873",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,25,,,,,,,,,,,,,,,WOS:000399842000001,0
J,"Lin, JH; Michailidis, G",,,,"Lin, Jiahe; Michailidis, George",,,Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dynamical systems comprising of multiple components that can be partitioned into distinct blocks originate in many scientific areas. A pertinent example is the interactions between financial assets and selected macroeconomic indicators, which has been studied at aggregate level-e.g. a stock index and an employment index-extensively in the macroeconomics literature. A key shortcoming of this approach is that it ignores potential influences from other related components (e.g. Gross Domestic Product) that may impact the system's dynamics and structure and thus produces incorrect results. To mitigate this issue, we consider a multi-block linear dynamical system with Granger-causal ordering between blocks, wherein the blocks' temporal dynamics are described by vector autoregressive processes and are influenced by blocks higher in the system hierarchy. We derive the maximum likelihood estimator for the posited model for Gaussian data in the high-dimensional setting based on appropriate regularization schemes for the parameters of the block components. To optimize the underlying non-convex likelihood function, we develop an iterative algorithm with convergence guarantees. We establish theoretical properties of the maximum likelihood estimates, leveraging the decomposability of the regularizers and a careful analysis of the iterates. Finally, we develop testing procedures for the null hypothesis of whether a block Granger-causes another block of variables. The performance of the model and the testing procedures are evaluated on synthetic data, and illustrated on a data set involving log-returns of the US S&P100 component stocks and key macroeconomic variables for the 2001-16 period.",,,,,,"Lin, Jiahe/0000-0001-9523-0981",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,117,,,,,,,,,,,,,,,WOS:000424540300001,0
J,"Liu, WW; Tsang, IW; Muller, KR",,,,"Liu, Weiwei; Tsang, Ivor W.; Mueller, Klaus-Robert",,,An Easy-to-hard Learning Paradigm for Multiple Classes and Multiple Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"' Many applications, such as human action recognition and object detection, can be formulated as a multiclass classification problem. One-vs-rest (OVR) is one of the most widely used approaches for multiclass classification due to its simplicity and excellent performance. However, many confusing classes in such applications will degrade its results. For example, hand clap and boxing are two confusing actions. Hand clap is easily misclassified as boxing, and vice versa. Therefore, precisely classifying confusing classes remains a challenging task. To obtain better performance for multiclass classifications that have confusing classes, we first develop a classifier chain model for multiclass classification (CCMC) to transfer class information between classifiers. Then, based on an analysis of our proposed model, we propose an easy-to-hard learning paradigm for multiclass classification to automatically identify easy and hard classes and then use the predictions from simpler classes to help solve harder classes. Similar to CCMC, the classifier chain (CC) model is also proposed by Read et al. (2009) to capture the label dependency for multi-label classification. However, CC does not consider the order of difficulty of the labels and achieves degenerated performance when there are many confusing labels. Therefore, it is non-trivial to learn the appropriate label order for CC. Motivated by our analysis for CCMC, we also propose the easy-to-hard learning paradigm for multi-label classification to automatically identify easy and hard labels, and then use the predictions from simpler labels to help solve harder labels. We also demonstrate that our proposed strategy can be successfully applied to a wide range of applications, such as ordinal classification and relationship prediction. Extensive empirical studies validate our analysis and the effectiveness of our proposed easy-to-hard learning strategies.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,94,,,,,,,,,,,,,,,WOS:000412484700001,0
J,"Perrone, V; Jenkins, PA; Spano, D; Teh, YW",,,,"Perrone, Valerio; Jenkins, Paul A.; Spano, Dario; Teh, Yee Whye",,,Poisson Random Fields for Dynamic Feature Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.",,,,,,"Jenkins, Paul/0000-0001-7603-4205",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000424542700001,0
J,"Arias-Castro, E; Mason, D; Pelletier, B",,,,"Arias-Castro, Ery; Mason, David; Pelletier, Bruno",,,"On the Estimation of the Gradient Lines of a Density and the Consistency of the Mean-Shift Algorithm (vol 17, pg 1, 2016)",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,206,,,,,,,,,,,,,,,WOS:000391832200001,0
J,"Deshpande, Y; Montanari, A",,,,"Deshpande, Yash; Montanari, Andrea",,,Sparse PCA via Covariance Thresholding,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n x p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here each of the principal components v(1),...,v(r) has at most so non-zero entries. We are particularly interested in the high dimensional regime wherein p is comparable to, or even much larger than n. In an influential paper, Johnstone and Lu (2004) introduced a simple algorithm that estimates the support of the principal vectors v(1),...,v(r) by the largest entries in the diagonal of the empirical covariance. This method can be shown to identify the correct support with high probability if s(0) <= K-1 root n/log p, and to fail with high probability if s(0) >= K-2 root n/log p for two constants 0 < K-1, K-2 < infinity. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler, Vilenchik, et al. (2015). On the basis of numerical simulations (for the rank-one case), these authors conjectured that covariance thresholding correctly recover the support with high probability for s(0) <= K root n (assuming n of the same order as p). We prove this conjecture, and in fact establish a more general guarantee including higher-rank as well as n much smaller than p. Recent lower bounds (Berthet and Rigollet, 2013; Ma and Wigderson, 2015) suggest that no polynomial time algorithm can do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before. Using these, we also derive sharp bounds for estimating the population covariance, and the principal component (with l(2)-loss).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,141,,,,,,,,,,,,,,,WOS:000391660400001,0
J,"Savitsky, TD",,,,"Savitsky, Terrance D.",,,Scalable Approximate Bayesian Inference for Outlier Detection under Informative Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Government surveys of business establishments receive a large volume of submissions where a small subset contain errors. Analysts need a fast-computing algorithm to flag this subset due to a short time window between collection and reporting. We offer a computationally-scalable optimization method based on non-parametric mixtures of hierarchical Dirichlet processes that allows discovery of multiple industry-indexed local partitions linked to a set of global cluster centers. Outliers are nominated as those clusters containing few observations. We extend an existing approach with a new merge step that reduces sensitivity to hyperparameter settings. Survey data are typically acquired under an informative sampling design where the probability of inclusion depends on the surveyed response such that the distribution for the observed sample is different from the population. We extend the derivation of a penalized objective function to use a pseudo-posterior that incorporates sampling weights that undo the informative design. We provide a simulation study to demonstrate that our approach produces unbiased estimation for the outlying cluster under informative sampling. The method is applied for outlier nomination for the Current Employment Statistics survey conducted by the Bureau of Labor Statistics.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,226,,,,,,,,,,,,,,,WOS:000391914900001,0
J,"Honda, J; Takemura, A",,,,"Honda, Junya; Takemura, Akimichi",,,Non-Asymptotic Analysis of a New Bandit Algorithm for Semi-Bounded Reward,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we consider a stochastic multiarmed bandit problem. It is known in this problem that Deterministic Minimum Empirical Divergence (DMED) policy achieves the asymptotic theoretical bound for the model where each reward distribution is supported in a known bounded interval, say [0; 1]. However, the regret bound of DMED is described in an asymptotic form and the performance in finite time has been unknown. We modify this policy and derive a finite-time regret bound for the new policy, Indexed Minimum Empirical Divergence (IMED), by refining large deviation probabilities to a simple nonasymptotic form. Further, the refined analysis reveals that the finite-time regret bound is valid even in the case that the reward is not bounded from below. Therefore, our finitetime result applies to the case that the minimum reward (that is, the maximum loss) is unknown or unbounded. We also present some simulation results which shows that IMED much improves DMED and performs competitively to other state-of-the-art policies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3721,3756,,,,,,,,,,,,,,,,WOS:000369888000042,0
J,"Horta, D; Campello, RJGB",,,,"Horta, Danilo; Campello, Ricardo J. G. B.",,,Comparing Hard and Overlapping Clusterings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Similarity measures for comparing clusterings is an important component, e.g., of evaluating clustering algorithms, for consensus clustering, and for clustering stability assessment. These measures have been studied for over 40 years in the domain of exclusive hard clusterings (exhaustive and mutually exclusive object sets). In the past years, the literature has proposed measures to handle more general clusterings (e.g., fuzzy/probabilistic clusterings). This paper provides an overview of these new measures and discusses their drawbacks. We ultimately develop a corrected-for-chance measure (13AGRI) capable of comparing exclusive hard, fuzzy/probabilistic, non-exclusive hard, and possibilistic clusterings. We prove that 13AGRI and the adjusted Rand index (ARI, by Hubert and Arabie) are equivalent in the exclusive hard domain. The reported experiments show that only 13AGRI could provide both a fine-grained evaluation across clusterings with different numbers of clusters and a constant evaluation between random clusterings, showing all the four desirable properties considered here. We identified a high correlation between 13AGRI applied to fuzzy clusterings and ARI applied to hard exclusive clusterings over 14 real data sets from the UCI repository, which corroborates the validity of 13AGRI fuzzy clustering evaluation. 13AGRI also showed good results as a clustering stability statistic for solutions produced by the expectation maximization algorithm for Gaussian mixture. Implementation and supplementary figures can be found at http : //sn. im/25a9h8u.",,,,,"Campello, Ricardo J. G. B./F-1808-2011; Campello, Ricardo/GPK-2804-2022","Campello, Ricardo J. G. B./0000-0003-0266-3492; Campello, Ricardo/0000-0003-0266-3492",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2949,2997,,,,,,,,,,,,,,,,WOS:000369888000022,0
J,"Carpentier, A; Munos, R; Antos, A",,,,"Carpentier, Alexandra; Munos, Remi; Antos, Andras",,,Adaptive Strategy for Stratified Monte Carlo Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of stratified sampling for Monte Carlo integration of a random variable. We model this problem in a K-armed bandit, where the arms represent the K strata. The goal is to estimate the integral mean, that is a weighted average of the mean values of the arms. The learner is allowed to sample the variable n times, but it can decide on-line which stratum to sample next. We propose an UCB-type strategy that samples the arms according to an upper bound on their estimated standard deviations. We compare its performance to an ideal sample allocation that knows the standard deviations of the arms. For sub-Gaussian arm distributions, we provide bounds on the total regret: a distribution-dependent bound of order poly(lambda(-1)(min))(O) over tilde (n(-3/2))(1) that depends on a measure of the disparity lambda(min) of the per stratum variances and a distribution-free bound poly(K)(O) over tilde (n(-7/6)) that does not. We give similar, but somewhat sharper bounds on a proxy of the regret. The problem-independent bound for this proxy matches its recent minimax lower bound in terms of n up to a log n factor.",,,,,"Carpentier, Alexandra/R-8179-2018","Carpentier, Alexandra/0000-0002-1194-7385",,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2231,2271,,,,,,,,,,,,,,,,WOS:000369887600004,0
J,"Liu, J; Wright, SJ; Re, C; Bittorf, V; Sridhar, S",,,,"Liu, Ji; Wright, Stephen J.; Re, Christopher; Bittorf, Victor; Sridhar, Srikrishna",,,An Asynchronous Parallel Stochastic Coordinate Descent Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O (n(1/2)) in unconstrained optimization and O (n(1/4)) in the separable-constrained case, where n is the number of variables. We describe results from implementation on 40-core processors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2015,16,,,,,,285,322,,,,,,,,,,,,,,,,WOS:000369885800005,0
J,"Morales, GD; Bifet, A",,,,"De Francisci Morales, Gianmarco; Bifet, Albert",,,SAMOA: Scalable Advanced Massive Online Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"SAMOA (SCALABLE ADVANCED MASSIVE ONLINE ANALYSIS) is a platform for mining big data streams. It provides a collection of distributed streaming algorithms for the most common data mining and machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Storm, S4, and Samza. SAMOA is written in Java, is open source, and is available at http://samoa-project.net under the Apache Software License version 2.0.",,,,,"Bifet, Albert/E-4984-2017; De Francisci Morales, Gianmarco/AAK-2941-2021","Bifet, Albert/0000-0002-8339-7773; De Francisci Morales, Gianmarco/0000-0002-2415-494X",,,,,,,,,,,,,1532-4435,,,,,JAN,2015,16,,,,,,149,153,,,,,,,,,,,,,,,,WOS:000369885500005,0
J,"Desautels, T; Krause, A; Burdick, JW",,,,"Desautels, Thomas; Krause, Andreas; Burdick, Joel W.",,,Parallelizing Exploration-Exploitation Tradeoffs in Gaussian Process Bandit Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"How can we take advantage of opportunities for experimental parallelization in explorationexploitation tradeoffs? In many experimental scenarios, it is often desirable to execute experiments simultaneously or in batches, rather than only performing one at a time. Additionally, observations may be both noisy and expensive. We introduce Gaussian Process Batch Upper Confidence Bound (GP-BUCB), an upper confidence bound-based algorithm, which models the reward function as a sample from a Gaussian process and which can select batches of experiments to run in parallel. We prove a general regret bound for GP-BUCB, as well as the surprising result that for some common kernels, the asymptotic average regret can be made independent of the batch size. The GP-BUCB algorithm is also applicable in the related case of a delay between initiation of an experiment and observation of its results, for which the same regret bounds hold. We also introduce Gaussian Process Adaptive Upper Confidence Bound (GP-AUCB), a variant of GP-BUCB which can exploit parallelism in an adaptive manner. We evaluate GP-BUCB and GP-AUCB on several simulated and real data sets. These experiments show that GP-BUCB and GP-AUCB are competitive with state-of-the-art heuristics.",,,,,"Krause, Andreas/A-5888-2008","Krause, Andreas/0000-0001-7260-9673; Desautels, Thomas/0000-0001-6853-981X",,,,,,,,,,,,,1532-4435,,,,,DEC,2014,15,,,,,,3873,3923,,,,,,,,,,,,,,,,WOS:000354999700003,0
J,"Archer, E; Park, IM; Pillow, JW",,,,"Archer, Evan; Park, Il Memming; Pillow, Jonathan W.",,,Bayesian Entropy Estimation for Countable Discrete Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating Shannon's entropy H from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The Pitman-Yor process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over H can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and Pitman-Yor process priors. Moreover, we show that a fixed Dirichlet or Pitman-Yor process prior implies a narrow prior distribution over H, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous measures for mixing Pitman-Yor processes to produce an approximately flat prior over H. We show that the resulting Pitman-Yor Mixture (PYM) entropy estimator is consistent for a large class of distributions. Finally, we explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.",,,,,"Park, Memming/N-3092-2016","Park, Memming/0000-0002-4255-7750; Pillow, Jonathan/0000-0002-3638-8831",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,2833,2868,,,,,,,,,,,,,,,,WOS:000344638800001,0
J,"Chin, WS; Zhuang, Y; Juan, YC; Wu, F; Tung, HY; Yu, T; Wang, JP; Chang, CX; Yang, CP; Chang, WC; Huang, KH; Kuo, TM; Lin, SW; Lin, YS; Lu, YC; Su, YC; Wei, CK; Yin, TC; Li, CL; Lin, TW; Tsai, CH; Lin, SD; Lin, HT; Lin, CJ",,,,"Chin, Wei-Sheng; Zhuang, Yong; Juan, Yu-Chin; Wu, Felix; Tung, Hsiao-Yu; Yu, Tong; Wang, Jui-Pin; Chang, Cheng-Xia; Yang, Chun-Pai; Chang, Wei-Cheng; Huang, Kuan-Hao; Kuo, Tzu-Ming; Lin, Shan-Wei; Lin, Young-San; Lu, Yu-Chen; Su, Yu-Chuan; Wei, Cheng-Kuang; Yin, Tu-Chun; Li, Chun-Liang; Lin, Ting-Wei; Tsai, Cheng-Hao; Lin, Shou-De; Lin, Hsuan-Tien; Lin, Chih-Jen",,,Effective String Processing and Matching for Author Disambiguation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Track 2 of KDD Cup 2013 aims at determining duplicated authors in a data set from Microsoft Academic Search. This type of problems appears in many large-scale applications that compile information from different sources. This paper describes our solution developed at National Taiwan University to win the first prize of the competition. We propose an effective name matching framework and realize two implementations. An important strategy in our approach is to consider Chinese and non-Chinese names separately because of their different naming conventions. Post-processing including merging results of two predictions further boosts the performance. Our approach achieves Fl-score 0.99202 on the private leader board, while 0.99195 on the public leader board.",,,,,"Zhuang, Yong/AAM-1728-2020; Lin, Young-San/HDO-4676-2022",,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3037,3064,,,,,,,,,,,,,,,,WOS:000344638800007,0
J,"Agarwal, A; Chapelle, O; Dudik, M; Langford, J",,,,"Agarwal, Alekh; Chapelle, Oliveier; Dudik, Miroslav; Langford, John",,,A Reliable Effective Terascale Linear Learning System,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features,(1) billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature.(2) We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1111,1133,,,,,,,,,,,,,,,,WOS:000335458100009,0
J,"Lember, J; Koloydenko, AA",,,,"Lember, Jueri; Koloydenko, Alexey A.",,,Bridging Viterbi and Posterior Decoding: A Generalized Risk Approach to Hidden Path Inference Based on Hidden Markov Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by the unceasing interest in hidden Markov models (HMMs), this paper re-examines hidden path inference in these models, using primarily a risk-based framework. While the most common maximum a posteriori (MAP), or Viterbi, path estimator and the minimum error, or Posterior Decoder (PD) have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed elsewhere. The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk-based decoders are more clearly interpretable, and, most importantly, work out-of-the box in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in the conclusion.",,,,,"Koloydenko, Alexey/AAE-7636-2020",,,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,1,58,,,,,,,,,,,,,,,,WOS:000335457400001,0
J,"Gonen, A; Sabato, S; Shalev-Shwartz, S",,,,"Gonen, Alon; Sabato, Sivan; Shalev-Shwartz, Shai",,,Efficient Active Learning of Halfspaces: An Aggressive Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2583,2615,,,,,,,,,,,,,,,,WOS:000327007400004,0
J,"Li, YF; Tsang, IW; Kwok, JT; Zhou, ZH",,,,"Li, Yu-Feng; Tsang, Ivor W.; Kwok, James T.; Zhou, Zhi-Hua",,,Convex and Scalable Weakly Labeled SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WELLSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WELLSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WELLSVM is also readily applicable on large data sets.",,,,,"Li, yu/HHZ-5236-2022","Tsang, Ivor/0000-0001-8095-4637",,,,,,,,,,,,,1532-4435,,,,,JUL,2013,14,,,,,,2151,2188,,,,,,,,,,,,,,,,WOS:000323367000014,0
J,"McFowland, E; Speakman, S; Neill, DB",,,,"McFowland, Edward, III; Speakman, Skyler; Neill, Daniel B.",,,Fast Generalized Subset Scan for Anomalous Pattern Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets.",,,,,,"Neill, Daniel/0000-0001-6282-1240",,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1533,1561,,,,,,,,,,,,,,,,WOS:000322506400004,0
J,"Ho, CH; Lin, CJ",,,,"Ho, Chia-Hua; Lin, Chih-Jen",,,Large-scale Linear Support Vector Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector regression (SVR) and support vector classification (SVC) are popular learning techniques, but their use with kernels is often time consuming. Recently, linear SVC without kernels has been shown to give competitive accuracy for some applications, but enjoys much faster training/testing. However, few studies have focused on linear SVR. In this paper, we extend state-of-the-art training methods for linear SVC to linear SVR. We show that the extension is straightforward for some methods, but is not trivial for some others. Our experiments demonstrate that for some problems, the proposed linear-SVR training methods can very efficiently produce models that are as good as kernel SVR.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3323,3348,,,,,,,,,,,,,,,,WOS:000313200200007,0
J,"Salman, T; Baram, Y",,,,"Salman, Tamer; Baram, Yoram",,,Quantum Set Intersection and its Application to Associative Memory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a quantum algorithm for computing the intersection of two sets and its application to associative memory. The algorithm is based on a modification of Grover's quantum search algorithm (Grover, 1996). We present algorithms for pattern retrieval, pattern completion, and pattern correction. We show that the quantum associative memory can store an exponential number of memories and retrieve them in sub-exponential time. We prove that this model has advantages over known classical associative memories as well as previously proposed quantum models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3177,3206,,,,,,,,,,,,,,,,WOS:000313200200002,0
J,"Kloft, M; Blanchard, G",,,,"Kloft, Marius; Blanchard, Gilles",,,On the Convergence Rate of l(p)-Norm Multiple Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive an upper bound on the local Rademacher complexity of l(p)-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p - 1 only while our analysis covers all cases 1 <= p <= infinity, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order O( n(-)1+alpha/alpha where alpha is the minimum eigenvalue decay rate of the individual kernels.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2465,2502,,,,,,,,,,,,,,,,WOS:000308795200007,0
J,"Chiang, D",,,,"Chiang, David",,,Hope and Fear for Discriminative Training of Statistical Translation Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In machine translation, discriminative models have almost entirely supplanted the classical noisy-channel model, but are standardly trained using a method that is reliable only in low-dimensional spaces. Two strands of research have tried to adapt more scalable discriminative training methods to machine translation: the first uses log-linear probability models and either maximum likelihood or minimum risk, and the other uses linear models and large-margin methods. Here, we provide an overview of the latter. We compare several learning algorithms and describe in detail some novel extensions suited to properties of the translation task: no single correct output, a large space of structured outputs, and slow inference. We present experimental results on a large-scale Arabic-English translation task, demonstrating large gains in translation accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1159,1187,,,,,,,,,,,,,,,,WOS:000303773100009,0
J,"Cortes, C; Mohri, M; Rostamizadeh, A",,,,"Cortes, Corinna; Mohri, Mehryar; Rostamizadeh, Afshin",,,Algorithms for Learning Kernels Based on Centered Alignment,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,795,828,,,,,,,,,,,,,,,,WOS:000303772100012,0
J,"Patra, B",,,,"Patra, Benoit",,,Convergence of Distributed Asynchronous Learning Vector Quantization Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by the problem of effectively executing clustering algorithms on very large data sets, we address a model for large scale distributed clustering methods. To this end, we briefly recall some standards on the quantization problem and some results on the almost sure convergence of the competitive learning vector quantization (CLVQ) procedure. A general model for linear distributed asynchronous algorithms well adapted to several parallel computing architectures is also discussed. Our approach brings together this scalable model and the CLVQ algorithm, and we call the resulting technique the distributed asynchronous learning vector quantization algorithm (DALVQ). An in-depth analysis of the almost sure convergence of the DALVQ algorithm is performed. A striking result is that we prove that the multiple versions of the quantizers distributed among the processors in the parallel architecture asymptotically reach a consensus almost surely. Furthermore, we also show that these versions converge almost surely towards the same nearly optimal value for the quantization criterion.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2011,12,,,,,,3431,3466,,,,,,,,,,,,,,,,WOS:000299681200002,0
J,"Cesa-Bianchi, N; Shalev-Shwartz, S; Shamir, O",,,,"Cesa-Bianchi, Nicolo; Shalev-Shwartz, Shai; Shamir, Ohad",,,Efficient Learning with Partially Observed Attributes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate three variants of budgeted learning, a setting in which the learner is allowed to access a limited number of attributes from training or test examples. In the local budget setting, where a constraint is imposed on the number of available attributes per training example, we design and analyze an efficient algorithm for learning linear predictors that actively samples the attributes of each training instance. Our analysis bounds the number of additional examples sufficient to compensate for the lack of full information on the training set. This result is complemented by a general lower bound for the easier global budget setting, where it is only the overall number of accessible training attributes that is being constrained. In the third, prediction on a budget setting, when the constraint is on the number of available attributes per test example, we show that there are cases in which there exists a linear predictor with zero error but it is statistically impossible to achieve arbitrary accuracy without full information on test examples. Finally, we run simple experiments on a digit recognition problem that reveal that our algorithm has a good performance against both partial information and full information baselines.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2857,2878,,,,,,,,,,,,,,,,WOS:000298103200005,0
J,"Audibert, JY; Bubeck, S",,,,"Audibert, Jean-Yves; Bubeck, Sebastien",,,Regret Bounds and Minimax Policies under Partial Monitoring,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work deals with four classical prediction settings, namely full information, bandit, label efficient and bandit label efficient as well as four different notions of regret: pseudo-regret, expected regret, high probability regret and tracking the best expert regret. We introduce a new forecaster, INF (Implicitly Normalized Forecaster) based on an arbitrary function psi for which we propose a unified analysis of its pseudo-regret in the four games we consider. In particular, for psi(x) = exp(eta x) + gamma/K, INF reduces to the classical exponentially weighted average forecaster and our analysis of the pseudo-regret recovers known results while for the expected regret we slightly tighten the bounds. On the other hand with psi(x) = (eta/-x)(q) + gamma/K, which defines a new forecaster, we are able to remove the extraneous logarithmic factor in the pseudo-regret bounds for bandits games, and thus fill in a long open gap in the characterization of the minimax rate for the pseudo-regret in the bandit game. We also provide high probability bounds depending on the cumulative reward of the optimal action. Finally, we consider the stochastic bandit game, and prove that an appropriate modification of the upper confidence bound policy UCB1 (Auer et al., 2002a) achieves the distribution-free optimal rate while still having a distribution-dependent rate logarithmic in the number of plays.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2785,2836,,,,,,,,,,,,,,,,WOS:000284040000007,0
J,"Chechik, G; Sharma, V; Shalit, U; Bengio, S",,,,"Chechik, Gal; Sharma, Varun; Shalit, Uri; Bengio, Samy",,,Large Scale Online Learning of Image Similarity Through Ranking,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2010,11,,,,,,1109,1135,,,,,,,,,,,,,,,,WOS:000277186600003,0
J,"Lehmann, J",,,,"Lehmann, Jens",,,DL-Learner: Learning Concepts in Description Logics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the official W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service.",,,,,,"Lehmann, Jens/0000-0001-9108-4278",,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2639,2642,,,,,,,,,,,,,,,,WOS:000272346600008,0
J,"Jiang, WX",,,,"Jiang, Wenxin",,,"On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical learning theory of risk minimization depends heavily on probability bounds for uniform deviations of the empirical risks. Classical probability bounds using Hoeffding's inequality cannot accommodate more general situations with unbounded loss and dependent data. The current paper introduces an inequality that extends Hoeffding's inequality to handle these more general situations. We will apply this inequality to provide probability bounds for uniform deviations in a very general framework, which can involve discrete decision rules, unbounded loss, and a dependence structure that can be more general than either martingale or strong mixing. We will consider two examples with high dimensional predictors: autoregression (AR) with l(1)-loss, and ARX model with variable selection for sign classification, which uses both lagged responses and exogenous predictors.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,977,996,,,,,,,,,,,,,,,,WOS:000270824600007,0
J,"Bab, A; Brafman, RI",,,,"Bab, Avraham; Brafman, Ronen I.",,,Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the field, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to specific tasks, and may guide further research and formal analysis of the learning processes.",,,,,,"Brafman, Ronen/0000-0001-8227-5646",,,,,,,,,,,,,1532-4435,,,,,DEC,2008,9,,,,,,2635,2675,,,,,,,,,,,,,,,,WOS:000263240700001,0
J,"Chen, SC; Gordon, GJ; Murphy, RF",,,,"Chen, Shann-Ching; Gordon, Geoffrey J.; Murphy, Robert F.",,,"Graphical models for structured classification, with an application to interpreting images of protein subcellular location patterns",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In structured classification problems, there is a direct conflict between expressive models and efficient inference: while graphical models such as Markov random fields or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efficient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efficient inference in practical structured classification problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classification accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,651,682,,,,,,,,,,,,,,,,WOS:000256642100005,0
J,"Gabrilovich, E; Markovitch, S",,,,"Gabrilovich, Evgeniy; Markovitch, Shaul",,,"Harnessing the expertise of 70,000 human editors: Knowledge-based feature generation for text categorization",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-specific and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two significant problems in natural language processing-synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets confirm improved performance compared to the bag of words document representation.",,,,,,"Gabrilovich, Evgeniy/0000-0001-7933-1926",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2297,2345,,,,,,,,,,,,,,,,WOS:000252744800004,0
J,"Liu, CC; Dai, DQ; Yan, H",,,,"Liu, Chao-Chun; Dai, Dao-Qing; Yan, Hong",,,Local discriminant wavelet packet coordinates for face recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Face recognition is a challenging problem due to variations in pose, illumination, and expression. Techniques that can provide effective feature representation with enhanced discriminability are crucial. Wavelets have played an important role in image processing for its ability to capture localized spatial-frequency information of images. In this paper, we propose a novel local discriminant coordinates method based on wavelet packet for face recognition to compensate for these variations. Traditional wavelet-based methods for face recognition select or operate on the most discriminant subband, and neglect the scattered characteristic of discriminant features. The proposed method selects the most discriminant coordinates uniformly from all spatial frequency subbands to overcome the deficiency of traditional wavelet-based methods. To measure the discriminability of coordinates, a new dilation invariant entropy and a maximum a posterior logistic model are put forward. Moreover, a new triangle square ratio criterion is used to improve classification using the Euclidean distance and the cosine criterion. Experimental results show that the proposed method is robust for face recognition under variations in illumination, pose and expression.",,,,,,"YAN, Hong/0000-0001-9661-3095",,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1165,1195,,,,,,,,,,,,,,,,WOS:000248351700010,0
J,"Sugiyama, M",,,,"Sugiyama, Masashi",,,Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called locality-preserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classification tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1027,1061,,,,,,,,,,,,,,,,WOS:000248351700005,0
J,"Ying, YM; Zhou, DX",,,,"Ying, Yiming; Zhou, Ding-Xuan",,,Learnability of Gaussians with flexible variances,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian kernels with flexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with flexible variances is a uniform Glivenko-Cantelli (uGC) class. This result confirms a conjecture concerning learnability of Gaussian kernels and verifies the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when flexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with flexible variances we present explicit learning rates for regression with least square loss and classification with hinge loss.",,,,,"Ying, Yiming/AGD-7246-2022; Ying, Yiming/A-4196-2013; Zhou, Ding-Xuan/B-3160-2013","Ying, Yiming/0000-0001-7345-6672; Ying, Yiming/0000-0001-7345-6672; Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,249,276,,,,,,,,,,,,,,,,WOS:000247002600003,0
J,"Angluin, D; Chen, J",,,,"Angluin, Dana; Chen, Jiang",,,Learning a hidden hypergraph,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning a hypergraph using edge-detecting queries. In this model, the learner may query whether a set of vertices induces an edge of the hidden hypergraph or not. We show that an r-uniform hypergraph with m edges and n vertices is learnable with O(2(4r)m. poly (r,logn)) queries with high probability. The queries can be made in O (min(2(r)(logm + r)(2); (logm + r)(3))) rounds. We also give an algorithm that learns an almost uniform hypergraph of dimension r using O(2(O((1 + Delta/2)r)) . m(1 + Delta/2) . poly(logn)) queries with high probability, where Delta is the difference between the maximum and the minimum edge sizes. This upper bound matches our lower bound of Omega((m/1 + Delta/2)(1 + Delta/2)) for this class of hypergraphs in terms of dependence on m. The queries can also be made in O((1 + Delta) . min (2(r)(logm + r)(2), (logm + r)(3))) rounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2006,7,,,,,,2215,2236,,,,,,,,,,,,,,,,WOS:000245390500010,0
J,"Kok, JR; Vlassis, N",,,,"Kok, Jelle R.; Vlassis, Nikos",,,Collaborative multiagent reinforcement learning by payoff propagation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article we describe a set of scalable techniques for learning the behavior of a group of agents in a collaborative multiagent setting. As a basis we use the framework of coordination graphs of Guestrin, Koller, and Parr (2002a) which exploits the dependencies between agents to decompose the global payoff function into a sum of local terms. First, we deal with the single-state case and describe a payoff propagation algorithm that computes the individual actions that approximately maximize the global payoff function. The method can be viewed as the decision-making analogue of belief propagation in Bayesian networks. Second, we focus on learning the behavior of the agents in sequential decision-making tasks. We introduce different model-free reinforcement-learning techniques, unitedly called Sparse Cooperative Q-learning, which approximate the global action-value function based on the topology of a coordination graph, and perform updates using the contribution of the individual agents to the maximal global action value. The combined use of an edge-based decomposition of the action-value function and the payoff propagation algorithm for efficient action selection, result in an approach that scales only linearly in the problem size. We provide experimental evidence experimental evidence that our method outperforms related multiagent reinforcement-learning methods based on temporal differences.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2006,7,,,,,,1789,1828,,,,,,,,,,,,,,,,WOS:000245389400001,0
J,"Bennett, KP; Parrado-Hernandez, E",,,,"Bennett, Kristin P.; Parrado-Hernandez, Emilio",,,The interplay of optimization and machine learning research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.",,,,,"PARRADO-HERNANDEZ, EMILIO/ABH-2027-2020","PARRADO-HERNANDEZ, EMILIO/0000-0003-2146-2135",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1265,1281,,,,,,,,,,,,,,,,WOS:000245388800005,0
J,"Cesa-Bianchi, N; Gentile, C; Zaniboni, L",,,,"Cesa-Bianchi, Nicolo; Gentile, Claudio; Zaniboni, Luca",,,Worst-case analysis of selective sampling for linear classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A selective sampling algorithm is a learning algorithm for classification that, based on the past observed data, decides whether to ask the label of each new instance to be classified. In this paper, we introduce a general technique for turning linear-threshold classification algorithms from the general additive family into randomized selective sampling algorithms. For the most popular algorithms in this family we derive mistake bounds that hold for individual sequences of examples. These bounds show that our semi-supervised algorithms can achieve, on average, the same accuracy as that of their fully supervised counterparts, but using fewer labels. Our theoretical results are corroborated by a number of experiments on real-world textual data. The outcome of these experiments is essentially predicted by our theoretical results: Our selective sampling algorithms tend to perform as well as the algorithms receiving the true label after each classification, while observing in practice substantially fewer labels.",,,,,"Cesa-Bianchi, Nicol√≤/C-3721-2013","Cesa-Bianchi, Nicol√≤/0000-0001-8477-4748",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1205,1230,,,,,,,,,,,,,,,,WOS:000245388800003,0
J,"Shalev-Shwartz, S; Singer, Y",,,,"Shalev-Shwartz, Shai; Singer, Yoram",,,Efficient learning of label ranking by soft projections onto polyhedra,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discuss the problem of learning to rank labels from a real valued feedback associated with each label. We cast the feedback as a preferences graph where the nodes of the graph are the labels and edges express preferences over labels. We tackle the learning problem by defining a loss function for comparing a predicted graph with a feedback graph. This loss is materialized by decomposing the feedback graph into bipartite sub-graphs. We then adopt the maximum-margin framework which leads to a quadratic optimization problem with linear constraints. While the size of the problem grows quadratically with the number of the nodes in the feedback graph, we derive a problem of a significantly smaller size and prove that it attains the same minimum. We then describe an efficient algorithm, called SOPOPO, for solving the reduced problem by employing a soft projection onto the polyhedron defined by a reduced set of constraints. We also describe and analyze a wrapper procedure for batch learning when multiple graphs are provided for training. We conclude with a set of experiments which show significant improvements in run time over a state of the art interior-point algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1567,1599,,,,,,,,,,,,,,,,WOS:000245388800017,0
J,"Sonnenburg, S; Ratsch, G; Schafer, C; Scholkopf, B",,,,"Sonnenburg, Soeren; Raetsch, Gunnar; Schaefer, Christin; Schoelkopf, Bernhard",,,Large scale multiple kernel learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun.",,,,,"R√§tsch, Gunnar/O-5914-2017; R√§tsch, Gunnar/B-8182-2009; Sch√∂lkopf, Bernhard/A-7570-2013; Sonnenburg, Soeren S/F-2230-2010","R√§tsch, Gunnar/0000-0001-5486-8532; Sch√∂lkopf, Bernhard/0000-0002-8177-0925; ",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1531,1565,,,,,,,,,,,,,,,,WOS:000245388800016,0
J,"Goldberg, P",,,,"Goldberg, P",,,Some discriminant-based PAC algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,14th Annual Conference on Computational Learning Theory (COLT 2001)/5th European Conference on Computational Learning Theory (EuroCOLT 2001),"JUL 16-19, 2001","AMSTERDAM, NETHERLANDS","Natl Res Inst Math & Comp Sci,Amsterdam Hist Museum,Netherlands Org Sci Res",,,,"A classical approach in multi-class pattern classification is the following. Estimate the probability distributions that generated the observations for each label class, and then label new instances by applying the Bayes classifier to the estimated distributions. That approach provides more useful information than just a class label; it also provides estimates of the conditional distribution of class labels, in situations where there is class overlap. We would like to know whether it is harder to build accurate classifiers via this approach, than by techniques that may process all data with distinct labels together. In this paper we make that question precise by considering it in the context of PAC learnability. We propose two restrictions on the PAC learning framework that are intended to correspond with the above approach, and consider their relationship with standard PAC learning. Our main restriction of interest leads to some interesting algorithms that show that the restriction is not stronger (more restrictive) than various other well-known restrictions on PAC learning. An alternative slightly milder restriction turns out to be almost equivalent to unrestricted PAC learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,283,306,,,,,,,,,,,,,,,,WOS:000236331700004,0
J,"Kitzelmann, E; Schmid, U",,,,"Kitzelmann, E; Schmid, U",,,Inductive synthesis of functional programs: An explanation based generalization approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe an approach to the inductive synthesis of recursive equations from input/output examples which is based on the classical two-step approach to induction of functional Lisp programs of Summers (1977). In a first step, I/O-examples are rewritten to traces which explain the outputs given the respective inputs based on a datatype theory. These traces can be integrated into one conditional expression which represents a non-recursive program. In a second step, this initial program term is generalized into recursive equations by searching for syntactical regularities in the term. Our approach extends the classical work in several aspects. The most important extensions are that we are able to induce a set of recursive equations in one synthesizing step, the equations may contain more than one recursive call, and additionally needed parameters are automatically introduced.",,,,,,"Schmid, Ute/0000-0002-1301-0326",,,,,,,,,,,,,1532-4435,,,,,FEB,2006,7,,,,,,429,454,,,,,,,,,,,,,,,,WOS:000236331700009,0
J,"Drineas, P; Mahoney, MW",,,,"Drineas, P; Mahoney, MW",,,On the Nystrom method for approximating a gram matrix for improved kernel-based learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A problem for many kernel-based methods is that the amount of computation required to find the solution scales as O(n(3)), where n is the number of training examples. We develop and analyze an algorithm to compute an easily-interpretable low-rank approximation to an n x n Gram matrix G such that computations of interest may be performed more rapidly. The approximation is of the form (G) over tilde (k) = CWk+ C-T, where C is a matrix consisting of a small number c of columns of G and W-k is the best rank-k approximation to W, the matrix formed by the intersection between those c columns of G and the corresponding c rows of G. An important aspect of the algorithm is the probability distribution used to randomly sample the columns; we will use a judiciously-chosen and data-dependent nonuniform probability distribution. Let parallel to(.)parallel to(2) and parallel to(.)parallel to(F) denote the spectral norm and the Frobenius norm, respectively, of a matrix, and let G(k) be the best rank-k approximation to G. We prove that by choosing O(k/epsilon(4)) columns parallel to G-(CWk+CT)parallel to(xi) <=parallel to G-G(k)parallel to(xi)+epsilon Sigma(n)(i=1) G(ii)(2) both in expectation and with high probability, for both xi = 2, F, and for all k : 0 <= k <= rank(W). This approximation can be computed using O(n) additional space and time, after making two passes over the data from external storage. The relationships between this algorithm, other related matrix decompositions, and the Nystrom method from integral equation theory are discussed.",,,,,,"Drineas, Petros/0000-0003-1994-8670",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2153,2175,,,,,,,,,,,,,,,,WOS:000236331100010,0
J,"Shani, G; Heckerman, D; Brafman, RI",,,,"Shani, G; Heckerman, D; Brafman, RI",,,An MDP-based recommender system,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the first to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach.",,,,,"; Shani, Guy/F-1634-2012","Brafman, Ronen/0000-0001-8227-5646; Shani, Guy/0000-0003-4131-0382",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1265,1295,,,,,,,,,,,,,,,,WOS:000236330100001,0
J,"Steinwart, I",,,,"Steinwart, I",,,Sparseness of support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines (SVMs) construct decision functions that are linear combinations of kernel evaluations on the training set. The samples with non-vanishing coefficients are called support vectors. In this work we establish lower (asymptotical) bounds on the number of support vectors. On our way we prove several results which are of great importance for the understanding of SVMs. In particular, we describe to which limit SVM decision functions tend, discuss the corresponding notion of convergence and provide some results on the stability of SVMs using subdifferential calculus in the associated reproducing kernel Hilbert space.",,,,,,"Steinwart, Ingo/0000-0002-4436-7109",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,1071,1105,,10.1162/1532443041827925,0,,,,,,,,,,,,,WOS:000231002600006,0
J,"Clark, A; Thollard, F",,,,"Clark, A; Thollard, F",,,PAC-learnability of Probabilistic Deterministic Finite State Automata,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the learnability of Probabilistic Deterministic Finite State Automata under a modified PAC-learning criterion. We argue that it is necessary to add additional parameters to the sample complexity polynomial, namely a bound on the expected length of strings generated from any state, and a bound on the distinguishability between states. With this, we demonstrate that the class of PDFAs is PAC-learnable using a variant of a standard state-merging algorithm and the Kullback-Leibler divergence as error function.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2004,5,,,,,,473,497,,,,,,,,,,,,,,,,WOS:000236327500002,0
J,"Sugiyama, M; Muller, KR",,,,"Sugiyama, M; Muller, KR",,,The subspace information criterion for infinite dimensional hypothesis spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A central problem in learning is selection of an appropriate model. This is typically done by estimating the unknown generalization errors of a set of models to be selected from and then choosing the model with minimal generalization error estimate. In this article, we discuss the problem of model selection and generalization error estimation in the context of kernel regression models, e.g., kernel ridge regression, kernel subset regression or Gaussian process regression. Previously, a non-asymptotic generalization error estimator called the subspace information criterion (SIC) was proposed, that could be successfully applied to finite dimensional subspace models. SIC is an unbiased estimator of the generalization error for the finite sample case under the conditions that the learning target function belongs to a specified reproducing kernel Hilbert space (RKHS) H and the reproducing kernels centered on training sample points span the whole space R. These conditions hold only if dim H less than or equal to l, where l (< infinity) is the number of training examples. Therefore, SIC could be applied only to finite dimensional RKHSs. In this paper, we extend the range of applicability of SIC, and show that even if the reproducing kernels centered on training sample points do not span the whole space R, SIC is an unbiased estimator of an essential part of the generalization error. Our extension allows the use of any RKHSs including infinite dimensional ones, i.e., richer function classes commonly used in Gaussian processes, support vector machines or boosting. We further show that when the kernel matrix is invertible, SIC can be expressed in a much simpler form, making its computation highly efficient. In computer simulations on ridge parameter selection with real and artificial data sets, SIC is compared favorably with other standard model selection techniques for instance leave-one-out cross-validation or an empirical Bayesian method.",,,,,"Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013","Mueller, Klaus-Robert/0000-0002-3861-7685; ",,,,,,,,,,,,,1532-4435,,,,,Feb-15,2003,3,2,,,,,323,359,,10.1162/153244303765208412,0,,,,,,,,,,,,,WOS:000182488500006,0
J,"Gao, ZJ; Hastie, T",,,,"Gao, Zijun; Hastie, Trevor",,,LinCDE: Conditional Density Estimation via Lindsey's Method,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional density estimation is a fundamental problem in statistics, with scientific and practical applications in biology, economics, finance and environmental studies, to name a few. In this paper, we propose a conditional density estimator based on gradient boosting and Lindsey's method (LinCDE). LinCDE admits flexible modeling of the density family and can capture distributional characteristics like modality and shape. In particular, when suitably parametrized, LinCDE will produce smooth and non-negative density estimates. Furthermore, like boosted regression trees, LinCDE does automatic feature selection. We demonstrate LinCDE's efficacy through extensive simulations and three real data examples.",,,,,,"Hastie, Trevor/0000-0002-0164-3142",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,55,,,,,,,,,,,,,,,,WOS:000752280200001,0
J,"Liu, S; Garrepalli, R; Hendrycks, D; Fern, A; Mondal, D; Dietterich, TG",,,,"Liu, Si; Garrepalli, Risheek; Hendrycks, Dan; Fern, Alan; Mondal, Debashis; Dietterich, Thomas G.",,,PAC Guarantees and Effective Algorithms for Detecting Novel Categories,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Open category detection is the problem of detecting alien test instances that belong to categories or classes that were not present in the training data (Liu et al., 2018). In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a clean training set that contains only the target categories of interest and an unlabeled contaminated training set that contains a fraction alpha of alien examples. Under the assumption that we know an upper bound on alpha, we develop an algorithm that gives PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Given an overall budget on the amount of training data, we also derive the optimal allocation of samples between the mixture and the clean data sets. Experiments on synthetic and standard benchmark datasets evaluate the regimes in which the algorithm can be effective and provide a baseline for further advancements. In addition, for the situation when an upper bound for alpha is not available, we employ nine different anomaly proportion estimators, and run experiments on both synthetic and standard benchmark data sets to compare their performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,47,,,,,,,,,,,,,,,,WOS:000766894100001,0
J,"Mourtada, J; Gaiffas, S",,,,"Mourtada, Jaouad; Gaiffas, Stephane",,,An improper estimator with optimal excess risk in misspecified density estimation and logistic regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a procedure for conditional density estimation under logarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator minimizes a new general excess risk bound for statistical learning. On standard examples, this bound scales as d n with d the model dimension and n the sample size, and critically remains valid under model misspecification. Being an improper (out-of-model) procedure, SMP improves over within-model estimators such as the maximum likelihood estimator, whose excess risk degrades under misspecification. Compared to approaches reducing to the sequential problem, our bounds remove suboptimal log n factors and can handle unbounded classes. For the Gaussian linear model, the predictions and risk bound of SMP are governed by leverage scores of covariates, nearly matching the optimal risk in the well-specified case without conditions on the noise variance or approximation error of the linear model. For logistic regression, SMP provides a non-Bayesian approach to calibration of probabilistic predictions relying on virtual samples, and can be computed by solving two logistic regressions. It achieves a non-asymptotic excess risk of O((d+ (BR2)-R-2)/n), where R bounds the norm of features and B that of the comparison parameter; by contrast, no within-model estimator can achieve better rate than min(BR/root n, e(BR)/n) in general (Hazan et al., 2014). This provides a more practical alternative to Bayesian approaches, which require approximate posterior sampling, thereby partly addressing a question raised by Foster et al. (2018).",,,,,,"Mourtada, Jaouad/0000-0002-7830-9783",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,49,,,,,,,,,,,,,,,,WOS:000752276800001,0
J,"Ben Arous, G; Gheissari, R; Jagannath, A",,,,"Ben Arous, Gerard; Gheissari, Reza; Jagannath, Aukosh",,,Online stochastic gradient descent on non-convex losses from high-dimensional inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic gradient descent (SGD) is a popular algorithm for optimization problems arising in high-dimensional inference tasks. Here one produces an estimator of an unknown parameter from independent samples of data by iteratively optimizing a loss function. This loss function is random and often non-convex. We study the performance of the simplest version of SGD, namely online SGD, from a random start in the setting where the parameter space is high-dimensional. We develop nearly sharp thresholds for the number of samples needed for consistent estimation as one varies the dimension. Our thresholds depend only on an intrinsic property of the population loss which we call the information exponent. In particular, our results do not assume uniform control on the loss itself, such as convexity or uniform derivative bounds. The thresholds we obtain are polynomial in the dimension and the precise exponent depends explicitly on the information exponent. As a consequence of our results, we find that except for the simplest tasks, almost all of the data is used simply in the initial search phase to obtain non-trivial correlation with the ground truth. Upon attaining non-trivial correlation, the descent is rapid and exhibits law of large numbers type behavior. We illustrate our approach by applying it to a wide set of inference tasks such as phase retrieval, and parameter estimation for generalized linear models, online PCA, and spiked tensor models, as well as to supervised learning for single-layer networks with general activation functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,106,,,,,,,,,,,,,,,WOS:000663161500001,0
J,"Cai, HQ; Hamm, K; Huang, LX; Needell, D",,,,"Cai, HanQin; Hamm, Keaton; Huang, Longxiu; Needell, Deanna",,,Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Low rank tensor approximation is a fundamental tool in modern machine learning and data science. In this paper, we study the characterization, perturbation analysis, and an efficient sampling strategy for two primary tensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact tensor CUR decompositions for low multilinear rank tensors. We also present theoretical error bounds of the tensor CUR approximations when (adversarial or Gaussian) noise appears. Moreover, we show that low cost uniform sampling is sufficient for tensor CUR approximations if the tensor has an incoherent structure. Empirical performance evaluations, with both synthetic and real-world datasets, establish the speed advantage of the tensor CUR approximations over other state-of-the-art low multilinear rank tensor approximations.",,,,,"Cai, Hanqin/AHD-4307-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687116500001,0
J,"Grosnit, A; Cowen-Rivers, AI; Tutunov, R; Griffiths, RR; Wang, J; Bou-Ammar, H",,,,"Grosnit, Antoine; Cowen-Rivers, Alexander, I; Tutunov, Rasul; Griffiths, Ryan-Rhys; Wang, Jun; Bou-Ammar, Haitham",,,Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian optimisation presents a sample-efficient methodology for global optimisation. Within this framework, a crucial performance-determining subroutine is the maximisation of the acquisition function, a task complicated by the fact that acquisition functions tend to be non-convex and thus nontrivial to optimise. In this paper, we undertake a comprehensive empirical study of approaches to maximise the acquisition function. Additionally, by deriving novel, yet mathematically equivalent, compositional forms for popular acquisition functions, we recast the maximisation task as a compositional optimisation problem, allowing us to benefit from the extensive literature in this field. We highlight the empirical advantages of the compositional approach to acquisition function maximisation across 3958 individual experiments comprising synthetic optimisation tasks as well as tasks from Bayesmark. Given the generality of the acquisition function maximisation subroutine, we posit that the adoption of compositional optimisers has the potential to yield performance improvements across all domains in which Bayesian optimisation is currently being applied. An open-source implementation is made available at https://github.com/huawei-noah/noah-research/tree/CompBO/BO/HEBO/CompBO",,,,,"Griffiths, Ryan-Rhys/AAP-4545-2020","Griffiths, Ryan-Rhys/0000-0003-3117-4559",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687193700001,0
J,"Martin, CH; Mahoney, MW",,,,"Martin, Charles H.; Mahoney, Michael W.",,,Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization, implicitly sculpting a more regularized energy or penalty landscape. In particular, the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization, such as Dropout or Weight Norm constraints. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, and applying them to these empirical results, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a size scale separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems (such as classical models of actual neural activity). This results from correlations arising at all size scales, which for DNNs arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. In particular, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. Our results suggest that large, well-trained DNN architectures should exhibit Heavy-Tailed Self-Regularization, and we discuss the theoretical and practical implications of this.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687037600001,0
J,"Wang, W; Stephens, M",,,,"Wang, Wei; Stephens, Matthew",,,Empirical Bayes Matrix Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Matrix factorization methods, which include Factor analysis (FA) and Principal Components Analysis (PCA), are widely used for inferring and summarizing structure in multivariate data. Many such methods use a penalty or prior distribution to achieve sparse representations (Sparse FA/PCA), and a key question is how much sparsity to induce. Here we introduce a general Empirical Bayes approach to matrix factorization (EBMF), whose key feature is that it estimates the appropriate amount of sparsity by estimating prior distributions from the observed data. The approach is very flexible: it allows for a wide range of different prior families and allows that each component of the matrix factorization may exhibit a different amount of sparsity. The key to this flexibility is the use of a variational approximation, which we show effectively reduces fitting the EBMF model to solving a simpler problem, the so-called normal means problem. We demonstrate the benefits of EBMF with sparse priors through both numerical comparisons with competing methods and through analysis of data from the GTEx (Genotype Tissue Expression) project on genetic associations across 44 human tissues. In numerical comparisons EBMF often provides more accurate inferences than other methods. In the GTEx data, EBMF identifies interpretable structure that agrees with known relationships among human tissues. Software implementing our approach is available at https://github.com/stephenslab/flashr.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,40,,,,,,,,,,,,,,,,WOS:000663172100001,0
J,"Wynne, G; Briol, FX; Girolami, M",,,,"Wynne, George; Briol, Francois-Xavier; Girolami, Mark",,,Convergence Guarantees for Gaussian Process Means With Misspecified Likelihoods and Smoothness,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian processes are ubiquitous in machine learning, statistics, and applied mathematics. They provide a flexible modelling framework for approximating functions, whilst simultaneously quantifying uncertainty. However, this is only true when the model is well-specified, which is often not the case in practice. In this paper, we study the properties of Gaussian process means when the smoothness of the model and the likelihood function are misspecified. In this setting, an important theoretical question of practical relevance is how accurate the Gaussian process approximations will be given the chosen model and the extent of the misspecification. The answer to this problem is particularly useful since it can inform our choice of model and experimental design. In particular, we describe how the experimental design and choice of kernel and kernel hyperparameters can be adapted to alleviate model misspecification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,40,123,,,,,,,,,,,,,,,WOS:000663173800001,0
J,"Berrendero, JR; Bueno-Larraz, B; Cuevas, A",,,,"Berrendero, Jose R.; Bueno-Larraz, Beatriz; Cuevas, Antonio",,,On Mahalanobis Distance in Functional Settings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.",,,,,"Berrendero, Jos√© R./E-9932-2016","Berrendero, Jos√© R./0000-0003-0728-7748",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300009,0
J,"Bezakova, I; Blanca, A; Chen, ZC; Stefankovic, D; Vigoda, E",,,,"Bezakova, Ivona; Blanca, Antonio; Chen, Zongchen; Stefankovic, Daniel; Vigoda, Eric",,,Lower Bounds for Testing Graphical Models: Colorings and Antiferromagnetic Ising Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the identity testing problem in the context of spin systems or undirected graphical models, where it takes the following form: given the parameter specification of the model M and a sampling oracle for the distribution mu(M*) of an unknown model M*, can we efficiently determine if the two models M and M* are the same? We consider identity testing for both soft-constraint and hard-constraint systems. In particular, we prove hardness results in two prototypical cases, the Ising model and proper colorings, and explore whether identity testing is any easier than structure learning. For the ferromagnetic (attractive) Ising model, Daskalakis et al. (2018) presented a polynomial time algorithm for identity testing. We prove hardness results in the anti-ferromagnetic (repulsive) setting in the same regime of parameters where structure learning is known to require a super-polynomial number of samples. Specifically, for n-vertex graphs of maximum degree d, we prove that if vertical bar beta vertical bar d = omega (log n) (where beta is the inverse temperature parameter), then there is no polynomial running time identity testing algorithm unless RP = NP. In the hard-constraint setting, we present hardness results for identity testing for proper colorings. Our results are based on the presumed hardness of #BIS, the problem of (approximately) counting independent sets in bipartite graphs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300025,0
J,"da Silva, AB; Gazeau, M",,,,"da Silva, Andre Belotto; Gazeau, Maxime",,,A General System of Differential Equations to Model First-Order Adaptive Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"First-order optimization algorithms play a major role in large scale machine learning. A new class of methods, called adaptive algorithms, were recently introduced to adjust iteratively the learning rate for each coordinate. Despite great practical success in deep learning, their behavior and performance on more general loss functions are not well understood. In this paper, we derive a non-autonomous system of differential equations, which is the continuous time limit of adaptive optimization methods. We study the convergence of its trajectories and give conditions under which the differential system, underlying all adaptive algorithms, is suitable for optimization. We discuss convergence to a critical point in the non-convex case and give conditions for the dynamics to avoid saddle points and local maxima. For convex loss function, we introduce a suitable Lyapunov functional which allows us to study its rate of convergence. Several other properties of both the continuous and discrete systems are briefly discussed. The differential system studied in the paper is general enough to encompass many other classical algorithms (such as Heavy Ball and Nesterov's accelerated method) and allow us to recover several known results for these algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,129,,,,,,,,,,,,,,,WOS:000556296600001,0
J,"Gassiat, E; Le Corff, S; Lehericy, L",,,,"Gassiat, Elisabeth; Le Corff, Sylvain; Lehericy, Luc",,,Identifiability and Consistent Estimation of Nonparametric Translation Hidden Markov Models with General State Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers hidden Markov models where the observations are given as the sum of a latent state which lies in a general state space and some independent noise with unknown distribution. It is shown that these fully nonparametric translation models are identifiable with respect to both the distribution of the latent variables and the distribution of the noise, under mostly a light tail assumption on the latent variables. Two nonparametric estimation methods are proposed and we prove that the corresponding estimators are consistent for the weak convergence topology. These results are illustrated with numerical experiments.",,,,,,"Le Corff, Sylvain/0000-0001-5211-2328",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,115,,,,,,,,,,,,,,,WOS:000546630800001,0
J,"Knop, S; Spurek, P; Tabor, J; Podolak, I; Mazur, M; Jastrzebski, S",,,,"Knop, Szymon; Spurek, Przemyslaw; Tabor, Jacek; Podolak, Igor; Mazur, Marcin; Jastrzebski, Stanislaw",,,Cramer-Wold Auto-Encoder,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The computation of the distance to the true distribution is a key component of most state-of-the-art generative models. Inspired by prior works on the Sliced-Wasserstein Auto-Encoders (SWAE) and the Wasserstein Auto-Encoders with MMD-based penalty (WAE-MMD), we propose a new generative model - a Cramer-Wold Auto-Encoder (CWAE). A fundamental component of CWAE is the characteristic kernel, the construction of which is one of the goals of this paper, from here on referred to as the Cramer-Wold kernel. Its main distinguishing feature is that it has a closed-form of the kernel product of radial Gaussians. Consequently, CWAE model has a closed-form for the distance between the posterior and the normal prior, which simplifies the optimization procedure by removing the need to sample in order to compute the loss function. At the same time, CWAE performance often improves upon WAE-MMD and SWAE on standard benchmarks.",,,,,"Mazur, Marcin/C-1080-2013; Podolak, Igor/HGA-7735-2022; Jastrzƒôbski, Stanis≈Çaw/AAX-4621-2020","Mazur, Marcin/0000-0002-3440-8173; Jastrzƒôbski, Stanis≈Çaw/0000-0003-4138-1818",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,164,,,,,,,,,,,,,,,WOS:000570152500001,0
J,"Tibo, A; Jaeger, M; Frasconi, P",,,,"Tibo, Alessandro; Jaeger, Manfred; Frasconi, Paolo",,,Learning and Interpreting Multi-Multi-Instance Learning Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce an extension of the multi-instance learning problem where examples are organized as nested bags of instances (e.g., a document could be represented as a bag of sentences, which in turn are bags of words). This framework can be useful in various scenarios, such as text and image classification, but also supervised learning over graphs. As a further advantage, multi-multi instance learning enables a particular way of interpreting predictions and the decision function. Our approach is based on a special neural network layer, called bag-layer, whose units aggregate bags of inputs of arbitrary size. We prove theoretically that the associated class of functions contains all Boolean functions over sets of sets of instances and we provide empirical evidence that functions of this kind can be actually learned on semi-synthetic datasets. We finally present experiments on text classification, on citation graphs, and social graph data, which show that our model obtains competitive results with respect to accuracy when compared to other approaches such as convolutional networks on graphs, while at the same time it supports a general approach to interpret the learnt model, as well as explain individual predictions.",,,,,,"TIBO, ALESSANDRO/0000-0002-9070-740X",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,193,,,,,,,,,,,,,,,WOS:000590000600001,0
J,"Wang, Y; Wu, SQ; Yu, B",,,,"Wang, Yu; Wu, Siqi; Yu, Bin",,,Unique Sharp Local Minimum in l(1)-minimization Complete Dictionary Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of globally recovering a dictionary from a set of signals via l(1)-minimization. We assume that the signals are generated as i.i.d. random linear combinations of the K atoms from a complete reference dictionary D* is an element of R-KxK, where the linear combination coefficients are from either a Bernoulli type model or exact sparse model. First, we obtain a necessary and sufficient norm condition for the reference dictionary D to be a sharp local minimum of the expected l(1) objective function. Our result substantially extends that of Wu and Yu (2018) and allows the combination coefficient to be non-negative. Secondly, we obtain an explicit bound on the region within which the objective value of the reference dictionary is minimal. Thirdly, we show that the reference dictionary is the unique sharp local minimum, thus establishing the first known global property of l(1)-minimization dictionary learning. Motivated by the theoretical results, we introduce a perturbation based test to determine whether a dictionary is a sharp local minimum of the objective function. In addition, we also propose a new dictionary learning algorithm based on Block Coordinate Descent, called DL-BCD, which is guaranteed to decrease the obective function monotonically. Simulation studies show that DL-BCD has competitive performance in terms of recovery rate compared to other state-of-the-art dictionary learning algorithms when the reference dictionary is generated from random Gaussian matrices.",,,,,"Wang, Yu/U-8482-2019","Wang, Yu/0000-0002-5329-7739",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000015,0
J,"Yuan, XT; Li, P",,,,"Yuan, Xiao-Tong; Li, Ping",,,"On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The DANE algorithm is an approximate Newton method popularly used for communication-efficient distributed machine learning. Reasons for the interest in DANE include scalability and efficiency. Convergence of DANE, however, can be tricky; its appealing convergence rate is only rigorous for quadratic objective function, and for more general convex functions the known results are no stronger than those of the classic first-order methods. To remedy these drawbacks, we propose in this article some new alternatives of DANE which are more suitable for analysis. We first introduce a simple variant of DANE equipped with backtracking line search, for which global asymptotic convergence and sharper local non-asymptotic convergence guarantees can be proved for both quadratic and non-quadratic strongly convex functions. Then we propose a heavy-ball method to accelerate the convergence of DANE, showing that the near-tight local rate of convergence can be established for strongly convex functions, and with proper modification of the algorithm about the same result applies globally to linear prediction models. Numerical evidence is provided to confirm the theoretical and practical advantages of our methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,206,,,,,,,,,,,,,,,WOS:000590009400001,0
J,"Braun, G; Pokutta, S; Zink, D",,,,"Braun, Gabor; Pokutta, Sebastian; Zink, Daniel",,,Lazifying Conditional Gradient Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained signi fi cant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,1,42,71,,,,,,,,,,,,,,,WOS:000467895000001,0
J,"Cai, HQ; Cai, JF; Wei, K",,,,"Cai, HanQin; Cai, Jian-Feng; Wei, Ke",,,Accelerated Alternating Projections for Robust Principal Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study robust PCA for the fully observed setting, which is about separating a low rank matrix L and a sparse matrix S from their sum D = L + S. In this paper, a new algorithm, dubbed accelerated alternating projections, is introduced for robust PCA which signi fi cantly improves the computational e ffi ciency of the existing alternating projections proposed in (Netrapalli et al., 2014) when updating the low rank factor. The acceleration is achieved by fi rst projecting a matrix onto some low dimensional subspace before obtaining a new estimate of the low rank matrix via truncated SVD. Exact recovery guarantee has been established which shows linear convergence of the proposed algorithm. Empirical performance evaluations establish the advantage of our algorithm over other state-of-theart algorithms for robust PCA.",,,,,"Cai, Jian-Feng/L-1808-2016; Cai, Jianfeng/AAB-9478-2019; Cai, Hanqin/AHD-4307-2022","Cai, Jian-Feng/0000-0003-2571-570X; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,20,,,,,,,,,,,,,,,WOS:000458667400001,0
J,"Chen, Y; Han, C; Li, Y; Huang, Z; Jiang, Y; Wang, N; Zhang, Z",,,,"Chen, Yuntao; Han, Chenxia; Li, Yanghao; Huang, Zehao; Jiang, Yi; Wang, Naiyan; Zhang, Zhaoxiang",,,SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet covers a wide range of models including both high-performance and high-speed ones. SimpleDet is well-optimized for both low precision training and distributed training and achieves 70% higher throughput for the Mask R-CNN detector compared with existing frameworks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,156,,,,,,,,,,,,,,,WOS:000491132200020,0
J,"Chung, J; Pedigo, BD; Bridgeford, EW; Varjavand, BK; Helm, HS; Vogelstein, JT",,,,"Chung, Jaewon; Pedigo, Benjamin D.; Bridgeford, Eric W.; Varjavand, Bijan K.; Helm, Hayden S.; Vogelstein, Joshua T.",,,GraSPy: Graph Statistics in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce GraSPy, a Python library devoted to statistical inference, machine learning, and visualization of random graphs and graph populations. This package provides flexible and easy-to-use algorithms for analyzing and understanding graphs with a scikit-learn compliant API. GraSPy can be downloaded from Python Package Index (PyPi), and is released under the Apache 2.0 open-source license.",,,,,"Vogelstein, Joshua/AAG-5489-2019","Vogelstein, Joshua/0000-0003-2487-6237; Chung, Jaewon/0000-0002-8940-3417",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,158,,,,,,,,,,,,,,,WOS:000491132200022,0
J,"Harris, DG; Li, S; Pensyl, T; Srinivasan, A; Trinh, K",,,,"Harris, David G.; Li, Shi; Pensyl, Thomas; Srinivasan, Aravind; Trinh, Khoa",,,Approximation algorithms for stochastic clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider stochastic settings for clustering, and develop provably-good approximation algorithms for a number of these notions. These algorithms yield better approximation ratios compared to the usual deterministic clustering setting. Additionally, they offer a number of advantages including clustering which is fairer and has better long-term behavior for each user. In particular, they ensure that every user is guaranteed to get good service (on average). We also complement some of these with impossibility results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,153,,,,,,,,,,,,,,,WOS:000491132200017,0
J,"Maggioni, M; Murphy, JM",,,,"Maggioni, Mauro; Murphy, James M.",,,Learning by Unsupervised Nonlinear Diffusion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes and analyzes a novel clustering algorithm, called learning by unsupervised nonlinear diffusion (LUND), that combines graph-based diffusion geometry with techniques based on density and mode estimation. LUND is suitable for data generated from mixtures of distributions with densities that are both multimodal and supported near nonlinear sets. A crucial aspect of this algorithm is the use of time of a data-adapted diffusion process, and associated diffusion distances, as a scale parameter that is different from the local spatial scale parameter used in many clustering algorithms. We prove estimates for the behavior of diffusion distances with respect to this time parameter under a flexible nonparametric data model, identifying a range of times in which the mesoscopic equilibria of the underlying process are revealed, corresponding to a gap between within-cluster and between-cluster diffusion distances. These structures may be missed by the top eigenvectors of the graph Laplacian, commonly used in spectral clustering. This analysis is leveraged to prove sufficient conditions guaranteeing the accuracy of LUND. We implement LUND and confirm its theoretical properties on illustrative data sets, demonstrating its theoretical and empirical advantages over both spectral and density-based clustering.",,,,,"ARSLAN, Okan/AAA-3232-2020","Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,160,,,,,,,,,,,,,,,WOS:000493915400001,0
J,"Trillos, NG; Sanz-Alonso, D; Yang, RY",,,,"Trillos, Nicolas Garcia; Sanz-Alonso, Daniel; Yang, Ruiyi",,,Local Regularization of Noisy Point Clouds: Improved Global Geometric Estimates and Data Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Several data analysis techniques employ similarity relationships between data points to uncover the intrinsic dimension and geometric structure of the underlying data-generating mechanism. In this paper we work under the model assumption that the data is made of random perturbations of feature vectors lying on a low- dimensional manifold. We study two questions: how to define the similarity relationships over noisy data points, and what is the resulting impact of the choice of similarity in the extraction of global geometric information from the underlying manifold. We provide concrete mathematical evidence that using a local regularization of the noisy data to define the similarity improves the approximation of the hidden Euclidean distance between unperturbed points. Furthermore, graph-based objects constructed with the locally regularized similarity function satisfy better error bounds in their recovery of global geometric ones. Our theory is supported by numerical experiments that demonstrate that the gain in geometric understanding facilitated by local regularization translates into a gain in classification accuracy in simulated and real data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,136,,,,,,,,,,,,,,,WOS:000487068900020,0
J,"Amjad, M; Shah, D; Shen, D",,,,"Amjad, Muhammad; Shah, Devavrat; Shen, Dennis",,,Robust Synthetic Control,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a robust generalization of the synthetic control method for comparative case studies. Like the classical method cf. Abadie and Gardeazabal (2003), we present an algorithm to estimate the unobservable counterfactual of a treatment unit. A distinguishing feature of our algorithm is that of de-noising the data matrix via singular value thresholding, which renders our approach robust in multiple facets: it automatically identifies a good subset of donors for the synthetic control, overcomes the challenges of missing data, and continues to work well in settings where covariate information may not be provided. We posit that the setting can be viewed as an instance of the Latent Variable Model and provide the first finite sample analysis (coupled with asymptotic results) for the estimation of the counterfactual. Our algorithm accurately imputes missing entries and filters corrupted observations in producing a consistent estimator of the underlying signal matrix, provided p = Omega(T-1+zeta) for some zeta > 0; here, p is the fraction of observed data and T is the time interval of interest. Under the same proportion of observations, we demonstrate that the mean-squared error in our counterfactual estimation scales as O (sigma(2)/p + 1/root T), where sigma(2) is the variance of the inherent noise. Additionally, we introduce a Bayesian framework to quantify the estimation uncertainty. Our experiments, using both synthetic and real-world datasets, demonstrate that our robust generalization yields an improvement over the classical synthetic control method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,22,,,,,,,,,,,,,,,WOS:000443228500001,0
J,"Das, A; Kempe, D",,,,"Das, Abhimanyu; Kempe, David",,,"Approximate Submodularity and its Applications: Subset Selection, Sparse Approximation and Dictionary Selection",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce the submodularity ratio as a measure of how close to submodular a set function f is. We show that when f has submodularity ratio gamma, the greedy algorithm for maximizing f provides a (1 - e(-gamma)) approximation. Furthermore, when gamma is bounded away from 0, the greedy algorithm for minimum submodular cover also provides essentially an O(log n) approximation for a universe of n elements. As a main application of this framework, we study the problem of selecting a subset of k random variables from a large set, in order to obtain the best linear prediction of another variable of interest. We analyze the performance of widely used greedy heuristics; in particular, by showing that the submodularity ratio is lower-bounded by the smallest 2 k sparse eigenvalue of the covariance matrix, we obtain the strongest known approximation guarantees for the Forward Regression and Orthogonal Matching Pursuit algorithms. As a second application, we analyze greedy algorithms for the dictionary selection problem, and significantly improve the previously known guarantees. Our theoretical analysis is complemented by experiments on real-world and synthetic data sets; in particular, we focus on an analysis of how tight various spectral parameters and the submodularity ratio are in terms of predicting the performance of the greedy algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,3,,,,,,,,,,,,,,,WOS:000443221700001,0
J,"El Karoui, N; Purdom, E",,,,"El Karoui, Noureddine; Purdom, Elizabeth",,,Can We Trust the Bootstrap in High-dimensions? The Case of Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of beta (where beta is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression-residual bootstrap and pairs bootstrap give very poor inference on beta as the ratio p/n grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio p/n grows. We also show that the jackknife resampling technique for estimating the variance of <(beta)over cap> severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,5,,,,,,,,,,,,,,,WOS:000443222300001,0
J,"Montiel, J; Read, J; Bifet, A; Abdessalem, T",,,,"Montiel, Jacob; Read, Jesse; Bifet, Albert; Abdessalem, Talel",,,Scikit-Multiflow: A Multi-output Streaming Framework,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"scikit-multiflow is a framework for learning from data streams and multi-output learning in Python. Conceived to serve as a platform to encourage the democratization of stream learning research, it provides multiple state-of-the-art learning methods, data generators and evaluators for different stream learning problems, including single-output, multi-output and multi-label. scikit-multiflow builds upon popular open source frameworks including scikit-learn, MOA and MEKA. Development follows the FOSS principles. Quality is enforced by complying with PEP8 guidelines, using continuous integration and functional testing. The source code is available at https://github.com/scikit-multiflow/scikit-multiflow.",,,,,"Montiel, Jacob/AAH-8641-2020","Montiel, Jacob/0000-0003-2245-0718",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000452056400001,0
J,"Rohe, K; Tao, J; Han, XT; Binkiewicz, N",,,,"Rohe, Karl; Tao, Jun; Han, Xintian; Binkiewicz, Norbert",,,A Note on Quickly Sampling a Sparse Matrix with Low Rank Expectation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given matrices X, Y epsilon R-nxK and S epsilon R-KxK with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix A with low rank expectation E (A)= XSYT and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges m and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive element-wise algorithm requires O (n(2)) operations to generate the n x n adjacency matrix A. In sparse graphs, where m = O (n), ignoring log terms, fastRG runs in time O (n). An implementation in R is available on github. A computational experiment in Section 2.4 simulates graphs up to n = 10; 000; 000 nodes with m = 100; 000; 000 edges. For example, on a graph with n = 500; 000 and m = 5; 000; 000, fastRG runs in less than one second on a 3.5 GHz Intel i5.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000454478600001,0
J,"Tarzanagh, DA; Michailidis, G",,,,"Tarzanagh, Davoud Ataee; Michailidis, George",,,Estimation of Graphical Models through Structured Norm Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely employed, is that of sparsity of the underlying model. In this paper, we study the problem of estimating such models exhibiting a more intricate structure comprising simultaneously of sparse, structured sparse and dense components. Such structures naturally arise in several scientific fields, including molecular biology, finance and political science. We introduce a general framework based on a novel structured norm that enables us to estimate such complex structures from high-dimensional data. The resulting optimization problem is convex and we introduce a linearized multi-block alternating direction method of multipliers (ADMM) algorithm to solve it efficiently. We illustrate the superior performance of the proposed framework on a number of synthetic data sets generated from both random and structured networks. Further, we apply the method to a number of real data sets and discuss the results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,209,,,,,,,,,,,,,,,WOS:000435628200001,0
J,"Wu, SQ; Yu, B",,,,"Wu, Siqi; Yu, Bin",,,Local Identifiability of`l(1)-minimization Dictionary Learning: a Sufficient and Almost Necessary Condition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the theoretical properties of learning a dictionary from N signals x(i) is an element of R-K for i - 1,..., N via l(1)-minimization. We assume that xi's are i.i.d. random linear combinations of the K columns from a complete (i.e., square and invertible) reference dictionary D-0 is an element of R-KxK. Here, the random linear coefficients are generated from either the s-sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary Do to be locally identifiable, i.e., a strict local minimum of the expected l(1)-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete mu-coherent reference dictionary, i.e., a dictionary with absolute pairwisecolumn inner-product at most mu is an element of [0, 1), local identifiability holds even when the random linear coefficient vector has up to O(mu(-2)) nonzero entries. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals N scales as O (K log K).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000433256500001,0
J,"Ndiaye, E; Fercoq, O; Gramfort, A; Salmon, J",,,,"Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph",,,Gap Safe Screening Rules for Sparsity Enforcing Penalties,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as l(1) or l(1)/l(2) norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse-Group Lasso, multi-task Lasso, binary and multinomial logistic regression, etc., we report significant speed-ups compared to previously proposed safe rules on all tested data sets.",,,,,,"Gramfort, Alexandre/0000-0001-9791-4404",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,128,,,,,,,,,,,,,,,WOS:000424542800001,0
J,"Chichignoud, M; Lederer, J; Wainwright, MJ",,,,"Chichignoud, Michael; Lederer, Johannes; Wainwright, Martin J.",,,A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality Guarantees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a novel scheme for choosing the regularization parameter in high-dimensional linear regression with Lasso. This scheme, inspired by Lepski's method for bandwidth selection in non-parametric regression, is equipped with both optimal finite-sample guarantees and a fast algorithm. In particular, for any design matrix such that the Lasso has low sup-norm error under an oracle choice of the regularization parameter, we show that our method matches the oracle performance up to a small constant factor, and show that it can be implemented by performing simple tests along a single Lasso path. By applying the Lasso to simulated and real data, we find that our novel scheme can be faster and more accurate than standard schemes such as Cross-Validation.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,229,,,,,,,,,,,,,,,WOS:000391917200001,0
J,"Ghavamzadeh, M; Engel, Y; Valko, M",,,,"Ghavamzadeh, Mohammad; Engel, Yaakov; Valko, Michal",,,Bayesian Policy Gradient and Actor-Critic Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy gradient methods are reinforcement learning algorithms that adapt a parameterized policy by following a performance gradient estimate. Many conventional policy gradient methods use Monte-Carlo techniques to estimate this gradient. The policy is improved by adjusting the parameters in the direction of the gradient estimate. Since Monte-Carlo methods tend to have high variance, a large number of samples is required to attain accurate estimates, resulting in slow convergence. In this paper, we fi rst propose a Bayesian framework for policy gradient, based on modeling the policy gradient as a Gaussian process. This reduces the number of samples needed to obtain accurate gradient estimates. Moreover, estimates of the natural gradient as well as a measure of the uncertainty in the gradient estimates, namely, the gradient covariance, are provided at little extra cost. Since the proposed Bayesian framework considers system trajectories as its basic observable unit, it does not require the dynamics within trajectories to be of any particular form, and thus, can be easily extended to partially observable problems. On the downside, it cannot take advantage of the Markov property when the system is Markovian. To address this issue, we proceed to supplement our Bayesian policy gradient framework with a new actor-critic learning model in which a Bayesian class of non-parametric critics, based on Gaussian process temporal di ff erence learning, is used. Such critics model the action-value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over action-value functions, conditioned on the observed data. Appropriate choices of the policy parameterization and of the prior covariance (kernel) between action-values allow us to obtain closed-form expressions for the posterior distribution of the gradient of the expected return with respect to the policy parameters. We perform detailed experimental comparisons of the proposed Bayesian policy gradient and actor-critic algorithms with classic Monte-Carlo based policy gradient methods, as well as with each other, on a number of reinforcement learning problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,66,,,,,,,,,,,,,,,WOS:000391522600001,0
J,"Gomez-Rodriguez, M; Song, L; Daneshmand, H; Scholkopf, B",,,,"Gomez-Rodriguez, Manuel; Song, Le; Daneshmand, Hadi; Schoelkopf, Bernhard",,,"Estimating Diffusion Networks: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees? Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l(1)-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d(3)logN) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding network inference algorithm which demonstrate the match between our theoretical prediction and empirical results. In practice, this new algorithm also outperforms other alternatives in terms of the accuracy of recovering hidden diffusion networks.",,,,,"Rodriguez, Manuel Gomez/AAB-5005-2021; Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Gomez Rodriguez, Manuel/0000-0003-3930-1161",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,90,,,,,,,,,,,,,,,WOS:000391534000001,0
J,"Levine, S; Finn, C; Darrell, T; Abbeel, P",,,,"Levine, Sergey; Finn, Chelsea; Darrell, Trevor; Abbeel, Pieter",,,End-to-End Training of Deep Visuomotor Policies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",,,,,,"Levine, Sergey/0000-0001-6764-2743",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,39,,,,,,,,,,,,,,,WOS:000391482200001,0
J,"Neykov, M; Liu, JS; Cai, TX",,,,"Neykov, Matey; Liu, Jun S.; Cai, Tianxi",,,L1-Regularized Least Squares for Support Recovery of High Dimensional Single Index Models with Gaussian Designs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is known that for a certain class of single index models (SIMs) Y=f(X(p)(T)x(1)beta(0),epsilon)Y=f(Xpx1?0,e), support recovery is impossible when X similar to N(0,I) and a model complexity adjusted sample size is below a critical threshold. Recently, optimal algorithms based on Sliced Inverse Regression (SIR) were suggested. These algorithms work provably under the assumption that the design X comes from an i.i.d. Gaussian distribution. In the present paper we analyze algorithms based on covariance screening and least squares with L1L1 penalization (i.e. LASSO) and demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample size in terms of support recovery, albeit under slightly different assumptions on ff and ee compared to the SIR based algorithms. Furthermore, we show more generally, that LASSO succeeds in recovering the signed support of beta(0) if X similar to N(0,S)X similar to N(0,Sigma), and the covariance SS satisfies he irrepresentable condition. Our work extends existing results on the support recovery of LASSO for the linear model, to a more general class of SIMs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,87,,,,,,,,,,,,,,,WOS:000391529900001,0
J,"Russo, D; Van Roy, B",,,,"Russo, Daniel; Van Roy, Benjamin",,,An Information-Theoretic Analysis of Thompson Sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We provide an information-theoretic analysis of Thompson sampling that applies across a broad range of online optimization problems in which a decision-maker must learn from partial feedback. This analysis inherits the simplicity and elegance of information theory and leads to regret bounds that scale with the entropy of the optimal-action distribution. This strengthens preexisting results and yields new insight into how information improves performance.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,68,,,,,,,,,,,,,,,WOS:000391522800001,0
J,"Shen, D; Shen, HP; Marron, JS",,,,"Shen, Dan; Shen, Haipeng; Marron, J. S.",,,A General Framework for Consistency of Principal Component Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A general asymptotic framework is developed for studying consistency properties of principal component analysis (PCA). Our framework includes several previously studied domains of asymptotics as special cases and allows one to investigate interesting connections and transitions among the various domains. More importantly, it enables us to investigate asymptotic scenarios that have not been considered before, and gain new insights into the consistency, subspace consistency and strong inconsistency regions of PCA and the boundaries among them. We also establish the corresponding convergence rate within each region. Under general spike covariance models, the dimension (or number of variables) discourages the consistency of PCA, while the sample size and spike information (the relative size of the population eigenvalues) encourage PCA consistency. Our framework nicely illustrates the relationship among these three types of information in terms of dimension, sample size and spike size, and rigorously characterizes how their relationships affect PCA consistency.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,150,,,,,,,,,,,,,,,WOS:000391662900001,0
J,"Addario-Berry, L; Bhamidi, S; Bubeck, S; Devroye, L; Lugosi, G; Oliveira, RI",,,,"Addario-Berry, Louigi; Bhamidi, Shankar; Bubeck, Sebastien; Devroye, Luc; Lugosi, Gabor; Oliveira, Roberto Imbuzeiro",,,Exceptional Rotations of Random Graphs: A VC Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we explore maximal deviations of large random structures from their typical behavior. We introduce a model for a high-dimensional random graph process and ask analogous questions to those of Vapnik and Chervonenkis for deviations of averages: how rich does the process have to be so that one sees atypical behavior. In particular, we study a natural process of Erdos-Renyi random graphs indexed by unit vectors in R-d. We investigate the deviations of the process with respect to three fundamental properties: clique number, chromatic number, and connectivity. In all cases we establish upper and lower bounds for the minimal dimension d that guarantees the existence of exceptional directions in which the random graph behaves atypically with respect to the property. For each of the three properties, four theorems are established, to describe upper and lower bounds for the threshold dimension in the subcritical and supercritical regimes.",,,,,"Oliveira, Roberto/ABD-9316-2020","Oliveira, Roberto/0000-0002-1064-3398",,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1893,1922,,,,,,,,,,,,,,,,WOS:000369887300008,0
J,"Geramifard, A; Dann, C; Klein, RH; Dabney, W; How, JP",,,,"Geramifard, Alborz; Dann, Christoph; Klein, Robert H.; Dabney, William; How, Jonathan P.",,,RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"RLPy is an object-oriented reinforcement learning software package with a focus on value function-based methods using linear function approximation and discrete actions. The framework was designed for both educational and research purposes. It provides a rich library of fine-grained, easily exchangeable components for learning agents (e.g., policies or representations of value functions), facilitating recently increased specialization in reinforcement learning. RLPy is written in Python to allow fast prototyping, but is also suitable for large-scale experiments through its built-in support for optimized numerical libraries and parallelization. Code profiling, domain visualizations, and data analysis are integrated in a self-contained package available under the Modified BSD License at http : //github. com/rlpy/rlpy. All of these properties allow users to compare various reinforcement learning algorithms with little effort.",,,,,,"How, Jonathan/0000-0001-8576-1930",,,,,,,,,,,,,1532-4435,,,,,AUG,2015,16,,,,,,1573,1578,,,,,,,,,,,,,,,,WOS:000369887100008,0
J,"Honorio, J; Ortiz, L",,,,"Honorio, Jean; Ortiz, Luis",,,Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider learning, from strictly behavioral data, the structure and parameters of linear influence games (LIGs), a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic inference (CSI) : Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by pure-strategy Nash equilibria (PS NE). Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including convex loss minimization (CLM) and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2015,16,,,,,,1157,1210,,,,,,,,,,,,,,,,WOS:000369886600002,0
J,"Germain, P; Lacasse, A; Laviolette, F; Marchand, M; Roy, JF",,,,"Germain, Pascal; Lacasse, Alexandre; Laviolette, Francois; Marchand, Mario; Roy, Jean-Francis",,,Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine.",,,,,,"Marchand, Mario/0000-0002-7078-7393; Germain, Pascal/0000-0003-3998-9533",,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,787,860,,,,,,,,,,,,,,,,WOS:000369886300006,0
J,"Mannor, S; Perchet, V; Stoltz, G",,,,"Mannor, Shie; Perchet, Vianney; Stoltz, Gilles",,,Set-Valued Approachability and Online Learning with Partial Monitoring,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Approachability has become a standard tool in analyzing learning algorithms in the adversarial online learning setup. We develop a variant of approachability for games where there is ambiguity in the obtained reward: it belongs to a set rather than being a single vector. Using this variant we tackle the problem of approachability in games with partial monitoring and develop a simple and generally efficient strategy (i.e., with constant per-step complexity) for this setup. As an important example, we instantiate our general strategy to the case when external regret or internal regret is to be minimized under partial monitoring.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3247,3295,,,,,,,,,,,,,,,,WOS:000344638800014,0
J,"Lan, AS; Waters, AE; Studer, C; Baraniuk, RG",,,,"Lan, Andrew S.; Waters, Andrew E.; Studer, Christoph; Baraniuk, Richard G.",,,Sparse Factor Analysis for Learning and Content Analytics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood-based solution and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.",,,,,"Baraniuk, Richard/ABA-1743-2020; lan, A/HHZ-6588-2022",,,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,1959,2008,,,,,,,,,,,,,,,,WOS:000344638300003,0
J,"Mizutani, T",,,,"Mizutani, Tomohiko",,,Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy Separability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a numerical algorithm for nonnegative matrix factorization (NMF) problems under noisy separability. An NMF problem under separability can be stated as one of finding all vertices of the convex hull of data points. The research interest of this paper is to find the vectors as close to the vertices as possible in a situation in which noise is added to the data points. Our algorithm is designed to capture the shape of the convex hull of data points by using its enclosing ellipsoid. We show that the algorithm has correctness and robustness properties from theoretical and practical perspectives; correctness here means that if the data points do not contain any noise, the algorithm can find the vertices of their convex hull; robustness means that if the data points contain noise, the algorithm can find the near-vertices. Finally, we apply the algorithm to document clustering, and report the experimental results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2014,15,,,,,,1011,1039,,,,,,,,,,,,,,,,WOS:000335458100006,0
J,"Opper, M; Paquet, U; Winther, O",,,,"Opper, Manfred; Paquet, Ulrich; Winther, Ole",,,Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian field, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model's partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP's local matching of moments. Through the expansion, we see that EP is correct to first order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution.",,,,,,"Winther, Ole/0000-0002-1966-3205",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2857,2898,,,,,,,,,,,,,,,,WOS:000327007400012,0
J,"Verma, N",,,,"Verma, Nakul",,,Distance Preserving Embeddings for General n-Dimensional Manifolds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic finite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general n-dimensional manifold into R-d (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2415,2448,,,,,,,,,,,,,,,,WOS:000324799600008,0
J,"Frezza-Buet, H; Geist, M",,,,"Frezza-Buet, Herve; Geist, Matthieu",,,A C++ Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces the rllib as an original C++ template-based library oriented toward value function estimation. Generic programming is promoted here as a way of having a good fit between the mathematics of reinforcement learning and their implementation in a library. The main concepts of rllib are presented, as well as a short example.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,625,628,,,,,,,,,,,,,,,,WOS:000315981900010,0
J,"Nakajima, S; Sugiyama, M; Babacan, SD; Tomioka, R",,,,"Nakajima, Shinichi; Sugiyama, Masashi; Babacan, S. Derin; Tomioka, Ryota",,,Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, finding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF-the global solution can be analytic ally computed. More specifically, the global solution is are weighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefficients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis.",,,,,"Sugiyama, Masashi/AEO-1176-2022","Sugiyama, Masashi/0000-0001-6658-6743",,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,1,37,,,,,,,,,,,,,,,,WOS:000314530200001,0
J,"Dekel, O; Gentile, C; Sridharan, K",,,,"Dekel, Ofer; Gentile, Claudio; Sridharan, Karthik",,,Selective Sampling and Active Learning from Single and Multiple Teachers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a new online learning algorithm in the selective sampling framework, where labels must be actively queried before they are revealed. We prove bounds on the regret of our algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances. Our bounds both generalize and strictly improve over previous bounds in similar settings. Additionally, our selective sampling algorithm can be converted into an efficient statistical active learning algorithm. We extend our algorithm and analysis to the multiple-teacher setting, where the algorithm can choose which subset of teachers to query for each label. Finally, we demonstrate the effectiveness of our techniques on a real-world Internet search problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2012,13,,,,,,2655,2697,,,,,,,,,,,,,,,,WOS:000309580600006,0
J,"Skolidis, G; Sanguinetti, G",,,,"Skolidis, Grigorios; Sanguinetti, Guido",,,A Case Study on Meta-Generalising: A Gaussian Processes Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a novel model for meta-generalisation, that is, performing prediction on novel tasks based on information from multiple different but related tasks. The model is based on two coupled Gaussian processes with structured covariance function; one model performs predictions by learning a constrained covariance function encapsulating the relations between the various training tasks, while the second model determines the similarity of new tasks to previously seen tasks. We demonstrate empirically on several real and synthetic data sets both the strengths of the approach and its limitations due to the distributional assumptions underpinning it.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,691,721,,,,,,,,,,,,,,,,WOS:000303772100008,0
J,"Telgarsky, M",,,,"Telgarsky, Matus",,,A Primal-Dual Convergence Analysis of Boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Boosting combines weak learners into a predictor with low empirical risk. Its dual constructs a high entropy distribution upon which weak learners and training labels are uncorrelated. This manuscript studies this primal-dual relationship under a broad family of losses, including the exponential loss of AdaBoost and the logistic loss, revealing: Weak learnability aids the whole loss family: for any epsilon > 0, O(ln(1/epsilon)) iterations suffice to produce a predictor with empirical risk epsilon-close to the infimum; The circumstances granting the existence of an empirical risk minimizer may be characterized in terms of the primal and dual problems, yielding a new proof of the known rate O(ln(1/epsilon)); Arbitrary instances may be decomposed into the above two, granting rate O(1/epsilon), with a matching lower bound provided for the logistic loss.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2012,13,,,,,,561,606,,,,,,,,,,,,,,,,WOS:000303772100004,0
J,"Zhao, PL; Hoi, SCH; Jin, R",,,,"Zhao, Peilin; Hoi, Steven C. H.; Jin, Rong",,,Double Updating Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In most kernel based online learning algorithms, when an incoming instance is misclassified, it will be added into the pool of support vectors and assigned with a weight, which often remains unchanged during the rest of the learning process. This is clearly insufficient since when a new support vector is added, we generally expect the weights of the other existing support vectors to be updated in order to reflect the influence of the added support vector. In this paper, we propose a new online learning method, termed Double Updating Online Learning, or DUOL for short, that explicitly addresses this problem. Instead of only assigning a fixed weight to the misclassified example received at the current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors. We show that the mistake bound can be improved by the proposed online learning method. We conduct an extensive set of empirical evaluations for both binary and multi-class online learning tasks. The experimental results show that the proposed technique is considerably more effective than the state-of-the-art online learning algorithms. The source code is available to public at http://www.cais.ntu.edu.sg/similar to chhoi/DUOL/.",,,,,"HOI, Steven C. H./A-3736-2011","Hoi, Steven/0000-0002-4584-3453",,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1587,1615,,,,,,,,,,,,,,,,WOS:000292304000005,0
J,"Zhang, ZH; Dai, GA; Jordan, MI",,,,"Zhang, Zhihua; Dai, Guang; Jordan, Michael I.",,,Bayesian Generalized Kernel Mixed Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a fully Bayesian methodology for generalized kernel mixed models (GKMMs), which are extensions of generalized linear mixed models in the feature space induced by a reproducing kernel. We place a mixture of a point-mass distribution and Silverman's g-prior on the regression vector of a generalized kernel model (GKM). This mixture prior allows a fraction of the components of the regression vector to be zero. Thus, it serves for sparse modeling and is useful for Bayesian computation. In particular, we exploit data augmentation methodology to develop a Markov chain Monte Carlo (MCMC) algorithm in which the reversible jump method is used for model selection and a Bayesian model averaging method is used for posterior prediction. When the feature basis expansion in the reproducing kernel Hilbert space is treated as a stochastic process, this approach can be related to the Karhunen-Loeve expansion of a Gaussian process (GP). Thus, our sparse modeling framework leads to a flexible approximation method for GPs.",,,,,"Jordan, Michael I/C-5253-2013","Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,111,139,,,,,,,,,,,,,,,,WOS:000287938500005,0
J,"Robinson, JW; Hartemink, AJ",,,,"Robinson, Joshua W.; Hartemink, Alexander J.",,,Learning Non-Stationary Dynamic Bayesian Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning dynamic Bayesian network structures provides a principled mechanism for identifying conditional dependencies in time-series data. An important assumption of traditional DBN structure learning is that the data are generated by a stationary process, an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical model called a non-stationary dynamic Bayesian network, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. Some examples of evolving networks are transcriptional regulatory networks during an organism's development, neural pathways during learning, and traffic patterns during the day. We define the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3647,3680,,,,,,,,,,,,,,,,WOS:000286637200012,0
J,"Shalev-Shwartz, S; Shamir, O; Srebro, N; Sridharan, K",,,,"Shalev-Shwartz, Shai; Shamir, Ohad; Srebro, Nathan; Sridharan, Karthik",,,"Learnability, Stability and Uniform Convergence",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of characterizing learnability is the most basic question of statistical learning theory. A fundamental and long-standing answer, at least for the case of supervised classification and regression, is that learnability is equivalent to uniform convergence of the empirical risk to the population risk, and that if a problem is learnable, it is learnable via empirical risk minimization. In this paper, we consider the General Learning Setting (introduced by Vapnik), which includes most statistical learning problems as special cases. We show that in this setting, there are non-trivial learning problems where uniform convergence does not hold, empirical risk minimization fails, and yet they are learnable using alternative mechanisms. Instead of uniform convergence, we identify stability as the key necessary and sufficient condition for learnability. Moreover, we show that the conditions for learnability in the general setting are significantly more complex than in supervised classification and regression.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2010,11,,,,,,2635,2670,,,,,,,,,,,,,,,,WOS:000284040000003,0
J,"El-Yaniv, R; Wiener, Y",,,,"El-Yaniv, Ran; Wiener, Yair",,,On the Foundations of Noise-free Selective Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider selective classification, a term we adopt here to refer to 'classification with a reject option.' The essence in selective classification is to trade-off classifier coverage for higher accuracy. We term this trade-off the risk-coverage (RC) trade-off. Our main objective is to characterize this trade-off and to construct algorithms that can optimally or near optimally achieve the best possible trade-offs in a controlled manner. For noise-free models we present in this paper a thorough analysis of selective classification including characterizations of RC trade-offs in various interesting settings.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1605,1641,,,,,,,,,,,,,,,,WOS:000282522000002,0
J,"Rieger, C; Zwicknagl, B",,,,"Rieger, Christian; Zwicknagl, Barbara",,,Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a new technique for the analysis of kernel-based regression problems. The basic tools are sampling inequalities which apply to all machine learning problems involving penalty terms induced by kernels related to Sobolev spaces. They lead to explicit deterministic results concerning the worst case behaviour of epsilon- and nu-SVRs. Using these, we show how to adjust regularization parameters to get best possible approximation orders for regression. The results are illustrated by some numerical examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2115,2132,,,,,,,,,,,,,,,,WOS:000272346100005,0
J,"Kumar, MP; Kolmogorov, V; Torr, PHS",,,,"Kumar, M. Pawan; Kolmogorov, Vladimir; Torr, Philip H. S.",,,An Analysis of Convex Relaxations for MAP Estimation of Discrete MRFs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of obtaining the maximum a posteriori estimate of a general discrete Markov random field (i.e., a Markov random field defined using a discrete set of labels) is known to be NP-hard. However, due to its central importance in many applications, several approximation algorithms have been proposed in the literature. In this paper, we present an analysis of three such algorithms based on convex relaxations: (i) LP-S: the linear programming (LP) relaxation proposed by Schlesinger (1976) for a special case and independently in Chekuri et al. (2001), Koster et al. (1998), and Wainwright et al. (2005) for the general case; (ii) QP-RL: the quadratic programming (QP) relaxation of Ravikumar and Lafferty (2006); and (iii) SOCP-MS: the second order cone programming (SOCP) relaxation first proposed by Muramatsu and Suzuki (2003) for two label problems and later extended by Kumar et al. (2006) for a general label set. We show that the SOCP-MS and the QP-RL relaxations are equivalent. Furthermore, we prove that despite the flexibility in the form of the constraints/objective function offered by QP and SOCP, the LP-S relaxation strictly dominates (i.e., provides a better approximation than) QP-RL and SOCP-MS. We generalize these results by defining a large class of SOCP (and equivalent QP) relaxations which is dominated by the LP-S relaxation. Based on these results we propose some novel SOCP relaxations which define constraints using random variables that form cycles or cliques in the graphical model representation of the random field. Using some examples we show that the new SOCP relaxations strictly dominate the previous approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2009,10,,,,,,71,106,,,,,,,,,,,,,,,,WOS:000270824100003,0
J,"Dalalyan, AS; Juditsky, A; Spokoiny, V",,,,"Dalalyan, Arnak S.; Juditsky, Anatoly; Spokoiny, Vladimir",,,A New Algorithm for Estimating the Effective Dimension-Reduction Subspace,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say (beta) over cap (1),...,(beta) over cap (L), nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize max(l-1,...,L)(beta) over cap (T)(l)(I - A)(beta) over cap (l) subject to A is an element of A(m)*, where A(m)* is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal to m*, with m* being the true structural dimension. Under mild assumptions, root n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors (beta) over cap (l) for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations.",,,,,"Dalalyan, Arnak S/G-7853-2011; Spokoiny, Vladimir G./L-5441-2015; Spokoiny, Vladimir/AAF-4942-2021","Dalalyan, Arnak S/0000-0003-4337-9500; Spokoiny, Vladimir G./0000-0002-2040-3427; Spokoiny, Vladimir/0000-0002-2040-3427",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1647,1678,,,,,,,,,,,,,,,,WOS:000262636800001,0
J,"Drton, M; Richardson, TS",,,,"Drton, Mathias; Richardson, Thomas S.",,,Graphical methods for efficient likelihood inference in Gaussian covariance models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identified with the vertices of the graph. We show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). Here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. In Gaussian models, this construction can be used for more efficient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts.",,,,,,"Drton, Mathias/0000-0001-5614-3025",,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,893,914,,,,,,,,,,,,,,,,WOS:000258645300003,0
J,"Gomez, F; Schmidhuber, J; Miikkulainen, R",,,,"Gomez, Faustino; Schmidhuber, Juergen; Miikkulainen, Risto",,,Accelerated neural evolution through cooperatively coevolved synapses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artificial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difficult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be significantly more efficient and powerful than the other methods on these tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2008,9,,,,,,937,965,,,,,,,,,,,,,,,,WOS:000258645300005,0
J,"Lin, CJ; Weng, RC; Keerthi, SS",,,,"Lin, Chih-Jen; Weng, Ruby C.; Keerthi, S. Sathiya",,,Trust region Newton method for large-scale logistic regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM).",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747; WENG, CHIU-HSING/0000-0001-5495-3817",,,,,,,,,,,,,1532-4435,,,,,APR,2008,9,,,,,,627,650,,,,,,,,,,,,,,,,WOS:000256642100004,0
J,"Globerson, A; Chechik, G; Pereira, F; Tishby, N",,,,"Globerson, Amir; Chechik, Gal; Pereira, Fernando; Tishby, Naftali",,,Euclidean embedding of co-occurrence data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are specified. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semidefinite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and significantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis.",,,,,,"Globerson, Amir/0000-0003-2557-1742",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2265,2295,,,,,,,,,,,,,,,,WOS:000252744800003,0
J,"Sonnenburg, S; Braun, ML; Ong, CS; Bengio, S; Bottou, L; Holmes, G; LeCun, Y; Muller, KR; Pereira, F; Rasmussen, CE; Ratsch, G; Scholkopf, B; Smola, A; Vincent, P; Weston, J; Williamson, RC",,,,"Sonnenburg, Soeren; Braun, Mikio L.; Ong, Cheng Soon; Bengio, Samy; Bottou, Leon; Holmes, Geoffrey; LeCun, Yann; Mueller, Klaus-Robert; Pereira, Fernando; Rasmussen, Carl Edward; Raetsch, Gunnar; Schoelkopf, Bernhard; Smola, Alexander; Vincent, Pascal; Weston, Jason; Williamson, Robert C.",,,The need for open source software in machine learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",,,,,"Mueller, Klaus-Robert/Y-3547-2019","Mueller, Klaus-Robert/0000-0002-3861-7685; Holmes, Geoffrey/0000-0003-0433-8925; Ratsch, Gunnar/0000-0001-5486-8532",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2443,2466,,,,,,,,,,,,,,,,WOS:000252744800008,0
J,"Taylor, ME; Stone, P; Liu, YX",,,,"Taylor, Matthew E.; Stone, Peter; Liu, Yaxin",,,Transfer learning via inter-task mappings for temporal difference learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artificial neural network ( ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005).",,,,,,"Taylor, Matthew/0000-0001-8946-0211",,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,2125,2167,,,,,,,,,,,,,,,,WOS:000252744600006,0
J,"Osadchy, M; Le Cun, Y; Miller, ML",,,,"Osadchy, Margarita; Le Cun, Yann; Miller, Matthew L.",,,Synergistic face detection and pose estimation with energy-based models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets-for frontal views, rotated faces, and profiles is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2007,8,,,,,,1197,1215,,,,,,,,,,,,,,,,WOS:000248351700011,0
J,"Kim, S; Smyth, P",,,,"Kim, Seyoung; Smyth, Padhraic",,,Segmental hidden Markov models with random effects for waveform modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper proposes a general probabilistic framework for shape-based modeling and classification of waveform data. A segmental hidden Markov model (HMM) is used to characterize waveform shape and shape variation is captured by adding random effects to the segmental model. The resulting probabilistic framework provides a basis for learning of waveform models from data as well as parsing and recognition of new waveforms. Expectation-maximization (EM) algorithms are derived and investigated for fitting such models to data. In particular, the expectation conditional maximization either (ECME) algorithm is shown to provide significantly faster convergence than a standard EM procedure. Experimental results on two real-world data sets demonstrate that the proposed approach leads to improved accuracy in classification and segmentation when compared to alternatives such as Euclidean distance matching, dynamic time warping, and segmental HMMs without random effects.",,,,,,"Smyth, Padhraic/0000-0001-9971-8378",,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,945,969,,,,,,,,,,,,,,,,WOS:000245388400002,0
J,"Chu, W; Ghahramani, Z",,,,"Chu, W; Ghahramani, Z",,,Gaussian processes for ordinal regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1019,1041,,,,,,,,,,,,,,,,WOS:000236329900001,0
J,"Markatou, M; Tian, H; Biswas, S; Hripcsak, G",,,,"Markatou, M; Tian, H; Biswas, S; Hripcsak, G",,,Analysis of variance of cross-validation estimators of the generalization error,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper brings together methods from two different disciplines: statistics and machine learning. We address the problem of estimating the variance of cross-validation (CV) estimators of the generalization error. In particular, we approach the problem of variance estimation of the CV estimators of generalization error as a problem in approximating the moments of a statistic. The approximation illustrates the role of training and test sets in the performance of the algorithm. It provides a unifying approach to evaluation of various methods used in obtaining training and test sets and it takes into account the variability due to different training and test sets. For the simple problem of predicting the sample mean and in the case of smooth loss functions, we show that the variance of the CV estimator of the generalization error is a function of the moments of the random variables Y = Card(S-j boolean AND S-j') and Y* = Card(S-j(c)boolean AND S-j'(c)), where S-j, S-j' are two training sets, and S-j(c), S-j'(c) are the corresponding test sets. We prove that the distribution of Y and Y* is hypergeometric and we compare our estimator with the one proposed by Nadeau and Bengio (2003). We extend these results in the regression case and the case of absolute error loss, and indicate how the methods can be extended to the classification case. We illustrate the results through simulation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1127,1168,,,,,,,,,,,,,,,,WOS:000236329900005,0
J,"Kawanabe, M; Muller, KR",,,,"Kawanabe, M; Muller, KR",,,Estimating functions for blind separation when sources have variance dependencies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A blind separation problem where the sources are not independent, but have variance dependencies is discussed. For this scenario Hyvarinen and Hurri (2004) proposed an algorithm which requires no assumption on distributions of sources and no parametric model of dependencies between components. In this paper, we extend the semiparametric approach of Amari and Cardoso (1997) to variance dependencies and study estimating functions for blind separation of such dependent sources. In particular, we show that many ICA algorithms are applicable to the variance-dependent model as well under mild conditions, although they should in principle not. Our results indicate that separation can be done based only on normalized sources which are adjusted to have stationary variances and is not affected by the dependent activity levels. We also study the asymptotic distribution of the quasi maximum likelihood method and the stability of the natural gradient learning in detail. Simulation results of artificial and realistic examples match well with our theoretical findings.",,,,,"Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013","Mueller, Klaus-Robert/0000-0002-3861-7685; ",,,,,,,,,,,,,1532-4435,,,,,APR,2005,6,,,,,,453,482,,,,,,,,,,,,,,,,WOS:000236329600004,0
J,"Jaeger, SA",,,,"Jaeger, SA",,,Generalization bounds and complexities based on sparsity and clustering for convex combinations of functions from random classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A unified approach is taken for deriving new generalization data dependent bounds for several classes of algorithms explored in the existing literature by different approaches. This unified approach is based on an extension of Vapnik's inequality for VC classes of sets to random classes of sets - that is, classes depending on the random data, invariant under permutation of the data and possessing the increasing property. Generalization bounds are derived for convex combinations of functions from random classes with certain properties. Algorithms, such as SVMs (support vector machines), boosting with decision stumps, radial basis function networks, some hierarchies of kernel machines or convex combinations of indicator functions over sets with finite VC dimension, generate classifier functions that fall into the above category. We also explore the individual complexities of the classifiers, such as sparsity of weights and weighted variance over clusters from the convex combination introduced by Koltchinskii and Panchenko (2004), and show sparsity-type and cluster-variance-type generalization bounds for random classes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2005,6,,,,,,307,340,,,,,,,,,,,,,,,,WOS:000236329400003,0
J,"Yu, L; Liu, H",,,,"Yu, L; Liu, H",,,Efficient feature selection via analysis of relevance and redundancy,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2004,5,,,,,,1205,1224,,,,,,,,,,,,,,,,WOS:000236328300001,0
J,"Sallans, B; Hinton, GE",,,,"Sallans, B; Hinton, GE",,,Reinforcement learning with factored states and actions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 2(40).,,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,1063,1088,,,,,,,,,,,,,,,,WOS:000236328000008,0
J,"Botta, M; Giordana, A; Saitta, L; Sebag, M",,,,"Botta, M; Giordana, A; Saitta, L; Sebag, M",,,Relational learning as search in a critical region,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,10th International Conference on Inductive Logic Programming (ILP2000),"JUL, 2000","London, ENGLAND",,,,,"Machine learning strongly relies on the covering test to assess whether a candidate hypothesis covers training examples. The present paper investigates learning relational concepts from examples, termed relational learning or inductive logic programming. In particular, it investigates the chances of success and the computational cost of relational learning, which appears to be severely affected by the presence of a phase transition in the covering test. To this aim, three up-to-date relational learners have been applied to a wide range of artificial, fully relational learning problems. A first experimental observation is that the phase transition behaves as an attractor for relational learning; no matter which region the learning problem belongs to, all three learners produce hypotheses lying within or close to the phase transition region. Second, a failure region appears. All three learners fail to learn any accurate hypothesis in this region. Quite surprisingly, the probability of failure does not systematically increase with the size of the underlying target concept: under some circumstances, longer concepts may be easier to accurately approximate than shorter ones. Some interpretations for these findings are proposed and discussed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,431,463,,10.1162/153244304773936018,0,,,,,,,,,,,,,WOS:000221345700003,0
J,"Langseth, H; Nielsen, TD",,,,"Langseth, H; Nielsen, TD",,,Fusion of domain knowledge with data for structural learning in object oriented domains,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Knowledge-Data Fusion,"JUN, 2000","Stanford Univ, Stanford, CA",,Stanford Univ,,,"When constructing a Bayesian network, it can be advantageous to employ structural learning algorithms to combine knowledge captured in databases with prior information provided by domain experts. Unfortunately, conventional learning algorithms do not easily incorporate prior information, if this information is too vague to be encoded as properties that are local to families of variables. For instance, conventional algorithms do not exploit prior information about repetitive structures, which are often found in object oriented domains such as computer networks, large pedigrees and genetic analysis. In this paper we propose a method for doing structural learning in object oriented domains. It is demonstrated that this method is more efficient than conventional algorithms in such domains, and it is argued that the method supports a natural approach for expressing and incorporating prior information provided by domain experts.",,,,,,"Nielsen, Thomas Dyhre/0000-0002-4823-6341",,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,339,368,,10.1162/153244304773633852,0,,,,,,,,,,,,,WOS:000221043900005,0
J,"Bshouty, NH; Burroughs, L",,,,"Bshouty, NH; Burroughs, L",,,On the proper learning of axis-parallel concepts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the proper learnability of axis-parallel concept classes in the PAC-learning and exact-learning models. These classes include union of boxes, DNF, decision trees and multivariate polynomials. For constant-dimensional axis-parallel concepts C we show that the following problems have time complexities that are within a polynomial factor of each other. 1. C is alpha-property exactly learnable (with hypotheses of size at most a times the target size) from membership and equivalence queries. 2. C is alpha-properly PAC learnable (without membership queries) under any product distribution. 3. There is an a-approximation algorithm for the MINEQUI C problem (given a g G C find a minimal size f is an element of C that is logically equivalent to g). In particular, if one has polynomial time complexity, they all do. Using this we give the first proper-teaming algorithm of constant-dimensional decision trees and the first negative results in proper teaming from membership and equivalence queries for many classes. For axis-parallel concepts over a nonconstant dimension we show that with the equivalence oracle (1) (3). We use this to show that (binary) decision trees are not properly learnable in polynomial time (assuming P=NP) and DNF is not s(epsilon)-properly learnable (epsilon 1) in polynomial time even with an NP-oracle (assuming Sigma(2)(P) = P-NP).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Feb-15,2004,4,2,,,,,157,176,,10.1162/153244304322972676,0,,,,,,,,,,,,,WOS:000221043600002,0
J,"Tsuda, K; Akaho, S; Asai, K",,,,"Tsuda, K; Akaho, S; Asai, K",,,The em algorithm for kernel matrix completion with auxiliary data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In biological data, it is often the case that observed data are available only for a subset of samples. When a kernel matrix is derived from such data, we have to leave the entries for unavailable samples as missing. In this paper, the missing entries are completed by exploiting an auxiliary kernel matrix derived from another information source. The parametric model of kernel matrices is created as a set of spectral variants of the auxiliary kernel matrix, and the missing entries are estimated by fitting this model to the existing entries. For model fitting, we adopt the em algorithm (distinguished from the EM algorithm of Dempster et al., 1977) based on the information geometry of positive definite matrices. We will report promising results on bacteria clustering experiments using two marker sequences: 16S and gyrB.",,,,,"Akaho, Shotaro/N-3401-2016; Asai, Kiyoshi/AAT-8344-2020","Akaho, Shotaro/0000-0002-4623-2718; Asai, Kiyoshi/0000-0003-0909-4982",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,67,81,,10.1162/153244304322765649,0,,,,,,,,,,,,,WOS:000221043500004,0
J,"Crammer, K; Singer, Y",,,,"Crammer, K; Singer, Y",,,Ultraconservative online algorithms for multiclass problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MASSACHUSETTS",,,,,"In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,951,991,,10.1162/jmlr.2003.3.4-5.951,0,,,,,,,,,,,,,WOS:000184926200014,0
J,"Ben-Hur, A; Horn, D; Siegelmann, HT; Vapnik, V",,,,"Ben-Hur, A; Horn, D; Siegelmann, HT; Vapnik, V",,,Support vector clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,Workshop on Kernel Methods,01-Dec-00,"BRECKENRIDGE, CO",,,,,"We present a novel clustering method using the approach of support vector machines. Data points are mapped by means of a Gaussian kernel to a high dimensional feature space, where we search for the minimal enclosing sphere. This sphere, when mapped back to data space, can separate into several components, each enclosing a separate cluster of points. We present a simple algorithm for identifying these clusters. The width of the Gaussian kernel controls the scale at which the data is probed while the soft margin constant helps coping with outliers and overlapping clusters. The structure of a dataset is explored by varying the two parameters, maintaining a minimal number of support vectors to assure smooth cluster boundaries. We demonstrate the performance of our algorithm on several datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,2,,,,,125,137,,10.1162/15324430260185565,0,,,,,,,,,,,,,WOS:000176055300003,0
J,"Bertsimas, D; Cory-Wright, R; Pauphilet, J",,,,"Bertsimas, Dimitris; Cory-Wright, Ryan; Pauphilet, Jean",,,Solving Large-Scale Sparse PCA to Certifiable (Near) Optimality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse principal component analysis (PCA) is a popular dimensionality reduction technique for obtaining principal components which are linear combinations of a small subset of the original features. Existing approaches cannot supply certifiably optimal principal components with more than p = 100s of variables. By reformulating sparse PCA as a convex mixed-integer semidefinite optimization problem, we design a cutting-plane method which solves the problem to certifiable optimality at the scale of selecting k = 5 covariates from p = 300 variables, and provides small bound gaps at a larger scale. We also propose a convex relaxation and greedy rounding scheme that provides bound gaps of 1 - 2% in practice within minutes for p = 100s or hours for p = 1, 000s and is therefore a viable alternative to the exact method at scale. Using real-world financial and medical data sets, we illustrate our approach's ability to derive interpretable principal components tractably at scale.",,,,,,"Cory-Wright, Ryan/0000-0002-4485-0619",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766876400001,0
J,"Pacchiardi, L; Dutta, R",,,,"Pacchiardi, Lorenzo; Dutta, Ritabrata",,,Score Matched Neural Exponential Families for Likelihood-Free Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distributions for stochastic models with intractable likelihood, by relying on model simulations. In Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to the observation in order to sample from an approximate posterior, whose form depends on the chosen statistics. In this work, we introduce a new way to learn ABC statistics: we first generate parameter-simulation pairs from the model independently on the observation; then, we use Score Matching to train a neural conditional exponential family to approximate the likelihood. The exponential family is the largest class of distributions with fixed-size sufficient statistics; thus, we use them in ABC, which is intuitively appealing and has state-of-the-art performance. In parallel, we insert our likelihood approximation in an MCMC for doubly intractable distributions to draw posterior samples. We can repeat that for any number of observations with no additional model simulations, with performance comparable to related approaches. We validate our methods on toy models with known likelihood and a large-dimensional time-series model.",,,,,"dutta, ritabrata/GWC-8579-2022",,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,71,,,,,,,,,,,,,,,,WOS:000766883400001,0
J,"Peng, X; Li, YF; Tsang, IW; Zhu, HY; Lv, JC; Zhou, JT",,,,"Peng, Xi; Li, Yunfan; Tsang, Ivor W.; Zhu, Hongyuan; Lv, Jiancheng; Zhou, Joey Tianyi",,,XAI Beyond Classification: Interpretable Neural Clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study two challenging problems in explainable AI (XAI) and data clustering. The first is how to directly design a neural network with inherent interpretability, rather than giving post-hoc explanations of a black-box model. The second is implementing discrete k-means with a differentiable neural network that embraces the advantages of parallel computing, online clustering, and clustering-favorable representation learning. To address these two challenges, we design a novel neural network, which is a differentiable reformulation of the vanilla k-means, called inTerpretable nEuraL cLustering (TELL). Our contributions are threefold. First, to the best of our knowledge, most existing XAI works focus on supervised learning paradigms. This work is one of the few XAI studies on unsupervised learning, in particular, data clustering. Second, TELL is an interpretable, or the so-called intrinsically explainable and transparent model. In contrast, most existing XAI studies resort to various means for understanding a black-box model with post-hoc explanations. Third, from the view of data clustering, TELL possesses many properties highly desired by k-means, including but not limited to online clustering, plug-and-play module, parallel computing, and provable convergence. Extensive experiments show that our method achieves superior performance comparing with 14 clustering approaches on three challenging data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,28,,,,,,,,,,,,,,,,WOS:000752355200001,0
J,"da Costa, VGT; Fini, E; Nabi, M; Sebe, N; Ricci, E",,,,"Turrisi da Costa, Victor G.; Fini, Enrico; Nabi, Moin; Sebe, Nicu; Ricci, Elisa",,,solo-learn: A Library of Self-supervised Methods for Visual Representation Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks. Our goal is to provide an easy-touse library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn.",,,,,,"Sebe, Niculae/0000-0002-6597-7248",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,6,,,,,,,,,,,,,,,,WOS:000752291000001,0
J,"Wang, XY; Tong, L",,,,"Wang, Xinyi; Tong, Lang",,,Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An innovations sequence of a time series is a sequence of independent and identically distributed random variables with which the original time series has a causal representation. The innovation at a time is statistically independent of the history of the time series. As such, it represents the new information contained at present but not in the past. Because of its simple probability structure, the innovations sequence is the most efficient signature of the original. Unlike the principle or independent component representations, an innovations sequence preserves not only the complete statistical properties but also the temporal order of the original time series. An long-standing open problem is to find a computationally tractable way to extract an innovations sequence of non-Gaussian processes. This paper presents a deep learning approach, referred to as Innovations Autoencoder (IAE), that extracts innovations sequences using a causal convolutional neural network. An application of IAE to the one-class anomalous sequence detection problem with unknown anomaly and anomaly-free models is also presented.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,27,,,,,,,,,,,,,,,,WOS:000753501800001,0
J,"Balasubramanian, K",,,,"Balasubramanian, Krishnakumar",,,Nonparametric Modeling of Higher-Order Interactions via Hypergraphons,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study statistical and algorithmic aspects of using hypergraphons, that are limits of large hypergraphs, for modeling higher-order interactions. Although hypergraphons are extremely powerful from a modeling perspective, we consider a restricted class of Simple Lipschitz Hypergraphons (SLH), that are amenable to practically efficient estimation. We also provide rates of convergence for our estimator that are optimal for the class of SLH. Simulation results are provided to corroborate the theory.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700316900001,0
J,"Banerjee, T; Liu, Q; Mukherjee, G; Sun, WG",,,,"Banerjee, Trambak; Liu, Qiang; Mukherjee, Gourab; Sun, Wenguang",,,A General Framework for Empirical Bayes Estimation in Discrete Linear Exponential Family,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a Nonparametric Empirical Bayes (NEB) framework for compound estimation in the discrete linear exponential family, which includes a wide class of discrete distributions frequently arising from modern big data applications. We propose to directly estimate the Bayes shrinkage factor in the generalized Robbins' formula via solving a convex program, which is carefully developed based on a RKHS representation of the Stein's discrepancy measure. The new NEB estimation framework is flexible for incorporating various structural constraints into the data driven rule, and provides a unified approach to compound estimation with both regular and scaled squared error losses. We develop theory to show that the class of NEB estimators enjoys strong asymptotic properties. Comprehensive simulation studies as well as analyses of real data examples are carried out to demonstrate the superiority of the NEB estimator over competing methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656369000001,0
J,"Cai, T; Li, HZ; Ma, R",,,,"Cai, Tony; Li, Hongzhe; Ma, Rong",,,Optimal Structured Principal Subspace Estimation: Metric Entropy and Minimax Rates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Driven by a wide range of applications, several principal subspace estimation problems have been studied individually under different structural constraints. This paper presents a unified framework for the statistical analysis of a general structured principal subspace estimation problem which includes as special cases sparse PCA/SVD, non-negative PCA/SVD, subspace constrained PCA/SVD, and spectral clustering. General minimax lower and upper bounds are established to characterize the interplay between the information-geometric complexity of the constraint set for the principal subspaces, the signal-to-noise ratio (SNR), and the dimensionality. The results yield interesting phase transition phenomena concerning the rates of convergence as a function of the SNRs and the fundamental limit for consistent estimation. Applying the general results to the specific settings yields the minimax rates of convergence for those problems, including the previous unknown optimal rates for sparse SVD, non-negative PCA/SVD and subspace constrained PCA/SVD.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500046,0
J,"Feurer, M; van Rijn, JN; Kadra, A; Gijsbers, P; Mallik, N; Ravi, S; Muller, A; Vanschoren, J; Hutter, F",,,,"Feurer, Matthias; van Rijn, Jan N.; Kadra, Arlind; Gijsbers, Pieter; Mallik, Neeratyoy; Ravi, Sahithya; Muller, Andreas; Vanschoren, Joaquin; Hutter, Frank",,,OpenML-Python: an extensible Python API for OpenML,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper, we introduce OpenML-Python, a client API for Python, which opens up the OpenML platform for a wide range of Python-based machine learning tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn extension and an extension mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation are available at https://github.com/openml/openml- python/.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,100,,,,,,,,,,,,,,,WOS:000663153000001,0
J,"Frogner, C; Claici, S; Chien, E; Solomon, J",,,,"Frogner, Charlie; Claici, Sebastian; Chien, Edward; Solomon, Justin",,,Incorporating Unlabeled Data into Distributionally-Robust Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study a robust alternative to empirical risk minimization called distributionally robust learning (DRL), in which one learns to perform against an adversary who can choose the data distribution from a specified set of distributions. We illustrate a problem with current DRL formulations, which rely on an overly broad definition of allowed distributions for the adversary, leading to learned classifiers that are unable to predict with any confidence. We propose a solution that incorporates unlabeled data into the DRL problem to further constrain the adversary. We show that this new formulation is tractable for stochastic gradient-based optimization and yields a computable guarantee on the future performance of the learned classifier, analogous to-but tighter than-guarantees from conventional DRL. We examine the performance of this new formulation on 14 real data sets and find that it often yields effective classifiers with nontrivial performance guarantees in situations where conventional DRL produces neither. Inspired by these results, we extend our DRL formulation to active learning with a novel, distributionally-robust version of the standard model-change heuristic. Our active learning algorithm often achieves superior learning performance to the original heuristic on real data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000656355600001,0
J,"Mingard, C; Valle-Perez, G; Skalse, J; Louis, AA",,,,"Mingard, Chris; Valle-Perez, Guillermo; Skalse, Joar; Louis, Ard A.",,,"Is SGD a Bayesian sampler? Well, almost",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep neural networks (DNNs) generalise remarkably well in the overparameterised regime, suggesting a strong inductive bias towards functions with low generalisation error. We empirically investigate this bias by calculating, for a range of architectures and datasets, the probability P-SGD(f vertical bar S) that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function f consistent with a training set S. We also use Gaussian processes to estimate the Bayesian posterior probability P-B(f vertical bar S) that the DNN expresses f upon random sampling of its parameters, conditioned on S. Our main findings are that P-SGD(f vertical bar S) correlates remarkably well with P-B(f vertical bar S) and that P-B(f vertical bar S) is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines P-B(f vertical bar S)), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior P-B(f vertical bar S) is the first order determinant of P-SGD(f vertical bar S), there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on P(SGD()f vertical bar S) and/or P-B(f vertical bar S), can shed light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",,,,,,"Mingard, Chris/0000-0002-4519-9709",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656393100001,0
J,"Perez, J; Barcelo, P; Marinkovic, J",,,,"Perez, Jorge; Barcelo, Pablo; Marinkovic, Javier",,,Attention is Turing Complete,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Alternatives to recurrent neural networks, in particular, architectures based on self-attention, are gaining momentum for processing input sequences. In spite of their relevance, the computational properties of such networks have not yet been fully explored. We study the computational power of the Transformer, one of the most paradigmatic architectures exemplifying self-attention. We show that the Transformer with hard-attention is Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. Our study also reveals some minimal sets of elements needed to obtain this completeness result.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656387300001,0
J,"Salehkaleybar, S; Sharifnassab, A; Golestani, SJ",,,,"Salehkaleybar, Saber; Sharifnassab, Arsalan; Golestani, S. Jamaloddin",,,One-Shot Federated Learning: Theoretical Limits and Algorithms to Achieve Them,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider distributed statistical optimization in one-shot setting, where there are m machines each observing n i.i.d. samples. Based on its observed samples, each machine sends a B-bit-long message to a server. The server then collects messages from all machines, and estimates a parameter that minimizes an expected convex loss function. We investigate the impact of communication constraint, B, on the expected error and derive a tight lower bound on the error achievable by any algorithm. We then propose an estimator, which we call Multi-Resolution Estimator (MRE), whose expected error (when B >= d log mn where d is the dimension of parameter) meets the aforementioned lower bound up to a poly-logarithmic factor in mn. The expected error of MRE, unlike existing algorithms, tends to zero as the number of machines (m) goes to infinity, even when the number of samples per machine (n) remains upper bounded by a constant. We also address the problem of learning under tiny communication budget, and present lower and upper error bounds for the case that the budget B is a constant.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706874000001,0
J,"Shamir, O",,,,"Shamir, Ohad",,,Gradient Methods Never Overfit On Separable Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A line of recent works established that when training linear predictors over separable data, using gradient methods and exponentially-tailed losses, the predictors asymptotically converge in direction to the max-margin predictor. As a consequence, the predictors asymptotically do not overfit. However, this does not address the question of whether overfitting might occur non-asymptotically, after some bounded number of iterations. In this paper, we formally show that standard gradient methods (in particular, gradient flow, gradient descent and stochastic gradient descent) never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of (O) over tilde (1/gamma T-2) up till T approximate to m, at which point the generalization error remains fixed at an essentially optimal level of (O) over tilde (1/gamma(2)m) regardless of how large T is. Along the way, we present non-asymptotic bounds on the number of margin violations over the dataset, and prove their tightness.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656404200001,0
J,"Sun, HW; Wu, Q",,,,"Sun, Hongwei; Wu, Qiang",,,Optimal Rates of Distributed Regression with Imperfect Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Distributed machine learning systems have been receiving increasing attentions for their efficiency to process large scale data. Many distributed frameworks have been proposed for different machine learning tasks. In this paper, we study the distributed kernel regression via the divide and conquer approach. The learning process consists of three stages. Firstly, the data is partitioned into multiple subsets. Then a base kernel regression algorithm is applied to each subset to learn a local regression model. Finally the local models are averaged to generate the final regression model for the purpose of predictive analytics or statistical inference. This approach has been proved asymptotically minimax optimal if the kernel is perfectly selected so that the true regression function lies in the associated reproducing kernel Hilbert space. However, this is usually, if not always, impractical because kernels that can only be selected via prior knowledge or a tuning process are hardly perfect. Instead it is more common that the kernel is good enough but imperfect in the sense that the true regression can be well approximated by but does not lie exactly in the kernel space. We show distributed kernel regression can still achieve capacity independent optimal rate in this case. To this end, we first establish a general framework that allows to analyze distributed regression with response weighted base algorithms by bounding the error of such algorithms on a single data set, provided that the error bounds have factored the impact of unexplained variance of the response variable. Then we perform a leave one out analysis of the kernel ridge regression and bias corrected kernel ridge regression, which in combination with the aforementioned framework allows us to derive sharp error bounds and capacity independent optimal rates for the associated distributed kernel regression algorithms. As a byproduct of the thorough analysis, we also prove the kernel ridge regression can achieve rates faster than O(N-1) (where N is the sample size) in the noise free setting which, to our best knowledge, are first observed and novel in regression learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700317400001,0
J,"Ye, SS; Padilla, OHM",,,,"Ye, Steven Siwei; Padilla, Oscar Hernan Madrid",,,Non-parametric Quantile Regression via the K-NN Fused Lasso,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Quantile regression is a statistical method for estimating conditional quantiles of a response variable. In addition, for mean estimation, it is well known that quantile regression is more robust to outliers than l(2)-based methods. By using the fused lasso penalty over a K-nearest neighbors graph, we propose an adaptive quantile estimator in a non-parametric setup. We show that the estimator attains optimal rate of n(-1/d) up to a logarithmic factor, under mild assumptions on the data generation mechanism of the d-dimensional data. We develop algorithms to compute the estimator and discuss methodology for model selection. Numerical experiments on simulated and real data demonstrate clear advantages of the proposed estimator over state of the art methods. All codes that implement the algorithms and the datasets used in the experiments are publicly available on the author's Github page (https://github. com/stevenysw/qt_knnf1).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,38,111,,,,,,,,,,,,,,,WOS:000663167000001,0
J,"Zhou, SL; Xiu, NH; Qi, HD",,,,"Zhou, Shenglong; Xiu, Naihua; Qi, Hou-Duo",,,Global and Quadratic Convergence of Newton Hard-Thresholding Pursuit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithms based on the hard thresholding principle have been well studied with sounding theoretical guarantees in the compressed sensing and more general sparsity-constrained optimization. It is widely observed in existing empirical studies that when a restricted Newton step was used (as the debiasing step), the hard-thresholding algorithms tend to meet halting conditions in a significantly low number of iterations and are very efficient. Hence, the thus obtained Newton hard-thresholding algorithms call for stronger theoretical guarantees than for their simple hard-thresholding counterparts. This paper provides a theoretical justification for the use of the restricted Newton step. We build our theory and algorithm, Newton Hard-Thresholding Pursuit (NHTP), for the sparsity-constrained optimization. Our main result shows that NHTP is quadratically convergent under the standard assumption of restricted strong convexity and smoothness. We also establish its global convergence to a stationary point under a weaker assumption. In the special case of the compressive sensing, NHTP effectively reduces to some of the existing hard-thresholding algorithms with a Newton step. Consequently, our fast convergence result justifies why those algorithms perform better than without the Newton step. The efficiency of NHTP was demonstrated on both synthetic and real data in compressed sensing and sparse logistic regression.",,,,,,"Qi, Hou-Duo/0000-0003-3481-4814; Zhou, Shenglong/0000-0003-2843-1614",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500012,0
J,"Zhu, YZ; Liu, RX",,,,"Zhu, Yunzhang; Liu, Renxiong",,,An algorithmic view of l(2) regularization and some path-following algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We establish an equivalence between the l(2)-regularized solution path for a convex loss function, and the solution of an ordinary differentiable equation (ODE). Importantly, this equivalence reveals that the solution path can be viewed as the flow of a hybrid of gradient descent and Newton method applying to the empirical loss, which is similar to a widely used optimization technique called trust region method. This provides an interesting algorithmic view of l(2) regularization, and is in contrast to the conventional view that the l(2) regularization solution path is similar to the gradient flow of the empirical loss. New path-following algorithms based on homotopy methods and numerical ODE solvers are proposed to numerically approximate the solution path. In particular, we consider respectively Newton method and gradient descent method as the basis algorithm for the homotopy method, and establish their approximation error rates over the solution path. Importantly, our theory suggests novel schemes to choose grid points that guarantee an arbitrarily small suboptimality for the solution path. In terms of computational cost, we prove that in order to achieve an f-suboptimality for the entire solution path, the number of Newton steps required for the Newton method is O(f(-1/2)), while the number of gradient steps required for the gradient descent method is O (f(-1)ln(f(-1))). Finally, we use l(2)-regularized logistic regression as an illustrating example to demonstrate the effectiveness of the proposed path-following algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687174600001,0
J,"Alcobaca, E; Siqueira, F; Rivolli, A; Garcia, LPF; Oliva, JT; de Carvalho, ACPLF",,,,"Alcobaca, Edesio; Siqueira, Felipe; Rivolli, Adriano; Garcia, Luis P. F.; Oliva, Jefferson T.; de Carvalho, Andre C. P. L. F.",,,MFE: Towards reproducible meta-feature extraction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Automated recommendation of machine learning algorithms is receiving a large deal of attention, not only because they can recommend the most suitable algorithms for a new task, but also because they can support efficient hyper-parameter tuning, leading to better machine learning solutions. The automated recommendation can be implemented using meta-learning, learning from previous learning experiences, to create a meta-model able to associate a data set to the predictive performance of machine learning algorithms. Although a large number of publications report the use of meta-learning, reproduction and comparison of meta-learning experiments is a difficult task. The literature lacks extensive and comprehensive public tools that enable the reproducible investigation of the different meta-learning approaches. An alternative to deal with this difficulty is to develop a meta-feature extractor package with the main characterization measures, following uniform guidelines that facilitate the use and inclusion of new meta-features. In this paper, we propose two Meta-Feature Extractor (MFE) packages, written in both Python and R, to fill this lack. The packages follow recent frameworks for meta-feature extraction, aiming to facilitate the reproducibility of meta-learning experiments.",,,,,"Ponce de Leon Ferreira de de Carvalho, Andre Carlos/A-6321-2008; Rivolli, Adriano/AAZ-6130-2021","Ponce de Leon Ferreira de de Carvalho, Andre Carlos/0000-0002-4765-6459; Rivolli, Adriano/0000-0001-6445-3007; Pinto de Souza Alcobaca Neto, Edesio/0000-0001-9175-8535",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,111,,,,,,,,,,,,,,,WOS:000546629400001,0
J,"Hoiles, W; Krishnamurthy, V; Pattanayak, K",,,,"Hoiles, William; Krishnamurthy, Vikram; Pattanayak, Kunal",,,Rationally Inattentive Inverse Reinforcement Learning Explains YouTube Commenting Behavior,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a novel application of inverse reinforcement learning with behavioral economics constraints to model, learn and predict the commenting behavior of YouTube viewers. Each group of users is modeled as a rationally inattentive Bayesian agent which solves a contextual bandit problem. Our methodology integrates three key components. First, to identify distinct commenting patterns, we use deep embedded clustering to estimate framing information (essential extrinsic features) that clusters users into distinct groups. Second, we present an inverse reinforcement learning algorithm that uses Bayesian revealed preferences to test for rationality: does there exist a utility function that rationalizes the given data, and if yes, can it be used to predict commenting behavior? Finally, we impose behavioral economics constraints stemming from rational inattention to characterize the attention span of groups of users. The test imposes a Renyi mutual information cost constraint which impacts how the agent can select attention strategies to maximize their expected utility. After a careful analysis of a massive YouTube dataset, our surprising result is that in most YouTube user groups, the commenting behavior is consistent with optimizing a Bayesian utility with rationally inattentive constraints. The paper also highlights how the rational inattention model can accurately predict commenting behavior. The massive YouTube dataset and analysis used in this paper are available on GitHub and completely reproducible.",,,,,"Pattanayak, Kunal/ABE-4502-2020",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,170,,,,,,,,,,,,,,,WOS:000570156100001,0
J,"Kim, Y; Gao, C",,,,"Kim, Youngseok; Gao, Chao",,,Bayesian Model Selection with Graph Structured Sparsity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a general algorithmic framework for Bayesian model selection. A spike-and-slab Laplacian prior is introduced to model the underlying structural assumption. Using the notion of effective resistance, we derive an EM-type algorithm with closed-form iterations to efficiently explore possible candidates for Bayesian model selection. The deterministic nature of the proposed algorithm makes it more scalable to large-scale and high-dimensional data sets compared with existing stochastic search algorithms. When applied to sparse linear regression, our framework recovers the EMVS algorithm (Rockova and George, 2014) as a special case. We also discuss extensions of our framework using tools from graph algebra to incorporate complex Bayesian models such as biclustering and submatrix localization. Extensive simulation studies and real data applications are conducted to demonstrate the superior performance of our methods over its frequentist competitors such as l(0) or l(1) penalization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,109,,,,,,,,,,,,,,,WOS:000546628600001,0
J,"Ramsey, JD; Malinsky, D; Bui, KV",,,,"Ramsey, Joseph D.; Malinsky, Daniel; Bui, Kevin, V",,,algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this report we describe a tool for comparing the performance of graphical causal structure learning algorithms implemented in the TETRAD freeware suite of causal analysis methods. Currently the tool is available as package in the TETRAD source code (written in Java). Simulations can be done varying the number of runs, sample sizes, and data modalities. Performance on this simulated data can then be compared for a number of algorithms, with parameters varied and with performance statistics as selected, producing a publishable report. The package presented here may also be used to compare structure learning methods across platforms and programming languages, i.e., to compare algorithms implemented in TETRAD with those implemented in MATLAB, Python, or R.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,238,,,,,,,,,,,,,,,WOS:000608909300001,0
J,"Rashid, T; Samvelyan, M; de Witt, CS; Farquhar, G; Foerster, J; Whiteson, S",,,,"Rashid, Tabish; Samvelyan, Mikayel; de Witt, Christian Schroeder; Farquhar, Gregory; Foerster, Jakob; Whiteson, Shimon",,,Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.",,,,,,"Foerster, Jakob/0000-0001-9688-2498",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,178,,,,,,,,,,,,,,,WOS:000570234000001,0
J,"Tavenard, R; Faouzi, J; Vandewiele, G; Divo, F; Androz, G; Holtz, C; Payne, M; Yurchak, R; Russwurm, M; Kolar, K; Woods, E",,,,"Tavenard, Romain; Faouzi, Johann; Vandewiele, Gilles; Divo, Felix; Androz, Guillaume; Holtz, Chester; Payne, Marie; Yurchak, Roman; Russwurm, Marc; Kolar, Kushal; Woods, Eli",,,"Tslearn, A Machine Learning Toolkit for Time Series Data",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects.",,,,,,"Vandewiele, Gilles/0000-0001-9531-0623; Kolar, Kushal/0000-0002-8668-0275; Faouzi, Johann/0000-0003-0542-9968",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,118,,,,,,,,,,,,,,,WOS:000546631900001,0
J,"Ye, HS; Luo, L; Zhang, ZH",,,,"Ye, Haishan; Luo, Luo; Zhang, Zhihua",,,Nesterov's Acceleration for Approximate Newton,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimization plays a key role in machine learning. Recently, stochastic second-order methods have attracted considerable attention because of their low computational cost in each iteration. However, these methods might suffer from poor performance when the Hessian is hard to be approximate well in a computation-efficient way. To overcome this dilemma, we resort to Nesterov's acceleration to improve the convergence performance of these second-order methods and propose accelerated approximate Newton. We give the theoretical convergence analysis of accelerated approximate Newton and show that Nesterov's acceleration can improve the convergence rate. Accordingly, we propose an accelerated regularized sub-sampled Newton (ARS SN) which performs much better than the conventional regularized sub-sampled Newton empirically and theoretically. Moreover, we show that ARS SN has better performance than classical first-order methods empirically.",,,,,"zhang, zh/GWV-4677-2022",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,142,,,,,,,,,,,,,,,WOS:000558804700001,0
J,"Zhai, YX; Yang, ZT; Liao, ZY; Wright, J; Ma, Y",,,,"Zhai, Yuexiang; Yang, Zitong; Liao, Zhenyu; Wright, John; Ma, Yi",,,Complete Dictionary Learning via l(4)-Norm Maximization over the Orthogonal Group,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the fundamental problem of learning a complete (orthogonal) dictionary from samples of sparsely generated signals. Most existing methods solve the dictionary (and sparse representations) based on heuristic algorithms, usually without theoretical guarantees for either optimality or complexity. The recent l(1)-minimization based methods do provide such guarantees but the associated algorithms recover the dictionary one column at a time. In this work, we propose a new formulation that maximizes the l(4)-norm over the orthogonal group, to learn the entire dictionary. We prove that under a random data model, with nearly minimum sample complexity, the global optima of the l(4)-norm are very close to signed permutations of the ground truth. Inspired by this observation, we give a conceptually simple and yet effective algorithm based on matching, stretching, and projection (MSP). The algorithm provably converges locally and cost per iteration is merely an SVD. In addition to strong theoretical guarantees, experiments show that the new algorithm is significantly more efficient and effective than existing methods, including KSVD and l(1)-based methods. Preliminary experimental results on mixed real imagery data clearly demonstrate advantages of so learned dictionary over classic PCA bases.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,165,,,,,,,,,,,,,,,WOS:000570153000001,0
J,"Becker, S; Cheridito, P; Jentzen, A",,,,"Becker, Sebastian; Cheridito, Patrick; Jentzen, Arnulf",,,Deep Optimal Stopping,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we develop a deep learning method for optimal stopping problems which directly learns the optimal stopping rule from Monte Carlo samples. As such, it is broadly applicable in situations where the underlying randomness can efficiently be simulated. We test the approach on three problems: the pricing of a Bermudan max-call option, the pricing of a callable multi barrier reverse convertible and the problem of optimally stopping a fractional Brownian motion. In all three cases it produces very accurate results in high-dimensional situations with short computing times.",,,,,"Jentzen, Arnulf/O-8237-2016; Bauer, Sebastian/HHR-9319-2022","Jentzen, Arnulf/0000-0002-9840-3339; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,74,,,,,,,,,,,,,,,WOS:000467895500001,0
J,"Bohn, B; Rieger, C; Griebel, M",,,,"Bohn, Bastian; Rieger, Christian; Griebel, Michael",,,A Representer Theorem for Deep Kernel Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we provide a finite-sample and an infinite-sample representer theorem for the concatenation of (linear combinations of) kernel functions of reproducing kernel Hilbert spaces. These results serve as mathematical foundation for the analysis of machine learning algorithms based on compositions of functions. As a direct consequence in the finite-sample case, the corresponding in finite-dimensional minimization problems can be recast into (nonlinear) finite-dimensional minimization problems, which can be tackled with nonlinear optimization algorithms. Moreover, we show how concatenated machine learning problems can be reformulated as neural networks and how our representer theorem applies to a broad class of state-of-the-art deep learning methods.",,,,,,"Bohn, Bastian/0000-0001-5663-7527",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,64,,,,,,,,,,,,,,,WOS:000467880300001,0
J,"Dance, CR; Silander, T",,,,"Dance, Christopher R.; Silander, Tomi",,,Optimal Policies for Observing Time Series and Related Restless Bandit Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance. It is based on a new verification theorem by Nifio-Mora that guarantees threshold structure for Markov decision processes, and on the relation between binary sequences known as Christoffel words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology. This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index policy. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,35,,,,,,,,,,,,,,,WOS:000463318300001,0
J,"Mourtada, J; Gaiffas, S",,,,"Mourtada, Jaouad; Gaiffas, Stephane",,,On the optimality of the Hedge algorithm in the stochastic regime,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the behavior of the Hedge algorithm in the online stochastic setting. We prove that anytime Hedge with decreasing learning rate, which is one of the simplest algorithm for the problem of prediction with expert advice, is remarkably both worst-case optimal and adaptive to the easier stochastic and adversarial with a gap problems. This shows that, in spite of its small, non-adaptive learning rate, Hedge possesses the same optimal regret guarantee in the stochastic case as recently introduced adaptive algorithms. Moreover, our analysis exhibits qualitative differences with other versions of the Hedge algorithm, such as the fixed-horizon variant (with constant learning rate) and the one based on the so-called doubling trick, both of which fail to adapt to the easier stochastic setting. Finally, we determine the intrinsic limitations of anytime Hedge in the stochastic case, and discuss the improvements provided by more adaptive algorithms.",,,,,,"Mourtada, Jaouad/0000-0002-7830-9783",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,83,,,,,,,,,,,,,,,WOS:000467897900001,0
J,"Osband, I; Van Roy, B; Russo, DJ; Wen, Z",,,,"Osband, Ian; Van Roy, Benjamin; Russo, Daniel J.; Wen, Zheng",,,Deep Exploration via Randomized Value Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.,,,,,"wen, zheng/HII-3705-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,124,,,,,,,,,,,,,,,WOS:000487068900008,0
J,"Shen, YN; Chen, TY; Giannakis, GB",,,,"Shen, Yanning; Chen, Tianyi; Giannakis, Georgios B.",,,Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel-based methods exhibit well-documented performance in various nonlinear learning tasks. Most of them rely on a preselected kernel, whose prudent choice presumes task-specific prior information. Especially when the latter is not available, multi-kernel learning has gained popularity thanks to its flexibility in choosing kernels from a prescribed kernel dictionary. Leveraging the random feature approximation and its recent orthogonality-promoting variant, the present contribution develops a scalable multi-kernel learning scheme (termed Raker) to obtain the sought nonlinear learning function 'on the fly,' first for static environments. To further boost performance in dynamic environments, an adaptive multi-kernel learning scheme (termed AdaRaker) is developed. AdaRaker accounts not only for data-driven learning of kernel combination, but also for the unknown dynamics. Performance is analyzed in terms of both static and dynamic regrets. AdaRaker is uniquely capable of tracking nonlinear learning functions in environments with unknown dynamics, and with with analytic performance guarantees. Tests with synthetic and real datasets are carried out to showcase the effectiveness of the novel algorithms.(1)",,,,,"Shen, Yanning/W-6185-2019; Giannakis, Georgios/Z-4413-2019","Giannakis, Georgios/0000-0002-0196-0260",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,22,,,,,,,,,,,,,,,WOS:000458667900001,0
J,"Li, LS; Jamieson, K; DeSalvo, G; Rostamizadeh, A; Talwalkar, A",,,,"Li, Lisha; Jamieson, Kevin; DeSalvo, Giulia; Rostamizadeh, Afshin; Talwalkar, Ameet",,,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, HYPERBAND, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare HYPERBAND with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that HYPERBAND can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,185,,,,,,,,,,,,,,,WOS:000435449300001,0
J,"Park, G; Raskutti, G",,,,"Park, Gunwoong; Raskutti, Garvesh",,,Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional p > n setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms. We also demonstrate through a real data example involving multi-variate count data, that our ODS algorithm is well-suited to estimating DAG models for count data in comparison to other methods used for discrete data.",,,,,"Park, Gunwoong/AAA-5853-2020",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,224,,,,,,,,,,,,,,,WOS:000438190400001,0
J,"Ray, A; Neeman, J; Sanghavi, S; Shakkottai, S",,,,"Ray, Avik; Neeman, Joe; Sanghavi, Sujay; Shakkottai, Sanjay",,,The Search Problem in Mixture Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the task of learning the parameters of a single component of a mixture model, for the case when we are given side information about that component; we call this the search problem in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,206,,,,,,,,,,,,,,,WOS:000435627800001,0
J,"Agarwal, N; Bullins, B; Hazan, E",,,,"Agarwal, Naman; Bullins, Brian; Hazan, Elad",,,Second-Order Stochastic Optimization for Machine Learning in Linear Time,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.",,,,,,"Hazan, Elad/0000-0002-1566-3216",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,116,,,,,,,,,,,,,,,WOS:000424539900001,0
J,"Al-Shedivat, M; Wilson, AG; Saatchi, Y; Hu, ZT; Xing, EP",,,,"Al-Shedivat, Maruan; Wilson, Andrew Gordon; Saatchi, Yunus; Hu, Zhiting; Xing, Eric P.",,,Learning Scalable Deep Kernels with Recurrent Structure,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.",,,,,"salama, khaled Nabil/K-3689-2019","salama, khaled Nabil/0000-0001-7742-1282",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,82,,,,,,,,,,,,,,,WOS:000412062600001,0
J,"Lauly, S; Zheng, Y; Allauzen, A; Larochelle, H",,,,"Lauly, Stanislas; Zheng, Yin; Allauzen, Alexandre; Larochelle, Hugo",,,Document Neural Autoregressive Distribution Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present an approach based on feed-forward neural networks for learning the distribution over textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator (NADE) model which has been shown to be a good estimator of the distribution over discrete-valued high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to textual data, retaining the property that sampling or computing the probability of an observation can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-level context.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,113,,,,,,,,,,,,,,,WOS:000424539400001,0
J,"Roy, A; Pokutta, S",,,,"Roy, Aurko; Pokutta, Sebastian",,,Hierarchical Clustering via Spreading Metrics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most O (alpha(n) log n) times the cost of the optimal hierarchical clustering, where a n is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O (log(3/2) n) times the cost of the optimal solution. We improve this by giving an O (log n) -approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O (log n) -approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,88,,,,,,,,,,,,,,,WOS:000412070000001,0
J,"Tikka, S; Karvanen, J",,,,"Tikka, Santtu; Karvanen, Juha",,,Simplifying Probabilistic Expressions in Causal Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,36,,,,,,,,,,,,,,,WOS:000400519100001,0
J,"Bhaskar, SA",,,,"Bhaskar, Sonia A.",,,Probabilistic Low-Rank Matrix Completion from Quantized Measurements,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the recovery of a low rank real-valued matrix M given a subset of noisy discrete (or quantized) measurements. Such problems arise in several applications such as collaborative filtering, learning and content analytics, and sensor network localization. We consider constrained maximum likelihood estimation of M, under a constraint on the entry-wise infinity-norm of M and an exact rank constraint. We provide upper bounds on the Frobenius norm of matrix estimation error under this model. Previous theoretical investigations have focused on binary (1-bit) quantizers, and been based on convex relaxation of the rank. Compared to the existing binary results, our performance upper bound has faster convergence rate with matrix dimensions when the fraction of revealed observations is fixed. We also propose a globally convergent optimization algorithm based on low rank factorization of M and validate the method on synthetic and real data, with improved performance over previous methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,60,,,,,,,,,,,,,,,WOS:000391493300001,0
J,"Dai, WL; Tong, TJ; Genton, MG",,,,"Dai, Wenlin; Tong, Tiejun; Genton, Marc G.",,,Optimal Estimation of Derivatives in Nonparametric Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a simple framework for estimating derivatives without fitting the regression function in nonparametric regression. Unlike most existing methods that use the symmetric difference quotients, our method is constructed as a linear combination of observations. It is hence very flexible and applicable to both interior and boundary points, including most existing methods as special cases of ours. Within this framework, we define the variance-minimizing estimators for any order derivative of the regression function with a fixed bias-reduction level. For the equidistant design, we derive the asymptotic variance and bias of these estimators. We also show that our new method will, for the first time, achieve the asymptotically optimal convergence rate for difference-based estimators. Finally, we provide an effective criterion for selection of tuning parameters and demonstrate the usefulness of the proposed method through extensive simulation studies of the first- and second-order derivative estimators.",,,,,"Tong, Tiejun/F-3880-2010","Tong, Tiejun/0000-0003-0947-3990; Dai, Wenlin/0000-0002-4858-1145",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,164,,,,,,,,,,,,,,,WOS:000391669500001,0
J,"Fan, J; Wu, YR; Yuan, M; Page, D; Liu, J; Ong, IM; Peissig, P; Burnside, E",,,,"Fan, Jun; Wu, Yirong; Yuan, Ming; Page, David; Liu, Jie; Ong, Irene M.; Peissig, Peggy; Burnside, Elizabeth",,,Structure-Leveraged Methods in Breast Cancer Risk Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Predicting breast cancer risk has long been a goal of medical research in the pursuit of precision medicine. The goal of this study is to develop novel penalized methods to improve breast cancer risk prediction by leveraging structure information in electronic health records. We conducted a retrospective case-control study, garnering 49 mammography descriptors and 77 high-frequency/low-penetrance single-nucleotide polymorphisms (SNPs) from an existing personalized medicine data repository. Structured mammography reports and breast imaging features have long been part of a standard electronic health record (EHR), and genetic markers likely will be in the near future. Lasso and its variants are widely used approaches to integrated learning and feature selection, and our methodological contribution is to incorporate the dependence structure among the features into these approaches. More specifically, we propose a new methodology by combining group penalty and l(p) (1 <= p <= 2) fusion penalty to improve breast cancer risk prediction, taking into account structure information in mammography descriptors and SNPs. We demonstrate that our method provides bene fits that are both statistically significant and potentially significant to people's lives.",,,,,"Fan, Jun/O-4742-2017","Fan, Jun/0000-0001-8451-3484",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,85,,,,,,,,,,,,,,,WOS:000391527700001,0
J,"Luca, S; Clifton, DA; Vanrumste, B",,,,"Luca, Stijn; Clifton, David A.; Vanrumste, Bart",,,One-class classification of point patterns of extremes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Novelty detection or one-class classification starts from a model describing some type of 'normal behaviour' and aims to classify deviations from this model as being either novelties or anomalies. In this paper the problem of novelty detection for point patterns S = {X-1 ,..., X-k} subset of R-d is treated where examples of anomalies are very sparse, or even absent. The latter complicates the tuning of hyperparameters in models commonly used for novelty detection, such as one-class support vector machines and hidden Markov models. To this end, the use of extreme value statistics is introduced to estimate explicitly a model for the abnormal class by means of extrapolation from a statistical model X for the normal class. We show how multiple types of information obtained from any available extreme instances of S can be combined to reduce the high false-alarm rate that is typically encountered when classes are strongly imbalanced, as often occurs in the one-class setting (whereby 'abnormal' data are often scarce). The approach is illustrated using simulated data and then a real-life application is used as an exemplar, whereby accelerometry data from epileptic seizures are analysed - these are known to be extreme and rare with respect to normal accelerometer data.",,,,,"Vanrumste, Bart/C-5420-2018","Vanrumste, Bart/0000-0002-9409-935X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,191,,,,,,,,,,,,,,,WOS:000391826500001,0
J,"Ma, J; Michailidis, G",,,,"Ma, Jing; Michailidis, George",,,Joint Structural Estimation of Multiple Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian graphical models capture dependence relationships between random variables through the pattern of nonzero elements in the corresponding inverse covariance matrices. To date, there has been a large body of literature on both computational methods and analytical results on the estimation of a single graphical model. However, in many application domains, one has to estimate several related graphical models, a problem that has also received attention in the literature. The available approaches usually assume that all graphical models are globally related. On the other hand, in many settings different relationships between subsets of the node sets exist between different graphical models. We develop methodology that jointly estimates multiple Gaussian graphical models, assuming that there exists prior information on how they are structurally related. For many applications, such information is available from external data sources. The proposed method consists of first applying neighborhood selection with a group lasso penalty to obtain edge sets of the graphs, and a maximum likelihood refit for estimating the nonzero entries in the inverse covariance matrices. We establish consistency of the proposed method for sparse high- dimensional Gaussian graphical models and examine its performance using simulation experiments. Applications to a climate data set and a breast cancer data set are also discussed.",,,,,"Ma, Jing/J-2065-2019","Ma, Jing/0000-0001-6294-227X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,166,,,,,,,,,,,,,,,WOS:000391670900001,0
J,"Maurer, A; Pontil, M; Romera-Paredes, B",,,,"Maurer, Andreas; Pontil, Massimiliano; Romera-Paredes, Bernardino",,,The Benefit of Multitask Representation Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,81,,,,,,,,,,,,,,,WOS:000391525900001,0
J,"Meister, M; Steinwart, I",,,,"Meister, Mona; Steinwart, Ingo",,,Optimal Learning Rates for Localized SVMs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One of the limiting factors of using support vector machines (SVMs) in large scale applications are their super-linear computational requirements in terms of the number of training samples. To address this issue, several approaches that train SVMs on many small chunks separately have been proposed in the literature. With the exception of random chunks, which is also known as divide-and-conquer kernel ridge regression, however, these approaches have only been empirically investigated. In this work we investigate a spatially oriented method to generate the chunks. For the resulting localized SVM that uses Gaussian kernels and the least squares loss we derive an oracle inequality, which in turn is used to deduce learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function. In addition, we derive local learning rates that are based on the local smoothness of the regression function. We further introduce a data-dependent parameter selection method for our local SVM approach and show that this method achieves the same almost optimal learning rates. Finally, we present a few larger scale experiments for our localized SVM showing that it achieves essentially the same test error as a global SVM for a fraction of the computational requirements. In addition, it turns out that the computational requirements for the local SVMs are similar to those of a vanilla random chunk approach, while the achieved test errors are significantly better.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,194,,,,,,,,,,,,,,,WOS:000391826900001,0
J,"Pahikkala, T; Airola, A",,,,"Pahikkala, Tapio; Airola, Antti",,,RLScore: Regularized Least-Squares Learners,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"RLScore is a Python open source module for kernel based machine learning. The library provides implementations of several regularized least-squares (RLS) type of learners. RLS methods for regression and classification, ranking, greedy feature selection, multi-task and zero-shot learning, and unsupervised classification are included. Matrix algebra based computational short-cuts are used to ensure efficiency of both training and cross-validation. A simple API and extensive tutorials allow for easy use of RLScore.",,,,,"Pahikkala, Tapio/H-9659-2012; Airola, Antti/ABH-2605-2021","Pahikkala, Tapio/0000-0003-4183-2455; ",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,220,,,,,,,,,,,,,,,WOS:000391913700001,0
J,"Petersen, A; Simon, N; Witten, D",,,,"Petersen, Ashley; Simon, Noah; Witten, Daniela",,,Convex Regression with Interpretable Sharp Partitions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of predicting an outcome variable on the basis of a small number of covariates, using an interpretable yet non-additive model. We propose convex regression with interpretable sharp partitions (CRISP) for this task. CRISP partitions the covariate space into blocks in a data-adaptive way, and fits a mean model within each block. Unlike other partitioning methods, CRISP is fit using a non-greedy approach by solving a convex optimization problem, resulting in low-variance fits. We explore the properties of CRISP, and evaluate its performance in a simulation study and on a housing price data set.",,,,,,"Petersen, Ashley/0000-0001-7711-9657",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,94,,,,,,,,,,27635120,,,,,WOS:000391539200001,0
J,"Wiens, J; Guttag, J; Horvitz, E",,,,"Wiens, Jenna; Guttag, John; Horvitz, Eric",,,Patient Risk Stratification with Time-Varying Parameters: A Multitask Learning Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The proliferation of electronic health records (EHRs) frames opportunities for using machine learning to build models that help healthcare providers improve patient outcomes. However, building useful risk stratification models presents many technical challenges including the large number of factors (both intrinsic and extrinsic) influencing a patient's risk of an adverse outcome and the inherent evolution of that risk over time. We address these challenges in the context of learning a risk stratification model for predicting which patients are at risk of acquiring a Clostridium difficile infection (CDI). We take a novel data-centric approach, leveraging the contents of EHRs from nearly 50,000 hospital admissions. We show how, by adapting techniques from multitask learning, we can learn models for patient risk stratification with unprecedented classification performance. Our model, based on thousands of variables, both time-varying and time-invariant, changes over the course of a patient admission. Applied to a held out set of approximately 25,000 patient admissions, we achieve an area under the receiver operating characteristic curve of 0.81 (95% CI 0.78-0.84). The model has been integrated into the health record system at a large hospital in the US, and can be used to produce daily risk estimates for each inpatient. While more complex than traditional risk stratification methods, the widespread development and use of such data-driven models could ultimately enable cost-effective, targeted prevention strategies that lead to better patient outcomes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,209,,,,,,,,,,,,,,,WOS:000391833600001,0
J,"Zhang, YC; Chen, X; Zhou, DY; Jordan, MI",,,,"Zhang, Yuchen; Chen, Xi; Zhou, Dengyong; Jordan, Michael I.",,,Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Crowdsourcing is a popular paradigm for effectively collecting labels at low cost. The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.",,,,,", Gustavo/ABC-1706-2022; Jordan, Michael I/C-5253-2013; Zhang, Yuchen/GYI-8858-2022; Camps-Valls, Gustavo/A-2532-2011","Camps-Valls, Gustavo/0000-0003-1683-2138; Jordan, Michael/0000-0001-8935-817X",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,102,,,,,,,,,,,,,,,WOS:000391541500001,0
J,"Yang, EH; Ravikumar, P; Allen, GI; Liu, ZD",,,,"Yang, Eunho; Ravikumar, Pradeep; Allen, Genevera I.; Liu, Zhandong",,,Graphical Models via Univariate Exponential Family Distributions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Undirected graphical models, or Markov networks, are a popular class of statistical models, used in a wide variety of applications. Popular instances of this class include Gaussian graphical models and Ising models. In many settings, however, it might not be clear which subclass of graphical models to use, particularly for non-Gaussian and non-categorical data. In this paper, we consider a general sub-class of graphical models where the node-wise conditional distributions arise from exponential families. This allows us to derive multivariate graphical model distributions from univariate exponential family distributions, such as the Poisson, negative binomial, and exponential distributions. Our key contributions include a class of M-estimators to fit these graphical model distributions; and rigorous statistical analysis showing that these M-estimators recover the true graphical model structure exactly, with high probability. We provide examples of genomic and proteomic networks learned via instances of our class of graphical models derived from Poisson and exponential distributions.",,,,,"Yang, Eunho/K-8395-2016","Liu, Zhandong/0000-0002-7608-0831",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3813,3847,,,,,,,,,,,27570498,,,,,WOS:000369888000044,0
J,"Boumal, N; Mishra, B; Absil, PA; Sepulchre, R",,,,"Boumal, Nicolas; Mishra, Bamdev; Absil, P. -A.; Sepulchre, Rodolphe",,,"Manopt, a Matlab Toolbox for Optimization on Manifolds",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. By dealing internally with most of the differential geometry, the package aims particularly at lowering the entrance barrier.",,,,,,"Mishra, Bamdev/0000-0001-7430-2843",,,,,,,,,,,,,1532-4435,,,,,APR,2014,15,,,,,,1455,1459,,,,,,,,,,,,,,,,WOS:000338420000008,0
J,"Shah, RD; Meinshausen, N",,,,"Shah, Rajen Dinesh; Meinshausen, Nicolai",,,Random Intersection Trees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Finding interactions between variables in large and high-dimensional data sets is often a serious computational challenge. Most approaches build up interaction sets incrementally, adding variables in a greedy fashion. The drawback is that potentially informative high-order interactions may be overlooked. Here, we propose an alternative approach for classification problems with binary predictor variables, called Random Intersection Trees. It works by starting with a maximal interaction that includes all variables, and then gradually removing variables if they fail to appear in randomly chosen observations of a class of interest. We show that informative interactions are retained with high probability, and the computational complexity of our procedure is of order p(kappa), where p is the number of predictor variables. The value of kappa can reach values as low as 1 for very sparse data; in many more general settings, it will still beat the exponent s obtained when using a brute force search constrained to order s interactions. In addition, by using some new ideas based on min-wise hash schemes, we are able to further reduce the computational cost. Interactions found by our algorithm can be used for predictive modelling in various forms, but they are also often of interest in their own right as useful characterisations of what distinguishes a certain class from others.",,,,,,"Shah, Rajen/0000-0001-9073-3782",,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,629,654,,,,,,,,,,,,,,,,WOS:000335457700009,0
J,"Vats, D; Nowak, RD",,,,"Vats, Divyanshu; Nowak, Robert D.",,,A Junction Tree Framework for Undirected Graphical Model Selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An undirected graphical model is a joint probability distribution defined on an undirected graph G*, where the vertices in the graph index a collection of random variables and the edges encode conditional independence relationships among random variables. The undirected graphical model selection (UGMS) problem is to estimate the graph G* given observations drawn from the undirected graphical model. This paper proposes a framework for decomposing the UGMS problem into multiple subproblems over clusters and subsets of the separators in a junction tree. The junction tree is constructed using a graph that contains a superset of the edges in G*. We highlight three main properties of using junction trees for UGMS. First, different regularization parameters or different UGMS algorithms can be used to learn different parts of the graph. This is possible since the subproblems we identify can be solved independently of each other. Second, under certain conditions, a junction tree based UGMS algorithm can produce consistent results with fewer observations than the usual requirements of existing algorithms. Third, both our theoretical and experimental results show that the junction tree framework does a significantly better job at finding the weakest edges in a graph than existing methods. This property is a consequence of both the first and second properties. Finally, we note that our framework is independent of the choice of the UGMS algorithm and can be used as a wrapper around standard UGMS algorithms for more accurate graph estimation.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2014,15,,,,,,147,191,,,,,,,,,,,,,,,,WOS:000335457400005,0
J,"Rao, V; Teh, YW",,,,"Rao, Vinayak; Teh, Yee Whye",,,Fast MCMC Sampling for Markov Jump Processes and Extensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path, and then sampling a new path given the set of extant and virtual jump times. The first step involves simulating a piecewise-constant inhomogeneous Poisson process, while for the second, we use a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time-discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks, and show significant computational benefits over state-of-the-art MCMC samplers for these models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3295,3320,,,,,,,,,,,,,,,,WOS:000329786900003,0
J,"Guyader, A; Hengartner, N",,,,"Guyader, Arnaud; Hengartner, Nick",,,On the Mutual Nearest Neighbors Estimate in Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y vertical bar X = x] as follows: first identify the k nearest neighbors of x in the sample D-n, then keep only those for which x is itself one of the k nearest neighbors, and finally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2013,14,,,,,,2361,2376,,,,,,,,,,,,,,,,WOS:000324799600007,0
J,"Zhang, C; Liu, YF",,,,"Zhang, Chong; Liu, Yufeng",,,Multicategory Large-Margin Unified Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Hard and soft classifiers are two important groups of techniques for classification problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classifiers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classification task or not. In particular, soft classifiers predict the label based on the obtained class conditional probabilities, while hard classifiers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classification, it is unclear which one to use in a given situation. To tackle this problem, the Large-margin Unified Machine (LUM) was recently proposed as a unified family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classifiers. For multicategory cases, however, the concept of soft and hard classification becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classification under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classification and its transition behavior from soft to hard classifiers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1349,1386,,,,,,,,,,,24415909,,,,,WOS:000320709300005,0
J,"Gould, S",,,,"Gould, Stephen",,,DARWIN: A Framework for Machine Learning and Computer Vision Research and Development,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We present an open-source platform-independent C++ framework for machine learning and computer vision research. The framework includes a wide range of standard machine learning and graphical models algorithms as well as reference implementations for many machine learning and computer vision applications. The framework contains Matlab wrappers for core components of the library and an experimental graphical user interface for developing and visualizing machine learning data flows.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3533,3537,,,,,,,,,,,,,,,,WOS:000314529000003,0
J,"Grau, J; Keilwagen, J; Gohr, A; Haldemann, B; Posch, S; Grosse, I",,,,"Grau, Jan; Keilwagen, Jens; Gohr, Andre; Haldemann, Berit; Posch, Stefan; Grosse, Ivo",,,Jstacs: A Java Framework for Statistical Analysis and Classification of Biological Sequences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Jstacs is an object-oriented Java library for analysing and classifying sequence data, which emerged from the need for a standardized implementation of statistical models, learning principles, classifiers, and performance measures. In Jstacs, these components can be used, combined, and extended easily, which allows for a direct comparison of different approaches and fosters the development of new components. Jstacs is especially tailored to biological sequence data, but is also applicable to general discrete and continuous data. Jstacs is freely available at http://www.jstacs.de under the GNU GPL license including an API documentation, a cookbook, and code examples.",,,,,"Grau, Jan/H-5465-2019","Grau, Jan/0000-0003-2081-6405; Keilwagen, Jens/0000-0002-6792-7076",,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1967,1971,,,,,,,,,,,,,,,,WOS:000307020700008,0
J,"Rejchel, W",,,,"Rejchel, Wojciech",,,On Ranking and Generalization Bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The problem of ranking is to predict or to guess the ordering between objects on the basis of their observed features. In this paper we consider ranking estimators that minimize the empirical convex risk. We prove generalization bounds for the excess risk of such estimators with rates that are faster than 1/root n. We apply our results to commonly used ranking algorithms, for instance boosting or support vector machines. Moreover, we study the performance of considered estimators on real data sets.",,,,,"Rejchel, Wojciech/D-4813-2014","Rejchel, Wojciech/0000-0003-1148-1439",,,,,,,,,,,,,1532-4435,,,,,MAY,2012,13,,,,,,1373,1392,,,,,,,,,,,,,,,,WOS:000305456600004,0
J,"Rubinstein, BIP; Rubinstein, JH",,,,"Rubinstein, Benjamin I. P.; Rubinstein, J. Hyam",,,A Geometric Approach to Sample Compression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Sample Compression Conjecture of Littlestone & Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer's Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a first negative result on the former, through a systematic investigation of finite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin & Warmuth. A bijection between finite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in R-d have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any finite maximum class, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d + k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes.",,,,,,"Rubinstein, Joachim/0000-0002-5712-0113",,,,,,,,,,,,,1532-4435,,,,,APR,2012,13,,,,,,1221,1261,,,,,,,,,,,,,,,,WOS:000303773100011,0
J,"Gashler, M",,,,"Gashler, Mike",,,Waffles: A Machine Learning Toolkit,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We present a breadth-oriented collection of cross-platform command-line tools for researchers in machine learning called Waffles. The Waffles tools are designed to offer a broad spectrum of functionality in a manner that is friendly for scripted automation. All functionality is also available in a C++ class library. Waffles is available under the GNU Lesser General Public License.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2383,2387,,,,,,,,,,,,,,,,WOS:000293757900009,0
J,"Tsoumakas, G; Spyromitros-Xioufis, E; Vilcek, J; Vlahavas, I",,,,"Tsoumakas, Grigorios; Spyromitros-Xioufis, Eleftherios; Vilcek, Jozef; Vlahavas, Ioannis",,,MULAN: A Java Library for Multi-Label Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"MULAN is a Java library for learning from multi-label data. It offers a variety of classification, ranking, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures.",,,,,"Tsoumakas, Grigorios/B-4718-2008; Vlahavas, Ioannis/Q-1779-2017","Tsoumakas, Grigorios/0000-0002-7879-669X; Vlahavas, Ioannis/0000-0003-3477-8825",,,,,,,,,,,,,1532-4435,,,,,JUL,2011,12,,,,,,2411,2414,,,,,,,,,,,,,,,,WOS:000293757900011,0
J,"Ueno, T; Maeda, S; Kawanabe, M; Ishii, S",,,,"Ueno, Tsuyoshi; Maeda, Shin-ichi; Kawanabe, Motoaki; Ishii, Shin",,,Generalized TD Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Since the invention of temporal difference (TD) learning (Sutton, 1988), many new algorithms for model-free policy evaluation have been proposed. Although they have brought much progress in practical applications of reinforcement learning (RL), there still remain fundamental problems concerning statistical properties of the value function estimation. To solve these problems, we introduce a new framework, semiparametric statistical inference, to model-free policy evaluation. This framework generalizes TD learning and its extensions, and allows us to investigate statistical properties of both of batch and online learning procedures for the value function estimation in a unified way in terms of estimating functions. Furthermore, based on this framework, we derive an optimal estimating function with the minimum asymptotic variance and propose batch and online learning algorithms which achieve the optimality.",,,,,,"Ishii, Shin/0000-0001-9385-8230",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1977,2020,,,,,,,,,,,,,,,,WOS:000293757200007,0
J,"van der Vaart, A; van Zanten, H",,,,"van der Vaart, Aad; van Zanten, Harry",,,Information Rates of Nonparametric Gaussian Process Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the quality of learning a response function by a nonparametric Bayesian approach using a Gaussian process (GP) prior on the response function. We upper bound the quadratic risk of the learning procedure, which in turn is an upper bound on the Kullback-Leibler information between the predictive and true data distribution. The upper bound is expressed in small ball probabilities and concentration measures of the GP prior. We illustrate the computation of the upper bound for the Matern and squared exponential kernels. For these priors the risk, and hence the information criterion, tends to zero for all continuous response functions. However, the rate at which this happens depends on the combination of true response function and Gaussian prior, and is expressible in a certain concentration function. In particular, the results show that for good performance, the regularity of the GP prior should match the regularity of the unknown response function.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,2095,2119,,,,,,,,,,,,,,,,WOS:000293757200011,0
J,"Steinwart, I; Hush, D; Scovel, C",,,,"Steinwart, Ingo; Hush, Don; Scovel, Clint",,,Training SVMs Without Offset,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop, analyze, and test a training algorithm for support vector machine classifiers without offset. Key features of this algorithm are a new, statistically motivated stopping criterion, new warm start options, and a set of inexpensive working set selection strategies that significantly reduce the number of iterations. For these working set strategies, we establish convergence rates that, not surprisingly, coincide with the best known rates for SVMs with offset. We further conduct various experiments that investigate both the run time behavior and the performed iterations of the new training algorithm. It turns out, that the new algorithm needs significantly less iterations and also runs substantially faster than standard training algorithms for SVMs with offset.",,,,,,"Steinwart, Ingo/0000-0002-4436-7109",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,141,202,,,,,,,,,,,,,,,,WOS:000287938500006,0
J,"Shivaswamy, PK; Jebara, T",,,,"Shivaswamy, Pannagadatta K.; Jebara, Tony",,,Maximum Relative Margin and Data-Dependent Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Leading classification methods such as support vector machines (SVMs) and their counterparts achieve strong generalization performance by maximizing the margin of separation between data classes. While the maximum margin approach has achieved promising performance, this article identifies its sensitivity to affine transformations of the data and to directions with large data spread. Maximum margin solutions may be misled by the spread of data and preferentially separate classes along large spread directions. This article corrects these weaknesses by measuring margin not in the absolute sense but rather only relative to the spread of data in any projection direction. Maximum relative margin corresponds to a data-dependent regularization on the classification function while maximum absolute margin corresponds to an l(2) norm constraint on the classification function. Interestingly, the proposed improvements only require simple extensions to existing maximum margin formulations and preserve the computational efficiency of SVMs. Through the maximization of relative margin, surprising performance gains are achieved on real-world problems such as digit, text classification and on several other benchmark data sets. In addition, risk bounds are derived for the new formulation based on Rademacher averages.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,747,788,,,,,,,,,,,,,,,,WOS:000277186500013,0
J,"Varshney, KR; Willsky, AS",,,,"Varshney, Kush R.; Willsky, Alan S.",,,Classification Using Geometric Level Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A variational level set method is developed for the supervised classification problem. Nonlinear classifier decision boundaries are obtained by minimizing an energy functional that is composed of an empirical risk term with a margin-based loss and a geometric regularization term new to machine learning: the surface area of the decision boundary. This geometric level set classifier is analyzed in terms of consistency and complexity through the calculation of its epsilon-entropy. For multicategory classification, an efficient scheme is developed using a logarithmic number of decision functions in the number of classes rather than the typical linear number of decision functions. Geometric level set classification yields performance results on benchmark data sets that are competitive with well-established methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,491,516,,,,,,,,,,,,,,,,WOS:000277186500002,0
J,"Hellerstein, L; Rosell, B; Bach, E; Ray, S; Page, D",,,,"Hellerstein, Lisa; Rosell, Bernard; Bach, Eric; Ray, Soumya; Page, David",,,Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for finding relevant variables of correlation immune functions in the PDC model.",,,,,,"Ray, Soumya/0000-0002-6673-6796",,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2375,2411,,,,,,,,,,,,,,,,WOS:000272346400008,0
J,"Esposito, R; Radicioni, DP",,,,"Esposito, Roberto; Radicioni, Daniele P.",,,CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity. 1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always finds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus confirming it as a sound replacement.",,,,,"ESPOSITO, Roberto/AHI-8229-2022","ESPOSITO, Roberto/0000-0001-5366-292X; RADICIONI, DANIELE PAOLO/0000-0003-0443-7720",,,,,,,,,,,,,1532-4435,,,,,AUG,2009,10,,,,,,1851,1880,,,,,,,,,,,,,,,,WOS:000270825200004,0
J,"Dasgupta, S; Kalai, AT; Monteleoni, C",,,,"Dasgupta, Sanjoy; Kalai, Adam Tauman; Monteleoni, Claire",,,Analysis of Perceptron-Based Active Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We start by showing that in an active learning setting, the Perceptron algorithm needs Omega(1/epsilon(2)) labels to learn linear separators within generalization error epsilon. We then present a simple active learning algorithm for this problem, which combines a modification of the Perceptron update with an adaptive filtering rule for deciding which points to query. For data distributed uniformly over the unit sphere, we show that our algorithm reaches generalization error e after asking for just (O) over tilde (d log1/epsilon) labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm.",,,,,,"Monteleoni, Claire/0000-0002-9488-0517; Kalai, Adam Tauman/0000-0002-4559-8574",,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,281,299,,,,,,,,,,,,,,,,WOS:000270824200007,0
J,"Shah, A; Woolf, P",,,,"Shah, Abhik; Woolf, Peter",,,Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we introduce PEBL, a Python library and application for learning Bayesian network structure from data and prior knowledge that provides features unmatched by alternative software packages: the ability to use interventional data, flexible specification of structural priors, modeling with hidden variables and exploitation of parallel processing. PEBL is released under the MIT open-source license, can be installed from the Python Package Index and is available at http://pebl-project.googlecode.com.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,159,162,,,,,,,,,,,20161541,,,,,WOS:000270824200002,0
J,"Zhang, JJ",,,,"Zhang, Jiji",,,Causal reasoning with ancestral graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams ( rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The first result extends Pearl (1995)'s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl's calculus-the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the first result. The second result also improves the earlier, similar results due to Spirtes et al. (1993).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1437,1474,,,,,,,,,,,,,,,,WOS:000258646800006,0
J,"Henrich, FF; Obermayer, K",,,,"Henrich, Falk-Florian; Obermayer, Klaus",,,Active learning by spherical subdivision,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce a computationally feasible, constructive active learning method for binary classification. The learning algorithm is initially formulated for separable classification problems, for a hyperspherical data space with constant data density, and for great spheres as classifiers. In order to reduce computational complexity the version space is restricted to spherical simplices and learning procedes by subdividing the edges of maximal length. We show that this procedure optimally reduces a tight upper bound on the generalization error. The method is then extended to other separable classification problems using products of spheres as data spaces and isometries induced by charts of the sphere. An upper bound is provided for the probability of disagreement between classifiers (hence the generalization error) for non-constant data densities on the sphere. The emphasis of this work lies on providing mathematically exact performance estimates for active learning strategies.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2008,9,,,,,,105,130,,,,,,,,,,,,,,,,WOS:000256641400005,0
J,"Mahadevan, S; Maggioni, M",,,,"Mahadevan, Sridhar; Maggioni, Mauro",,,Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A specific instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a final parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A specific instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystrom extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are briefly summarized at the end.",,,,,,"Maggioni, Mauro/0000-0003-3258-9297",,,,,,,,,,,,,1532-4435,,,,,OCT,2007,8,,,,,,2169,2231,,,,,,,,,,,,,,,,WOS:000252744800001,0
J,"Reisert, M; Burkhardt, H",,,,"Reisert, Marco; Burkhardt, Hans",,,Learning equivariant functions with matrix valued kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of ( ir) reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant filter for 2D-images and propose an invariant object detector based on the generalized Hough transform.",,,,,"Burkhardt, Hans/M-5895-2019",,,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,385,408,,,,,,,,,,,,,,,,WOS:000247002700001,0
J,"Bach, FR; Heckerman, D; Horvitz, E",,,,"Bach, Francis R.; Heckerman, David; Horvitz, Eric",,,Considering cost asymmetry in learning classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Receiver Operating Characteristic (ROC) curves are a standard way to display the performance of a set of binary classifiers for all feasible ratios of the costs associated with false positives and false negatives. For linear classifiers, the set of classifiers is typically obtained by training once, holding constant the estimated slope and then varying the intercept to obtain a parameterized set of classifiers whose performances can be plotted in the ROC plane. We consider the alternative of varying the asymmetry of the cost function used for training. We show that the ROC curve obtained by varying both the intercept and the asymmetry, and hence the slope, always outperforms the ROC curve obtained by varying only the intercept. In addition, we present a path-following algorithm for the support vector machine (SVM) that can compute efficiently the entire ROC curve, and that has the same computational complexity as training a single classifier. Finally, we provide a theoretical analysis of the relationship between the asymmetric cost model assumed when training a classifier and the cost model assumed in applying the classifier. In particular, we show that the mismatch between the step function used for testing and its convex upper bounds, usually used for training, leads to a provable and quantifiable difference around extreme asymmetries.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2006,7,,,,,,1713,1741,,,,,,,,,,,,,,,,WOS:000245389200003,0
J,"Bergkvist, A; Damaschke, P; Luthi, M",,,,"Bergkvist, Anders; Damaschke, Peter; Luethi, Marcel",,,Linear programs for hypotheses selection in probabilistic inference models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider an optimization problem in probabilistic inference: Given n hypotheses H-j, m possible observations O-k, their conditional probabilities p(kj), and a particular O-k, select a possibly small subset of hypotheses excluding the true target only with some error probability epsilon. After specifying the optimization goal we show that this problem can be solved through a linear program in mn variables that indicate the probabilities to discard a hypothesis given an observation. Moreover, we can compute optimal strategies where only O(m+n) of these variables get fractional values. The manageable size of the linear programs and the mostly deterministic shape of optimal strategies makes the method practicable. We interpret the dual variables as worst-case distributions of hypotheses, and we point out some counterintuitive nonmonotonic behaviour of the variables as a function of the error bound epsilon. One of the open problems is the existence of a purely combinatorial algorithm that is faster than generic linear programming.",,,,,,"Luethi, Marcel/0000-0002-9686-2195",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1339,1355,,,,,,,,,,,,,,,,WOS:000245388800008,0
J,"Elisseeff, A; Evgeniou, T; Pontil, M",,,,"Elisseeff, A; Evgeniou, T; Pontil, M",,,Stability of randomized learning algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We extend existing theory on stability, namely how much changes in the training data influence the estimated models, and generalization performance of deterministic learning algorithms to the case of randomized algorithms. We give formal definitions of stability for randomized algorithms and prove non-asymptotic bounds on the difference between the empirical and expected error as well as the leave-one-out and expected error of such algorithms that depend on their random stability. The setup we develop for this purpose can be also used for generally studying randomized learning algorithms. We then use these general results to study the effects of bagging on the stability of a learning method and to prove non-asymptotic bounds on the predictive performance of bagging which have not been possible to prove with the existing theory of stability for deterministic learning algorithms.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,55,79,,,,,,,,,,,,,,,,WOS:000236328800003,0
J,"Schmitt, M",,,,"Schmitt, M",,,Some dichotomy theorems for neural learning problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The computational complexity of learning from binary examples is investigated for linear threshold neurons. We introduce combinatorial measures that create classes of infinitely many learning problems with sample restrictions. We analyze how the complexity of these problems depends on the values for the measures. The results are established as dichotomy theorems showing that each problem is either NP-complete or solvable in polynomial time. In particular, we consider consistency and maximum consistency problems for neurons with binary weights, and maximum consistency problems for neurons with arbitrary weights. We determine for each problem class the dividing line between the NP-complete and polynomial-time solvable problems. Moreover, all efficiently solvable problems are shown to have constructive algorithms that require no more than linear time on a random access machine model. Similar dichotomies are exhibited for neurons with bounded threshold. The results demonstrate on the one hand that the consideration of sample constraints can lead to the discovery of new efficient algorithms for non-trivial learning problems. On the other hand, hard learning problems may remain intractable even for severely restricted samples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2004,5,,,,,,891,912,,,,,,,,,,,,,,,,WOS:000236328000002,0
J,"Even-Dar, E; Mansour, Y",,,,"Even-Dar, E; Mansour, Y",,,Learning rates for Q-learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we derive convergence rates for Q-learning. We show an interesting relationship between the convergence rate and the learning rate used in Q-learning. For a polynomial learning rate, one which is 1/t(omega) at time t where omega epsilon ( 1/2, 1), we show that the convergence rate is polynomial in 1/(1-gamma), where g is the discount factor. In contrast we show that for a linear learning rate, one which is 1/t at time t, the convergence rate has an exponential dependence on 1/(1-gamma). In addition we show a simple example that proves this exponential behavior is inherent for linear learning rates.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2003,5,,,,,,1,25,,,,,,,,,,,,,,,,WOS:000236283600001,0
J,"Fairbank, M; Samothrakis, S; Citi, L",,,,"Fairbank, Michael; Samothrakis, Spyridon; Citi, Luca",,,Deep Learning in Target Space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep learning uses neural networks which are parameterised by their weights. The neural networks are usually trained by tuning the weights to directly minimise a given loss function. In this paper we propose to re-parameterise the weights into targets for the firing strengths of the individual nodes in the network. Given a set of targets, it is possible to calculate the weights which make the firing strengths best meet those targets. It is argued that using targets for training addresses the problem of exploding gradients, by a process which we call cascade untangling, and makes the loss-function surface smoother to traverse, and so leads to easier, faster training, and also potentially better generalisation, of the neural network. It also allows for easier learning of deeper and recurrent network structures. The necessary conversion of targets to weights comes at an extra computational expense, which is in many cases manageable. Learning in target space can be combined with existing neural-network optimisers, for extra gain. Experimental results show the speed of using target space, and examples of improved generalisation, for fully-connected networks and convolutional networks, and the ability to recall and process long time sequences and perform natural language processing with recurrent networks.",,,,,"Citi, Luca/AAE-3192-2022","Citi, Luca/0000-0001-8702-5654",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000766891800001,0
J,"Ghosh, T; Kirby, M",,,,"Ghosh, Tomojit; Kirby, Michael",,,Supervised Dimensionality Reduction and Visualization using Centroid-Encoder,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a new tool for visualizing complex, and potentially large and high-dimensional, data sets called Centroid-Enco der (CE). The architecture of the Centroid-Enco der is similar to the autoencoder neural network but it has a modified target, i.e., the class centroid in the ambient space. As such, CE incorporates label information and performs a supervised data visualization. The training of CE is done in the usual way with a training set whose parameters are tuned using a validation set. The evaluation of the resulting CE visualization is performed on a sequestered test set where the generalization of the model is assessed both visually and quantitatively. We present a detailed comparative analysis of the method using a wide variety of data sets and techniques, both supervised and Isomap, Parametric Embedding, supervised Neighbor Retrieval Visualizer, and Multiple Relational Embedding. An analysis of variance using PCA demonstrates that a non-linear preprocessing by the CE transformation of the data captures more variance than PCA by dimension.",,,,,"Ghosh, Tomojit/GPW-8476-2022",,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,34,,,,,,,,,,,,,,,,WOS:000766889600001,0
J,"Zheng, XB; Zhou, BX; Wang, YG; Zhuang, XS",,,,"Zheng, Xuebin; Zhou, Bingxin; Wang, Yu Guang; Zhuang, Xiaosheng",,,Decimated Framelet System on Graphs and Fast C-Framelet Transforms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Graph representation learning has many real-world applications, from self-driving LiDAR, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. This paper proposes a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored on a subgraph. Based on this, we establish decimated C- framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated C-framelet transforms, or FCT, that has linear computational complexity 0 (N) for a graph of size N. The effectiveness for constructing the decimated framelet system and the FCT is demonstrated by a simulated example of random graphs and real-world applications, including multiresolution analysis for traffic network and representation learning of graph neural networks for graph classification tasks.",,,,,"ZHUANG, Xiaosheng/A-2991-2013","ZHUANG, Xiaosheng/0000-0001-7238-0143",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,68,,,,,,,,,,,,,,,,WOS:000752276900001,0
J,"Bayraktar, E; Ekren, I; Zhang, X",,,,"Bayraktar, Erhan; Ekren, Ibrahim; Zhang, Xin",,,Prediction against a limited adversary,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of prediction with expert advice with adversarial corruption where the adversary can at most corrupt one expert. Using tools from viscosity theory, we characterize the long-time behavior of the value function of the game between the forecaster and the adversary. We provide lower and upper bounds for the growth rate of regret without relying on a comparison result. We show that depending on the description of regret, the limiting behavior of the game can significantly differ.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656375500001,0
J,"Blanchard, G; Deshmukh, AA; Dogan, U; Lee, G; Scott, C",,,,"Blanchard, Gilles; Deshmukh, Aniket Anand; Dogan, Urun; Lee, Gyemin; Scott, Clayton",,,Domain Generalization by Marginal Transfer Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced (Blanchard et al., 2011). We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500002,0
J,"Bonnevie, R; Schmidt, MN",,,,"Bonnevie, Rasmus; Schmidt, Mikkel N.",,,Matrix Product States for Inference in Discrete Probabilistic Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When faced with problems involving inference in discrete domains, solutions often involve appeals to conditional independence structure or mean-field approximations. We argue that this is insufficient for a number of interesting Bayesian problems, including mixture assignment posteriors and probabilistic relational models (e.g. the stochastic block model). These posteriors exhibit no conditional independence structure, precluding the use of graphical model methods, yet exhibit dependency between every single element of the posterior, making mean-field methods a poor fit. We propose using an expressive yet tractable approximation inspired by tensor factorization methods, alternately known as the tensor train or the matrix product state, and which can be construed of as a direct extension of the mean-field approximation to higher-order dependencies. We give a comprehensive introduction to the application of matrix product state in probabilistic inference, and illustrate how to efficiently perform marginalization, conditioning, sampling, normalization, some expectations, and approximate variational inference in our proposed model.",,,,,,"Schmidt, Mikkel N./0000-0001-6927-8869",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,187,,,,,,,,,,,,,,,WOS:000706446500001,0
J,"Chretien, S; Cucuringu, M; Lecue, G; Neirac, L",,,,"Chretien, Stephane; Cucuringu, Mihai; Lecue, Guillaume; Neirac, Lucie",,,Learning with semi-definite programming: statistical bounds based on fixed point analysis and excess risk curvature,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many statistical learning problems have recently been shown to be amenable to Semi Definite Programming (SDP), with community detection and clustering in Gaussian mixture models as the most striking instances Javanmard et al. (2016). Given the growing range of applications of SDP-based techniques to machine learning problems, and the rapid progress in the design of efficient algorithms for solving SDPs, an intriguing question is to understand how the recent advances from empirical process theory and Statistical Learning Theory can be leveraged for providing a precise statistical analysis of SDP estimators. In the present paper, we borrow cutting edge techniques and concepts from the Learning Theory literature, such as fixed point equations and excess risk curvature arguments, which yield general estimation and prediction results for a wide class of SDP estimators. From this perspective, we revisit some classical results in community detection from Gue acute accent don and Vershynin (2016) and Fei and Chen (2019b), and we obtain statistical guarantees for SDP estimators used in signed clustering, angular group synchronization (for both multiplicative and additive models) and MAX-CUT. Our theoretical findings are complemented by numerical experiments for each of the three problems considered, showcasing the competitiveness of the SDP estimators.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706450600001,0
J,"Cockayne, J; Ipsen, ICF; Oates, CJ; Reid, TW",,,,"Cockayne, Jon; Ipsen, Ilse C. F.; Oates, Chris J.; Reid, Tim W.",,,Probabilistic Iterative Methods for Linear Systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a probabilistic perspective on iterative methods for approximating the solution x is an element of Rd of a nonsingular linear system Ax = b. Classically, an iterative method produces a sequence xm of approximations that converge to x in Rd. Our approach, instead, lifts a standard iterative method to act on the set of probability distributions, P(Rd), outputting a sequence of probability distributions mu m is an element of P(Rd). The output of a probabilistic iterative method can provide both a best guess for x , for example by taking the mean of mu m , and also probabilistic uncertainty quantification for the value of x when it has not been exactly determined. A comprehensive theoretical treatment is presented in the case of a stationary linear iterative method, where we characterise both the rate of contraction of mu m to an atomic measure on x and the nature of the uncertainty quantification being provided. We conclude with an empirical illustration that highlights the potential for probabilistic iterative methods to provide insight into solution uncertainty.",,,,,,"Reid, Tim/0000-0003-0087-8481",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000706887700001,0
J,"Fei, Z; Li, Y",,,,"Fei, Zhe; Li, Yi",,,Estimation and Inference for High Dimensional Generalized Linear Models: A Splitting and Smoothing Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The focus of modern biomedical studies has gradually shifted to explanation and estimation of joint effects of high dimensional predictors on disease risks. Quantifying uncertainty in these estimates may provide valuable insight into prevention strategies or treatment decisions for both patients and physicians. High dimensional inference, including confidence intervals and hypothesis testing, has sparked much interest. While much work has been done in the linear regression setting, there is lack of literature on inference for high dimensional generalized linear models. We propose a novel and computationally feasible method, which accommodates a variety of outcome types, including normal, binomial, and Poisson data. We use a splitting and smoothing approach, which splits samples into two parts, performs variable selection using one part and conducts partial regression with the other part. Averaging the estimates over multiple random splits, we obtain the smoothed estimates, which are numerically stable. We show that the estimates are consistent, asymptotically normal, and construct confidence intervals with proper coverage probabilities for all predictors. We examine the finite sample performance of our method by comparing it with the existing methods and applying it to analyze a lung cancer cohort study.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,34531706,,,,,WOS:000656357500001,0
J,"Gupta, C; Balakrishnan, S; Ramdas, A",,,,"Gupta, Chirag; Balakrishnan, Sivaraman; Ramdas, Aaditya",,,Path Length Bounds for Gradient Descent and Flow,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive bounds on the path length zeta of gradient descent (GD) and gradient flow (GF) curves for various classes of smooth convex and nonconvex functions. Among other results, we prove that: (a) if the iterates are linearly convergent with factor (1 - c), then zeta is at most O(1/c); (b) under the Polyak-Kurdyka-Lojasiewicz (PKL) condition, zeta is at most O(root kappa), where kappa is the condition number, and at least (Omega) over tilde(root d boolean AND kappa(1/4)); (c) for quadratics, zeta is Theta(min {root d, root log kappa}) and in some cases can be independent of kappa; (d) assuming just convexity, zeta can be at most 2(4d log d); (e) for separable quasiconvex functions, zeta is Theta(root d). Thus, we advance current understanding of the properties of GD and GF curves beyond rates of convergence. We expect our techniques to facilitate future studies for other algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000656369800001,0
J,"Kim, J; Shin, J; Yang, I",,,,"Kim, Jeongho; Shin, Jaeuk; Yang, Insoon",,,Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose Q-learning algorithms for continuous-time deterministic optimal control problems with Lipschitz continuous controls. A new class of Hamilton-Jacobi- Bellman (HJB) equations is derived from applying the dynamic programming principle to continuous-time Q-functions. Our method is based on a novel semi-discrete version of the HJB equation, which is proposed to design a Q-learning algorithm that uses data collected in discrete time without discretizing or approximating the system dynamics. We identify the conditions under which the Q-function estimated by this algorithm converges to the optimal Q-function. For practical implementation, we propose the Hamilton-Jacobi DQN, which extends the idea of deep Q-networks (DQN) to our continuous control setting. This approach does not require actor networks or numerical solutions to optimization problems for greedy actions since the HJB equation provides a simple characterization of optimal controls via ordinary differential equations. We empirically demonstrate the performance of our method through benchmark tasks and high-dimensional linear-quadratic problems.",,,,,"Kim, Jeongho/GVS-6222-2022","Kim, Jeongho/0000-0002-5220-3139",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706868500001,0
J,"Wang, BX; Xu, Z; Zhang, H; Qiu, KX; Zhang, DY; Sun, CJ",,,,"Wang, Baoxun; Xu, Zhen; Zhang, Huan; Qiu, Kexin; Zhang, Deyuan; Sun, Chengjie",,,LocalGAN: Modeling Local Distributions for Adversarial Response Generation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a new methodology for modeling the local semantic distribution of responses to a given query in the human-conversation corpus, and on this basis, explores a specified adversarial learning mechanism for training Neural Response Generation (NRG) models to build conversational agents. Our investigation begins with the thorough discussions upon the objective function of general Generative Adversarial Nets (GAN) architectures, and the training instability problem is proved to be highly relative with the special local distributions of conversational corpora. Consequently, an energy function is employed to estimate the status of a local area restricted by the query and its responses in the semantic space, and the mathematical approximation of this energy-based distribution is finally found. Building on this foundation, a local distribution oriented objective is proposed and combined with the original objective, working as a hybrid loss for the adversarial training of response generation models, named as LocalGAN. Our experimental results demonstrate that the reasonable local distribution modeling of the query-response corpus is of great importance to adversarial NRG, and our proposed LocalGAN is promising for improving both the training stability and the quality of generated results.",,,,,"Sun, Chengjie/GQY-9276-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,101,,,,,,,,,,,,,,,WOS:000663154500001,0
J,"Bhattacharjee, M; Banerjee, M; Michailidis, G",,,,"Bhattacharjee, Monika; Banerjee, Moulinath; Michailidis, George",,,Change Point Estimation in a Dynamic Stochastic Block Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of estimating the location of a single change point in a network generated by a dynamic stochastic block model mechanism. This model produces community structure in the network that exhibits change at a single time epoch. We propose two methods of estimating the change point, together with the model parameters, before and after its occurrence. The first employs a least-squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises the following two steps: in the first one, a least-squares function is used and evaluated at each time point, but ignoring the community structure and only considering a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. The first method, since it requires knowledge of the community structure and hence clustering at every point in time, is significantly more computationally expensive than the second one. On the other hand, it requires a significantly less stringent identifiability condition for consistent estimation of the change point and the model parameters than the second method; however, it also requires a condition on the misclassification rate of misallocating network nodes to their respective communities that may fail to hold in many realistic settings. Despite the apparent stringency of the identifiability condition for the second method, we show that networks generated by a stochastic block mechanism exhibiting a change in their structure can easily satisfy this condition under a multitude of scenarios, including merging/splitting communities, nodes joining another community, etc. Further, for both methods under their respective identifiability and certain additional regularity conditions, we establish rates of convergence and derive the asymptotic distributions of the change point estimators. The results are illustrated on synthetic data. In summary, this work provides an in-depth investigation of the novel problem of change point analysis for networks generated by stochastic block models, identifies key conditions for the consistent estimation of the change point, and proposes a computationally fast algorithm that solves the problem in many settings that occur in applications. Finally, it discusses challenges posed by employing clustering algorithms in this problem, that require additional investigation for their full resolution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,107,,,,,,,,,,,,,,,WOS:000546627600001,0
J,"Cai, TT; Liang, TY; Rakhlin, A",,,,"Cai, T. Tony; Liang, Tengyuan; Rakhlin, Alexander",,,Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the misclassification error for community detection in general heterogeneous stochastic block models (SBM) with noisy or partial label information. We establish a connection between the misclassification rate and the notion of minimum energy on the local neighborhood of the SBM. We develop an optimally weighted message passing algorithm to reconstruct labels for SBM based on the minimum energy flow and the eigenvectors of a certain Markov transition matrix. The general SBM considered in this paper allows for unequal-size communities, degree heterogeneity, and different connection probabilities among blocks. We focus on how to optimally weigh the message passing to improve misclassification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300011,0
J,"Chi, EC; Gaines, BR; Sun, WW; Zhou, H; Yang, J",,,,"Chi, Eric C.; Gaines, Brian R.; Sun, Will Wei; Zhou, Hua; Yang, Jian",,,Provable Convex Co-clustering of Tensors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Cluster analysis is a fundamental tool for pattern discovery of complex heterogeneous data. Prevalent clustering methods mainly focus on vector or matrix-variate data and are not applicable to general-order tensors, which arise frequently in modern scientific and business applications. Moreover, there is a gap between statistical guarantees and computational efficiency for existing tensor clustering solutions due to the nature of their non-convex formulations. In this work, we bridge this gap by developing a provable convex formulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator enjoys stability guarantees and its computational and storage costs are polynomial in the size of the data. We further establish a non-asymptotic error bound for the CoCo estimator, which reveals a surprising blessing of dimensionality phenomenon that does not exist in vector or matrix-variate cluster analysis. Our theoretical findings are supported by extensive simulated studies. Finally, we apply the CoCo estimator to the cluster analysis of advertisement click tensor data from a major online company. Our clustering results provide meaningful business insights to improve advertising effectiveness.",,,,,,"Chi, Eric/0000-0003-4647-0895; Zhou, Hua/0000-0003-1320-7118",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,214,,,,,,,,,,33312074,,,,,WOS:000590024800001,0
J,"Hofmeyr, DP",,,,"Hofmeyr, David P.",,,Connecting Spectral Clustering to Maximum Margins and Level Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the connections between spectral clustering and the problems of maximum margin clustering, and estimation of the components of level sets of a density function. Specifically, we obtain bounds on the eigenvectors of graph Laplacian matrices in terms of the between cluster separation, and within cluster connectivity. These bounds ensure that the spectral clustering solution converges to the maximum margin clustering solution as the scaling parameter is reduced towards zero. The sensitivity of maximum margin clustering solutions to outlying points is well known, but can be mitigated by first removing such outliers, and applying maximum margin clustering to the remaining points. If outliers are identified using an estimate of the underlying probability density, then the remaining points may be seen as an estimate of a level set of this density function. We show that such an approach can be used to consistently estimate the components of the level sets of a density function under very mild assumptions.",,,,,"Hofmeyr, David/AAC-4042-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300018,0
J,"Lee, KY; Liu, TQ; Li, B; Zhao, HY",,,,"Lee, Kuang-Yao; Liu, Tianqi; Li, Bing; Zhao, Hongyu",,,Learning Causal Networks via Additive Faithfulness,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce a statistical model, called additively faithful directed acyclic graph (AFDAG), for causal learning from observational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical relation that shares many similarities with conditional independence but without resorting to multi-dimensional kernels. This distinct feature strikes a balance between a parametric model and a fully nonparametric model, which makes the proposed model attractive for handling large networks. We develop an estimator for AFDAG based on a linear operator that characterizes ACI, and establish the consistency and convergence rates of this estimator, as well as the uniform consistency of the estimated DAG. Moreover, we introduce a modified PC-algorithm to implement the estimating procedure efficiently, so that its complexity is determined by the level of sparseness rather than the dimension of the network. Through simulation studies we show that our method outperforms existing methods when commonly assumed conditions such as Gaussian or Gaussian copula distributions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through an application to a proteomics data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000004,0
J,"Liu, Y; De Brabanter, K",,,,"Liu, Yu; De Brabanter, Kris",,,Smoothed Nonparametric Derivative Estimation using Weighted Difference Quotients,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Derivatives play an important role in bandwidth selection methods (e.g., plug-ins), data analysis and bias-corrected confidence intervals. Therefore, obtaining accurate derivative information is crucial. Although many derivative estimation methods exist, the majority require a fixed design assumption. In this paper, we propose an effective and fully data-driven framework to estimate the first and second order derivative in random design. We establish the asymptotic properties of the proposed derivative estimator, and also propose a fast selection method for the tuning parameters. The performance and exibility of the method is illustrated via an extensive simulation study.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000017,0
J,"Locatello, F; Bauer, S; Lucic, M; Ratsch, G; Gelly, S; Scholkopf, B; Bachem, O",,,,"Locatello, Francesco; Bauer, Stefan; Lucic, Mario; Ratsch, Gunnar; Gelly, Sylvain; Schoelkopf, Bernhard; Bachem, Olivier",,,A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train over 14 000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on eight data sets. We observe that while the different methods successfully enforce properties encouraged by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, different evaluation metrics do not always agree on what should be considered disentangled and exhibit systematic differences in the estimation. Finally, increased disentanglement does not seem to necessarily lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.",,,,,"Locatello, Francesco/GQY-6025-2022","Locatello, Francesco/0000-0002-4850-0683",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,209,,,,,,,,,,,,,,,WOS:000590011800001,0
J,"Lu, WQ; Zhu, ZY; Lian, H",,,,"Lu, Wenqi; Zhu, Zhongyi; Lian, Heng",,,High-dimensional Quantile Tensor Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Quantile regression is an indispensable tool for statistical learning. Traditional quantile regression methods consider vector-valued covariates and estimate the corresponding coefficient vector. Many modern applications involve data with a tensor structure. In this paper, we propose a quantile regression model which takes tensors as covariates, and present an estimation approach based on Tucker decomposition. It effectively reduces the number of parameters, leading to efficient estimation and feasible computation. We also use a sparse Tucker decomposition, which is a popular approach in the literature, to further reduce the number of parameters when the dimension of the tensor is large. We propose an alternating update algorithm combined with alternating direction method of multipliers (ADMM). The asymptotic properties of the estimators are established under suitable conditions. The numerical performances are demonstrated via simulations and an application to a crowd density estimation problem.",,,,,,"LU, Wenqi/0000-0001-8719-7843",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,250,,,,,,,,,,,,,,,WOS:000608919700001,0
J,"Suarez, JL; Garcia, S; Herrera, F",,,,"Luis Suarez, Juan; Garcia, Salvador; Herrera, Francisco",,,pyDML: A Python Library for Distance Metric Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"pyDML is an open-source python library that provides a wide range of distance metric learning algorithms. Distance metric learning can be useful to improve similarity learning algorithms, such as the nearest neighbors classifier, and also has other applications, like dimensionality reduction. The pyDML package currently provides more than 20 algorithms, which can be categorized, according to their purpose, in: dimensionality reduction algorithms, algorithms to improve nearest neighbors or nearest centroids classifiers, information theory based algorithms or kernel based algorithms, among others. In addition, the library also provides some utilities for the visualization of classifier regions, parameter tuning and a stats website with the performance of the implemented algorithms. The package relies on the scipy ecosystem, it is fully compatible with scikit-learn, and is distributed under GPLv3 license. Source code and documentation can be found at https://github.com/j1suarezdiaz/pyDML.",,,,,"Garcia, Salvador/N-3624-2013","Garcia, Salvador/0000-0003-4494-7565",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,96,,,,,,,,,,,,,,,WOS:000545028700001,0
J,"Massias, M; Vaiter, S; Gramfort, A; Salmon, J",,,,"Massias, Mathurin; Vaiter, Samuel; Gramfort, Alexandre; Salmon, Joseph",,,Dual Extrapolation for Sparse Generalized Linear Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Generalized Linear Models (GLM) form a wide class of regression and classification models, where prediction is a function of a linear combination of the input variables. For statistical inference in high dimension, sparsity inducing regularizations have proven to be useful while offering statistical guarantees. However, solving the resulting optimization problems can be challenging: even for popular iterative algorithms such as coordinate descent, one needs to loop over a large number of variables. To mitigate this, techniques known as screening rules and working sets diminish the size of the optimization problem at hand, either by progressively removing variables, or by solving a growing sequence of smaller problems. For both techniques, significant variables are identified thanks to convex duality arguments. In this paper, we show that the dual iterates of a GLM exhibit a Vector AutoRegressive (VAR) behavior after sign identification, when the primal problem is solved with proximal gradient descent or cyclic coordinate descent. Exploiting this regularity, one can construct dual points that offer tighter certificates of optimality, enhancing the performance of screening rules and working set algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,1,33,234,,,,,,,,,,,,,,,WOS:000608906400001,0
J,"Ricatte, T; Gilleron, R; Tommasi, M",,,,"Ricatte, Thomas; Gilleron, Remi; Tommasi, Marc",,,Skill Rating for Multiplayer Games Introducing Hypernode Graphs and their Spectral Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem arg min(s) s(T) Delta s where Delta is a positive semidefinite matrix and s a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to ELO DUELLING (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006). As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hypernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as arg min(s) s(T) Delta s where Delta is the Laplacian of a hypernode graph constructed from a set of games. From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000001,0
J,"Khamaru, K; Wainwright, MJ",,,,"Khamaru, Koulik; Wainwright, Martin J.",,,Convergence guarantees for a class of non-convex and non-smooth optimization problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of finding critical points of functions that are non-convex and non-smooth. Studying a fairly broad class of such problems, we analyze the behavior of three gradient-based methods (gradient descent, proximal update, and Frank-Wolfe update). For each of these methods, we establish rates of convergence for general problems, and also prove faster rates for continuous sub-analytic functions. We also show that our algorithms can escape strict saddle points for a class of non-smooth functions, thereby generalizing known results for smooth functions. Our analysis leads to a simplification of the popular CCCP algorithm, used for optimizing functions that can be written as a difference of two convex functions. Our simplified algorithm retains all the convergence properties of CCCP, along with a significantly lower cost per iteration. We illustrate our methods and theory via applications to the problems of best subset selection, robust estimation, mixture density estimation, and shape-from-shading reconstruction.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,154,,,,,,,,,,,,,,,WOS:000491132200018,0
J,"Rebeschini, P; Tatikonda, S",,,,"Rebeschini, Patrick; Tatikonda, Sekhar",,,A New Approach to Laplacian Solvers and Flow Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates the behavior of the Min-Sum message passing scheme to solve systems of linear equations in the Laplacian matrices of graphs and to compute electric flows. Voltage and flow problems involve the minimization of quadratic functions and are fundamental primitives that arise in several domains. Algorithms that have been proposed are typically centralized and involve multiple graph-theoretic constructions or sampling mechanisms that make them difficult to implement and analyze. On the other hand, message passing routines are distributed, simple, and easy to implement. In this paper we establish a framework to analyze Min-Sum to solve voltage and flow problems. We characterize the error committed by the algorithm on general weighted graphs in terms of hitting times of random walks defined on the computation trees that support the operations of the algorithms with time. For d-regular graphs with equal weights, we show that the convergence of the algorithms is controlled by the total variation distance between the distributions of non-backtracking random walks defined on the original graph that start from neighboring nodes. The framework that we introduce extends the analysis of MinSum to settings where the contraction arguments previously considered in the literature (based on the assumption of walk summability or scaled diagonal dominance) can not be used, possibly in the presence of constraints.",,,,,,"Rebeschini, Patrick/0000-0001-7772-4160",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,36,,,,,,,,,,,,,,,WOS:000463318600001,0
J,"Saremi, S; Hyvarinen, A",,,,"Saremi, Saeed; Hyvarinen, Aapo",,,Neural Empirical Bayes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We unify kernel density estimation and empirical Bayes and address a set of problems in unsupervised machine learning with a geometric interpretation of those methods, rooted in the concentration of measure phenomenon. Kernel density is viewed symbolically as X (sic) Y where the random variable X is smoothed to Y = X + N(0, sigma(2)/(d)), and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as X Y. A learning objective is derived by combining these two, symbolically captured by X (sic) Y. Crucially, instead of using the original nonparametric estimators, we parametrize the energy function with a neural network denoted by phi; at optimality, del phi approximate to -del log f where f is the density of Y. The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic Gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a walk-jump sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for associative memory, called NEBULA, defined a la Hopfield by the gradient flow of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich creative memories as attractors of NEBULA for highly-overlapping spheres.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,181,,,,,,,,,,,,,,,WOS:000506403100021,0
J,"Jiang, BY; Wang, XY; Leng, CL",,,,"Jiang, Binyan; Wang, Xiangyu; Leng, Chenlei",,,A Direct Approach for Sparse Quadratic Discriminant Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Quadratic discriminant analysis (QDA) is a standard tool for classification due to its simplicity and flexibility. Because the number of its parameters scales quadratically with the number of the variables, QDA is not practical, however, when the dimensionality is relatively large. To address this, we propose a novel procedure named DA-QDA for QDA in analyzing high-dimensional data. Formulated in a simple and coherent framework, DA-QDA aims to directly estimate the key quantities in the Bayes discriminant function including quadratic interactions and a linear index of the variables for classification. Under appropriate sparsity assumptions, we establish consistency results for estimating the interactions and the linear index, and further demonstrate that the misclassification rate of our procedure converges to the optimal Bayes risk, even when the dimensionality is exponentially high with respect to the sample size. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed for finding interactions, which is much faster than its competitor in the literature. The promising performance of DA-QDA is illustrated via extensive simulation studies and the analysis of four real datasets.",,,,,,"JIANG, Binyan/0000-0002-1992-4815",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,31,,,,,,,,,,,,,,,WOS:000444402700001,0
J,"Lattimore, T",,,,"Lattimore, Tor",,,Refining the Confidence Level for Optimistic Bandit Strategies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is presented that very nearly matches the finite-time upper bound of the newly proposed strategy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,20,,,,,,,,,,,,,,,WOS:000443228200001,0
J,"Morstatter, F; Liu, H",,,,"Morstatter, Fred; Liu, Huan",,,In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,169,,,,,,,,,,,,,,,WOS:000435441200001,0
J,"Obuchi, T; Kabashima, Y",,,,"Obuchi, Tomoyuki; Kabashima, Yoshiyuki",,,Accelerating Cross-Validation in Multinomial Logistic Regression with l(1)-Regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop an approximate formula for evaluating a cross-validation estimator of predictive likelihood for multinomial logistic regression regularized by an l(1)-norm. This allows us to avoid repeated optimizations required for literally conducting cross-validation; hence, the computational time can be significantly reduced. The formula is derived through a perturbative approach employing the largeness of the data size and the model dimensionality. An extension to the elastic net regularization is also addressed. The usefulness of the approximate formula is demonstrated on simulated data and the ISOLET dataset from the UCI machine learning repository. MATLAB and python codes implementing the approximate formula are distributed in (Obuchi, 2017; Takahashi and Obuchi, 2017).",,,,,"Kabashima, Yoshiyuki/F-4719-2015","Kabashima, Yoshiyuki/0000-0002-2949-7108; Obuchi, Tomoyuki/0000-0003-1216-489X",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,52,,,,,,,,,,,,,,,WOS:000448376400001,0
J,"Chakrabarti, D; Funiak, S; Chang, J; Macskassy, SA",,,,"Chakrabarti, Deepayan; Funiak, Stanislav; Chang, Jonathan; Macskassy, Sofus A.",,,Joint Label Inference in Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called EDGE EXPLAIN, explicitly models these interactions, while still allowing scalable inference under a distributed message -passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EDGEEXPLAIN outperforms label propagation for several label types, with lifts of up to 120% for recall@ 1 and 60% for recall 3.",,,,,,"Chakrabarti, Deepayan/0000-0002-3863-4928",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,39,,,,,,,,,,,,,,,,WOS:000405991500001,0
J,"Denis, C; Hebiri, M",,,,"Denis, Christophe; Hebiri, Mohamed",,,Confidence Sets with Expected Sizes for Multiclass Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multiclass classification problems such as image annotation can involve a large number of classes. In this context, confusion between classes can occur, and single label classification may be misleading. We provide in the present paper a general device that, given an unlabeled dataset and a score function de fined as the minimizer of some empirical and convex risk, outputs a set of class labels, instead of a single one. Interestingly, this procedure does not require that the unlabeled dataset explores the whole classes. Even more, the method is calibrated to control the expected size of the output set while minimizing the classification risk. We show the statistical optimality of the procedure and establish rates of convergence under the Tsybakov margin condition. It turns out that these rates are linear on the number of labels. We apply our methodology to convex aggregation of confidence sets based on the V-fold cross validation principle also known as the superlearning principle (van der Laan et al., 2007). We illustrate the numerical performance of the procedure on real data and demonstrate in particular that with moderate expected size, w.r.t. the number of labels, the procedure provides significant improvement of the classification risk.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,102,,,,,,,,,,,,,,,WOS:000413448800001,0
J,"Flammarion, N; Palaniappan, B; Bach, F",,,,"Flammarion, Nicolas; Palaniappan, Balamurugan; Bach, Francis",,,Robust Discriminative Clustering with Sparse Regularizers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from noise-looking variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem; (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions; (c) we propose a natural extension for the multi-label scenario; (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form d = O (root n) for the affine invariant case and d = O (n) for the sparse case, where n is the number of examples and d the ambient dimension; and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to O (nd(2)), improving on earlier algorithms for discriminative clustering with the square loss, which had quadratic complexity in the number of examples.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,50,80,,,,,,,,,,,,,,,WOS:000412062000001,0
J,"Gottlieb, LA; Kontorovich, A; Nisnevitch, P",,,,"Gottlieb, Lee-Ad; Kontorovich, Aryeh; Nisnevitch, Pinhas",,,Nearly optimal classification for semimetrics,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We de fine the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius R and margin gamma, we show that it can be compressed down to roughly d = ( R/gamma)(dens) points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound.",,,,,"Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019","Kontorovich, Aryeh/0000-0001-8038-8671; ",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,37,,,,,,,,,,,,,,,WOS:000400520500001,0
J,"Guhaniyogi, R; Qamar, S; Dunson, DB",,,,"Guhaniyogi, Rajarshi; Qamar, Shaan; Dunson, David B.",,,Bayesian Tensor Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,31,,,,,,,,,,,,,,,,WOS:000412061900001,0
J,"Bayer, I",,,,"Bayer, Immanuel",,,fastFM: A Library for Factorization Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Factorization Machines (FM) are only used in a narrow range of applications and are not part of the standard toolbox of machine learning models. This is a pity, because even though FMs are recognized as being very successful for recommender system type applications they are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM's for a wide field of applications. This implementation has the potential to improve our understanding of the FM model and drive new development.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,184,,,,,,,,,,,,,,,WOS:000391824900001,0
J,"Fan, JQ; Zhou, WX",,,,"Fan, Jianqing; Zhou, Wen-Xin",,,Guarding against Spurious Discoveries in High Dimensions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many data mining and statistical machine learning algorithms have been developed to select a subset of covariates to associate with a response variable. Spurious discoveries can easily arise in high-dimensional data analysis due to enormous possibilities of such selections. How can we know statistically our discoveries better than those by chance? In this paper, we de fine a measure of goodness of spurious fit, which shows how good a response variable can be fitted by an optimally selected subset of covariates under the null model, and propose a simple and effective LAMM algorithm to compute it. It coincides with the maximum spurious correlation for linear models and can be regarded as a generalized maximum spurious correlation. We derive the asymptotic distribution of such goodness of spurious fit for generalized linear models and L-1 regression. Such an asymptotic distribution depends on the sample size, ambient dimension, the number of variables used in the fit, and the covariance information. It can be consistently estimated by multiplier bootstrapping and used as a benchmark to guard against spurious discoveries. It can also be applied to model selection, which considers only candidate models with goodness of fits better than those by spurious fits. The theory and method are convincingly illustrated by simulated examples and an application to the binary outcomes from German Neuroblastoma Trials.",,,,,"Zhou, Wenxin/ABB-9228-2020; Fan, Jianqing/B-2115-2008","Zhou, Wenxin/0000-0002-2761-485X; ",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,203,,,,,,,,,,,,,,,WOS:000391829800001,0
J,"Manukyan, A; Ceyhan, E",,,,"Manukyan, Artur; Ceyhan, Elvan",,,Classification of Imbalanced Data with a Geometric Digraph Family,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We use a geometric digraph family called class cover catch digraphs (CCCDs) to tackle the class imbalance problem in statistical classification. CCCDs provide graph theoretic solutions to the class cover problem and have been employed in classification. We assess the classification performance of CCCD classifiers by extensive Monte Carlo simulations, comparing them with other classifiers commonly used in the literature. In particular, we show that CCCD classifiers perform relatively well when one class is more frequent than the other in a two-class setting, an example of the cl ass imbalance problem. We also point out the relationship between class imbalance and class overlapping problems, and their influence on the performance of CCCD classifiers and other classification methods as well as some state-of-the-art algorithms which are robust to class imbalance by construction. Experiments on both simulated and real data sets indicate that CCCD classifiers are robust to the class imbalance problem. CCCDs substantially undersample from the majority class while preserving the information on the discarded points during the undersampling process. Many state-of-the-art methods, however, keep this information by means of ensemble classifiers, but CCCDs yield only a single classifier with the same property, making it both appealing and fast.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,189,,,,,,,,,,,,,,,WOS:000391826000001,0
J,"Williamson, RC; Vernet, E; Reid, MD",,,,"Williamson, Robert C.; Vernet, Elodie; Reid, Mark D.",,,Composite Multiclass Losses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a proper composite loss, which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We subsume results on classification calibration by relating it to properness. We determine the stationarity condition, Bregman representation, order-sensitivity, and quasi-convexity of multiclass proper losses. We then characterise the existence and uniqueness of the composite representation for multiclass losses. We show how the composite representation is related to other core properties of a loss: mixability, admissibility and (strong) convexity of multiclass losses which we characterise in terms of the Hessian of the Bayes risk. We show that the simple integral representation for binary proper losses can not be extended to multiclass losses but offer concrete guidance regarding how to design different loss functions. The conclusion drawn from these results is that the proper composite representation is a natural and convenient tool for the design of multiclass loss functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,52,223,,,,,,,,,,,,,,,WOS:000391914300001,0
J,"Zhang, C; Liu, YF; Wu, YC",,,,"Zhang, Chong; Liu, Yufeng; Wu, Yichao",,,On Quantile Regression in Reproducing Kernel Hilbert Spaces with the Data Sparsity Constraint,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"For spline regressions, it is well known that the choice of knots is crucial for the performance of the estimator. As a general learning framework covering the smoothing splines, learning in a Reproducing Kernel Hilbert Space (RKHS) has a similar issue. However, the selection of training data points for kernel functions in the RKHS representation has not been carefully studied in the literature. In this paper we study quantile regression as an example of learning in a RKHS. In this case, the regular squared norm penalty does not perform training data selection. We propose a data sparsity constraint that imposes thresholding on the kernel function coefficients to achieve a sparse kernel function representation. We demonstrate that the proposed data sparsity method can have competitive prediction performance for certain situations, and have comparable performance in other cases compared to that of the traditional squared norm penalty. Therefore, the data sparsity method can serve as a competitive alternative to the squared norm penalty method. Some theoretical properties of our proposed method using the data sparsity constraint are obtained. Both simulated and real data sets are used to demonstrate the usefulness of our data sparsity constraint.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,40,,,,,,,,,,,,,,,WOS:000391482600001,0
J,"Lin, T; Xue, HL; Wang, L; Huang, B; Zha, HB",,,,"Lin, Tong; Xue, Hanlin; Wang, Ling; Huang, Bo; Zha, Hongbin",,,Supervised Learning via Euler's Elastica Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper investigates the Euler's elastica (EE) model for high-dimensional supervised learning problems in a function approximation framework. In 1744 Euler introduced the elastica energy for a 2D curve on modeling torsion-free thin elastic rods. Together with its degenerate form of total variation (TV), Euler's elastica has been successfully applied to low-dimensional data processing such as image denoising and image inpainting in the last two decades. Our motivation is to apply Euler's elastica to high-dimensional supervised learning problems. To this end, a supervised learning problem is modeled as an energy functional minimization under a new geometric regularization scheme, where the energy is composed of a squared loss and an elastica penalty. The elastica penalty aims at regularizing the approximated function by heavily penalizing large gradients and high curvature values on all level curves. We take a computational PDE approach to minimize the energy functional. By using variational principles, the energy minimization problem is transformed into an Euler-Lagrange PDE. However, this PDE is usually high-dimensional and can not be directly handled by common low-dimensional solvers. To circumvent this difficulty, we use radial basis functions (RBF) to approximate the target function, which reduces the optimization problem to finding the linear coefficients of these basis functions. Some theoretical properties of this new model, including the existence and uniqueness of solutions and universal consistency, are analyzed. Extensive experiments have demonstrated the effectiveness of the proposed model for binary classification, multi-class classification, and regression tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,3637,3686,,,,,,,,,,,,,,,,WOS:000369888000040,0
J,"Ma, P; Mahoney, MW; Yu, B",,,,"Ma, Ping; Mahoney, Michael W.; Yu, Bin",,,A Statistical Perspective on Algorithmic Leveraging,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"One popular method for dealing with large-scale data sets is sampling. For example, by using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. This method has been successful in improving computational efficiency of algorithms for matrix problems such as least-squares approximation, least absolute deviations approximation, and low-rank matrix approximation. Existing work has focused on algorithmic issues such as worst-case running times and numerical issues associated with providing high-quality implementations, but none of it addresses statistical aspects of this method. In this paper, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with shrinkage leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). A detailed empirical evaluation of existing leverage-based methods as well as these two new methods is carried out on both synthetic and real data sets. The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance. For example, with the same computation reduction as in the original algorithmic leveraging approach, our proposed SLEV typically leads to improved biases and variances both unconditionally and conditionally (on the observed data), and our proposed LEVUNW typically yields improved unconditional biases and variances.",,,,,"Ma, Ping/M-7746-2015","Ma, Ping/0000-0002-5728-3596",,,,,,,,,,,,,1532-4435,,,,,APR,2015,16,,,,,,861,911,,,,,,,,,,,,,,,,WOS:000369886300007,0
J,"Anandkumar, A; Ge, R; Hsu, D; Kakade, SM",,,,"Anandkumar, Animashree; Ge, Rong; Hsu, Daniel; Kakade, Sham M.",,,A Tensor Approach to Learning Mixed Membership Community Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. (2008). This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g., singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model.",,,,,,"Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,1532-4435,,,,,JUN,2014,15,,,,,,2239,2312,,,,,,,,,,,,,,,,WOS:000344638300012,0
J,"Culp, MV; Ryan, KJ",,,,"Culp, Mark Vere; Ryan, Kenneth Joseph",,,Joint Harmonic Functions and Their Supervised Connections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The cluster assumption had a significant impact on the reasoning behind semi-supervised classification methods in graph-based learning. The literature includes numerous applications where harmonic functions provided estimates that conformed to data satisfying this well-known assumption, but the relationship between this assumption and harmonic functions is not as well-understood theoretically. We investigate these matters from the perspective of supervised kernel classification and provide concrete answers to two fundamental questions. (i) Under what conditions do semi-supervised harmonic approaches satisfy this assumption? (ii) If such an assumption is satisfied then why precisely would an observation sacrifice its own supervised estimate in favor of the cluster? First, a harmonic function is guaranteed to assign labels to data in harmony with the cluster assumption if a specific condition on the boundary of the harmonic function is satisfied. Second, it is shown that any harmonic function estimate within the interior is a probability weighted average of supervised estimates, where the weight is focused on supervised kernel estimates near labeled cases. We demonstrate that the uniqueness criterion for harmonic estimators is sensitive when the graph is sparse or the size of the boundary is relatively small. This sets the stage for a third contribution, a new regularized joint harmonic function for semi-supervised learning based on a joint optimization criterion. Mathematical properties of this estimator, such as its uniqueness even when the graph is sparse or the size of the boundary is relatively small, are proven. A main selling point is its ability to operate in circumstances where the cluster assumption may not be fully satisfied on real data by compromising between the purely harmonic and purely supervised estimators. The competitive stature of the new regularized joint harmonic approach is established.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3721,3752,,,,,,,,,,,,,,,,WOS:000335457100008,0
J,"Long, PM; Servedio, RA",,,,"Long, Philip M.; Servedio, Rocco A.",,,Algorithms and Hardness Results for Parallel Large Margin Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov's that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown gamma-margin halfspace over n dimensions using n . poly(1/gamma) processors and running in time (O) over tilde (1/gamma) + O(log n). In contrast, naive parallel algorithms that learn a gamma-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter gamma. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3105,3128,,,,,,,,,,,,,,,,WOS:000328603600006,0
J,"Yuan, XT; Zhang, T",,,,"Yuan, Xiao-Tong; Zhang, Tong",,,Truncated Power Method for Sparse Eigenvalue Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper considers the sparse eigenvalue problem, which is to extract dominant (largest) sparse eigenvectors with at most k non-zero components. We propose a simple yet effective solution called truncated power method that can approximately solve the underlying nonconvex optimization problem. A strong sparse recovery result is proved for the truncated power method, and this theory is our key motivation for developing the new algorithm. The proposed method is tested on applications such as sparse principal component analysis and the densest k-subgraph problem. Extensive experiments on several synthetic and real-world data sets demonstrate the competitive empirical performance of our method.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,899,925,,,,,,,,,,,,,,,,WOS:000318590500006,0
J,"Hu, T; Fan, J; Wu, Q; Zhou, DX",,,,"Hu, Ting; Fan, Jun; Wu, Qiang; Zhou, Ding-Xuan",,,Learning Theory Approach to Minimum Error Entropy Criterion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the minimum error entropy (MEE) criterion and an empirical risk minimization learning algorithm when an approximation of Renyi's entropy (of order 2) by Parzen windowing is minimized. This learning algorithm involves a Parzen windowing scaling parameter. We present a learning theory approach for this MEE algorithm in a regression setting when the scaling parameter is large. Consistency and explicit convergence rates are provided in terms of the approximation ability and capacity of the involved hypothesis space. Novel analysis is carried out for the generalization error associated with Renyi's entropy and a Parzen windowing function, to overcome technical difficulties arising from the essential differences between the classical least squares problems and the MEE setting. An involved symmetrized least squares error is introduced and analyzed, which is related to some ranking algorithms.",,,,,"Wu, Qiang/B-1620-2008; Fan, Jun/O-4742-2017; Zhou, Ding-Xuan/B-3160-2013","Wu, Qiang/0000-0002-4698-6966; Fan, Jun/0000-0001-8451-3484; Zhou, Ding-Xuan/0000-0003-0224-9216",,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,377,397,,,,,,,,,,,,,,,,WOS:000315981900004,0
J,"Drineas, P; Magdon-Ismail, M; Mahoney, MW; Woodruff, DP",,,,"Drineas, Petros; Magdon-Ismail, Malik; Mahoney, Michael W.; Woodruff, David P.",,,Fast Approximation of Matrix Coherence and Statistical Leverage,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The statistical leverage scores of a matrix A are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystrom-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n x d matrix A, with n >> d, and that returns as output relative-error approximations to a l l n of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of n and d) in O(nd log n) time, as opposed to the O (n d 2) time required by the naive algorithm that involves computing an orthogonal basis for the range of A. Our analysis may be viewed in terms of computing a relative-error approximation to an under constrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with n approximate to d, and the extension to streaming environments.",,,,,,"Drineas, Petros/0000-0003-1994-8670",,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3475,3506,,,,,,,,,,,,,,,,WOS:000314529000001,0
J,"Zhang, XH; Saha, A; Vishwanathan, SVN",,,,"Zhang, Xinhua; Saha, Ankan; Vishwanathan, S. V. N.",,,Smoothing Multivariate Performance Measures,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Optimizing multivariate performance measure is an important task in Machine Learning. Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can be shown that CPMs converge to an epsilon accurate solution in O(1/lambda epsilon) iterations, where lambda is the trade-off parameter between the regularizer and the loss function. Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved. We disprove this conjecture in this paper by constructing counter examples. However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov's accelerated gradient method, can find an e accurate solution in O* (min {1/epsilon, 1/root lambda epsilon}) iterations. Computationally, our smoothing technique is also particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs. Empirical evaluation on some of the largest publicly available data sets shows that our method converges significantly faster than CPMs without sacrificing generalization ability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2012,13,,,,,,3623,3680,,,,,,,,,,,,,,,,WOS:000314529000007,0
J,"Raskutti, G; Wainwright, MJ; Yu, B",,,,"Raskutti, Garvesh; Wainwright, Martin J.; Yu, Bin",,,Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sparse additive models are families of d-variate functions with the additive decomposition f* = Sigma(j is an element of S)f*(j), where S is an unknown subset of cardinality s << d. In this paper, we consider the case where each univariate component function f*(j) lies in a reproducing kernel Hilbert space (RKHS), and analyze a method for estimating the unknown function f* based on kernels combined with l(1)-type convex regularization. Working within a high-dimensional framework that allows both the dimension d and sparsity s to increase with n, we derive convergence rates in the L-2(P) and L-2(P-n) norms over the class F-d,F-s,F-H of sparse additive models with each univariate function f*(j) in the unit ball of a univariate RKHS with bounded kernel function. We complement our upper bounds by deriving minimax lower bounds on the L-2(P) error, thereby showing the optimality of our method. Thus, we obtain optimal minimax rates for many interesting classes of sparse additive models, including polynomials, splines, and Sobolev classes. We also show that if, in contrast to our univariate conditions, the d-variate function class is assumed to be globally bounded, then much faster estimation rates are possible for any sparsity s = Omega(root n), showing that global boundedness is a significant restriction in the high-dimensional setting.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,389,427,,,,,,,,,,,,,,,,WOS:000303046000006,0
J,"Bull, AD",,,,"Bull, Adam D.",,,Convergence Rates of Efficient Global Optimization Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the efficient global optimization problem, we minimize an unknown function f, using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2011,12,,,,,,2879,2904,,,,,,,,,,,,,,,,WOS:000298103200006,0
J,"Shalev-Shwartz, S; Tewari, A",,,,"Shalev-Shwartz, Shai; Tewari, Ambuj",,,Stochastic Methods for l(1)-regularized Loss Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe and analyze two stochastic methods for l(1) regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1865,1892,,,,,,,,,,,,,,,,WOS:000293757200003,0
J,"Pelletier, B; Pudlo, P",,,,"Pelletier, Bruno; Pudlo, Pierre",,,Operator Norm Convergence of Spectral Clustering on Level Sets,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Following Hartigan (1975), a cluster is defined as a connected component of the t-level set of the underlying density, that is, the set of points for which the density is greater than t. A clustering algorithm which combines a density estimate with spectral clustering techniques is proposed. Our algorithm is composed of two steps. First, a nonparametric density estimate is used to extract the data points for which the estimated density takes a value greater than t. Next, the extracted points are clustered based on the eigenvectors of a graph Laplacian matrix. Under mild assumptions, we prove the almost sure convergence in operator norm of the empirical graph Laplacian operator associated with the algorithm. Furthermore, we give the typical behavior of the representation of the data set into the feature space, which establishes the strong consistency of our proposed algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2011,12,,,,,,385,416,,,,,,,,,,,,,,,,WOS:000288896800002,0
J,"Jebara, T",,,,"Jebara, Tony",,,Multitask Sparsity via Maximum Entropy Discrimination,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A multitask learning framework is developed for discriminative classification and regression where multiple large-margin linear classifiers are estimated for different prediction problems. These classifiers operate in a common input space but are coupled as they recover an unknown shared representation. A maximum entropy discrimination (MED) framework is used to derive the multitask algorithm which involves only convex optimization problems that are straightforward to implement. Three multitask scenarios are described. The first multitask method produces multiple support vector machines that learn a shared sparse feature selection over the input space. The second multitask method produces multiple support vector machines that learn a shared conic kernel combination. The third multitask method produces a pooled classifier as well as adaptively specialized individual classifiers. Furthermore, extensions to regression, graphical model structure estimation and other sparse methods are discussed. The maximum entropy optimization problems are implemented via a sequential quadratic programming method which leverages recent progress in fast SVM solvers. Fast monotonic convergence bounds are provided by bounding the MED sparsifying cost function with a quadratic function and ensuring only a constant factor runtime increase above standard independent SVM solvers. Results are shown on multitask data sets and favor multitask learning over single-task or tabula rasa methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,75,110,,,,,,,,,,,,,,,,WOS:000287938500004,0
J,"Weng, RC; Lin, CJ",,,,"Weng, Ruby C.; Lin, Chih-Jen",,,A Bayesian Approximation Method for Online Ranking,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes a Bayesian approximation method to obtain online ranking algorithms for games with multiple teams and multiple players. Recently for Internet games large online ranking systems are much needed. We consider game models in which a k-team game is treated as several two-team games. By approximating the expectation of teams' (or players') performances, we derive simple analytic update rules. These update rules, without numerical integrations, are very easy to interpret and implement. Experiments on game data show that the accuracy of our approach is competitive with state of the art systems such as TrueSkill, but the running time as well as the code is much shorter.",,,,,,"Lin, Chih-Jen/0000-0003-4684-8747",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,267,300,,,,,,,,,,,,,,,,WOS:000287938500009,0
J,"Radovanovic, M; Nanopoulos, A; Ivanovic, M",,,,"Radovanovic, Milos; Nanopoulos, Alexandros; Ivanovic, Mirjana",,,Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent popular nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.",,,,,"Ivanovic, M./AAE-7711-2020; Radovanoviƒá, Milo≈°/ABB-7403-2022","Radovanovic, Milos/0000-0003-2225-7803; Ivanovic, Mirjana/0000-0003-1946-0384",,,,,,,,,,,,,1532-4435,,,,,SEP,2010,11,,,,,,2487,2531,,,,,,,,,,,,,,,,WOS:000282523400004,0
J,"Donmez, P; Lebanon, G; Balasubramanian, K",,,,"Donmez, Pinar; Lebanon, Guy; Balasubramanian, Krishnakumar",,,Unsupervised Supervised Learning I: Estimating Classification and Regression Errors without Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,Estimating the error rates of classifiers or regression models is a fundamental task in machine learning which has thus far been studied exclusively using supervised learning techniques. We propose a novel unsupervised framework for estimating these error rates using only unlabeled data and mild assumptions. We prove consistency results for the framework and demonstrate its practical applicability on both synthetic and real world data.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1323,1351,,,,,,,,,,,,,,,,WOS:000282521500005,0
J,"Vishwanathan, SVN; Schraudolph, NN; Kondor, R; Borgwardt, KM",,,,"Vishwanathan, S. V. N.; Schraudolph, Nicol N.; Kondor, Risi; Borgwardt, Karsten M.",,,Graph Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a unified framework to study graph kernels, special cases of which include the random walk (Gartner et al., 2003; Borgwardt et al., 2005) and marginalized (Kashima et al., 2003, 2004; Mahe et al., 2004) graph kernels. Through reduction to a Sylvester equation we improve the time complexity of kernel computation between unlabeled graphs with n vertices from O(n(6)) to O(n(3)). We find a spectral decomposition approach even more efficient when computing entire kernel matrices. For labeled graphs we develop conjugate gradient and fixed-point methods that take O(dn(3)) time per iteration, where d is the size of the label set. By extending the necessary linear algebra to Reproducing Kernel Hilbert Spaces (RKHS) we obtain the same result for d-dimensional edge kernels, and O(n(4)) in the infinite-dimensional case; on sparse graphs these algorithms only take O(n(2)) time per iteration in all cases. Experiments on graphs from bioinformatics and other application domains show that these techniques can speed up computation of the kernel by an order of magnitude or more. We also show that certain rational kernels (Cortes et al., 2002, 2003, 2004) when specialized to graphs reduce to our random walk graph kernel. Finally, we relate our framework to R-convolution kernels (Haussler, 1999) and provide a kernel that is close to the optimal assignment kernel of Frohlich et al. (2006) yet provably positive semi-definite.",,,,,,"Borgwardt, Karsten/0000-0001-7221-2393",,,,,,,,,,,,,1532-4435,,,,,APR,2010,11,,,,,,1201,1242,,,,,,,,,,,,,,,,WOS:000282521500001,0
J,"Strumbelj, E; Kononenko, I",,,,"Strumbelj, Erik; Kononenko, Igor",,,An Efficient Explanation of Individual Classifications using Game Theory,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,1,18,,,,,,,,,,,,,,,,WOS:000277186400001,0
J,"Weinberger, KQ; Saul, LK",,,,"Weinberger, Kilian Q.; Saul, Lawrence K.",,,Distance Metric Learning for Large Margin Nearest Neighbor Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,207,244,,,,,,,,,,,,,,,,WOS:000270824200005,0
J,"Loog, M",,,,"Loog, Marco",,,On the Equivalence of Linear Dimensionality-Reducing Transformations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this JMLR volume, Ye (2008) demonstrates the essential equivalence of two sets of solutions to a generalized Fisher criterion used for linear dimensionality reduction (see Ye, 2005; Loog, 2007). Here, I point out the basic flaw in this new contribution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2008,9,,,,,,2489,2490,,,,,,,,,,,,,,,,WOS:000262637600003,0
J,"Tarigan, B; van de Geer, SA",,,,"Tarigan, Bernadetta; van de Geer, Sara A.",,,A Moment Bound for Multi-hinge Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,The success of support vector machines in binary classification relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the so-called multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than finding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin.,,,,,"Tarigan, Bernadetta/AAD-1843-2019",,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2171,2185,,,,,,,,,,,,,,,,WOS:000262637300005,0
J,"Crammer, K; Kearns, M; Wortman, J",,,,"Crammer, Koby; Kearns, Michael; Wortman, Jennifer",,,Learning from Multiple Sources,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning accurate models from multiple sources of nearby data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classification and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations.",,,,,"Nunes, Lu√≠s/B-8732-2009","Nunes, Lu√≠s/0000-0001-7072-0925",,,,,,,,,,,,,1532-4435,,,,,AUG,2008,9,,,,,,1757,1774,,,,,,,,,,,,,,,,WOS:000262636800005,0
J,"Boulle, M",,,,"Boulle, Marc",,,Compression-based averaging of selective naive Bayes classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The naive Bayes classifier has proved to be very effective on many real data applications. Its performance usually benefits from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overfitting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and finally results in a naive Bayes classifier with soft variable selection. Extensive experiments show that the compression-based averaged classifier outperforms the Bayesian model averaging scheme.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1659,1685,,,,,,,,,,,,,,,,WOS:000249353700010,0
J,"Rifkin, RM; Lippert, RA",,,,"Rifkin, Ryan M.; Lippert, Ross A.",,,Value regularization and Fenchel duality,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Regularization is an approach to function learning that balances fit and smoothness. In practice, we search for a function f with a finite representation f = Sigma(i)c(i)phi(i) (.). In most treatments, the c(i) are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2007,8,,,,,,441,479,,,,,,,,,,,,,,,,WOS:000247002700003,0
J,"List, N; Simon, HU",,,,"List, Nikolas; Simon, Hans Ulrich",,,General polynomial time decomposition algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efficiently approaches an optimal solution. The number of iterations required to be within e of optimality grows linearly with 1/epsilon and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k(0) equality constraints for some fixed constant k(0) plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called sparse witness of sub-optimality. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions).",,,,,,"Simon, Hans/0000-0002-1587-0944",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,303,321,,,,,,,,,,,,,,,,WOS:000247002600006,0
J,"Belkin, M; Niyogi, P; Sindhwani, V",,,,"Belkin, Mikhail; Niyogi, Partha; Sindhwani, Vikas",,,Manifold regularization: A geometric framework for learning from labeled and unlabeled examples,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result ( in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2006,7,,,,,,2399,2434,,,,,,,,,,,,,,,,WOS:000245390700005,0
J,"Lecue, G",,,,"Lecue, Guillaume",,,Lower bounds and aggregation in density estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we prove the optimality of an aggregation procedure. We prove lower bounds for aggregation of model selection type of M density estimators for the Kullback-Leibler divergence (KL), the Hellinger's distance and the L-1-distance. The lower bound, with respect to the KL distance, can be achieved by the on-line type estimate suggested, among others, by Yang (2000a). Combining these results, we state that logM/n is an optimal rate of aggregation in the sense of Tsybakov (2003), where n is the sample size.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,971,981,,,,,,,,,,,,,,,,WOS:000245388400003,0
J,"Vishwanathan, SVN; Schraudolph, NN; Smola, AJ",,,,"Vishwanathan, S. V. N.; Schraudolph, Nicol N.; Smola, Alex J.",,,Step size adaptation in reproducing kernel Hilbert space,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents an online support vector machine (SVM) that uses the stochastic meta-descent (SMD) algorithm to adapt its step size automatically. We formulate the online learning problem as a stochastic gradient descent in reproducing kernel Hilbert space (RKHS) and translate SMD to the nonparametric setting, where its gradient trace parameter is no longer a coefficient vector but an element of the RKHS. We derive efficient updates that allow us to perform the step size adaptation in linear time. We apply the online SVM framework to a variety of loss functions, and in particular show how to handle structured output spaces and achieve efficient online multiclass classification. Experiments show that our algorithm outperforms more primitive methods for setting the gradient step size.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2006,7,,,,,,1107,1133,,,,,,,,,,,,,,,,WOS:000245388400009,0
J,"Gretton, A; Herbrich, R; Smola, A; Bousquet, O; Scholkopf, B",,,,"Gretton, A; Herbrich, R; Smola, A; Bousquet, O; Scholkopf, B",,,Kernel methods for measuring independence,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis.",,,,,"Sch√∂lkopf, Bernhard/A-7570-2013","Sch√∂lkopf, Bernhard/0000-0002-8177-0925; Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,1532-4435,,,,,DEC,2005,6,,,,,,2075,2129,,,,,,,,,,,,,,,,WOS:000236331100008,0
J,"Banerjee, Arindam; Merugu, S; Dhillon, IS; Ghosh, J",,,,"Banerjee, A; Merugu, S; Dhillon, IS; Ghosh, J",,,Clustering with Bregman divergences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A wide variety of distortion functions, such as squared Euclidean distance, Mahalanobis distance, Itakura-Saito distance and relative entropy, have been used for clustering. In this paper, we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences. The proposed algorithms unify centroid-based parametric clustering approaches, such as classical kmeans, the Linde-Buzo-Gray (LBG) algorithm and information-theoretic clustering, which arise by special choices of the Bregman divergence. The algorithms maintain the simplicity and scalability of the classical kmeans algorithm, while generalizing the method to a large class of clustering loss functions. This is achieved by first posing the hard clustering problem in terms of minimizing the loss in Bregman information, a quantity motivated by rate distortion theory, and then deriving an iterative algorithm that monotonically decreases this loss. In addition, we show that there is a bijection between regular exponential families and a large class of Bregman divergences, that we call regular Bregman divergences. This result enables the development of an alternative interpretation of an efficient EM scheme for learning mixtures of exponential family distributions, and leads to a simple soft clustering algorithm for regular Bregman divergences. Finally, we discuss the connection between rate distortion theory and Bregman clustering and present an information theoretic analysis of Bregman clustering algorithms in terms of a trade-off between compression and loss in Bregman information.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2005,6,,,,,,1705,1749,,,,,,,,,,,,,,,,WOS:000236330500003,0
J,"Tsochantaridis, I; Joachims, T; Hofmann, T; Altun, Y",,,,"Tsochantaridis, I; Joachims, T; Hofmann, T; Altun, Y",,,Large margin methods for structured and interdependent output variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.",,,,,,"Joachims, Thorsten/0000-0003-3654-3683",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1453,1484,,,,,,,,,,,,,,,,WOS:000236330100008,0
J,"Almeida, LB",,,,"Almeida, LB",,,Separating a real-life nonlinear image mixture,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When acquiring an image of a paper document, the image printed on the back page sometimes shows through. The mixture of the front- and back-page images thus obtained is markedly nonlinear, and thus constitutes a good real-life test case for nonlinear blind source separation. This paper addresses a difficult version of this problem, corresponding to the use of onion skin paper, which results in a relatively strong nonlinearity of the mixture, which becomes close to singular in the lighter regions of the images. The separation is achieved through the MISEP technique, which is an extension of the well known INFOMAX method. The separation results are assessed with objective quality measures. They show an improvement over the results obtained with linear separation, but have room for further improvement.",,,,,"Almeida, Luis/A-7651-2012","Almeida, Luis/0000-0002-1324-0068",,,,,,,,,,,,,1532-4435,,,,,JUL,2005,6,,,,,,1199,1229,,,,,,,,,,,,,,,,WOS:000236329900007,0
J,"Lee, TW; Cardoso, JF; Oja, E; Amari, S",,,,"Lee, TW; Cardoso, JF; Oja, E; Amari, S",,,Introduction to special issue on independent components analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,"Amari, Shun-ichi/A-5901-2016",,,,,,,,,,,,,,1532-4435,,,,,Oct-01,2004,4,07-Aug,,,,,1175,1176,,,,,,,,,,,,,,,,WOS:000224808300001,0
J,"Hu, JL; Wellman, MP",,,,"Hu, JL; Wellman, MP",,,Nash Q-learning for general-sum stochastic games,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance.",,,,,"Wellman, Michael/AAQ-7063-2020","Wellman, Michael/0000-0002-1691-6844",,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,1039,1069,,10.1162/1532443041827880,0,,,,,,,,,,,,,WOS:000231002600005,0
J,"Zhong, S; Ghosh, J",,,,"Zhong, S; Ghosh, J",,,A unified framework for model-based clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Model-based clustering techniques have been widely used and have shown promising results in many applications involving complex data. This paper presents a unified framework for probabilistic model-based clustering based on a bipartite graph view of data and models that highlights the commonalities and differences among existing model-based clustering algorithms. In this view, clusters are represented as probabilistic models in a model space that is conceptually separate from the data space. For partitional clustering, the view is conceptually similar to the Expectation-Maximization (EM) algorithm. For hierarchical clustering, the graph-based view helps to visualize critical/important distinctions between similarity-based approaches and model-based approaches. The framework also suggests several useful variations of existing clustering algorithms. Two new variations-balanced model-based clustering and hybrid model-based clustering-are discussed and empirically evaluated on a variety of data types.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Aug-15,2004,4,6,,,,,1001,1037,,10.1162/1532443041827943,0,,,,,,,,,,,,,WOS:000231002600004,0
J,"Blanchard, G; Lugosi, G; Vayatis, N",,,,"Blanchard, G; Lugosi, G; Vayatis, N",,,On the rate of convergence of regularized boosting classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,16th Annual Conference on Neural Information Processing Systems (NIPS),"DEC, 2002","VANCOUVER, CANADA",,,,,"A regularized boosting method is introduced, for which regularization is obtained through a penalization function. It is shown through oracle inequalities that this method is model adaptive. The rate of convergence of the probability of misclassification is investigated. It is shown that for quite a large class of distributions, the probability of error converges to the Bayes risk at a rate faster than n(-(V+2)/(4(V+1))) where V is the VC dimension of the base class whose elements are combined by boosting methods to obtain an aggregated classifier. The dimension-independent nature of the rates may partially explain the good behavior of these methods in practical problems. Under Tsybakov's noise condition the rate of convergence is even faster. We investigate the conditions necessary to obtain such rates for different base classes. The special case of boosting using decision stumps is studied in detail. We characterize the class of classifiers realizable by aggregating decision stumps. It is shown that some versions of boosting work especially well in high-dimensional logistic additive models. It appears that adding a limited labelling noise to the training data may in certain cases improve the convergence, as has been also suggested by other authors.",,,,,,"Lugosi, Gabor/0000-0003-1614-5901",,,,,,,,,,,,,1532-4435,,,,,Jul-01,2004,4,5,,,,,861,894,,10.1162/1532443041424319,0,,,,,,,,,,,,,WOS:000223238800006,0
J,"Friedman, C; Sandow, S",,,,"Friedman, C; Sandow, S",,,Learning Probabilistic models: An expected utility maximization approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning a probabilistic model from the viewpoint of an expected utility maximizing decision maker/investor who would use the model to make decisions (bets), which result in well defined payoffs. In our new approach, we seek good out-of-sample model performance by considering a one-parameter family of Pareto optimal models, which we define in terms of consistency with the training data and consistency with a prior (benchmark) model. We measure the former by means of the large-sample distribution of a vector of sample-averaged features, and the latter by means of a generalized relative entropy. We express each Pareto optimal model as the solution of a strictly convex optimization problem and its strictly concave (and tractable) dual. Each dual problem is a regularized maximization of expected utility over a well-defined family of functions. Each Pareto optimal model is robust: maximizing worst-case outperformance relative to the benchmark model. Finally, we select the Pareto optimal model with maximum (out-of-sample) expected utility. We show that our method reduces to the minimum relative entropy method if and only if the utility function is a member of a three-parameter logarithmic family.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,Apr-01,2004,4,3,,,,,257,291,,10.1162/153244304773633816,0,,,,,,,,,,,,,WOS:000221043900001,0
J,"Mannor, S; Shimkin, N",,,,"Mannor, S; Shimkin, N",,,A geometric approach to multi-criterion reinforcement learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of reinforcement learning in a controlled Markov environment with multiple objective functions of the long-term average reward type. The environment is initially unknown, and furthermore may be affected by the actions of other agents, actions that are observed but cannot be predicted beforehand. We capture this situation using a stochastic game model, where the learning agent is facing an adversary whose policy is arbitrary and unknown, and where the reward function is vector-valued. State recurrence conditions are imposed throughout. In our basic problem formulation, a desired target set is specified in the vector reward space, and the objective of the learning agent is to approach the target set, in the sense that the long-term average reward vector will belong to this set. We devise appropriate learning algorithms, that essentially use multiple reinforcement learning algorithms for the standard scalar reward problem, which are combined using the geometric insight from the theory of approachability for vector-valued stochastic games. We then address the more general and optimization-related problem, where a nested class of possible target sets is prescribed, and the goal of the learning agent is to approach the smallest possible target set (which will generally depend on the unknown system parameters). A particular case which falls into this framework is that of stochastic games with average reward constraints, and further specialization provides a reinforcement learning algorithm for constrained Markov decision processes. Some basic examples are provided to illustrate these results.",,,,,,"Mannor, Shie/0000-0003-4439-7647",,,,,,,,,,,,,1532-4435,,,,,APR,2004,5,,,,,,325,360,,,,,,,,,,,,,,,,WOS:000236327400002,0
J,"Lee, HKH; Clyde, MA",,,,"Lee, HKH; Clyde, MA",,,Lossless online Bayesian bagging,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bagging frequently improves the predictive performance of a model. An online version has recently been introduced, which attempts to gain the benefits of an online algorithm while approximating regular bagging. However, regular online bagging is an approximation to its batch counterpart and so is not loss less with respect to the bagging operation. By operating under the Bayesian paradigm, we introduce an online Bayesian version of bagging which is exactly equivalent to the batch Bayesian version, and thus when combined with a loss less learning algorithm gives a completely lossless online bagging algorithm. We also note that the Bayesian formulation resolves a theoretical problem with bagging, produces less variability in its estimates, and can improve predictive performance for smaller data sets.",,,,,"Clyde, Merlise/ABG-3538-2020","Clyde, Merlise/0000-0002-3595-1872",,,,,,,,,,,,,1532-4435,,,,,FEB,2004,5,,,,,,143,151,,,,,,,,,,,,,,,,WOS:000236327000001,0
J,"Marchand, M; Shawe-Taylor, J",,,,"Marchand, M; Shawe-Taylor, J",,,The set covering machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MASSACHUSETTS",,,,,"We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a general-purpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection.",,,,,,"Marchand, Mario/0000-0002-7078-7393; Shawe-Taylor, John/0000-0002-2030-0073",,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,723,746,,10.1162/jmlr.2003.3.4-5.723,0,,,,,,,,,,,,,WOS:000184926200006,0
J,"Sebban, M; Nock, R; Lallich, S",,,,"Sebban, M; Nock, R; Lallich, S",,,Stopping criterion for boosting-based data reduction techniques: From binary to multiclass problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"So far, boosting has been used to improve the quality of moderately accurate learning algorithms, by weighting and combining many of their weak hypotheses into a final classifier with theoretically high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt boosting properties to data reduction techniques. In this particular context, the objective was not only to improve the success rate, but also to reduce the time and space complexities due to the storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In that framework, each weak hypothesis, which is usually built and weighted from the learning set, is replaced by a single learning instance. The weight given by boosting defines in that case the relevance of the instance, and a statistical test allows one to decide whether it can be discarded without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed problems with two classes. It is the aim of the present paper to relax the class constraint, and extend our contribution to multiclass problems. Beyond data reduction, experimental results are also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting rule brings to weighted nearest neighbor classifiers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,863,885,,10.1162/jmlr.2003.3.4-5.863,0,,,,,,,,,,,,,WOS:000184926200011,0
J,"Biau, G; Sangnier, M; Tanielian, U",,,,"Biau, Gerard; Sangnier, Maxime; Tanielian, Ugo",,,Some Theoretical Insights into Wasserstein GANs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Generative Adversarial Networks (GANs) have been successful in producing outstanding results in areas as diverse as image, video, and text generation. Building on these successes, a large number of empirical studies have validated the benefits of the cousin approach called Wasserstein GANs (WGANs), which brings stabilization in the training process. In the present paper, we add a new stone to the edifice by proposing some theoretical advances in the properties of WGANs. First, we properly define the architecture of WGANs in the context of integral probability metrics parameterized by neural networks and highlight some of their basic mathematical features. We stress in particular interesting optimization properties arising from the use of a parametric 1-Lipschitz discriminator. Then, in a statistically-driven approach, we study the convergence of empirical WGANs as the sample size tends to infinity, and clarify the adversarial effects of the generator and the discriminator by underlining some trade-off properties. These features are finally illustrated with experiments using both synthetic and real-world datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,45,119,,,,,,,,,,,,,,,WOS:000663171700001,0
J,"Bietti, A; Agarwal, A; Langford, J",,,,"Bietti, Alberto; Agarwal, Alekh; Langford, John",,,A Contextual Bandit Bake-off,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically optimal and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to empirically evaluate contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate various components of contextual bandit algorithm design such as loss estimators. Overall, this is a thorough study and review of contextual bandit methodology.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700321700001,0
J,"Klink, P; Abdulsamad, H; Belousov, B; D'Eramo, C; Peters, J; Pajarinen, J",,,,"Klink, Pascal; Abdulsamad, Hany; Belousov, Boris; D'Eramo, Carlo; Peters, Jan; Pajarinen, Joni",,,A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Across machine learning, the use of curricula has shown strong empirical potential to improve learning from data by avoiding local optima of training objectives. For reinforcement learning (RL), curricula are especially interesting, as the underlying optimization has a strong tendency to get stuck in local optima due to the exploration-exploitation trade-off. Recently, a number of approaches for an automatic generation of curricula for RL have been shown to increase performance while requiring less expert knowledge compared to manually designed curricula. However, these approaches are seldomly investigated from a theoretical perspective, preventing a deeper understanding of their mechanics. In this paper, we present an approach for automated curriculum generation in RL with a clear theoretical underpinning. More precisely, we formalize the well-known self-paced learning paradigm as inducing a distribution over training tasks, which trades off between task complexity and the objective to match a desired task distribution. Experiments show that training on this induced distribution helps to avoid poor local optima across RL algorithms in different tasks with uninformative rewards and challenging exploration requirements.",,,,,"Peters, Jan/P-6027-2019; Pajarinen, Joni/C-4189-2014","Peters, Jan/0000-0002-5266-8091; D'Eramo, Carlo/0000-0003-2712-118X; Pajarinen, Joni/0000-0003-4469-8191",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687299400001,0
J,"Kovachki, N; Lanthaler, S; Mishra, S",,,,"Kovachki, Nikola; Lanthaler, Samuel; Mishra, Siddhartha",,,On Universal Approximation and Error Bounds for Fourier Neural Operators,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Fourier neural operators (FNOs) have recently been proposed as an effective framework for learning operators that map between infinite-dimensional spaces. We prove that FNOs are universal, in the sense that they can approximate any continuous operator to desired accuracy. Moreover, we suggest a mechanism by which FNOs can approximate operators associated with PDEs efficiently. Explicit error bounds are derived to show that the size of the FNO, approximating operators associated with a Darcy type elliptic PDE and with the incompressible Navier-Stokes equations of fluid dynamics, only increases sub (log)-linearly in terms of the reciprocal of the error. Thus, FNOs are shown to efficiently approximate operators arising in a large class of PDEs.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,76,,,,,,,,,,,,,,,,WOS:000726786300001,0
J,"Kreitzberg, P; Serang, O",,,,"Kreitzberg, Patrick; Serang, Oliver",,,On Solving Probabilistic Linear Diophantine Equations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Multiple methods exist for computing marginals involving a linear Diophantine constraint on random variables. Each of these extant methods has some limitation on the dimension and support or on the type of marginal computed (e.g., sum-product inference, max-product inference, maximum a posteriori, etc.). Here, we introduce the trimmed p-convolution tree an approach that generalizes the applicability of the existing methods and achieves a runtime within a log-factor or better compared to the best existing methods. A second form of trimming we call underflow/overflow trimming is introduced which aggregates events which land outside the supports for a random variable into the nearest support. Trimmed p-convolution trees with and without underflow/overflow trimming are used in different protein inference models. Then two different methods of approximating max-convolution using Cartesian product trees are introduced.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,87,,,,,,,,,,,,,,,WOS:000663144200001,0
J,"Richardson, E; Weiss, Y",,,,"Richardson, Eitan; Weiss, Yair",,,A Bayes-Optimal View on Adversarial Examples,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Since the discovery of adversarial examples - the ability to fool modern CNN classifiers with tiny perturbations of the input, there has been much discussion whether they are a bug that is specific to current neural architectures and training methods or an inevitable feature of high dimensional geometry. In this paper, we argue for examining adversarial examples from the perspective of Bayes-Optimal classification. We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and derive analytic conditions on the distributions under which these classifiers are provably robust against any adversarial attack even in high dimensions. Our results show that even when these gold standard optimal classifiers are robust, CNNs trained on the same datasets consistently learn a vulnerable classifier, indicating that adversarial examples are often an avoidable bug. We further show that RBF SVMs trained on the same data consistently learn a robust classifier. The same trend is observed in experiments with real images in different datasets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,1,28,,,,,,,,,,,,,,,,WOS:000711927900001,0
J,"Zhao, TT; Bouchard-Cote, A",,,,"Zhao, Tingting; Bouchard-Cote, Alexandre",,,Analysis of high-dimensional Continuous Time Markov Chains using the Local Bouncy Particle Sampler,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sampling the parameters of high-dimensional Continuous Time Markov Chains (CTMCs) is a challenging problem with important applications in many fields of applied statistics. In this work a recently proposed type of non-reversible rejection-free Markov Chain Monte Carlo (MCMC) sampler, the Bouncy Particle Sampler (BPS), is brought to bear to this problem. BPS has demonstrated its favourable computational efficiency compared with state-of-the-art MCMC algorithms, however to date applications to real-data scenario were scarce. An important aspect of practical implementation of BPS is the simulation of event times. Default implementations use conservative thinning bounds. Such bounds can slow down the algorithm and limit the computational performance. Our paper develops an algorithm with exact analytical solution to the random event times in the context of CTMCs. Our local version of BPS algorithm takes advantage of the sparse structure in the target factor graph and we also provide a graph-theoretic tool for assessing the computational complexity of local BPS algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,91,,,,,,,,,,,,,,,WOS:000663146100001,0
J,"Bacry, E; Bompaire, M; Gaiffas, S; Muzy, JF",,,,"Bacry, Emmanuel; Bompaire, Martin; Gaiffas, Stephane; Muzy, Jean-Francois",,,Sparse and low-rank multivariate Hawkes processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of unveiling the implicit network structure of node interactions (such as user interactions in a social network), based only on high-frequency timestamps. Our inference is based on the minimization of the least-squares loss associated with a multivariate Hawkes model, penalized by ;(1) and trace norm of the interaction tensor. We provide a first theoretical analysis for this problem, that includes sparsity and low-rank inducing penalizations. This result involves a new data-driven concentration inequality for matrix martingales in continuous time with observable variance, which is a result of independent interest and a broad range of possible applications since it extends to matrix martingales former results restricted to the scalar case. A consequence of our analysis is the construction of sharply tuned l(1) and trace-norm penalizations, that leads to a data-driven scaling of the variability of information available for each users. Numerical experiments illustrate the significant improvements achieved by the use of such data-driven penalizations.",,,,,"Bompaire, Martin/ABF-7984-2020",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,48,,,,,,,,,,,,,,,WOS:000529405000003,0
J,"Belhadji, A; Bardenet, R; Chainais, P",,,,"Belhadji, Ayoub; Bardenet, Remi; Chainais, Pierre",,,A determinantal point process for column subset selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Two popular approaches to dimensionality reduction are principal component analysis, which projects onto a small number of well-chosen but non-interpretable directions, and feature selection, which selects a small number of the original features. Feature selection can be abstracted as selecting the subset of columns of a matrix X is an element of R-Nxd which minimize the approximation error, i.e., the norm of the residual after projecting X onto the space spanned by the selected columns. Such a combinatorial optimization is usually impractical, and there has been interest in polynomial-cost, random subset selection algorithms that favour small values of this approximation error. We propose sampling from a projection determinantal point process, a repulsive distribution over column indices that favours diversity among the selected columns. We bound the ratio of the expected approximation error over the optimal error of PCA. These bounds improve over the state-of-the-art bounds of volume sampling when some realistic structural assumptions are satisfied for X. Numerical experiments suggest that our bounds are tight, and that our algorithms have comparable performance with the double phase algorithm, often considered the practical state-of-the-art.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,197,,,,,,,,,,,,,,,WOS:000590003100001,0
J,"Blum, A; Dick, T; Manoj, N; Zhang, HY",,,,"Blum, Avrim; Dick, Travis; Manoj, Naren; Zhang, Hongyang",,,Random Smoothing Might be Unable to Certify l(infinity) Robustness for High-Dimensional Images,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the l(p) ball of radius epsilon when p > 2. Although random smoothing has been well understood for the l(2) case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of p > 2. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the l(infinity) threat model. In this work, we show that any noise distribution D over R-d that provides l(p) robustness for all base classifiers with p > 2 must satisfy E eta(2)(i) = Omega(d(1-2/p)epsilon(2) (1 - delta/delta(2)) for 99% of the features (pixels) of vector eta similar to D, where epsilon is the robust radius and delta is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in [0; 255], the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,211,,,,,,,,,,,,,,,WOS:000590021600001,0
J,"Kallus, N",,,,"Kallus, Nathan",,,Generalized Optimal Matching Methods for Causal Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop an encompassing framework for matching, covariate balancing, and doubly-robust methods for causal inference from observational data called generalized optimal matching (GOM). The framework is given by generalizing a new functional-analytical formulation of optimal matching, giving rise to the class of GOM methods, for which we provide a single unified theory to analyze tractability and consistency. Many commonly used existing methods are included in GOM and, using their GOM interpretation, can be extended to optimally and automatically trade off balance for variance and outperform their standard counterparts. As a subclass, GOM gives rise to kernel optimal matching (KOM), which, as supported by new theoretical and empirical results, is notable for combining many of the positive properties of other methods in one. KOM, which is solved as a linearly-constrained convex-quadratic optimization problem, inherits both the interpretability and model-free consistency of matching but can also achieve the root n-consistency of well-specified regression and the bias reduction and robustness of doubly robust methods. In settings of limited overlap, KOM enables a very transparent method for interval estimation for partial identification and robust coverage. We demonstrate this in examples with both synthetic and real data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000014,0
J,"Li, BY; Cen, SC; Chen, YX; Chi, YJ",,,,"Li, Boyue; Cen, Shicong; Chen, Yuxin; Chi, Yuejie",,,Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There is growing interest in large-scale machine learning and optimization over decentralized networks, e.g. in the context of multi-agent learning and federated learning. Due to the imminent need to alleviate the communication burden, the investigation of communicationefficient distributed optimization algorithms - particularly for empirical risk minimization - has flourished in recent years. A large fraction of these algorithms have been developed for the master/slave setting, relying on the presence of a central parameter server that can communicate with all agents. This paper focuses on distributed optimization over networks, or decentralized optimization, where each agent is only allowed to aggregate information from its neighbors over a network (namely, no centralized coordination is present). By properly adjusting the global gradient estimate via local averaging in conjunction with proper correction, we develop a communication-efficient approximate Newton-type method, called Network-DANE, which generalizes DANE to accommodate decentralized scenarios. Our key ideas can be applied, in a systematic manner, to obtain decentralized versions of other master/slave distributed algorithms. A notable development is Network-SVRG/SARAH, which employs variance reduction at each agent to further accelerate local computation. We establish linear convergence of Network-DANE and Network-SVRG for strongly convex losses, and Network-SARAH for quadratic losses, which shed light on the impacts of data homogeneity, network connectivity, and local averaging upon the rate of convergence. We further extend Network-DANE to composite optimization by allowing a nonsmooth penalty term. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency. Our work suggests that by performing a judiciously chosen amount of local communication and computation per iteration, the overall efficiency can be substantially improved.",,,,,"Li, Boyue/ABG-1227-2021",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,180,,,,,,,,,,,,,,,WOS:000570234500001,0
J,"Li, H; Lin, ZC",,,,"Li, Huan; Lin, Zhouchen",,,On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need T(epsilon(-2)) iterations to achieve an epsilon-optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with T(epsilon(-1)) iterations to achieve an 6-optimal dual solution, where T(x) can be x or root x. In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same O (1/root epsilon) order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of O (n log n + root n/epsilon) in both primal space and dual space, where n is the number of samples. Our result takes out the (log 1/epsilon)factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal O (root n/epsilon) iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, e.g., the least absolute deviation and SVM.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000008,0
J,"Salehkaleybar, S; Ghassami, A; Kiyavash, N; Zhang, K",,,,"Salehkaleybar, Saber; Ghassami, AmirEmad; Kiyavash, Negar; Zhang, Kun",,,Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, the inferred causal relationships among the observed variables are often wrong. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among the observed variables. The next question is whether the causal effects can be uniquely identified as well. We show that causal effects among observed variables cannot be identified uniquely under mere assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we are able to propose an efficient method that identifies the set of all possible causal effects that are compatible with the observational data. We present additional structural conditions on the causal graph under which causal effects among observed variables can be determined uniquely. Furthermore, we provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm for learning causal models.",,,,,"Salehkaleybar, Saber/AAQ-3268-2021","Salehkaleybar, Saber/0000-0003-3934-9931",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000014,0
J,"Hofer, CD; Kwitt, R; Niethammer, M",,,,"Hofer, Christoph D.; Kwitt, Roland; Niethammer, Marc",,,Learning Representations of Persistence Barcodes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of supervised learning with summary representations of topological features in data. In particular, we focus on persistent homology, the prevalent tool used in topological data analysis. As the summary representations, referred to as barcodes or persistence diagrams, come in the unusual format of multi sets, equipped with computationally expensive metrics, they can not readily be processed with conventional learning techniques. While different approaches to address this problem have been proposed, either in the context of kernel-based learning, or via carefully designed vectorization techniques, it remains an open problem how to leverage advances in representation learning via deep neural networks. Appropriately handling topological summaries as input to neural networks would address the disadvantage of previous strategies which handle this type of data in a task-agnostic manner. In particular, we propose an approach that is designed to learn a task-specific representation of barcodes. In other words, we aim to learn a representation that adapts to the learning problem while, at the same time, preserving theoretical properties (such as stability). This is done by projecting barcodes into a finite dimensional vector space using a collection of parametrized functionals, so called structure elements, for which we provide a generic construction scheme. A theoretical analysis of this approach reveals sufficient conditions to preserve stability, and also shows that different choices of structure elements lead to great differences with respect to their suitability for numerical optimization. When implemented as a neural network input layer, our approach demonstrates compelling performance on various types of problems, including graph classification and eigenvalue prediction, the classification of 2D/3D object shapes and recognizing activities from EEG signals.",,,,,"Kwitt, Roland/HII-6060-2022; Kwitt, Roland/AFS-8639-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,126,,,,,,,,,,,,,,,WOS:000487068900010,0
J,"Kamper, F; Steel, SJ; du Preez, JA",,,,"Kamper, Francois; Steel, Sarel J.; du Preez, Johan A.",,,On the Convergence of Gaussian Belief Propagation with Nodes of Arbitrary Size,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper is concerned with a multivariate extension of Gaussian message passing applied to pairwise Markov graphs (MGs). Gaussian message passing applied to pairwise MGs is often labeled Gaussian belief propagation (GaBP) and can be used to approximate the marginal of each variable contained in the pairwise MG. We propose a multivariate extension of GaBP (we label this GaBP-m) that can be used to estimate higher-dimensional marginals. Beyond the ability to estimate higher-dimensional marginals, GaBP-m exhibits better convergence behavior than GaBP, and can also provide more accurate univariate marginals. The theoretical results of this paper are based on an extension of the computation tree analysis conducted on univariate nodes to the multivariate case. The main contribution of this paper is the development of a convergence condition for GaBP-m that moves beyond the walk-summability of the precision matrix. Based on this convergence condition, we derived an upper bound for the number of iterations required for convergence of the GaBP-m algorithm. An upper bound on the dissimilarity between the approximate and exact marginal covariance matrices was established. We argue that GaBP-m is robust towards a certain change in variables, a property not shared by iterative solvers of linear systems, such as the conjugate gradient (CG) and preconditioned conjugate gradient (PCG) methods. The advantages of using GaBP-m over GaBP are also illustrated empirically.",,,,,,"Kamper, Francois/0000-0001-9460-5219",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,165,,,,,,,,,,,,,,,WOS:000506403100005,0
J,"Li, B; Wu, YC",,,,"Li, Bin; Wu, Yik-Chung",,,Convergence of Gaussian Belief Propagation Under General Pairwise Factorization: Connecting Gaussian MRF with Pairwise Linear Gaussian Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gaussian belief propagation (BP) is a low-complexity and distributed method for computing the marginal distributions of a high-dimensional joint Gaussian distribution. However, Gaussian BP is only guaranteed to converge in singly connected graphs and may fail to converge in loopy graphs. Therefore, convergence analysis is a core topic in Gaussian BP. Existing conditions for verifying the convergence of Gaussian BP are all tailored for one particular pairwise factorization of the distribution in Gaussian Markov random field (MRF) and may not be valid for another pairwise factorization. On the other hand, convergence conditions of Gaussian BP in pairwise linear Gaussian model are developed independently from those in Gaussian MRF, making the convergence results highly scattered with diverse settings. In this paper, the convergence condition of Gaussian BP is investigated under a general pairwise factorization, which includes Gaussian MRF and pairwise linear Gaussian model as special cases. Upon this, existing convergence conditions in Gaussian MRF are extended to any pairwise factorization. Moreover, the newly established link between Gaussian MRF and pairwise linear Gaussian model reveals an easily verifiable sufficient convergence condition in pairwise linear Gaussian model, which provides a unified criterion for assessing the convergence of Gaussian BP in multiple applications. Numerical examples are presented to corroborate the theoretical results of this paper.",,,,,"Li, Bin/AAB-1223-2019; Li, Bin/J-1649-2014","Li, Bin/0000-0003-3894-9249",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,144,,,,,,,,,,,,,,,WOS:000491132200008,0
J,"Luo, ZY; Sun, DF; Toh, KC; Xiu, NH",,,,"Luo, Ziyan; Sun, Defeng; Toh, Kim-Chuan; Xiu, Naihua",,,Solving the OSCAR and SLOPE Models Using a Semismooth Newton-Based Augmented Lagrangian Method,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The octagonal shrinkage and clustering algorithm for regression (OSCAR), equipped with the l(1)-norm and a pair-wise l(infinity)-norm regularizer, is a useful tool for feature selection and grouping in high-dimensional data analysis. The computational challenge posed by OSCAR, for high dimensional and/or large sample size data, has not yet been well resolved due to the non-smoothness and non-separability of the regularizer involved. In this paper, we successfully resolve this numerical challenge by proposing a sparse semismooth Newton-based augmented Lagrangian method to solve the more general SLOPE (the sorted L-one penalized estimation) model. By appropriately exploiting the inherent sparse and low-rank property of the generalized Jacobian of the semismooth Newton system in the augmented Lagrangian subproblem, we show how the computational complexity can be substantially reduced. Our algorithm offers a notable computational advantage in the high-dimensional statistical regression settings. Numerical experiments are conducted on real data sets, and the results demonstrate that our algorithm is far superior, in both speed and robustness, to the existing state-of-the-art algorithms based on first-order iterative schemes, including the widely used accelerated proximal gradient (APG) method and the alternating direction method of multipliers (ADMM).",,,,,"Sun, Defeng/ABA-7211-2020","Sun, Defeng/0000-0003-0481-272X",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,106,,,,,,,,,,,,,,,WOS:000476623200001,0
J,"Rodrigo, EG; Aledo, JA; Gamez, JA",,,,"Rodrigo, Enrique G.; Aledo, Juan A.; Gamez, Jose A.",,,spark-crowd: A Spark Package for Learning from Crowdsourced Big Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As the data sets increase in size, the process of manually labeling data becomes unfeasible by small groups of experts. Thus, it is common to rely on crowdsourcing platforms which provide inexpensive, but noisy, labels. Although implementations of algorithms to tackle this problem exist, none of them focus on scalability, limiting the area of application to relatively small data sets. In this paper, we present spark-crowd, an Apache Spark package for learning from crowdsourced data with scalability in mind.",,,,,"Aledo, Juan A./N-3631-2014; Gamez, Jose/K-5098-2014","Aledo, Juan A./0000-0003-1786-8087; Gamez, Jose/0000-0003-1188-1117",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,19,,,,,,,,,,,,,,,WOS:000458667200001,0
J,"Shafieezadeh-Abadeh, S; Kuhn, D; Esfahani, PM",,,,"Shafieezadeh-Abadeh, Soroosh; Kuhn, Daniel; Esfahani, Peyman Mohajerin",,,Regularization via Mass Transportation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The goal of regression and classification methods in supervised learning is to minimize the empirical risk, that is, the expectation of some loss function quantifying the prediction error under the empirical distribution. When facing scarce training data, over fitting is typically mitigated by adding regularization terms to the objective that penalize hypothesis complexity. In this paper we introduce new regularization techniques using ideas from distributionally robust optimization, and we give new probabilistic interpretations to existing techniques. Specifically, we propose to minimize the worst-case expected loss, where the worst case is taken over the ball of all (continuous or discrete) distributions that have a bounded transportation distance from the (discrete) empirical distribution. By choosing the radius of this ball judiciously, we can guarantee that the worst-case expected loss provides an upper confidence bound on the loss on test data, thus offering new generalization bounds. We prove that the resulting regularized learning problems are tractable and can be tractably kernelized for many popular loss functions. The proposed approach to regluarization is also extended to neural networks. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,103,,,,,,,,,,,,,,,WOS:000476622400001,0
J,"Tan, KM; Lu, JW; Zhang, T; Liu, H",,,,"Tan, Kean Ming; Lu, Junwei; Zhang, Tong; Liu, Han",,,Layer-Wise Learning Strategy for Nonparametric Tensor Product Smoothing Spline Regression and Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Nonparametric estimation of multivariate functions is an important problem in statistical machine learning with many applications, ranging from nonparametric regression to nonparametric graphical models. Several authors have proposed to estimate multivariate functions under the smoothing spline analysis of variance (SSANOVA) framework, which assumes that the multivariate function can be decomposed into the summation of main effects, two-way interaction effects, and higher order interaction effects. However, existing methods are not scalable to the dimension of the random variables and the order of interactions. We propose a LAyer-wiSE leaRning strategy (LASER) to estimate multivariate functions under the SSANOVA framework. The main idea is to approximate the multivariate function sequentially starting from a model with only the main effects. Conditioned on the support of the estimated main effects, we estimate the two-way interaction effects only when the corresponding main effects are estimated to be non-zero. This process is continued until no more higher order interaction effects are identified. The proposed strategy provides a data-driven approach for estimating multivariate functions under the SSANOVA framework. Our proposal yields a sequence of estimators. To study the theoretical properties of the sequence of estimators, we establish the notion of post-selection persistency. Extensive numerical studies are performed to evaluate the performance of LASER.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,119,,,,,,,,,,,,,,,WOS:000487068900003,0
J,"Unser, M",,,,"Unser, Michael",,,A Representer Theorem for Deep Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose to optimize the activation functions of a deep neural network by adding a corresponding functional regularization to the cost function. We justify the use of a second-order total-variation criterion. This allows us to derive a general representer theorem for deep neural networks that makes a direct connection with splines and sparsity. Specifically, we show that the optimal network configuration can be achieved with activation functions that are nonuniform linear splines with adaptive knots. The bottom line is that the action of each neuron is encoded by a spline whose parameters (including the number of knots) are optimized during the training procedure. The scheme results in a computational structure that is compatible with existing deep-ReLU, parametric ReLU, APL (adaptive piecewise-linear) and MaxOut architectures. It also suggests novel optimization challenges and makes an explicit link with l(1) minimization and sparsity-promoting techniques.",,,,,,"Unser, Michael/0000-0003-1248-2513",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,110,,,,,,,,,,,,,,,WOS:000476623900001,0
J,"Ustun, B; Rudin, C",,,,"Ustun, Berk; Rudin, Cynthia",,,Learning Optimized Risk Scores,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Risk scores are simple classification models that let users make quick risk predictions by adding and subtracting a few small numbers. These models are widely used in medicine and criminal justice, but are difficult to learn from data because they need to be calibrated, sparse, use small integer coefficients, and obey application-specific constraints. In this paper, we introduce a machine learning method to learn risk scores. We formulate the risk score problem as a mixed integer nonlinear program, and present a cutting plane algorithm to recover its optimal solution. We improve our algorithm with specialized techniques that generate feasible solutions, narrow the optimality gap, and reduce data-related computation. Our algorithm can train risk scores in a way that scales linearly in the number of samples in a dataset, and that allows practitioners to address application-specific constraints without parameter tuning or post-processing. We benchmark the performance of different methods to learn risk scores on publicly available datasets, comparing risk scores produced by our method to risk scores built using methods that are used in practice. We also discuss the practical benefits of our method through a real-world application where we build a customized risk score for ICU seizure prediction in collaboration with the Massachusetts General Hospital.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,150,,,,,,,,,,,,,,,WOS:000491132200014,0
J,"Guedj, B; Desikan, BS",,,,"Guedj, Benjamin; Desikan, Bhargav Srinivasa",,,Pycobra: A Python Toolbox for Ensemble Learning and Visualisation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations. pycobra is fully scikit-learn compatible and is released under the MIT open-source license. pycobra can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at https://github.com/bhargavvader/pycobra and official documentation website is https://modal.lille.inria.fr/pycobra.",,,,,"Guedj, Benjamin/AAA-9872-2019","Guedj, Benjamin/0000-0003-1237-7430",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,190,,,,,,,,,,,,,,,WOS:000435450500001,0
J,"Hajek, B; Wu, YH; Xu, JM",,,,"Hajek, Bruce; Wu, Yihong; Xu, Jiaming",,,Submatrix localization via message passing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The principal submatrix localization problem deals with recovering a K x K principal submatrix of elevated mean j in a large n x n symmetric matrix subject to additive standard Gaussian noise, or more generally, mean zero, variance one, subgaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime Omega (root n) <= K <= o(n), the support of the submatrix can be weakly recovered (with o(K) misclassification errors on average) by an optimized message passing algorithm if lambda = mu(2) K-2/n, the signal-to-noise ratio, exceeds 1/e. This extends a result by Deshpande and Montanari previously obtained for K = circle dot(root n) and = mu = circle dot(1). In addition, the algorithm can be combined with a voting procedure to achieve the information-theoretic limit of exact recovery with sharp constants for all K >= n/logn (i/8e + o(1)). The total running time of the algorithm is O(n(2)log n). Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a K-1 x K-2 submatrix of elevated mean mu in a large n(1) x n(2) Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming Omega(root n(i)) <= K-i <= o(n(i)) and K-1 asymptotic to K-2. A sharp informationtheoretic condition for the weak recovery of both clusters is also identified.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,186,,,,,,,,,,,,,,,WOS:000435449600001,0
J,"Oliveira, IFD; Ailon, N; Davidov, O",,,,"Oliveira, Ivo F. D.; Ailon, Nir; Davidov, Ori",,,A New and Flexible Approach to the Analysis of Paired Comparison Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the situation where I items are ranked by paired comparisons. It is usually assumed that the probability that item i is preferred over item j is p(ij) = F(mu(i) - mu(j)) where F is a symmetric distribution function, which we refer to as the comparison function, and mu(i) and mu(j) are the merits or scores of the compared items. This modelling framework, which is ubiquitous in the paired comparison literature, strongly depends on the assumption that the comparison function F is known. In practice, however, this assumption is often unrealistic and may result in poor fit and erroneous inferences. This limitation has motivated us to relax the assumption that F is fully known and simultaneously estimate the merits of the objects and the underlying comparison function. Our formulation yields a flexible semi-definite programming problem that we use as a refinement step for estimating the paired comparison probability matrix. We provide a detailed sensitivity analysis and, as a result, we establish the consistency of the resulting estimators and provide bounds on the estimation and approximation errors. Some statistical properties of the resulting estimators as well as model selection criteria are investigated. Finally, using a large data-set of computer chess matches, we estimate the comparison function and find that the model used by the International Chess Federation does not seem to apply to computer chess.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,60,,,,,,,,,,,,,,,WOS:000452044300001,0
J,"Vural, E; Guillemot, C",,,,"Vural, Elif; Guillemot, Christine",,,A Study of the Classification of Low-Dimensional Data with Supervised Manifold Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets.",,,,,"Vural, Elif/AAZ-9275-2020","Guillemot, Christine/0000-0003-1604-967X",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,1,55,157,,,,,,,,,,,,,,,WOS:000433251800001,0
J,"Balachandran, P; Kolaczyk, ED; Viles, WD",,,,"Balachandran, Prakash; Kolaczyk, Eric D.; Viles, Weston D.",,,On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Our work in this paper is inspired by a statistical observation that is both elementary and broadly relevant to network analysis in practice-that the uncertainty in approximating some true graph G = (V; E) by some estimated graph (G) over cap = (V; (E) over cap) manifests as errors in our knowledge of the presence/absence of edges between vertex pairs, which must necessarily propagate to any estimates of network summaries eta(G) we seek. Motivated by the common practice of using plug-in estimates eta((G) over cap) as proxies for eta(G), our focus is on the problem of characterizing the distribution of the discrepancy D = eta((G) over cap) eta(G), in the case where eta(center dot) is a subgraph count. Specifically, we study the fundamental case where the statistic of interest is vertical bar E vertical bar, the number of edges in G. Our primary contribution in this paper is to show that in the empirically relevant setting of large graphs with low-rate measurement errors, the distribution of D-E = vertical bar(E) over cap vertical bar - vertical bar E vertical bar is well-characterized by a Skellam distribution, when the errors are independent or weakly dependent. Under an assumption of independent errors, we are able to further show conditions under which this characterization is strictly better than that of an appropriate normal distribution. These results derive from our formulation of a general result, quantifying the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by the difference of two independent Poisson random variables, i.e., by a Skellam distribution. This general result is developed through the use of Stein's method, and may be of some general interest. We finish with a discussion of possible extension of our work to subgraph counts eta(G) of higher order.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,61,,,,,,,,,,,,,,,WOS:000405991800001,0
J,"Charles, AS; Yin, D; Rozell, CJ",,,,"Charles, Adam S.; Yin, Dong; Rozell, Christopher J.",,,Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low-dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs.",,,,,"Charles, Adam/O-1533-2019","Charles, Adam/0000-0002-9045-3489",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000397019400001,0
J,"Raff, E",,,,"Raff, Edward",,,"JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abed et al., 2009). Almost all of the algorithms are independently implemented using an Object-Oriented framework. JSAT is made available under the GNU GPL license here: https://github.com/EdwardRaff/JSAT.",,,,,,"Raff, Edward/0000-0002-9900-1972",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,23,,,,,,,,,,,,,,,WOS:000399841800001,0
J,"Shi, TL; Zhu, J",,,,"Shi, Tianlin; Zhu, Jun",,,Online Bayesian Passive-Aggressive Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive-Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,33,,,,,,,,,,,,,,,WOS:000400517000001,0
J,"Wang, Y; Yu, AW; Singh, A",,,,"Wang, Yining; Yu, Adams Wei; Singh, Aarti",,,On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We derive computationally tractable methods to select a small subset of experiment settings from a large pool of given design points. The primary focus is on linear regression models, while the technique extends to generalized linear models and Delta's method (estimating functions of linear regression models) as well. The algorithms are based on a continuous relaxation of an otherwise intractable combinatorial optimization problem, with sampling or greedy procedures as post-processing steps. Formal approximation guarantees are established for both algorithms, and numerical results on both synthetic and real-world data confirm the effectiveness of the proposed methods.",,,,,,"Wang, Yining/0000-0001-9410-0392",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,143,,,,,,,,,,,,,,,WOS:000424549200001,0
J,"Arlot, S; Lerasle, M",,,,"Arlot, Sylvain; Lerasle, Matthieu",,,Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies V-fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing V in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for V-fold cross-validation and its bias-corrected version (V-fold penalization). In particular, this result implies that V-fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of V-fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on V like 1 + 4/(V 1), at least in some particular cases, suggesting that the performance increases much from V - 2 to V - 5 or 10, and then is almost constant. Overall, this can explain the common advice to take V - 5 - at least in our setting and when the computational power is limited-, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte-Carlo cross-validation, also known as repeated cross-validation, where the parameter V is replaced by the number B of random splits of the data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,208,,,,,,,,,,,,,,,WOS:000391833100001,0
J,"Lin, JH; Basu, S; Banerjee, M; Michailidis, G",,,,"Lin, Jiahe; Basu, Sumanta; Banerjee, Moulinath; Michailidis, George",,,Penalized Maximum Likelihood Estimation of Multi-layered Gaussian Graphical Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Analyzing multi-layered graphical models provides insight into understanding the conditional relationships among nodes within layers after adjusting for and quantifying the effects of nodes from other layers. We obtain the penalized maximum likelihood estimator for Gaussian multi-layered graphical models, based on a computational approach involving screening of variables, iterative estimation of the directed edges between layers and undirected edges within layers and a final refitting and stability selection step that provides improved performance in finite sample settings. We establish the consistency of the estimator in a high-dimensional setting. To obtain this result, we develop a strategy that leverages the biconvexity of the likelihood function to ensure convergence of the developed iterative algorithm to a stationary point, as well as careful uniform error control of the estimates over iterations. The performance of the maximum likelihood estimator is illustrated on synthetic data.",,,,,,"Lin, Jiahe/0000-0001-9523-0981",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,146,,,,,,,,,,,,,,,WOS:000391662000001,0
J,"Wang, ZT; Jin, C; Fan, K; Zhang, JQ; Huang, JL; Zhong, YQ; Wang, LW",,,,"Wang, Ziteng; Jin, Chi; Fan, Kai; Zhang, Jiaqi; Huang, Junliang; Zhong, Yiqiao; Wang, Liwei",,,Differentially Private Data Releasing for Smooth Queries,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the past few years, differential privacy has become a standard concept in the area of privacy. One of the most important problems in this field is to answer queries while preserving differential privacy. In spite of extensive studies, most existing work on differentially private query answering assumes the data are discrete (i.e., in {0, 1}(d)) and focuses on queries induced by Boolean functions. In real applications however, continuous data are at least as common as binary data. Thus, in this work we explore a less studied topic, namely, differential privately query answering for continuous data with continuous function. As a first step towards the continuous case, we study a natural class of linear queries on continuous data which we refer to as smooth queries. A linear query is said to be K-smooth if it is specified by a function de fined on [1; 1](d) whose partial derivatives up to order K are all bounded. We develop two is an element of-differentially private mechanisms which are able to answer all smooth queries. The first mechanism outputs a summary of the database and can then give answers to the queries. The second mechanism is an improvement of the first one and it outputs a synthetic database. The two mechanisms both achieve an accuracy of O(n-K/2d+K/is an element of). Here we assume that the dimension d is a constant. It turns out that even in this parameter setting (which is almost trivial in the discrete case), using existing discrete mechanisms to answer the smooth queries is difficult and requires more noise. Our mechanisms are based on L-infinity-approximation of (transformed) smooth functions by low-degree even trigonometric polynomials with uniformly bounded coefficients. We also develop practically efficient variants of the mechanisms with promising experimental results.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,1,42,51,,,,,,,,,,,,,,,WOS:000391486400001,0
J,"Zbontar, J; LeCun, Y",,,,"Zbontar, Jure; LeCun, Yann",,,Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,65,,,,,,,,,,,,,,,WOS:000391522500001,0
J,"Li, CL; Su, YC; Lin, TW; Tsai, CH; Chang, WC; Huang, KH; Kuo, TM; Lin, SW; Lin, YS; Lu, YC; Yang, CP; Chang, CX; Chin, WS; Juan, YC; Tung, HY; Wang, JP; Wei, CK; Wu, F; Yin, TC; Yu, T; Zhuang, Y; Lin, SD; Lin, HT; Lin, CJ",,,,"Li, Chun-Liang; Su, Yu-Chuan; Lin, Ting-Wei; Tsai, Cheng-Hao; Chang, Wei-Cheng; Huang, Kuan-Hao; Kuo, Tzu-Ming; Lin, Shan-Wei; Lin, Young-San; Lu, Yu-Chen; Yang, Chun-Pai; Chang, Cheng-Xia; Chin, Wei-Sheng; Juan, Yu-Chin; Tung, Hsiao-Yu; Wang, Jui-Pin; Wei, Cheng-Kuang; Wu, Felix; Yin, Tu-Chun; Yu, Tong; Zhuang, Yong; Lin, Shou-de; Lin, Hsuan-Tien; Lin, Chih-Jen",,,Combination of Feature Engineering and Ranking Models for Paper-Author Identification in KDD Cup 2013,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. The track 1 in KDD Cup 2013 considers the paper-author identification problem, which is to identify whether a paper is truly written by an author. First, we conduct feature engineering to transform the various types of provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of the Test set.",,,,,"Lin, Hsuan-Tien/AAE-4359-2020; Lin, Young-San/HDO-4676-2022; Zhuang, Yong/AAM-1728-2020","Lin, Chih-Jen/0000-0003-4684-8747; LIN, SHOU-DE/0000-0001-9970-1250; Lin, Hsuan-Tien/0000-0003-2968-0671",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2921,2947,,,,,,,,,,,,,,,,WOS:000369888000021,0
J,"Marusic, I; Worrell, J",,,,"Marusic, Ines; Worrell, James",,,Complexity of Equivalence and Learning for Multiplicity Tree Automata,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the query and computational complexity of learning multiplicity tree automata in Angluin's exact learning model. In this model, there is an oracle, called the Teacher, that can answer membership and equivalence queries posed by the Learner. Motivated by this feature, we first characterise the complexity of the equivalence problem for multiplicity tree automata, showing that it is logspace equivalent to polynomial identity testing. We then move to query complexity, deriving lower bounds on the number of queries needed to learn multiplicity tree automata over both fixed and arbitrary fields. In the latter case, the bound is linear in the size of the target automaton. The best known upper bound on the query complexity over arbitrary fields derives from an algorithm of Habrard and Oncina (2006), in which the number of queries is proportional to the size of the target automaton and the size of a largest counterexample, represented as a tree, that is returned by the Teacher. However, a smallest counterexample tree may already be exponential in the size of the target automaton. Thus the above algorithm has query complexity exponentially larger than our lower bound, and does not run in time polynomial in the size of the target automaton. We give a new learning algorithm for multiplicity tree automata in which counterexamples to equivalence queries are represented as DAGs. The query complexity of this algorithm is quadratic in the target automaton size and linear in the size of a largest counterexample. In particular, if the Teacher always returns DAG counterexamples of minimal size then the query complexity is quadratic in the target automaton size-almost matching the lower bound, and improving the best previously-known algorithm by an exponential factor.",,,,,,"Worrell, James/0000-0001-8151-2443",,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2465,2500,,,,,,,,,,,,,,,,WOS:000369888000005,0
J,"Watanabe, K; Roos, T",,,,"Watanabe, Kazuho; Roos, Teemu",,,Achievability of Asymptotic Minimax Regret by Horizon-Dependent and Horizon-Independent Strategies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The normalized maximum likelihood distribution achieves minimax coding (log-loss) regret given a fixed sample size, or horizon, n. It generally requires that n be known in advance. Furthermore, extracting the sequential predictions from the normalized maximum likelihood distribution is computationally infeasible for most statistical models. Several computationally feasible alternative strategies have been devised. We characterize the achievability of asymptotic minimaxity by horizon-dependent and horizon-independent strategies. We prove that no horizon-independent strategy can be asymptotically minimax in the multinomial case. A weaker result is given in the general case subject to a condition on the horizon-dependence of the normalized maximum likelihood. Motivated by these negative results, we demonstrate that an easily implementable Bayes mixture based on a conjugate Dirichlet prior with a simple dependency on n achieves asymptotic minimaxity for all sequences, simplifying earlier similar proposals. Our numerical experiments for the Bernoulli model demonstrate improved finite-sample performance by a number of novel horizon-dependent and horizon-independent algorithms.",,,,,,"Watanabe, Kazuho/0000-0001-6357-5141; Roos, Teemu/0000-0001-9470-3759",,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2357,2375,,,,,,,,,,,,,,,,WOS:000369887600007,0
J,"Zhang, J; Liu, C",,,,"Zhang, Jian; Liu, Chao",,,On Linearly Constrained Minimum Variance Beamforming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Beamforming is a widely used technique for source localization in signal processing and neuroimaging. A number of vector-beamformers have been introduced to localize neuronal activity by using magnetoencephalography (MEG) data in the literature. However, the existing theoretical analyses on these beamformers have been limited to simple cases, where no more than two sources are allowed in the associated model and the theoretical sensor covariance is also assumed known. The information about the effects of the MEG spatial and temporal dimensions on the consistency of vector-beamforming is incomplete. In the present study, we consider a class of vector-beamformers defined by thresholding the sensor covariance matrix, which include the standard vector-beamformer as a special case. A general asymptotic theory is developed for these vector-beamformers, which shows the extent of effects to which the MEG spatial and temporal dimensions on estimating the neuronal activity index. The performances of the proposed beamformers are assessed by simulation studies. Superior performances of the proposed beamformers are obtained when the signal-to-noise ratio is low. We apply the proposed procedure to real MEG data sets derived from five sessions of a human face-perception experiment, finding several highly active areas in the brain. A good agreement between these findings and the known neurophysiology of the MEG response to human face perception is shown.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2015,16,,,,,,2099,2145,,,,,,,,,,,,,,,,WOS:000369887600001,0
J,"Miller, JW; Harrison, MT",,,,"Miller, Jeffrey W.; Harrison, Matthew T.",,,Inconsistency of Pitman-Yor Process Mixtures for the Number of Components,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians).",,,,,,"Miller, Jeffrey/0000-0001-7718-1581",,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3333,3370,,,,,,,,,,,,,,,,WOS:000344638800016,0
J,"Wu, JX; Cheng, J",,,,"Wu, Jiaxiang; Cheng, Jian",,,Bayesian Co-Boosting for Multi-modal Gesture Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"With the development of data acquisition equipment, more and more modalities become available for gesture recognition. However, there still exist two critical issues for multimodal gesture recognition: how to select discriminative features for recognition and how to fuse features from different modalities. In this paper, we propose a novel Bayesian Co-Boosting framework for multi-modal gesture recognition. Inspired by boosting learning and co-training method, our proposed framework combines multiple collaboratively trained weak classifiers to construct the final strong classifier for the recognition task. During each iteration round, we randomly sample a number of feature subsets and estimate weak classifier's parameters for each subset. The optimal weak classifier and its corresponding feature subset are retained for strong classifier construction. Furthermore, we define an upper bound of training error and derive the update rule of instance's weight, which guarantees the error upper bound to be minimized through iterations. For demonstration, we present an implementation of our framework using hidden Markov models as weak classifiers. We perform extensive experiments using the ChaLearn MMGR and ChAirGest data sets, in which our approach achieves 97.63% and 96.53% accuracy respectively on each publicly available data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2014,15,,,,,,3013,3036,,,,,,,,,,,,,,,,WOS:000344638800006,0
J,"Clark, A",,,,"Clark, Alexander",,,Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially infinite, set of strings generated by some target grammar. Here we define the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modification of Gold's identification in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3537,3559,,,,,,,,,,,,,,,,WOS:000335457100002,0
J,"Tong, X",,,,"Tong, Xin",,,A Plug-in Approach to Neyman-Pearson Classification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The Neyman-Pearson (NP) paradigm in binary classification treats type I and type II errors with different priorities. It seeks classifiers that minimize type II error, subject to a type I error constraint under a user specified level a. In this paper, plug-in classifiers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classifiers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classifiers handle different sampling schemes. This work focuses on theoretical properties of the proposed classifiers; in particular, we derive oracle inequalities that can be viewed as finite sample versions of risk bounds. NP classification can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a specific form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2013,14,,,,,,3011,3040,,,,,,,,,,,,,,,,WOS:000328603600003,0
J,"Picard, D; Thome, N; Cord, M",,,,"Picard, David; Thome, Nicolas; Cord, Matthieu",,,JKernelMachines: A Simple Framework for Kernel Machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classification performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning).",,,,,"Picard, David/AAV-8841-2021","Picard, David/0000-0002-6296-4222",,,,,,,,,,,,,1532-4435,,,,,MAY,2013,14,,,,,,1417,1421,,,,,,,,,,,,,,,,WOS:000320709300007,0
J,"Niehren, J; Champavere, J; Lemay, A; Gilleron, R",,,,"Niehren, Joachim; Champavere, Jerome; Lemay, Aurelien; Gilleron, Remi",,,Query Induction with Schema-Guided Pruning Strategies,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Inference algorithms for tree automata that define node selecting queries in unranked trees rely on tree pruning strategies. These impose additional assumptions on node selection that are needed to compensate for small numbers of annotated examples. Pruning-based heuristics in query learning algorithms for Web information extraction often boost the learning quality and speed up the learning process. We will distinguish the class of regular queries that are stable under a given schema-guided pruning strategy, and show that this class is learnable with polynomial time and data. Our learning algorithm is obtained by adding pruning heuristics to the traditional learning algorithm for tree automata from positive and negative examples. While justified by a formal learning model, our learning algorithm for stable queries also performs very well in practice of XML information extraction.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2013,14,,,,,,927,964,,,,,,,,,,,,,,,,WOS:000318590500007,0
J,"Mukherjee, I; Schapire, RE",,,,"Mukherjee, Indraneel; Schapire, Robert E.",,,A Theory of Multiclass Boosting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Boosting combines weak classifiers to form highly accurate predictors. Although the case of binary classification is well understood, in the multiclass setting, the correct requirements on the weak classifier, or the notion of the most efficient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classifier, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,437,497,,,,,,,,,,,,,,,,WOS:000315981900006,0
J,"De Brabanter, K; De Brabanter, J; De Moor, B; Gijbels, I",,,,"De Brabanter, Kris; De Brabanter, Jos; De Moor, Bart; Gijbels, Irene",,,Derivative Estimation with Local Polynomial Fitting,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a fully automated framework to estimate derivatives nonparametrically without estimating the regression function. Derivative estimation plays an important role in the exploration of structures in curves (jump detection and discontinuities), comparison of regression curves, analysis of human growth data, etc. Hence, the study of estimating derivatives is equally important as regression estimation itself. Via empirical derivatives we approximate the qth order derivative and create a new data set which can be smoothed by any nonparametric regression estimator. We derive L-1 and L-2 rates and establish consistency of the estimator. The new data sets created by this technique are no longer independent and identically distributed (i.i.d.) random variables anymore. As a consequence, automated model selection criteria (data-driven procedures) break down. Therefore, we propose a simple factor method, based on bimodal kernels, to effectively deal with correlated data in the local polynomial regression framework.",,,,,"Gijbels, Irene/AAI-1699-2019","Gijbels, Irene/0000-0002-4443-9803",,,,,,,,,,,,,1532-4435,,,,,JAN,2013,14,,,,,,281,301,,,,,,,,,,,,,,,,WOS:000314530200009,0
J,"Rieck, K; Wressnegger, C; Bikadorov, A",,,,"Rieck, Konrad; Wressnegger, Christian; Bikadorov, Alexander",,,Sally: A Tool for Embedding Strings in Vector Spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Strings and sequences are ubiquitous in many areas of data analysis. However, only few learning methods can be directly applied to this form of data. We present Sally, a tool for embedding strings in vector spaces that allows for applying a wide range of learning methods to string data. Sally implements a generalized form of the bag-of-words model, where strings are mapped to a vector space that is spanned by a set of string features, such as words or n-grams of words. The implementation of Sally builds on efficient string algorithms and enables processing millions of strings and features. The tool supports several data formats and is capable of interfacing with common learning environments, such as Weka, Shogun, Matlab, or Pylab. Sally has been successfully applied for learning with natural language text, DNA sequences and monitored program behavior.",,,,,,"Rieck, Konrad/0000-0002-5054-8758",,,,,,,,,,,,,1532-4435,,,,,NOV,2012,13,,,,,,3247,3251,,,,,,,,,,,,,,,,WOS:000313200200004,0
J,"Hauser, A; Buhlmann, P",,,,"Hauser, Alain; Buehlmann, Peter",,,Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph ( also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study.",,,,,"B√ºhlmann, Peter/A-2107-2013","B√ºhlmann, Peter/0000-0002-1782-6015; Hauser, Alain/0000-0002-1553-5812",,,,,,,,,,,,,1532-4435,,,,,AUG,2012,13,,,,,,2409,2464,,,,,,,,,,,,,,,,WOS:000308795200006,0
J,"Tamar, A; Di Castro, D; Meir, R",,,,"Tamar, Aviv; Di Castro, Dotan; Meir, Ron",,,Integrating a Partial Model into Model Free Reinforcement Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In reinforcement learning an agent uses online feedback from the environment in order to adaptively select an effective policy. Model free approaches address this task by directly mapping environmental states to actions, while model based methods attempt to construct a model of the environment, followed by a selection of optimal actions based on that model. Given the complementary advantages of both approaches, we suggest a novel procedure which augments a model free algorithm with a partial model. The resulting hybrid algorithm switches between a model based and a model free mode, depending on the current state and the agent's knowledge. Our method relies on a novel definition for a partially known model, and an estimator that incorporates such knowledge in order to reduce uncertainty in stochastic approximation iterations. We prove that such an approach leads to improved policy evaluation whenever environmental knowledge is available, without compromising performance when such knowledge is absent. Numerical simulations demonstrate the effectiveness of the approach on policy gradient and Q-learning algorithms, and its usefulness in solving a call admission control problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,1927,1966,,,,,,,,,,,,,,,,WOS:000307020700007,0
J,"Huang, GB; Kae, A; Doersch, C; Learned-Miller, E",,,,"Huang, Gary B.; Kae, Andrew; Doersch, Carl; Learned-Miller, Erik",,,Bounding the Probability of Error for High Precision Optical Character Recognition,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with near certainty, they can be conditioned upon, allowing further inference to be done efficiently. Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This clean set is subsequently used as document-specific training data. While OCR systems produce confidence measures for the identity of each letter or word, thresholding these values still produces a significant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difficult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1% from an initial OCR system's translation.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2012,13,,,,,,363,387,,,,,,,,,,,,,,,,WOS:000303046000005,0
J,"Wang, HX; Shen, XT; Pan, W",,,,"Wang, Huixin; Shen, Xiaotong; Pan, Wei",,,Large Margin Hierarchical Classification with Mutually Exclusive Class Membership,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In hierarchical classification, class labels are structured, that is each label value corresponds to one non-root node in a tree, where the inter-class relationship for classification is specified by directed paths of the tree. In such a situation, the focus has been on how to leverage the interclass relationship to enhance the performance of flat classification, which ignores such dependency. This is critical when the number of classes becomes large relative to the sample size. This paper considers single-path or partial-path hierarchical classification, where only one path is permitted from the root to a leaf node. A large margin method is introduced based on a new concept of generalized margins with respect to hierarchy. For implementation, we consider support vector machines and psi-learning. Numerical and theoretical analyses suggest that the proposed method achieves the desired objective and compares favorably against strong competitors in the literature, including its flat counterparts. Finally, an application to gene function prediction is discussed. Keywords: difference convex programming, gene function annotation, margins, multi-class classification, structured learning",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2011,12,,,,,,2721,2748,,,,,,,,,,,,,,,,WOS:000298102900006,0
J,"De Brabanter, K; De Brabanter, J; Suykens, JAK; De Moor, B",,,,"De Brabanter, Kris; De Brabanter, Jos; Suykens, Johan A. K.; De Moor, Bart",,,Kernel Regression in the Presence of Correlated Errors,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is a well-known problem that obtaining a correct bandwidth and/or smoothing parameter in non-parametric regression is difficult in the presence of correlated errors. There exist a wide variety of methods coping with this problem, but they all critically depend on a tuning procedure which requires accurate information about the correlation structure. We propose a bandwidth selection procedure based on bimodal kernels which successfully removes the correlation without requiring any prior knowledge about its structure and its parameters. Further, we show that the form of the kernel is very important when errors are correlated which is in contrast to the independent and identically distributed (i.i.d.) case. Finally, some extensions are proposed to use the proposed criterion in support vector machines and least squares support vector machines for regression.",,,,,"Suykens, Johan A.K./C-9781-2014","Suykens, Johan A.K./0000-0002-8846-6352",,,,,,,,,,,,,1532-4435,,,,,JUN,2011,12,,,,,,1955,1976,,,,,,,,,,,,,,,,WOS:000293757200006,0
J,"Bubeck, S; Munos, R; Stoltz, G; Szepesvari, C",,,,"Bubeck, Sebastien; Munos, Remi; Stoltz, Gilles; Szepesvari, Csaba",,,X-Armed Bandits,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a generalization of stochastic bandits where the set of arms, X, is allowed to be a generic measurable space and the mean-payoff function is locally Lipschitz with respect to a dissimilarity function that is known to the decision maker. Under this condition we construct an arm selection policy, called HOO (hierarchical optimistic optimization), with improved regret bounds compared to previous results for a large class of problems. In particular, our results imply that if X is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally continuous with a known smoothness degree, then the expected regret of HOO is bounded up to a logarithmic factor by root n, that is, the rate of growth of the regret is independent of the dimension of the space. We also prove the minimax optimality of our algorithm when the dissimilarity is a metric. Our basic strategy has quadratic computational complexity as a function of the number of time steps and does not rely on the doubling trick. We also introduce a modified strategy, which relies on the doubling trick but runs in linearithmic time. Both results are improvements with respect to previous approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2011,12,,,,,,1655,1695,,,,,,,,,,,,,,,,WOS:000292304000007,0
J,"McAuley, JJ; Caetano, TS",,,,"McAuley, Julian J.; Caetano, Tiberio S.",,,Faster Algorithms for Max-Product Message-Passing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Maximum A Posteriori inference in graphical models is often solved via message-passing algorithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this problem is well-known to be exponential in the size of the maximal cliques of the triangulated model, while approximate inference is typically exponential in the size of the model's factors. In this paper, we take advantage of the fact that many models have maximal cliques that are larger than their constituent factors, and also of the fact that many factors consist only of latent variables (i.e., they do not depend on an observation). This is a common case in a wide variety of applications that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We demonstrate that message-passing operations in such models are equivalent to some variant of matrix multiplication in the tropical semiring, for which we offer an O(N-2.5) expected-case solution.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2011,12,,,,,,1349,1388,,,,,,,,,,,,,,,,WOS:000290096100006,0
J,"Omlor, L; Giese, MA",,,,"Omlor, Lars; Giese, Martin A.",,,Anechoic Blind Source Separation Using Wigner Marginals,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Blind source separation problems emerge in many applications, where signals can be modeled as superpositions of multiple sources. Many popular applications of blind source separation are based on linear instantaneous mixture models. If specific invariance properties are known about the sources, for example, translation or rotation invariance, the simple linear model can be extended by inclusion of the corresponding transformations. When the sources are invariant against translations (spatial displacements or time shifts) the resulting model is called an anechoic mixing model. We present a new algorithmic framework for the solution of anechoic problems in arbitrary dimensions. This framework is derived from stochastic time-frequency analysis in general, and the marginal properties of the Wigner-Ville spectrum in particular. The method reduces the general anechoic problem to a set of anechoic problems with non-negativity constraints and a phase retrieval problem. The first type of subproblem can be solved by existing algorithms, for example by an appropriate modification of non-negative matrix factorization (NMF). The second subproblem is solved by established phase retrieval methods. We discuss and compare implementations of this new algorithmic framework for several example problems with synthetic and real-world data, including music streams, natural 2D images, human motion trajectories and two-dimensional shapes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAR,2011,12,,,,,,1111,1148,,,,,,,,,,,,,,,,WOS:000289635000011,0
J,"Ni, YZ; Saunders, C; Szedmak, S; Niranjan, M",,,,"Ni, Yizhao; Saunders, Craig; Szedmak, Sandor; Niranjan, Mahesan",,,Exploitation of Machine Learning Techniques in Modelling Phrase Movements for Machine Translation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a distance phrase reordering model (DPR) for statistical machine translation (SMT), where the aim is to learn the grammatical rules and context dependent changes using a phrase reordering classification framework. We consider a variety of machine learning techniques, including state-of-the-art structured prediction methods. Techniques are compared and evaluated on a Chinese-English corpus, a language pair known for the high reordering characteristics which cannot be adequately captured with current models. In the reordering classification task, the method significantly outperforms the baseline against which it was tested, and further, when integrated as a component of the state-of-the-art machine translation system, MOSES, it achieves improvement in translation results.",,,,,,"Niranjan, Mahesan/0000-0001-7021-140X; Ni, Yizhao/0000-0001-8599-454X",,,,,,,,,,,,,1532-4435,,,,,JAN,2011,12,,,,,,1,30,,,,,,,,,,,,,,,,WOS:000287938500001,0
J,"Cohen, SB; Smith, NA",,,,"Cohen, Shay B.; Smith, Noah A.",,,Covariance in Unsupervised Learning of Probabilistic Grammars,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text. Their symbolic component is amenable to inspection by humans, while their probabilistic component helps resolve ambiguity. They also permit the use of well-understood, general-purpose learning algorithms. There has been an increased interest in using probabilistic grammars in the Bayesian setting. To date, most of the literature has focused on using a Dirichlet prior. The Dirichlet prior has several limitations, including that it cannot directly model covariance between the probabilistic grammar's parameters. Yet, various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties. In this paper, we suggest an alternative to the Dirichlet prior, a family of logistic normal distributions. We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction, demonstrating performance improvements with our priors on a set of six tree-banks in different natural languages. Our covariance framework permits soft parameter tying within grammars and across grammars for text in different languages, and we show empirical gains in a novel learning setting using bilingual, non-parallel data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3017,3051,,,,,,,,,,,,,,,,WOS:000285643600003,0
J,"Rasmussen, CE; Nickisch, H",,,,"Rasmussen, Carl Edward; Nickisch, Hannes",,,Gaussian Processes for Machine Learning (GPML) Toolbox,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.",,,,,"Nickisch, Hannes/I-7049-2017","Nickisch, Hannes/0000-0003-1604-6647; Rasmussen, Carl Edward/0000-0001-8899-7850",,,,,,,,,,,,,1532-4435,,,,,NOV,2010,11,,,,,,3011,3015,,,,,,,,,,,,,,,,WOS:000285643600002,0
J,"Laparra, V; Gutierrez, J; Camps-Valls, G; Malo, J",,,,"Laparra, Valero; Gutierrez, Juan; Camps-Valls, Gustavo; Malo, Jesus",,,Image Denoising with Kernels Based on Natural Image Relations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A successful class of image denoising methods is based on Bayesian approaches working in wavelet representations. The performance of these methods improves when relations among the local frequency coefficients are explicitly included. However, in these techniques, analytical estimates can be obtained only for particular combinations of analytical models of signal and noise, thus precluding its straightforward extension to deal with other arbitrary noise sources. In this paper, we propose an alternative non-explicit way to take into account the relations among natural image wavelet coefficients for denoising: we use support vector regression (SVR) in the wavelet domain to enforce these relations in the estimated signal. Since relations among the coefficients are specific to the signal, the regularization property of SVR is exploited to remove the noise, which does not share this feature. The specific signal relations are encoded in an anisotropic kernel obtained from mutual information measures computed on a representative image database. In the proposed scheme, training considers minimizing the Kullback-Leibler divergence (KLD) between the estimated and actual probability functions (or histograms) of signal and noise in order to enforce similarity up to the higher (computationally estimable) order. Due to its non-parametric nature, the method can eventually cope with different noise sources without the need of an explicit re-formulation, as it is strictly necessary under parametric Bayesian formalisms. Results under several noise levels and noise sources show that: (1) the proposed method outperforms conventional wavelet methods that assume coefficient independence, (2) it is similar to state-of-the-art methods that do explicitly include these relations when the noise source is Gaussian, and (3) it gives better numerical and visual performance when more complex, realistic noise sources are considered. Therefore, the proposed machine learning approach can be seen as a more flexible (model-free) alternative to the explicit description of wavelet coefficient relations for image denoising.",,,,,"Laparra, Valero/I-2705-2015; Gutierr√©z-Aguado, Juan/L-8202-2014; Camps-Valls, Gustavo/A-2532-2011; Laparra, Valero/AAD-1937-2019; Malo, Jesus/K-9235-2017; , Gustavo/ABC-1706-2022","Laparra, Valero/0000-0001-7531-9890; Gutierr√©z-Aguado, Juan/0000-0001-5527-8091; Camps-Valls, Gustavo/0000-0003-1683-2138; Laparra, Valero/0000-0001-7531-9890; Malo, Jesus/0000-0002-5684-8591; ",,,,,,,,,,,,,1532-4435,,,,,FEB,2010,11,,,,,,873,903,,,,,,,,,,,,,,,,WOS:000277186500017,0
J,"Di Castro, D; Meir, R",,,,"Di Castro, Dotan; Meir, Ron",,,A Convergent Online Single Time Scale Actor Critic Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Actor-Critic based approaches were among the first to address reinforcement learning in a general setting. Recently, these algorithms have gained renewed interest due to their generality, good convergence properties, and possible biological relevance. In this paper, we introduce an online temporal difference based actor-critic algorithm which is proved to converge to a neighborhood of a local maximum of the average reward. Linear function approximation is used by the critic in order estimate the value function, and the temporal difference signal, which is passed from the critic to the actor. The main distinguishing feature of the present convergence proof is that both the actor and the critic operate on a similar time scale, while in most current convergence proofs they are required to have very different time scales in order to converge. Moreover, the same temporal difference signal is used to update the parameters of both the actor and the critic. A limitation of the proposed approach, compared to results available for two time scale convergence, is that convergence is guaranteed only to a neighborhood of an optimal value, rather to an optimal value itself. The single time scale and identical temporal difference signal used by the actor and the critic, may provide a step towards constructing more biologically realistic models of reinforcement learning in the brain.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2010,11,,,,,,367,410,,,,,,,,,,,,,,,,WOS:000277186400011,0
J,"Klivans, AR; Long, PM; Servedio, RA",,,,"Klivans, Adam R.; Long, Philip M.; Servedio, Rocco A.",,,Learning Halfspaces with Malicious Noise,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We give new algorithms for learning halfspaces in the challenging malicious noise model, where an adversary may corrupt both the labels and the underlying distribution of examples. Our algorithms can tolerate malicious noise rates exponentially larger than previous work in terms of the dependence on the dimension n, and succeed for the fairly broad class of all isotropic log-concave distributions. We give poly(n, 1/epsilon)-time algorithms for solving the following problems to accuracy epsilon: Learning origin-centered halfspaces in R-n with respect to the uniform distribution on the unit ball with malicious noise rate eta = Omega(epsilon(2)/log(n/epsilon)). (The best previous result was Omega(epsilon/(nlog(n/epsilon))(1/4)).) Learning origin-centered halfspaces with respect to any isotropic log-concave distribution on R-n with malicious noise rate eta = Omega(epsilon(3)/log(2)(n/epsilon)). This is the first efficient algorithm for learning under isotropic log-concave distributions in the presence of malicious noise. We also give a poly(n, 1/epsilon)-time algorithm for learning origin-centered halfspaces under any isotropic log-concave distribution on R-n in the presence of adversarial label noise at rate eta - Omega(epsilon(3)/log(1/epsilon)). In the adversarial label noise setting (or agnostic model), labels can be noisy, but not example points themselves. Previous results could handle eta = W(epsilon) but had running time exponential in an unspecified function of 1/epsilon. Our analysis crucially exploits both concentration and anti-concentration properties of isotropic log-concave distributions. Our algorithms combine an iterative outlier removal procedure using Principal Component Analysis together with smooth boosting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2009,10,,,,,,2715,2740,,,,,,,,,,,,,,,,WOS:000273877300001,0
J,"Orabona, F; Keshet, J; Caputo, B",,,,"Orabona, Francesco; Keshet, Joseph; Caputo, Barbara",,,Bounded Kernel-Based Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the first online bounded algorithm that can learn complex classification tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA(2). Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy.",,,,,"Orabona, Francesco/G-6758-2012; Caputo, Barbara/F-3928-2011; Caputo, Barbara/J-8976-2015","Caputo, Barbara/0000-0001-7169-0158; orabona, francesco/0000-0001-8523-6845; Keshet, Joseph/0000-0003-2332-5783",,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2643,2666,,,,,,,,,,,,,,,,WOS:000272346600009,0
J,"Zhu, J; Xing, EP",,,,"Zhu, Jun; Xing, Eric P.",,,Maximum Entropy Discrimination Markov Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The standard maximum margin approach for structured prediction lacks a straightforward probabilistic interpretation of the learning scheme and the prediction rule. Therefore its unique advantages such as dual sparseness and kernel tricks cannot be easily conjoined with the merits of a probabilistic model such as Bayesian regularization, model averaging, and ability to model hidden variables. In this paper, we present a new general framework called maximum entropy discrimination Markov networks (MaxEnDNet, or simply, MEDN), which integrates these two approaches and combines and extends their merits. Major innovations of this approach include: 1) It extends the conventional max-entropy discrimination learning of classification rules to a new structural max-entropy discrimination paradigm of learning a distribution of Markov networks. 2) It generalizes the extant Markov network structured-prediction rule based on a point estimator of model coefficients to an averaging model akin to a Bayesian predictor that integrates over a learned posterior distribution of model coefficients. 3) It admits flexible entropic regularization of the model during learning. By plugging in different prior distributions of the model coefficients, it subsumes the well-known maximum margin Markov networks ((MN)-N-3) as a special case, and leads to a model similar to an L-1-regularized (MN)-N-3 that is simultaneously primal and dual sparse, or other new types of Markov networks. 4) It applies a modular learning algorithm that combines existing variational inference techniques and convex-optimization based (MN)-N-3 solvers as subroutines. Essentially, MEDN can be understood as a jointly maximum likelihood and maximum margin estimate of Markov network. It represents the first successful attempt to combine maximum entropy learning (a dual form of maximum likelihood learning) with maximum margin learning of Markov network for structured input/output problems; and the basic principle can be generalized to learning arbitrary graphical models, such as the generative Bayesian networks or models with structured hidden variables. We discuss a number of theoretical properties of this approach, and show that empirically it outperforms a wide array of competing methods for structured input/output learning on both synthetic and real OCR and web data extraction data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2009,10,,,,,,2531,2569,,,,,,,,,,,,,,,,WOS:000272346600005,0
J,"Liu, H; Lafferty, J; Wasserman, L",,,,"Liu, Han; Lafferty, John; Wasserman, Larry",,,The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Recent methods for estimating sparse undirected graphs for real-valued data in high dimensional problems rely heavily on the assumption of normality. We show how to use a semiparametric Gaussian copula-or nonparanormal- for high dimensional inference. Just as additive models extend linear models by replacing linear functions with a set of one-dimensional smooth functions, the nonparanormal extends the normal by transforming the variables by smooth functions. We derive a method for estimating the nonparanormal, study the method's theoretical properties, and show that it works well in many examples.",,,,,"Liu, Han/P-7105-2018",,,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2295,2328,,,,,,,,,,,,,,,,WOS:000272346400005,0
J,"Quadrianto, N; Smola, AJ; Caetano, TS; Le, QV",,,,"Quadrianto, Novi; Smola, Alex J.; Caetano, Tiberio S.; Le, Quoc V.",,,Estimating Labels from Label Proportions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.",,,,,,"Quadrianto, Novi/0000-0001-8819-306X",,,,,,,,,,,,,1532-4435,,,,,OCT,2009,10,,,,,,2349,2374,,,,,,,,,,,,,,,,WOS:000272346400007,0
J,"Gorissen, D; Dhaene, T; De Turck, F",,,,"Gorissen, Dirk; Dhaene, Tom; De Turck, Filip",,,Evolutionary Model Type Selection for Global Surrogate Modeling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Due to the scale and computational complexity of currently used simulation codes, global surrogate (metamodels) models have become indispensable tools for exploring and understanding the design space. Due to their compact formulation they are cheap to evaluate and thus readily facilitate visualization, design space exploration, rapid prototyping, and sensitivity analysis. They can also be used as accurate building blocks in design packages or larger simulation environments. Consequently, there is great interest in techniques that facilitate the construction of such approximation models while minimizing the computational cost and maximizing model accuracy. Many surrogate model types exist ( Support Vector Machines, Kriging, Neural Networks, etc.) but no type is optimal in all circumstances. Nor is there any hard theory available that can help make this choice. In this paper we present an automatic approach to the model type selection problem. We describe an adaptive global surrogate modeling environment with adaptive sampling, driven by speciated evolution. Different model types are evolved cooperatively using a Genetic Algorithm ( heterogeneous evolution) and compete to approximate the iteratively selected data. In this way the optimal model type and complexity for a given data set or simulation code can be dynamically determined. Its utility and performance is demonstrated on a number of problems where it outperforms traditional sequential execution of each model type.",,,,,"Dhaene, Tom/A-4541-2009","Dhaene, Tom/0000-0003-2899-4636; De Turck, Filip/0000-0003-4824-1199",,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2039,2078,,,,,,,,,,,,,,,,WOS:000272346100003,0
J,"Dugas, C; Bengio, Y; Belisle, F; Nadeau, C; Garcia, R",,,,"Dugas, Charles; Bengio, Yoshua; Belisle, Francois; Nadeau, Claude; Garcia, Rene",,,Incorporating Functional Knowledge in Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz(1) functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the a priori constraints.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2009,10,,,,,,1239,1262,,,,,,,,,,,,,,,,WOS:000270824900002,0
J,"Zhang, T",,,,"Zhang, Tong",,,On the Consistency of Feature Selection using Greedy Least Squares Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches infinity. The condition is identical to a corresponding condition for Lasso. Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefficient is larger than a constant times the noise level. In comparison, Lasso may require the coefficients to be larger than O(root s) times the noise level in the worst case, where s is the number of nonzero coefficients.",,,,,"Zhang, Tong/HGC-1090-2022",,,,,,,,,,,,,,1532-4435,,,,,MAR,2009,10,,,,,,555,568,,,,,,,,,,,,,,,,WOS:000270824500001,0
J,"Arlot, S; Massart, P",,,,"Arlot, Sylvain; Massart, Pascal",,,Data-driven Calibration of Penalties for Least-Squares Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Penalization procedures often suffer from their dependence on multiplying factors, whose optimal values are either unknown or hard to estimate from data. We propose a completely data-driven calibration algorithm for these parameters in the least-squares regression framework, without assuming a particular shape for the penalty. Our algorithm relies on the concept of minimal penalty, recently introduced by Birge and Massart (2007) in the context of penalized least squares for Gaussian homoscedastic regression. On the positive side, the minimal penalty can be evaluated from the data themselves, leading to a data-driven estimation of an optimal penalty which can be used in practice; on the negative side, their approach heavily relies on the homoscedastic Gaussian nature of their stochastic framework. The purpose of this paper is twofold: stating a more general heuristics for designing a data-driven penalty (the slope heuristics) and proving that it works for penalized least-squares regression with a random design, even for heteroscedastic non-Gaussian data. For technical reasons, some exact mathematical results will be proved only for regressogram bin-width selection. This is at least a first step towards further results, since the approach and the method that we use are indeed general.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,245,279,,,,,,,,,,,,,,,,WOS:000270824200006,0
J,"Krupka, E; Navot, A; Tishby, N",,,,"Krupka, Eyal; Navot, Amir; Tishby, Naftali",,,Learning to Select Features using their Properties,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Feature selection is the task of choosing a small subset of features that is sufficient to predict the target labels well. Here, instead of trying to directly determine which features are better, we attempt to learn the properties of good features. For this purpose we assume that each feature is represented by a set of properties, referred to as meta-features. This approach enables prediction of the quality of features without measuring their value on the training instances. We use this ability to devise new selection algorithms that can efficiently search for new good features in the presence of a huge number of features, and to dramatically reduce the number of feature measurements needed. We demonstrate our algorithms on a handwritten digit recognition problem and a visual object category recognition problem. In addition, we show how this novel viewpoint enables derivation of better generalization bounds for the joint learning problem of selection and classification, and how it contributes to a better understanding of the problem. Specifically, in the context of object recognition, previous works showed that it is possible to find one set of features which fits most object categories (aka a universal dictionary). Here we use our framework to analyze one such universal dictionary and find that the quality of features in this dictionary can be predicted accurately by its meta-features.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,OCT,2008,9,,,,,,2349,2376,,,,,,,,,,,,,,,,WOS:000262637300011,0
J,"Koo, JY; Lee, Y; Kim, Y; Park, C",,,,"Koo, Ja-Yong; Lee, Yoonkyung; Kim, Yuwon; Park, Changyi",,,A Bahadur representation of the linear support vector machine,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefficients of the linear support vector machine. A Bahadur type representation of the coefficients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences.",,,,,"Lee, Yoonkyung/K-4360-2015","Lee, Yoonkyung/0000-0002-5756-6588; Park, Changyi/0000-0002-5210-4739",,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1343,1368,,,,,,,,,,,,,,,,WOS:000258646800003,0
J,"Claeskens, G; Croux, C; Van Kerckhoven, J",,,,"Claeskens, Gerda; Croux, Christophe; Van Kerckhoven, Johan",,,An information criterion for variable selection in support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support vector machines for classification have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the definition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same findings for comparison on some real-world benchmark data sets.",,,,,"Claeskens, Gerda/AAA-1567-2020; Croux, Christophe/E-4386-2016","Croux, Christophe/0000-0003-2912-3437; Claeskens, Gerda/0000-0002-3863-5197",,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,541,558,,,,,,,,,,,,,,,,WOS:000256642000008,0
J,"Bickel, PJ; Ritov, Y",,,,"Bickel, Peter J.; Ritov, Ya'acov",,,"Response to Mease and Wyner, evidence contrary to the statistical view of boosting, JMLR 9 : 131-156, 2008: And yet it overfits",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2008,9,,,,,,181,186,,,,,,,,,,,,,,,,WOS:000256641800006,0
J,"Audibert, JY; Bousquet, O",,,,"Audibert, Jean-Yves; Bousquet, Olivier",,,Combining PAC-Bayesian and generic chaining bounds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, first, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester ( 1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand ( see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,863,889,,,,,,,,,,,,,,,,WOS:000247002800007,0
J,"Biehl, M; Ghosh, A; Hammer, B",,,,"Biehl, Michael; Ghosh, Anarta; Hammer, Barbara",,,Dynamics and generalization ability of LVQ algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning vector quantization (LVQ) schemes constitute intuitive, powerful classification heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in high-dimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen's LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difficulty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations confirm our analytical findings.",,,,,"Biehl, Michael/B-5105-2009; Hammer, Barbara/E-8624-2010","Biehl, Michael/0000-0001-5148-4568; Hammer, Barbara/0000-0002-0935-5591",,,,,,,,,,,,,1532-4435,,,,,FEB,2007,8,,,,,,323,360,,,,,,,,,,,,,,,,WOS:000247002600007,0
J,"Kolter, JZ; Maloof, MA",,,,"Kolter, J. Zico; Maloof, Marcus A.",,,Learning to detect and classify malicious executables in the wild,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe the use of machine learning and data mining to detect and classify malicious executables as they appear in the wild. We gathered 1; 971 benign and 1; 651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the ROC curve of 0.996. Results suggest that our methodology will scale to larger collections of executables. We also evaluated how well the methods classified executables based on the function of their payload, such as opening a backdoor and mass-mailing. Areas under the ROC curve for detecting payload function were in the neighborhood of 0.9, which were smaller than those for the detection task. However, we attribute this drop in performance to fewer training examples and to the challenge of obtaining properly labeled examples, rather than to a failing of the methodology or to some inherent difficulty of the classification task. Finally, we applied detectors to 291 malicious executables discovered after we gathered our original collection, and boosted decision trees achieved a true-positive rate of 0.98 for a desired false-positive rate of 0.05. This result is particularly important, for it suggests that our methodology could be used as the basis for an operational system for detecting previously undiscovered malicious executables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2721,2744,,,,,,,,,,,,,,,,WOS:000245390800009,0
J,"Lippert, RA; Rifkin, RM",,,,"Lippert, Ross A.; Rifkin, Ryan M.",,,Infinite-sigma limits for Tikhonov regularization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of Tikhonov regularization with a general convex loss function: this formalism includes support vector machines and regularized least squares. For a family of kernels that includes the Gaussian, parameterized by a bandwidth parameter sigma, we characterize the limiting solution as sigma -> infinity. In particular, we show that if we set the regularization parameter lambda = (lambda) over tilde sigma(-2p), the regularization term of the Tikhonov problem tends to an indicator function on polynomials of degree [p] (with residual regularization in the case where p is an element of Z). The proof rests on two key ideas: epi-convergence, a notion of functional convergence under which limits of minimizers converge to minimizers of limits, and a value-based formulation of learning, where we work with regularization on the function output values (y) as opposed to the function expansion coefficients in the RKHS. Our result generalizes and unifies previous results in this area.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2006,7,,,,,,855,876,,,,,,,,,,,,,,,,WOS:000240173400006,0
J,"Lafferty, J; Lebanon, G",,,,"Lafferty, J; Lebanon, G",,,Diffusion kernels on statistical manifolds,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JAN,2005,6,,,,,,129,163,,,,,,,,,,,,,,,,WOS:000236328800005,0
J,"Koivisto, M; Sood, K",,,,"Koivisto, M; Sood, K",,,Exact Bayesian structure discovery in Bayesian networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Learning a Bayesian network structure from data is a well-motivated but computationally hard task. We present an algorithm that computes the exact posterior probability of a subnetwork, e. g., a directed edge; a modified version of the algorithm finds one of the most probable network structures. This algorithm runs in time O(n2(n) + n(k+1)C(m)), where n is the number of network variables, k is a constant maximum in-degree, and C(m) is the cost of computing a single local marginal conditional likelihood for m data instances. This is the first algorithm with less than super-exponential complexity with respect to n. Exact computation allows us to tackle complex cases where existing Monte Carlo methods and local search procedures potentially fail. We show that also in domains with a large number of variables, exact computation is feasible, given suitable a priori restrictions on the structures; combining exact and inexact methods is also possible. We demonstrate the applicability of the presented algorithm on four synthetic data sets with 17, 22, 37, and 100 variables.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,MAY,2004,5,,,,,,549,573,,,,,,,,,,,,,,,,WOS:000236327500005,0
J,"Collobert, R; Bengio, S",,,,"Collobert, R; Bengio, S",,,SVMTorch: Support vector machines for large-scale regression problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Support Vector Machines (SVMs) for regression problems are trained by solving a quadratic optimization problem which needs on the-order of l(2) memory and time resources to solve, where l is the number of training examples. In this paper, we propose a decomposition algorithm, SVMTorch(1), which is similar to SVM-Light proposed by Joachims (1999) for classification problems, but adapted to regression problems. With this algorithm, one can now efficiently solve large-scale regression problems (more than 20000 examples). Comparisons with Nodelib, another publicly available SVM algorithm for large-scale regression problems from Flake and Lawrence (2000) yielded significant time improvements. Finally, based on a recent paper from Lin (2000), we show that a convergence proof exists for our algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2001,1,2,,,,,143,160,,10.1162/15324430152733142,0,,,,,,,,,,,,,WOS:000173336800003,0
J,"Kara, AD; Yuksel, S",,,,"Kara, Ali Devran; Yuksel, Serdar",,,Near Optimality of Finite Memory Feedback Policies in Partially Observed Markov Decision Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In the theory of Partially Observed Markov Decision Processes (POMDPs), existence of optimal policies have in general been established via converting the original partially observed stochastic control problem to a fully observed one on the belief space, leading to a belief-MDP. However, computing an optimal policy for this fully observed model, and so for the original POMDP, using classical dynamic or linear programming methods is challenging even if the original system has finite state and action spaces, since the state space of the fully observed belief-MDP model is always uncountable. Furthermore, there exist very few rigorous value function approximation and optimal policy approximation results, as regularity conditions needed often require a tedious study involving the spaces of probability measures leading to properties such as Feller continuity. In this paper, we study a planning problem for POMDPs where the system dynamics and measurement channel model are assumed to be known. We construct an approximate belief model by discretizing the belief space using only finite window information variables. We then find optimal policies for the approximate model and we rigorously establish near optimality of the constructed finite window control policies in POMDPs under mild non-linear filter stability conditions and the assumption that the measurement and action sets are finite (and the state space is real vector valued). We also establish a rate of convergence result which relates the finite window memory size and the approximation error bound, where the rate of convergence is exponential under explicit and testable exponential filter stability conditions. While there exist many experimental results and few rigorous asymptotic convergence results, an explicit rate of convergence result is new in the literature, to our knowledge.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,,,,,,,,,,,,,,,,,,WOS:000766877400001,0
J,"Aziz, M; Kaufmann, E; Riviere, MK",,,,"Aziz, Maryam; Kaufmann, Emilie; Riviere, Marie-Karelle",,,On Multi-Armed Bandit Designs for Dose-Finding Clinical Trials,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the problem of finding the optimal dosage in early stage clinical trials through the multi-armed bandit lens. We advocate the use of the Thompson Sampling principle, a flexible algorithm that can accommodate different types of monotonicity assumptions on the toxicity and efficacy of the doses. For the simplest version of Thompson Sampling, based on a uniform prior distribution for each dose, we provide finite-time upper bounds on the number of sub-optimal dose selections, which is unprecedented for dose-finding algorithms. Through a large simulation study, we then show that variants of Thompson Sampling based on more sophisticated prior distributions outperform state-of-the-art dose identification algorithms in different types of dose-finding studies that occur in phase I or phase I/II trials.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500014,0
J,"Jiang, GX; Wang, WJ; Qian, YH; Liang, JY",,,,"Jiang, Gaoxia; Wang, Wenjian; Qian, Yuhua; Liang, Jiye",,,A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The existence of output noise will bring difficulties to supervised learning. Noise filtering, aiming to detect and remove polluted samples, is one of the main ways to deal with the noise on outputs. However, most of the filters are heuristic and could not explain the filtering influence on the generalization error (GE) bound. The hyper-parameters in various filters are specified manually or empirically, and they are usually unable to adapt to the data environment. The filter with an improper hyper-parameter may overclean, leading to a weak generalization ability. This paper proposes a unified framework of optimal sample selection (OSS) for the output noise filtering from the perspective of error bound. The covering distance filter (CDF) under the framework is presented to deal with noisy outputs in regression and ordinal classification problems. Firstly, two necessary and sufficient conditions for a fixed goodness of fit in regression are deduced from the perspective of GE bound. They provide the unified theoretical framework for determining the filtering effectiveness and optimizing the size of removed samples. The optimal sample size has the adaptability to the environmental changes in the sample size, the noise ratio, and noise variance. It offers a choice of tuning the hyper-parameter and could prevent filters from overcleansing. Meanwhile, the OSS framework can be integrated with any noise estimator and produces a new filter. Then the covering interval is proposed to separate low-noise and high-noise samples, and the effectiveness is proved in regression. The covering distance is introduced as an unbiased estimator of high noises. Further, the CDF algorithm is designed by integrating the cover distance with the OSS framework. Finally, it is verified that the CDF not only recognizes noise labels correctly but also brings down the prediction errors on real apparent age data set. Experimental results on benchmark regression and ordinal classification data sets demonstrate that the CDF outperforms the state-of-the-art filters in terms of prediction ability, noise recognition, and efficiency.",,,,,,"Jiang, Gaoxia/0000-0002-2343-1132",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500018,0
J,"Pastore, A; Gastpar, M",,,,"Pastore, Adriano; Gastpar, Michael",,,Locally Differentially-Private Randomized Response for Discrete Distribution Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a setup in which confidential i.i.d. samples X-1, ..., X-n from an unknown finite-support distribution p are passed through n copies of a discrete privatization channel (a.k.a. mechanism) producing outputs Y-1, ..., Y-n. The channel law guarantees a local differential privacy of epsilon. Subject to a prescribed privacy level epsilon, the optimal channel should be designed such that an estimate of the source distribution based on the channel outputs Y-1, ..., Y-n converges as fast as possible to the exact value p. For this purpose we study the convergence to zero of three distribution distance metrics: f-divergence, mean-squared error and total variation. We derive the respective normalized first-order terms of convergence (as n -> infinity), which for a given target privacy epsilon represent a rule-of-thumb factor by which the sample size must be augmented so as to achieve the same estimation accuracy as that of a non-randomizing channel. We formulate the privacy-fidelity trade-off problem as being that of minimizing said first-order term under a privacy constraint epsilon. We further identify a scalar quantity that captures the essence of this trade-off, and prove bounds and data-processing inequalities on this quantity. For some specific instances of the privacy-fidelity trade-off problem, we derive inner and outer bounds on the optimal trade-off curve.",,,,,"Pastore, Adriano/ABF-6786-2021","Pastore, Adriano/0000-0002-6986-6481",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687207000001,0
J,"Sun, DF; Toh, KC; Yuan, YC",,,,"Sun, Defeng; Toh, Kim-Chuan; Yuan, Yancheng",,,"Convex Clustering: Model, Theoretical Guarantee and Efficient Algorithm",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clustering is a fundamental problem in unsupervised learning. Popular methods like K-means, may suffer from poor performance as they are prone to get stuck in its local minima. Recently, the sum-of-norms (SON) model (also known as the convex clustering model) has been proposed by Pelckmans et al. (2005), Lindsten et al. (2011) and Hocking et al. (2011). The perfect recovery properties of the convex clustering model with uniformly weighted all-pairwise-differences regularization have been proved by Zhu et al. (2014) and Panahi et al. (2017). However, no theoretical guarantee has been established for the general weighted convex clustering model, where better empirical results have been observed. In the numerical optimization aspect, although algorithms like the alternating direction method of multipliers (ADMM) and the alternating minimization algorithm (AMA) have been proposed to solve the convex clustering model (Chi and Lange, 2015), it still remains very challenging to solve large-scale problems. In this paper, we establish sufficient conditions for the perfect recovery guarantee of the general weighted convex clustering model, which include and improve existing theoretical results in (Zhu et al., 2014; Panahi et al., 2017) as special cases. In addition, we develop a semismooth Newton based augmented Lagrangian method for solving large-scale convex clustering problems. Extensive numerical experiments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale problems. Moreover, the numerical results also show the superior performance and scalability of our algorithm comparing to the existing first-order methods. In particular, our algorithm is able to solve a convex clustering problem with 200,000 points in R-3 in about 6 minutes.",,,,,,"Yuan, Yancheng/0000-0002-8243-4683",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500009,0
J,"Tauzin, G; Lupo, U; Tunstall, L; Perez, JB; Caorsi, M; Medina-Mardones, AM; Dassatti, A; Hess, K",,,,"Tauzin, Guillaume; Lupo, Umberto; Tunstall, Lewis; Perez, Julian Burella; Caorsi, Matteo; Medina-Mardones, Anibal M.; Dassatti, Alberto; Hess, Kathryn",,,giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce giotto-tda, a Python library that integrates high-performance topological data analysis with machine learning via a scikit-learn {compatible API and state-of-the-art C++ implementations. The library's ability to handle various types of data is rooted in a wide range of preprocessing techniques, and its strong focus on data exploration and interpretability is aided by an intuitive plotting API. Source code, binaries, examples, and documentation can be found at https://github.com/giotto-ai/giotto-tda",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500039,0
J,"Wang, FF; Zhang, JNL; Li, YC; Deng, K; Liu, JS",,,,"Wang, Feifei; Zhang, Junni L.; Li, Yichao; Deng, Ke; Liu, Jun S.",,,Bayesian Text Classification and Summarization via A Class-Specified Topic Model,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose the class-specified topic model (CSTM) to deal with the tasks of text classification and class-specific text summarization. The model assumes that in addition to a set of latent topics that are shared across classes, there is a set of class-specific latent topics for each class. Each document is a probabilistic mixture of the class-specific topics associated with its class and the shared topics. Each class-specific or shared topic has its own probability distribution over a given dictionary. We develop a Bayesian inference of CSTM in the semisupervised scenario, with the supervised scenario as a special case. We analyze in detail the 20 Newsgroups dataset, a benchmark dataset for text classification, and demonstrate that CSTM has better performance than a two-stage approach based on latent Dirichlet allocation (LDA), several existing supervised extensions of LDA, and an L-1 penalized logistic regression. The favorable performance of CSTM is also demonstrated through Monte Carlo simulations and an analysis of the Reuters dataset.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,89,,,,,,,,,,,,,,,WOS:000663145000001,0
J,"Weis, MA; Chitta, K; Sharma, Y; Brendel, W; Bethge, M; Geiger, A; Ecker, AS",,,,"Weis, Marissa A.; Chitta, Kashyap; Sharma, Yash; Brendel, Wieland; Bethge, Matthias; Geiger, Andreas; Ecker, Alexander S.",,,Benchmarking Unsupervised Object Representations for Video Sequences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: VIMON, a video-extension of MONET, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.",,,,,"Ecker, Alexander S/A-5184-2010","Ecker, Alexander S/0000-0003-2392-5105",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700307700001,0
J,"Wilson, JT; Borovitskiy, V; Terenin, A; Mostowsky, P; Deisenroth, MP",,,,"Wilson, James T.; Borovitskiy, Viacheslav; Terenin, Alexander; Mostowsky, Peter; Deisenroth, Marc Peter",,,Pathwise Conditioning of Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.",,,,,"Borovitskiy, Viacheslav/R-7216-2017","Borovitskiy, Viacheslav/0000-0002-3539-333X",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,105,,,,,,,,,,,,,,,WOS:000663160300001,0
J,"Yang, L; Li, J; Sun, DF; Toh, KC",,,,"Yang, Lei; Li, Jia; Sun, Defeng; Toh, Kim-Chuan",,,A Fast Globally Linearly Convergent Algorithm for the Computation of Wasserstein Barycenters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of computing a Wasserstein barycenter for a set of discrete probability distributions with finite supports, which finds many applications in areas such as statistics, machine learning and image processing. When the support points of the barycenter are pre-specified, this problem can be modeled as a linear programming (LP) problem whose size can be extremely large. To handle this large-scale LP, we analyse the structure of its dual problem, which is conceivably more tractable and can be reformulated as a well-structured convex problem with 3 kinds of block variables and a coupling linear equality constraint. We then adapt a symmetric Gauss-Seidel based alternating direction method of multipliers (sGS-ADMM) to solve the resulting dual problem and establish its global convergence and global linear convergence rate. As a critical component for efficient computation, we also show how all the subproblems involved can be solved exactly and efficiently. This makes our method suitable for computing a Wasserstein barycenter on a large-scale data set, without introducing an entropy regularization term as is commonly practiced. In addition, our sGS-ADMM can be used as a subroutine in an alternating minimization method to compute a barycenter when its support points are not pre-specified. Numerical results on synthetic data sets and image data sets demonstrate that our method is highly competitive for solving large-scale Wasserstein barycenter problems, in comparison to two existing representative methods and the commercial software Gurobi.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500021,0
J,"Cruz, RMO; Hafemann, LG; Sabourin, R; Cavalcanti, GDC",,,,"Cruz, Rafael M. O.; Hafemann, Luiz G.; Sabourin, Robert; Cavalcanti, George D. C.",,,DESlib: A Dynamic ensemble selection library in Python,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) dcs, containing the implementation of dynamic classifier selection methods (DCS); (ii) des, containing the implementation of dynamic ensemble selection methods (DES); (iii) static, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page:",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300008,0
J,"Gu, B; Xian, WH; Huo, ZY; Deng, C; Huang, H",,,,"Gu, Bin; Xian, Wenhan; Huo, Zhouyuan; Deng, Cheng; Huang, Heng",,,A Unified q-Memorization Framework for Asynchronous Stochastic Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Asynchronous stochastic algorithms with various variance reduction techniques (such as SVRG, S2GD, SAGA and q-SAGA) are popular in solving large scale learning problems. Recently, Reddi et al. (2015) proposed an unified variance reduction framework (i.e., HSAG) to analyze the asynchronous stochastic gradient optimization. However, the HSAG framework cannot incorporate the S2GD technique, the analysis of the HSAG framework is limited to the SVRG and SAGA techniques on the smooth convex optimization. They did not analyze other important various variance techniques (e.g., S2GD and q-SAGA) and other important optimization problems (e.g., convex optimization with non-smooth regularization and non-convex optimization with cardinality constraint). In this paper, we bridge this gap by using an unified q-memorization framework for various variance reduction techniques (including SVRG, S2GD, SAGA, q-SAGA) to analyze asynchronous stochastic algorithms for three important optimization problems. Specifically, based on the q-memorization framework, 1) we propose an asynchronous stochastic gradient hard thresh-olding algorithm with q-memorization (AsySGHT-qM) for the non-convex optimization with cardinality constraint, and prove that the convergence rate of AsySGHT-qM before reaching the inherent error induced by gradient hard thresholding methods is geometric. 2) We propose an asynchronous stochastic proximal gradient algorithm (AsySPG-qM) for the convex optimization with non-smooth regularization, and prove that AsySPG-q M can achieve a linear convergence rate. 3) We propose an asynchronous stochastic gradient descent algorithm (AsySGD-qM) for the general non-convex optimization problem, and prove that AsySGD-qM can achieve a sublinear convergence rate to stationary points. The experimental results on various large-scale datasets confirm the fast convergence of our AsySGHT-qM, AsySPG-qM and AsySGD-qM through concrete realizations of SVRG and SAGA.",,,,,,"Deng, Cheng/0000-0003-2620-3247",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,190,,,,,,,,,,,,,,,WOS:000589998700001,0
J,"Lin, JH; Cevher, V",,,,"Lin, Junhong; Cevher, Volkan",,,Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We first investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds (up to logarithmic factor) can be retained for distributed SGM provided that the partition level is not too large. We then extend our results to spectral algorithms (SA), including kernel ridge regression (KRR), kernel principal component regression, and gradient methods. Our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed KRR and classic SGM. Moreover, even for a general non-distributed SA, they provide optimal, capacity-dependent convergence rates, for the case that the regression function may not be in the RKHS in the well-conditioned regimes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,147,,,,,,,,,,,,,,,WOS:000570103200001,0
J,"Liu, FH; Huang, XL; Gong, C; Yang, J; Li, L",,,,"Liu, Fanghui; Huang, Xiaolin; Gong, Chen; Yang, Jie; Li, Li",,,Learning Data-adaptive Non-parametric Kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we propose a data-adaptive non-parametric kernel learning framework in margin based kernel methods. In model formulation, given an initial kernel matrix, a data-adaptive matrix with two constraints is imposed in an entry-wise scheme. Learning this data-adaptive matrix in a formulation-free strategy enlarges the margin between classes and thus improves the model flexibility. The introduced two constraints are imposed either exactly (on small data sets) or approximately (on large data sets) in our model, which provides a controllable trade-off between model flexibility and complexity with theoretical demonstration. In algorithm optimization, the objective function of our learning framework is proven to be gradient-Lipschitz continuous. Thereby, kernel and classifier/regressor learning can be efficiently optimized in a unified framework via Nesterov's acceleration. For the scalability issue, we study a decomposition-based approach to our model in the large sample case. The effectiveness of this approximation is illustrated by both empirical studies and theoretical guarantees. Experimental results on various classification and regression benchmark data sets demonstrate that our non-parametric kernel learning framework achieves good performance when compared with other representative kernel learning based algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,208,,,,,,,,,,,,,,,WOS:000590010900001,0
J,"Mohammadi, R; Pratola, M; Kaptein, M",,,,"Mohammadi, Reza; Pratola, Matthew; Kaptein, Maurits",,,Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Decision trees are flexible models that are well suited for many statistical regression problems. In the Bayesian framework for regression trees, Markov Chain Monte Carlo (MCMC) search algorithms are required to generate samples of tree models according to their posterior probabilities. The critical component of such MCMC algorithms is to construct good Metropolis-Hastings steps to update the tree topology. Such algorithms frequently suffer from poor mixing and local mode stickiness; therefore, the algorithms are slow to converge. Hitherto, authors have primarily used discrete-time birth/death mechanisms for Bayesian (sums of) regression tree models to explore the tree-model space. These algorithms are efficient, in terms of computation and convergence, only if the rejection rate is low which is not always the case. We overcome this issue by developing a novel search algorithm which is based on a continuous-time birth-death Markov process. The search algorithm explores the tree-model space by jumping between parameter spaces corresponding to different tree structures. The jumps occur in continuous time corresponding to the birth-death events which are modeled as independent Poisson processes. In the proposed algorithm, the moves between models are always accepted which can dramatically improve the convergence and mixing properties of the search algorithm. We provide theoretical support of the algorithm for Bayesian regression tree models and demonstrate its performance in a simulated example.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,201,,,,,,,,,,,,,,,WOS:000590006000001,0
J,"Mooij, JM; Magliacane, S; Claassen, T",,,,"Mooij, Joris M.; Magliacane, Sara; Claassen, Tom",,,Joint Causal Inference from Multiple Contexts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.",,,,,"Magliacane, Sara/ABD-8241-2020",,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,99,,,,,,,,,,,,,,,WOS:000546144100001,0
J,"Zhang, X; Mai, Q; Zou, H",,,,"Zhang, Xin; Mai, Qing; Zou, Hui",,,The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Sufficient dimension reduction (SDR) is a very useful concept for exploratory analysis and data visualization in regression, especially when the number of covariates is large. Many SDR methods have been proposed for regression with a continuous response, where the central subspace (CS) is the target of estimation. Various conditions, such as the linearity condition and the constant covariance condition, are imposed so that these methods can estimate at least a portion of the CS. In this paper we study SDR for regression and discriminant analysis with categorical response. Motivated by the exploratory analysis and data visualization aspects of SDR, we propose a new geometric framework to reformulate the SDR problem in terms of manifold optimization and introduce a new concept called Maximum Separation Subspace (MASES). The MASES naturally preserves the sufficiency in SDR without imposing additional conditions on the predictor distribution, and directly inspires a semi-parametric estimator. Numerical studies show MASES exhibits superior performance as compared with competing SDR methods in specific settings.",,,,,"Zou, Hui/B-7023-2014","Zou, Hui/0000-0003-4798-9904",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000004,0
J,"Bun, M; Nissim, K; Stemmer, U",,,,"Bun, Mark; Nissim, Kobbi; Stemmer, Uri",,,Simultaneous Private Learning of Multiple Concepts,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate the direct-sum problem in the context of differentially private PAC learning: What is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual example consists of a domain element x labeled by k unknown concepts (c(1), ..., c(k)). The goal of a multi-learner is to output k hypotheses (h(1), ..., h(k)) that generalize the input examples. Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy of learning each hypothesis independently yields sample complexity that grows polynomially with k. For some concept classes, we give multi-learners that require fewer samples than the basic strategy. Unfortunately, however, we also give lower bounds showing that even for very simple concept classes, the sample cost of private multi-learning must grow polynomially in k.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,94,,,,,,,,,,,,,,,WOS:000470908500001,0
J,"Elsken, T; Metzen, JH; Hutter, F",,,,"Elsken, Thomas; Metzen, Jan Hendrik; Hutter, Frank",,,Neural Architecture Search: A Survey,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,55,,,,,,,,,,,,,,,WOS:000463324400001,0
J,"Grigoryeva, L; Ortega, JP",,,,"Grigoryeva, Lyudmila; Ortega, Juan-Pablo",,,Differentiable reservoir computing,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Numerous results in learning and approximation theory have evidenced the importance of differentiability at the time of countering the curse of dimensionality. In the context of reservoir computing, much effort has been devoted in the last two decades to characterize the situations in which systems of this type exhibit the so-called echo state (ESP) and fading memory (FMP) properties. These important features amount, in mathematical terms, to the existence and continuity of global reservoir system solutions. That research is complemented in this paper with the characterization of the differentiability of reservoir filters for very general classes of discrete-time deterministic inputs. This constitutes a novel strong contribution to the long line of research on the ESP and the FMP and, in particular, links to existing research on the input-dependence of the ESP. Differentiability has been shown in the literature to be a key feature in the learning of attractors of chaotic dynamical systems. A Volterra-type series representation for reservoir filters with semi -infinite discrete-time inputs is constructed in the analytic case using Taylor's theorem and corresponding approximation bounds are provided. Finally, it is shown as a corollary of these results that any fading memory filter can be uniformly approximated by a finite Volterra series with finite memory.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,179,,,,,,,,,,,,,,,WOS:000506403100019,0
J,"Grunewalder, S; Khaleghi, A",,,,"Grunewalder, Steffen; Khaleghi, Azadeh",,,Approximations of the Restless Bandit Problem,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The multi-armed restless bandit problem is studied in the case where the pay-off distributions are stationary phi-mixing. This version of the problem provides a more realistic model for most real-world applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The objective of this paper is to characterize a sub-class of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that under some conditions on the phi-mixing coefficients, a modified version of UCB can prove effective. The main challenge is that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same characteristics as those of the original bandit arms. In particular, the phi-mixing property does not necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on the pay-off distributions. Some of the proof techniques developed in this paper can be more generally used in the context of online sampling under dependence. Proposed algorithms are accompanied with corresponding regret analysis.",,,,,,"Grunewalder, Steffen/0000-0002-4017-2048; Khaleghi, Azadeh/0000-0001-8643-5416",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,14,,,,,,,,,,,,,,,WOS:000458666400001,0
J,"Sommerfeld, M; Schrieber, J; Zemel, Y; Munk, A",,,,"Sommerfeld, Max; Schrieber, Joern; Zemel, Yoav; Munk, Axel",,,Optimal Transport: Fast Probabilistic Approximation with Exact Solvers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances on finite spaces. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,105,,,,,,,,,,,,,,,WOS:000476623100001,0
J,"Chen, C; Ren, M; Zhang, M; Zhang, DB",,,,"Chen, Chen; Ren, Min; Zhang, Min; Zhang, Dabao",,,A Two-Stage Penalized Least Squares Method for Constructing Large Systems of Structural Equations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a two-stage penalized least squares method to build large systems of structural equations based on the instrumental variables view of the classical two-stage least squares method. We show that, with large numbers of endogenous and exogenous variables, the system can be constructed via consistent estimation of a set of conditional expectations at the first stage, and consistent selection of regulatory effects at the second stage. While the consistent estimation at the first stage can be obtained via the ridge regression, the adaptive lasso is employed at the second stage to achieve the consistent selection. This method is computationally fast and allows for parallel implementation. We demonstrate its effectiveness via simulation studies and real data analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,2,,,,,,,,,,,,,,,WOS:000443221600001,0
J,"Price, BS; Sherwood, B",,,,"Price, Bradley S.; Sherwood, Ben",,,A Cluster Elastic Net for Multivariate Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an L-1 penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for p >> n. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods.",,,,,"Price, Bradley/ABB-3754-2020","Price, Bradley/0000-0002-0619-3347",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,232,,,,,,,,,,,,,,,WOS:000440886400001,0
J,"Sabato, S; Hess, T",,,,"Sabato, Sivan; Hess, Tom",,,"Interactive Algorithms: Pool, Stream and Precognitive Stream",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool-based algorithms can select elements at any order, while stream-based algorithms observe elements in sequence, and can only select elements immediately after observing them. We further consider an intermediate setting, which we term precognitive stream, in which the algorithm knows in advance the identity of all the elements in the sequence, but can select them only in the order of their appearance. For all settings, we assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size, in the stream-based setting and in the precognitive stream setting. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further derive nearly matching upper and lower bounds on the gap between the two settings for the special case of active learning for binary classification.",,,,,"Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044",,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,229,,,,,,,,,,,,,,,WOS:000440885900001,0
J,"Awasthi, P; Balcan, MF; Voevodski, K",,,,"Awasthi, Pranjal; Balcan, Maria Florina; Voevodski, Konstantin",,,Local algorithms for interactive clustering,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn-changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice.,,,,,,"Voevodski, Konstantin/0000-0002-7518-8242",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000397017800001,0
J,"Jaeger, H",,,,"Jaeger, Herbert",,,Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Biological brains can learn, recognize, organize, and re-generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and focussed. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content-addressed in ways that are analog to recalling static patterns in Hop field networks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,13,,,,,,,,,,,,,,,WOS:000399838000001,0
J,"Kleindessner, M; von Luxburg, U",,,,"Kleindessner, Matthaeus; von Luxburg, Ulrike",,,Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as d(A, B) < d(C, D). For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the k-relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,52,58,,,,,,,,,,,,,,,WOS:000405991400001,0
J,"Lecue, G; Mendelson, S",,,,"Lecue, Guillaume; Mendelson, Shahar",,,Regularization and the small-ball method II: complexity dependent error rates,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study estimation properties of regularized procedures of the form (f) over capf is an element of argmin(f is an element of F) (1/N Sigma(N)(i=1) (Y-i - f(X-i))(2) + lambda psi(f)) for a convex class of functions F, regularization function psi (.) and some well chosen regularization parameter lambda, where the given data is an independent sample (X-i, Y-i)(i=1)(N). We obtain bounds on the L-2 estimation error rate that depend on the complexity of the true model F* :- {f is an element of F : psi (f) <= psi (f*)}, where f* is an element of argmin(f is an element of F) E(Y - f(X))(2) and the (X (i), Y-i)'s are independent and distributed as (X; Y). Our estimate holds under weak stochastic assumptions - one of which being a small-ball condition satisfied by F - and for rather flexible choices of regularization functions psi(.). Moreover, the result holds in the learning theory framework: we do not assume any a-priori connection between the output Y and the input X. As a proof of concept, we apply our general estimation bound to various choices of psi, for example, the l(p) and S-p-norms (for p >= 1), weak-l(p), atomic norms, max-norm and SLOPE. In many cases, the estimation rate almost coincides with the minimax rate in the class F*.",,,,,,"Mendelson, Shahar/0000-0002-5673-7576",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,146,,,,,,,,,,,,,,,WOS:000424549800001,0
J,"McMahan, HB",,,,"McMahan, H. Brendan",,,A Survey of Algorithms and Analysis for Adaptive Online Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, proxfunction or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000412482000001,0
J,"Sadeghi, K",,,,"Sadeghi, Kayvan",,,Faithfulness of Probability Distributions and Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A main question in graphical models and causal inference is whether, given a probability distribution P (which is usually an underlying distribution of data), there is a graph (or graphs) to which P is faithful. The main goal of this paper is to provide a theoretical answer to this problem. We work with general independence models, which contain probabilistic independence models as a special case. We exploit a generalization of ordering, called preordering, of the nodes of (mixed) graphs. This allows us to provide sufficient conditions for a given independence model to be Markov to a graph with the minimum possible number of edges, and more importantly, necessary and sufficient conditions for a given probability distribution to be faithful to a graph. We present our results for the general case of mixed graphs, but specialize the definitions and results to the better-known subclasses of undirected (concentration) and bidirected (covariance) graphs as well as directed acyclic graphs.",,,,,,"Sadeghi, Kayvan/0000-0001-7314-744X",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,148,,,,,,,,,,,,,,,WOS:000424550100001,0
J,"Sriperumbudur, B; Fukumizu, K; Gretton, A; Hyvarinen, A; Kumar, R",,,,"Sriperumbudur, Bharath; Fukumizu, Kenji; Gretton, Arthur; Hyvarinen, Aapo; Kumar, Revant",,,Density Estimation in Infinite Dimensional Exponential Families,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider an infinite dimensional exponential family P of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space and show it to be quite rich in the sense that a broad class of densities on R-d can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in P. Motivated by this approximation property, the paper addresses the question of estimating an unknown density p(0) through an element in P. Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between p(0) and P, do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. We propose an estimator fin based on minimizing the Fisher divergence, J(po parallel to p) between p(0) and p is an element of P, which involves solving a simple finite-dimensional linear system. When p(0) is an element of P, we show that the proposed estimator is consistent, and provide a convergence rate of n(-min{2/3, 2 beta+1/2 beta+2}) in Fisher divergence under the smoothness assumption that log p(0) is an element of R(C-beta) for some beta >= 0, where C is a certain Hilbert-Schmidt operator on H and R(C-beta) denotes the image of C-beta. We also investigate the misspecified case of p(0) is not an element of P and show that J(p(0)parallel to(p) over cap (n)) -> inf(p is an element of P) J(p(0)parallel to p) as n -> infinity, and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non-parametric kernel density estimator, and that the advantage of the proposed estimator grows as d increases.",,,,,,"Fukumizu, Kenji/0000-0002-3488-2625; Gretton, Arthur/0000-0003-3169-7624",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000405991200001,0
J,"Tasche, D",,,,"Tasche, Dirk",,,Fisher Consistency for Prior Probability Shift,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test data sets under prior probability and more general data set shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Count, EM-algorithm and CDE-Iterate. We find that Adjusted Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities.",,,,,,"Tasche, Dirk/0000-0002-2750-2970",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,32,,,,,,,,,,,,,,,WOS:000412485000001,0
J,"Tran, AC; Dietrich, J; Guesgen, HW; Marsland, S",,,,"Tran, An C.; Dietrich, Jens; Guesgen, Hans W.; Marsland, Stephen",,,Parallel Symmetric Class Expression Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added bene fit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers.",,,,,,"Dietrich, Jens/0000-0001-9019-6550; Guesgen, Hans Werner/0000-0002-8160-5946",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000412056600001,0
J,"Zhang, YC; Xiao, L",,,,"Zhang, Yuchen; Xiao, Lin",,,Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variables. An extrapolation step on the primal variables is performed to obtain accelerated convergence rate. We also develop a minibatch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.",,,,,"Zhang, Yuchen/GYI-8858-2022",,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,1,,,,,,,,,,,,,,,WOS:000412063100001,0
J,"Ahmed, B; Thesen, T; Blckmon, KE; Kuzniekcy, R; Devinsky, O; Brodley, CE",,,,"Ahmed, Bilala; Thesen, Thomas; Blckmon, Karen E.; Kuzniekcy, Ruben; Devinsky, Orrin; Brodley, Carla E.",,,Decrypting Cryptogenic Epilepsy: Semi-supervised Hierarchical Conditional Random Fields For Detecting Cortical Lesions In MRI-Negative Patients,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Focal cortical dysplasia (FCD) is the most common cause of pediatric epilepsy and the third most common cause in adults with treatment-resistant epilepsy. Surgical resection of the lesion is the most effective treatment to stop seizures. Technical advances in MRI have revolutionized the diagnosis of FCD, leading to high success rates for resective surgery. However, 45% of histologically confirmed FCD patients have normal MRIs (MRI-negative). Without a visible lesion, the success rate of surgery drops from 66% to 29%. In this work, we cast the problem of detecting potential FCD lesions using MRI scans of MRI-negative patients in an image segmentation framework based on hierarchical conditional random fields (HCRF). We use surface based morphometry to model the cortical surface as a two-dimensional surface which is then segmented at multiple scales to extract superpixels of different sizes. Each superpixel is assigned an outlier score by comparing it to a control population. The lesion is detected by fusing the outlier probabilities across multiple scales using a tree-structured HCRF. The proposed method achieves a higher detection rate, with superior recall and precision on a sample of twenty MRI-negative FCD patients as compared to a baseline across four morphological features and their combinations.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,112,,,,,,,,,,,,,,,WOS:000391549400001,0
J,"Goudie, RJB; Mukherjee, S",,,,"Goudie, Robert J. B.; Mukherjee, Sach",,,A Gibbs Sampler for Learning DAGs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose a Gibbs sampler for structure learning in directed acyclic graph (DAG) models. The standard Markov chain Monte Carlo algorithms used for learning DAGs are random-walk Metropolis-Hastings samplers. These samplers are guaranteed to converge asymptotically but often mix slowly when exploring the large graph spaces that arise in structure learning. In each step, the sampler we propose draws entire sets of parents for multiple nodes from the appropriate conditional distribution. This provides an efficient way to make large moves in graph space, permitting faster mixing whilst retaining asymptotic guarantees of convergence. The conditional distribution is related to variable selection with candidate parents playing the role of covariates or inputs. We empirically examine the performance of the sampler using several simulated and real data examples. The proposed method gives robust results in diverse settings, outperforming several existing Bayesian and frequentist methods. In addition, our empirical results shed some light on the relative merits of Bayesian and constraint-based methods for structure learning.",,,,,,"Goudie, Robert/0000-0001-9554-1499",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,30,,,,,,,,,,28331463,,,,,WOS:000391478700001,0
J,"Samo, YLK; Roberts, SJ",,,,"Samo, Yves-Laurent Kom; Roberts, Stephen J.",,,String and Membrane Gaussian Processes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we introduce a novel framework for making exact nonparametric Bayesian inference on latent functions that is particularly suitable for Big Data tasks. Firstly, we introduce a class of stochastic processes we refer to as string Gaussian processes (string GPs which are not to be mistaken for Gaussian processes operating on text). We construct string GPs so that their finite-dimensional marginals exhibit suitable local conditional independence structures, which allow for scalable, distributed, and flexible nonparametric Bayesian inference, without resorting to approximations, and while ensuring some mild global regularity constraints. Furthermore, string GP priors naturally cope with heterogeneous input data, and the gradient of the learned latent function is readily available for explanatory analysis. Secondly, we provide some theoretical results relating our approach to the standard GP paradigm. In particular, we prove that some string GPs are Gaussian processes, which provides a complementary global perspective on our framework. Finally, we derive a scalable and distributed MCMC scheme for supervised learning tasks under string GP priors. The proposed MCMC scheme has computational time complexity O(N) and memory requirement O (dN), where N is the data size and d the dimension of the input space. We illustrate the efficacy of the proposed approach on several synthetic and real-world data sets, including a data set with 6 millions input points and 8 attributes.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,131,,,,,,,,,,,,,,,WOS:000391656700001,0
J,"Wang, X; Bi, JB; Yu, SP; Sun, JW; Song, MH",,,,"Wang, Xin; Bi, Jinbo; Yu, Shipeng; Sun, Jiangwen; Song, Minghu",,,Multiplicative Multitask Feature Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate a general framework of multiplicative multitask feature learning which decomposes individual task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods can be proved to be special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effects of different regularizers. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. An efficient blockwise coordinate descent algorithm is developed suitable for solving the entire family of formulations with rigorous convergence analysis. Simulation studies have identified the statistical properties of data that would be in favor of the new formulations. Extensive empirical studies on various classification and regression benchmark data sets have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,80,,,,,,,,,,28428735,,,,,WOS:000391525500001,0
J,"Williamson, SA",,,,"Williamson, Sinead A.",,,Nonparametric Network Models for Link Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many data sets can be represented as a sequence of interactions between entities-for example communications between individuals in a social network, protein-protein interactions or DNA-protein interactions in a biological context, or vehicles' journeys between cities. In these contexts, there is often interest in making predictions about future interactions, such as who will message whom. A popular approach to network modeling in a Bayesian context is to assume that the observed interactions can be explained in terms of some latent structure. For example, traffic patterns might be explained by the size and importance of cities, and social network interactions might be explained by the social groups and interests of individuals. Unfortunately, while elucidating this structure can be useful, it often does not directly translate into an effective predictive tool. Further, many existing approaches are not appropriate for sparse networks, a class that includes many interesting real-world situations. In this paper, we develop models for sparse networks that combine structure elucidation with predictive performance. We use a Bayesian nonparametric approach, which allows us to predict interactions with entities outside our training set, and allows the both the latent dimensionality of the model and the number of nodes in the network to grow in expectation as we see more data. We demonstrate that we can capture latent structure while maintaining predictive power, and discuss possible extensions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,202,,,,,,,,,,,,,,,WOS:000391829600001,0
J,"He, YB; Jia, JZ; Yu, B",,,,"He, Yangbo; Jia, Jinzhu; Yu, Bin",,,Counting and Exploring Sizes of Markov Equivalence Classes of Directed Acyclic Graphs,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"When learning a directed acyclic graph (DAG) model via observational data, one generally cannot identify the underlying DAG, but can potentially obtain a Markov equivalence class. The size (the number of DAGs) of a Markov equivalence class is crucial to infer causal effects or to learn the exact causal DAG via further interventions. Given a set of Markov equivalence classes, the distribution of their sizes is a key consideration in developing learning methods. However, counting the size of an equivalence class with many vertices is usually computationally infeasible, and the existing literature reports the size distributions only for equivalence classes with ten or fewer vertices. In this paper, we develop a method to compute the size of a Markov equivalence class. We first show that there are five types of Markov equivalence classes whose sizes can be formulated as five functions of the number of vertices respectively. Then we introduce a new concept of a rooted sub-class. The graph representations of rooted subclasses of a Markov equivalence class are used to partition this class recursively until the sizes of all rooted subclasses can be computed via the five functions. The proposed size counting is efficient for Markov equivalence classes of sparse DAGs with hundreds of vertices. Finally, we explore the size and edge distributions of Markov equivalence classes and find experimentally that, in general, (1) most Markov equivalence classes are half completed and their average sizes are small, and (2) the sizes of sparse classes grow approximately exponentially with the numbers of vertices.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2015,16,,,,,,2589,2609,,,,,,,,,,,,,,,,WOS:000369888000008,0
J,"Gammerman, A; Vovk, V",,,,"Gammerman, Alex; Vovk, Vladimir",,,Alexey Chervonenkis's Bibliography,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,2067,2080,,,,,,,,,,,,,,,,WOS:000369887300014,0
J,"Koltchinskii, V; Xia, D",,,,"Koltchinskii, Vladimir; Xia, Dong",,,Optimal Estimation of Low Rank Density Matrices,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The density matrices are positively semi-definite Hermitian matrices of unit trace that describe the state of a quantum system. The goal of the paper is to develop minimax lower bounds on error rates of estimation of low rank density matrices in trace regression models used in quantum state tomography (in particular, in the case of Pauli measurements) with explicit dependence of the bounds on the rank and other complexity parameters. Such bounds are established for several statistically relevant distances, including quantum versions of Kullback-Leibler divergence (relative entropy distance) and of Hellinger distance (so called Bures distance), and Schatten p-norm distances. Sharp upper bounds and oracle inequalities for least squares estimator with von Neumann entropy penalization are obtained showing that minimax lower bounds are attained (up to logarithmic factors) for these distances.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2015,16,,,,,,1757,1792,,,,,,,,,,,,,,,,WOS:000369887300004,0
J,"Sabato, S; Shalev-Shwartz, S; Srebro, N; Hsu, D; Zhang, T",,,,"Sabato, Sivan; Shalev-Shwartz, Shai; Srebro, Nathan; Hsu, Daniel; Zhang, Tong",,,Learning Sparse Low-Threshold Linear Classifiers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of learning a non-negative linear classifier with a l(1)-norm of at most k, and a fixed threshold, under the hinge-loss. This problem generalizes the problem of learning a k-monotone disjunction. We prove that we can learn efficiently in this setting, at a rate which is linear in both k and the size of the threshold, and that this is the best possible rate. We provide an efficient online learning algorithm that achieves the optimal rate, and show that in the batch case, empirical risk minimization achieves this rate as well. The rates we show are tighter than the uniform convergence rate, which grows with k(2.)",,,,,"Zhang, Tong/HGC-1090-2022; Sabato, Sivan/U-4730-2017","Sabato, Sivan/0000-0002-7975-0044; Hsu, Daniel/0000-0002-3495-7113",,,,,,,,,,,,,1532-4435,,,,,JUL,2015,16,,,,,,1275,1304,,,,,,,,,,,,,,,,WOS:000369886800002,0
J,"Rakhlin, A; Sridharan, K; Tewari, A",,,,"Rakhlin, Alexander; Sridharan, Karthik; Tewari, Ambuj",,,Online Learning via Sequential Complexities,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of sequential prediction and provide tools to study the minimax value of the associated game. Classical statistical learning theory provides several useful complexity measures to study learning with i.i.d. data. Our proposed sequential complexities can be seen as extensions of these measures to the sequential setting. The developed theory is shown to yield precise learning guarantees for the problem of sequential prediction. In particular, we show necessary and sufficient conditions for online learnability in the setting of supervised learning. Several examples show the utility of our framework: we can establish learnability without having to exhibit an explicit online learning algorithm.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2015,16,,,,,,155,186,,,,,,,,,,,,,,,,WOS:000369885800001,0
J,"Moore, BL; Pyeatt, LD; Kulkarni, V; Panousis, P; Padrez, K; Doufaas, AG",,,,"Moore, Brett L.; Pyeatt, Larry D.; Kulkarni, Vivekanand; Panousis, Periklis; Padrez, Kevin; Doufaas, Anthony G.",,,Reinforcement Learning for Closed-Loop Propofol Anesthesia: A Study in Human Volunteers,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Clinical research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index of the electroencephalogram as the controlled variable. These controllers have evolved to yield patient-specific anesthesia, which is associated with improved patient outcomes. Despite progress, the problem of patient-specific anesthesia remains unsolved. A variety of factors confound good control, including variations in human physiology, imperfect measures of drug effect, and delayed, hysteretic response to drug delivery. Reinforcement learning (RL) appears to be uniquely equipped to overcome these challenges; however, the literature offers no precedent for RL in anesthesia. To begin exploring the role RL might play in improving anesthetic care, we investigated the method's application in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to performance metrics reported in the anesthesia literature, RL demonstrated patient-specific control marked by improved accuracy and stability. Furthermore, these results suggest that RL may be considered a viable alternative for solving other difficult closed-loop control problems in medicine. More rigorous clinical study, beyond the confines of controlled human volunteer studies, is needed to substantiate these findings.",,,,,"Doufas, Anthony G/L-9297-2019",,,,,,,,,,,,,,1532-4435,,,,,FEB,2014,15,,,,,,655,696,,,,,,,,,,,,,,,,WOS:000335457700010,0
J,"Ahlgren, J; Yuen, SY",,,,"Ahlgren, John; Yuen, Shiu yin",,,Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph's default) and Progol's A* search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A*, NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A* substantially sacrificed accuracy to induce faster, and one in which Progol A* was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for.",,,,,"Yuen, Shiu Yin/B-7569-2008","YUEN, Shiu Yin Kelvin/0000-0002-5889-8808",,,,,,,,,,,,,1532-4435,,,,,DEC,2013,14,,,,,,3649,3682,,,,,,,,,,,,,,,,WOS:000335457100006,0
J,"Rudin, C; Letham, B; Madigan, D",,,,"Rudin, Cynthia; Letham, Benjamin; Madigan, David",,,Learning Theory Analysis for Association Rules and Sequential Event Prediction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called sequential event prediction. In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer's online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the cold start problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classification, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an adjusted confidence measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2013,14,,,,,,3441,3492,,,,,,,,,,,,,,,,WOS:000329786900008,0
J,"Noorshams, N; Wainwright, MJ",,,,"Noorshams, Nima; Wainwright, Martin J.",,,Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP fixed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefficients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a delta-neighborhood of the unique BP fixed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefficients as a function of the desired approximation accuracy delta and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical flow estimation.",,,,,,"Wainwright, Martin J./0000-0002-8760-2236",,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2799,2835,,,,,,,,,,,,,,,,WOS:000327007400010,0
J,"Wu, W; Lu, ZD; Li, H",,,,"Wu, Wei; Lu, Zhengdong; Li, Hang",,,Learning Bilinear Model for Matching Queries and Documents,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The task of matching data from two heterogeneous domains naturally arises in various areas such as web search, collaborative filtering, and drug design. In web search, existing work has designed relevance models to match queries and documents by exploiting either user clicks or content of queries and documents. To the best of our knowledge, however, there has been little work on principled approaches to leveraging both clicks and content to learn a matching model for search. In this paper, we propose a framework for learning to match heterogeneous objects. The framework learns two linear mappings for two objects respectively, and matches them via the dot product of their images after mapping. Moreover, when different regularizations are enforced, the framework renders a rich family of matching models. With orthonormal constraints on mapping functions, the framework subsumes Partial Least Squares (PLS) as a special case. Alternatively, with a l(1) + l(2) regularization, we obtain a new model called Regularized Mapping to Latent Structures (RMLS). RMLS enjoys many advantages over PLS, including lower time complexity and easy parallelization. To further understand the matching framework, we conduct generalization analysis and apply the result to both PLS and RMLS. We apply the framework to web search and implement both PLS and RMLS using a click-through bipartite with metadata representing features of queries and documents. We test the efficacy and scalability of RMLS and PLS on large scale web search problems. The results show that both PLS and RMLS can significantly outperform baseline methods, while RMLS substantially speeds up the learning process.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2013,14,,,,,,2519,2548,,,,,,,,,,,,,,,,WOS:000327007400002,0
J,"Dhillon, PS; Foster, DP; Kakade, SM; Ungar, LH",,,,"Dhillon, Paramveer S.; Foster, Dean P.; Kakade, Sham M.; Ungar, Lyle H.",,,A Risk Comparison of Ordinary Least Squares vs Ridge Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a finite dimensional subspace (as specified by a principal component analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace. This note shows that the risk of this ordinary least squares method (PCA-OLS) is within a constant factor (namely 4) of the risk of ridge regression (RR).",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2013,14,,,,,,1505,1511,,,,,,,,,,,,,,,,WOS:000322506400002,0
J,"Slivkins, A; Radlinski, F; Gollapudi, S",,,,"Slivkins, Aleksandrs; Radlinski, Filip; Gollapudi, Sreenivas",,,Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisfied users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justifications for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2013,14,,,,,,399,436,,,,,,,,,,,,,,,,WOS:000315981900005,0
J,"Genovese, CR; Jin, JS; Wasserman, L; Yao, ZG",,,,"Genovese, Christopher R.; Jin, Jiashun; Wasserman, Larry; Yao, Zhigang",,,A Comparison of the Lasso and Marginal Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUN,2012,13,,,,,,2107,2143,,,,,,,,,,,,,,,,WOS:000307020700014,0
J,"Voevodski, K; Balcan, MF; Roglin, H; Teng, SH; Xia, Y",,,,"Voevodski, Konstantin; Balcan, Maria-Florina; Roeglin, Heiko; Teng, Shang-Hua; Xia, Yu",,,Active Clustering of Biological Sequences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s is an element of S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. Our algorithm uses an active selection strategy to choose a small set of points that we call landmarks, and considers only the distances between landmarks and other points to produce a clustering. We use our procedure to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire data set. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.",,,,,"Xia, Yu/W-6150-2019","Xia, Yu/0000-0002-5596-5518; Voevodski, Konstantin/0000-0002-7518-8242",,,,,,,,,,,,,1532-4435,,,,,JAN,2012,13,,,,,,203,225,,,,,,,,,,,,,,,,WOS:000303045100007,0
J,"Dmochowski, JP; Sajda, P; Parra, LC",,,,"Dmochowski, Jacek P.; Sajda, Paul; Parra, Lucas C.",,,"Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds",JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The presence of asymmetry in the misclassification costs or class prevalences is a common occurrence in the pattern classification domain. While much interest has been devoted to the study of cost-sensitive learning techniques, the relationship between cost-sensitive learning and the specification of the model set in a parametric estimation framework remains somewhat unclear. To that end, we differentiate between the case of the model including the true posterior, and that in which the model is misspecified. In the former case, it is shown that thresholding the maximum likelihood (ML) estimate is an asymptotically optimal solution to the risk minimization problem. On the other hand, under model misspecification, it is demonstrated that thresholded ML is suboptimal and that the risk-minimizing solution varies with the misclassification cost ratio. Moreover, we analytically show that the negative weighted log likelihood (Elkan, 2001) is a tight, convex upper bound of the empirical loss. Coupled with empirical results on several real-world data sets, we argue that weighted ML is the preferred cost-sensitive technique.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2010,11,,,,,,3313,3332,,,,,,,,,,,,,,,,WOS:000286637200002,0
J,"Bouckaert, RR; Frank, E; Hall, MA; Holmes, G; Pfahringer, B; Reutemann, P; Witten, IH",,,,"Bouckaert, Remco R.; Frank, Eibe; Hall, Mark A.; Holmes, Geoffrey; Pfahringer, Bernhard; Reutemann, Peter; Witten, Ian H.",,,WEKA-Experiences with a Java Open-Source Project,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software's functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project.",,,,,"Witten, Ian H/A-3366-2012; Frank, Eibe/A-1434-2008","Witten, Ian H/0000-0001-6428-8988; Holmes, Geoffrey/0000-0003-0433-8925; Reutemann, Peter/0000-0002-1226-0948; Frank, Eibe/0000-0001-6152-7111",,,,,,,,,,,,,1532-4435,,,,,SEP,2010,11,,,,,,2533,2541,,,,,,,,,,,,,,,,WOS:000282523400005,0
J,"Hyvarinen, A; Zhang, K; Shimizu, S; Hoyer, PO",,,,"Hyvarinen, Aapo; Zhang, Kun; Shimizu, Shohei; Hoyer, Patrik O.",,,Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregression (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR's. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure. We propose computationally efficient methods for estimating the model, as well as methods to assess the significance of the causal influences. The model is successfully applied on financial and brain imaging data.",,,,,"Hyvarinen, Aapo/E-9006-2012; Shimizu, Shohei/B-4425-2010; Ottaviano, Panfilo Andrea/AAV-4616-2020","Shimizu, Shohei/0000-0002-1931-0733; Ottaviano, Panfilo Andrea/0000-0003-3353-540X; Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,MAY,2010,11,,,,,,1709,1731,,,,,,,,,,,,,,,,WOS:000282522000005,0
J,"Tanner, B; White, A",,,,"Tanner, Brian; White, Adam",,,RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"RL-Glue is a standard, language-independent software package for reinforcement-learning experiments. The standardization provided by RL-Glue facilitates code sharing and collaboration. Code sharing reduces the need to re-engineer tasks and experimental apparatus, both common barriers to comparatively evaluating new ideas in the context of the literature. Our software features a minimalist interface and works with several languages and computing platforms. RL-Glue compatibility can be extended to any programming language that supports network socket communication. RL-Glue has been used to teach classes, to run international competitions, and is currently used by several other open-source software and hardware projects.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2009,10,,,,,,2133,2136,,,,,,,,,,,,,,,,WOS:000272346100006,0
J,"Ramon, J; Nijssen, S",,,,"Ramon, Jan; Nijssen, Siegfried",,,Polynomial-Delay Enumeration of Monotonic Graph Classes,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Algorithms that list graphs such that no two listed graphs are isomorphic, are important building blocks of systems for mining and learning in graphs. Algorithms are already known that solve this problem efficiently for many classes of graphs of restricted topology, such as trees. In this article we introduce the concept of a dense augmentation schema, and introduce an algorithm that can be used to enumerate any class of graphs with polynomial delay, as long as the class of graphs can be described using a monotonic predicate operating on a dense augmentation schema. In practice this means that this is the first enumeration algorithm that can be applied theoretically efficiently in any frequent subgraph mining algorithm, and that this algorithm generalizes to situations beyond the standard frequent subgraph mining setting.",,,,,"Ramon, Jan/E-8956-2010","Ramon, Jan/0000-0002-0558-7176",,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,907,929,,,,,,,,,,,,,,,,WOS:000270824600004,0
J,"Zakai, A; Ritov, Y",,,,"Zakai, Alon; Ritov, Ya'acov",,,Consistency and Localizability,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show that all consistent learning methods-that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X, Y)-are necessarily localizable, by which we mean that they do not significantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be defined in a non-local manner, such as support vector machines in classification and least-squares estimators in regression. Aside from showing that consistency implies a specific form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the method's global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2009,10,,,,,,827,856,,,,,,,,,,,,,,,,WOS:000270824600001,0
J,"Li, JN; Wang, ZJ",,,,"Li, Junning; Wang, Z. Jane",,,Controlling the False Discovery Rate of the Association/Causality Structure Learned with the PC Algorithm,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In real world applications, graphical statistical models are not only a tool for operations such as classification or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-specified level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-specified level, and a heuristic modification of the method is able to control the FDR more accurately around the user-specified level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,FEB,2009,10,,,,,,475,514,,,,,,,,,,,,,,,,WOS:000270824200013,0
J,"Pellet, JP; Elisseeff, A",,,,"Pellet, Jean-Philippe; Elisseeff, Andre",,,Using Markov blankets for causal structure learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children's parents (or spouses), also known as the Markov blanket of X. Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We first show an efficient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TCbw, with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on five artificial networks, we argue that Markov blanket algorithms such as TC/TCbw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2008,9,,,,,,1295,1342,,,,,,,,,,,,,,,,WOS:000258646800002,0
J,"Panait, L; Tuyls, K; Luke, S",,,,"Panait, Liviu; Tuyls, Karl; Luke, Sean",,,Theoretical advantages of lenient learners: An evolutionary game theoretic perspective,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the benefits of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insignificant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning.",,,,,"Tuyls, Karl P/Q-7328-2018","Tuyls, Karl P/0000-0001-7929-1944",,,,,,,,,,,,,1532-4435,,,,,MAR,2008,9,,,,,,423,457,,,,,,,,,,,,,,,,WOS:000256642000003,0
J,"Xu, YS; Zhang, HZ",,,,"Xu, Yuesheng; Zhang, Haizhang",,,Refinable kernels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Motivated by mathematical learning from training data, we introduce the notion of refinable kernels. Various characterizations of refinable kernels are presented. The concept of refinable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a refinable kernel that forms a Riesz basis. In particular, we characterize refinable translation invariant kernels, and refinable kernels defined by refinable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces.",,,,,"Zhang, Haizhang/GPX-1222-2022; Xu, yue/HGE-1737-2022","Zhang, Haizhang/0000-0002-8241-3145; ",,,,,,,,,,,,,1532-4435,,,,,SEP,2007,8,,,,,,2083,2120,,,,,,,,,,,,,,,,WOS:000252744600004,0
J,"Gunter, S; Schraudolph, NN; Vishwanathan, SVN",,,,"Guenter, Simon; Schraudolph, Nicol N.; Vishwanathan, S. V. N.",,,Fast iterative kernel principal component analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop gain adaptation methods that improve convergence of the kernel Hebbian algorithm (KHA) for iterative kernel PCA (Kim et al., 2005). KHA has a scalar gain parameter which is either held constant or decreased according to a predetermined annealing schedule, leading to slow convergence. We accelerate it by incorporating the reciprocal of the current estimated eigenvalues as part of a gain vector. An additional normalization term then allows us to eliminate a tuning parameter in the annealing schedule. Finally we derive and apply stochastic meta-descent (SMD) gain vector adaptation (Schraudolph, 1999, 2002) in reproducing kernel Hilbert space to further speed up convergence. Experimental results on kernel PCA and spectral clustering of USPS digits, motion capture and image denoising, and image super-resolution tasks confirm that our methods converge substantially faster than conventional KHA. To demonstrate scalability, we perform kernel PCA on the entire MNIST data set.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2007,8,,,,,,1893,1918,,,,,,,,,,,,,,,,WOS:000252744400008,0
J,"Rigollet, P",,,,"Rigollet, Philippe",,,Generalization error bounds in semi-supervised classification under the cluster assumption,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,We consider semi-supervised classification when part of the available data is unlabeled. These unlabeled data can be useful for the classification problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger ( 2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples.,,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2007,8,,,,,,1369,1392,,,,,,,,,,,,,,,,WOS:000249353700001,0
J,"Owen, AB",,,,"Owen, Art B.",,,Infinitely imbalanced logistic regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In binary classification problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the infinitely imbalanced case where one class has a finite sample size and the other class's sample size grows without bound. For logistic regression, the infinitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefficient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2007,8,,,,,,761,773,,,,,,,,,,,,,,,,WOS:000247002800002,0
J,"Garcia-Pedrajas, N; Garcia-Osorio, C; Fyfe, C",,,,"Garcia-Pedrajas, Nicolas; Garcia-Osorio, Cesar; Fyfe, Colin",,,Nonlinear boosting projections for ensemble construction,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper we propose a novel approach for ensemble construction based on the use of nonlinear projections to achieve both accuracy and diversity of individual classifiers. The proposed approach combines the philosophy of boosting, putting more effort on difficult instances, with the basis of the random subspace method. Our main contribution is that instead of using a random subspace, we construct a projection taking into account the instances which have posed most difficulties to previous classifiers. In this way, consecutive nonlinear projections are created by a neural network trained using only incorrectly classified instances. The feature subspace induced by the hidden layer of this network is used as the input space to a new classifier. The method is compared with bagging and boosting techniques, showing an improved performance on a large set of 44 problems from the UCI Machine Learning Repository. An additional study showed that the proposed approach is less sensitive to noise in the data than boosting methods.",,,,,"Garc√≠a-Osorio, C√©sar/F-4232-2011; Garcia-Pedrajas, Nicolas E./H-6806-2015","Garc√≠a-Osorio, C√©sar/0000-0002-1206-1084; Garcia-Pedrajas, Nicolas E./0000-0002-4488-6849",,,,,,,,,,,,,1532-4435,,,,,JAN,2007,8,,,,,,1,33,,,,,,,,,,,,,,,,WOS:000247002500001,0
J,"Fumera, G; Pillai, I; Roli, F",,,,"Fumera, Giorgio; Pillai, Ignazio; Roli, Fabio",,,Spam filtering based on the analysis of text information embedded into images,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In recent years anti-spam filters have become necessary tools for Internet service providers to face up to the continuously growing spam phenomenon. Current server-side anti-spam filters are made up of several modules aimed at detecting different features of spam e-mails. In particular, text categorisation techniques have been investigated by researchers for the design of modules for the analysis of the semantic content of e-mails, due to their potentially higher generalisation capability with respect to manually derived classification rules used in current server-side filters. However, very recently spammers introduced a new trick consisting of embedding the spam message into attached images, which can make all current techniques based on the analysis of digital text in the subject and body fields of e-mails ineffective. In this paper we propose an approach to anti-spam filtering which exploits the text information embedded into images sent as attachments. Our approach is based on the application of state-of-the-art text categorisation techniques to the analysis of text extracted by OCR tools from images attached to e-mails. The effectiveness of the proposed approach is experimentally evaluated on two large corpora of spam e-mails.",,,,,,"ROLI, FABIO/0000-0003-4103-9190",,,,,,,,,,,,,1532-4435,,,,,DEC,2006,7,,,,,,2699,2720,,,,,,,,,,,,,,,,WOS:000245390800008,0
J,"Zhou, J; Foster, DP; Stine, RA; Ungar, LH",,,,"Zhou, Jing; Foster, Dean P.; Stine, Robert A.; Ungar, Lyle H.",,,Streamwise feature selection,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In streamwise feature selection, new features are sequentially considered for addition to a predictive model. When the space of potential features is large, streamwise feature selection offers many advantages over traditional feature selection methods, which assume that all features are known in advance. Features can be generated dynamically, focusing the search for new features on promising subspaces, and overfitting can be controlled by dynamically adjusting the threshold for adding features to the model. In contrast to traditional forward feature selection algorithms such as stepwise regression in which at each step all possible features are evaluated and the best one is selected, streamwise feature selection only evaluates each feature once when it is generated. We describe information-investing and alpha-investing, two adaptive complexity penalty methods for streamwise feature selection which dynamically adjust the threshold on the error reduction required for adding a new feature. These two methods give false discovery rate style guarantees against overfitting. They differ from standard penalty methods such as AIC, BIC and RIC, which always drastically over- or under-fit in the limit of infinite numbers of non-predictive features. Empirical results show that streamwise regression is competitive with (on small data sets) and superior to (on large data sets) much more compute-intensive feature selection methods such as stepwise regression, and allows feature selection on problems with millions of potential features.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,SEP,2006,7,,,,,,1861,1885,,,,,,,,,,,,,,,,WOS:000245389400003,0
J,"Zanni, L; Serafini, T; Zanghirati, G",,,,"Zanni, Luca; Serafini, Thomas; Zanghirati, Gaetano",,,Parallel software for training large scale support vector machines on multiprocessor systems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Parallel software for solving the quadratic program arising in training support vector machines for classification problems is introduced. The software implements an iterative decomposition technique and exploits both the storage and the computing resources available on multiprocessor systems, by distributing the heaviest computational tasks of each decomposition iteration. Based on a wide range of recent theoretical advances, relevant decomposition issues, such as the quadratic sub-problem solution, the gradient updating, the working set selection, are systematically described and their careful combination to get an effective parallel tool is discussed. A comparison with state-of-the-art packages on benchmark problems demonstrates the good accuracy and the remarkable time saving achieved by the proposed software. Furthermore, challenging experiments on real-world data sets with millions training samples highlight how the software makes large scale standard nonlinear support vector machines effectively tractable on common multiprocessor systems. This feature is not shown by any of the available codes.",,,,,"Zanni, Luca/O-6568-2016; Zanghirati, Gaetano/F-6460-2015","Zanni, Luca/0000-0001-9471-9128; Zanghirati, Gaetano/0000-0002-5282-9158",,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1467,1492,,,,,,,,,,,,,,,,WOS:000245388800013,0
J,"Zhang, Y; Burer, S; Street, WN",,,,"Zhang, Yi; Burer, Samuel; Street, W. Nick",,,Ensemble pruning via semi-definite programming,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"An ensemble is a group of learning models that jointly solve a problem. However, the ensembles generated by existing techniques are sometimes unnecessarily large, which can lead to extra memory usage, computational costs, and occasional decreases in effectiveness. The purpose of ensemble pruning is to search for a good subset of ensemble members that performs as well as, or better than, the original ensemble. This subset selection problem is a combinatorial optimization problem and thus finding the exact optimal solution is computationally prohibitive. Various heuristic methods have been developed to obtain an approximate solution. However, most of the existing heuristics use simple greedy search as the optimization method, which lacks either theoretical or empirical quality guarantees. In this paper, the ensemble subset selection problem is formulated as a quadratic integer programming problem. By applying semi-definite programming (SDP) as a solution technique, we are able to get better approximate solutions. Computational experiments show that this SDP-based pruning algorithm outperforms other heuristics in the literature. Its application in a classifier-sharing study also demonstrates the effectiveness of the method.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,JUL,2006,7,,,,,,1315,1338,,,,,,,,,,,,,,,,WOS:000245388800007,0
J,"Brown, G; Wyatt, JL; Tino, P",,,,"Brown, G; Wyatt, JL; Tino, P",,,Managing diversity in regression ensembles,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ensembles are a widely used and effective technique in machine learning - their success is commonly attributed to the degree of disagreement, or 'diversity', within the ensemble. For ensembles where the individual estimators output crisp class labels, this 'diversity' is not well understood and remains an open research issue. For ensembles of regression estimators, the diversity can be exactly formulated in terms of the covariance between individual estimator outputs, and the optimum level is expressed in terms of a bias-variance-covariance trade-off. Despite this, most approaches to learning ensembles use heuristics to encourage the right degree of diversity. In this work we show how to explicitly control diversity through the error function. The first contribution of this paper is to show that by taking the combination mechanism for the ensemble into account we can derive an error function for each individual that balances ensemble diversity with individual accuracy. We show the relationship between this error function and an existing algorithm called negative correlation learning, which uses a heuristic penalty term added to the mean squared error function. It is demonstrated that these methods control the bias-variance-covariance trade-off systematically, and can be utilised with any estimator capable of minimising a quadratic error function, for example MLPs, or RBF networks. As a second contribution, we derive a strict upper bound on the coefficient of the penalty term, which holds for any estimator that can be cast in a generalised linear regression framework, with mild assumptions on the basis functions. Finally we present the results of an empirical study, showing significant improvements over simple ensemble learning, and finding that this technique is competitive with a variety of methods, including boosting, bagging, mixtures of experts, and Gaussian processes, on a number of tasks.",,,,,"Tino, Peter/Z-5748-2019","Wyatt, Jeremy/0000-0001-6991-356X; Tino, Peter/0000-0003-2330-128X",,,,,,,,,,,,,1532-4435,,,,,SEP,2005,6,,,,,,1621,1650,,,,,,,,,,,,,,,,WOS:000236330100013,0
J,"Bar-Hillel, AB; Hertz, T; Shental, N; Weinshall, D",,,,"Bar-Hillel, AB; Hertz, T; Shental, N; Weinshall, D",,,Learning a Mahalanobis metric from equivalence constraints,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many learning algorithms use a metric defined over the input space as a principal tool, and their performance critically depends on the quality of this metric. We address the problem of learning metrics using side-information in the form of equivalence constraints. Unlike labels, we demonstrate that this type of side-information can sometimes be automatically obtained without the need of human intervention. We show how such side-information can be used to modify the representation of the data, leading to improved clustering and classification. Specifically, we present the Relevant Component Analysis (RCA) algorithm, which is a simple and efficient algorithm for learning a Mahalanobis metric. We show that RCA is the solution of an interesting optimization problem, founded on an information theoretic basis. If dimensionality reduction is allowed within RCA, we show that it is optimally accomplished by a version of Fisher's linear discriminant that uses constraints. Moreover, under certain Gaussian assumptions, RCA can be viewed as a Maximum Likelihood estimation of the within class covariance matrix. We conclude with extensive empirical evaluations of RCA, showing its advantage over alternative methods.",,,,,"Hillel, Aharon Bar/R-2656-2016; Hertz, Tomer/S-5744-2016","Hertz, Tomer/0000-0002-0561-1578; Bar-Hillel, Aharon/0000-0002-7303-0687",,,,,,,,,,,,,1532-4435,,,,,JUN,2005,6,,,,,,937,965,,,,,,,,,,,,,,,,WOS:000236329800001,0
J,"Rudin, C; Daubechies, I; Schapire, RE",,,,"Rudin, C; Daubechies, I; Schapire, RE",,,The dynamics of AdaBoost: Cyclic behavior and convergence of margins,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In order to study the convergence properties of the AdaBoost algorithm, we reduce AdaBoost to a nonlinear iterated map and study the evolution of its weight vectors. This dynamical systems approach allows us to understand AdaBoost's convergence properties completely in certain cases; for these cases we find stable cycles, allowing us to explicitly solve for AdaBoost's output. Using this unusual technique, we are able to show that AdaBoost does not always converge to a maximum margin combined classifier, answering an open question. In addition, we show that nonoptimal AdaBoost ( where the weak learning algorithm does not necessarily choose the best weak classifier at each iteration) may fail to converge to a maximum margin classifier, even if optimal AdaBoost produces a maximum margin. Also, we show that if AdaBoost cycles, it cycles among support vectors, i.e., examples that achieve the same smallest margin.",,,,,"Daubechies, Ingrid/B-5886-2012",,,,,,,,,,,,,,1532-4435,,,,,DEC,2004,5,,,,,,1557,1595,,,,,,,,,,,,,,,,WOS:000236328500001,0
J,"Leslie, C; Kuang, R",,,,"Leslie, C; Kuang, R",,,Fast string kernels using inexact matching for protein sequences,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We describe several families of k-mer based string kernels related to the recently presented mismatch kernel and designed for use with support vector machines (SVMs) for classification of protein sequence data. These new kernels - restricted gappy kernels, substitution kernels, and wildcard kernels - are based on feature spaces indexed by k-length subsequences (k-mers) from the string alphabet Sigma. However, for all kernels we define here, the kernel value K( x, y) can be computed in O(c(K)(vertical bar x vertical bar+vertical bar y vertical bar)) time, where the constant c(K) depends on the parameters of the kernel but is independent of the size vertical bar Sigma vertical bar of the alphabet. Thus the computation of these kernels is linear in the length of the sequences, like the mismatch kernel, but we improve upon the parameter-dependent constant c(K) = k(m+1) vertical bar Sigma vertical bar(m) of the (k, m)-mismatch kernel. We compute the kernels efficiently using a trie data structure and relate our new kernels to the recently described transducer formalism. In protein classification experiments on two benchmark SCOP data sets, we show that our new faster kernels achieve SVM classification performance comparable to the mismatch kernel and the Fisher kernel derived from profile hidden Markov models, and we investigate the dependence of the kernels on parameter choice.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,NOV,2004,5,,,,,,1435,1455,,,,,,,,,,,,,,,,WOS:000236328400002,0
J,"Page, D; Srinivasan, A",,,,"Page, D; Srinivasan, A",,,ILP: A short look back and a longer look forward,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,10th International Conference on Inductive Logic Programming (ILP2000),"JUL, 2000","London, ENGLAND",,,,,"Inductive logic programming (ILP) is built on a foundation laid by research in machine learning and computational logic. Armed with this strong foundation, ILP has been applied to important and interesting problems in the life sciences, engineering and the arts. This paper begins by briefly reviewing some example applications, in order to illustrate the benefits of ILP. In turn, the applications have brought into focus the need for more research into specific topics. We enumerate and elaborate five of these: (1) novel search methods; (2) incorporation of explicit probabilities; (3) incorporation of special-purpose reasoners; (4) parallel execution using commodity components; and (5) enhanced human interaction. It is our hypothesis that progress in each of these areas can greatly improve the contributions that can be made with ILP; and that, with assistance from research workers in other areas, significant progress in each of these areas is possible.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2004,4,4,,,,,415,430,,10.1162/153244304773936009,0,,,,,,,,,,,,,WOS:000221345700002,0
J,"Higuchi, I; Eguchi, S",,,,"Higuchi, I; Eguchi, S",,,Robust principal component analysis with adaptive selection for tuning parameters,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The present paper discusses robustness against outliers in a principal component analysis (PCA). We propose a class of procedures for PCA based on the minimum psi principle, which unifies various approaches, including the classical procedure and recently proposed procedures. The reweighted matrix algorithm for off-line data and the gradient algorithm for on-line data are both investigated with respect to robustness. The reweighted matrix algorithm is shown to satisfy a desirable property with local convergence, and the on-line gradient algorithm is shown to satisfy an asymptotical stability of convergence. Some procedures in the class involve tuning parameters, which control sensitivity to outliers. We propose a shape-adaptive selection rule for tuning parameters using K-fold cross validation.",,,,,"Eguchi, Shinto/A-9103-2012; HIGUCHI, Isao/D-8996-2011","Eguchi, Shinto/0000-0002-3974-618X; ",,,,,,,,,,,,,1532-4435,,,,,MAY,2004,5,,,,,,453,471,,,,,,,,,,,,,,,,WOS:000236327500001,0
J,"Quist, M; Yona, G",,,,"Quist, M; Yona, G",,,Distributional scaling: An algorithm for structure-preserving embedding of metric and nonmetric spaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We present a novel approach for embedding general metric and nonmetric spaces into low-dimensional Euclidean spaces. As opposed to traditional multidimensional scaling techniques, which minimize the distortion of pairwise distances, our embedding algorithm seeks a low-dimensional representation of the data that preserves the structure (geometry) of the original data. The algorithm uses a hybrid criterion function that combines the pairwise distortion with what we call the geometric distortion. To assess the geometric distortion, we explore functions that reflect geometric properties. Our approach is different from the Isomap and LLE algorithms in that the discrepancy in distributional information is used to guide the embedding. We use clustering algorithms in conjunction with our embedding algorithm to direct the embedding process and improve its convergence properties. We test our method on metric and nonmetric data sets, and in the presence of noise. We demonstrate that our method preserves the structural properties of embedded data better than traditional MDS, and that its performance is robust with respect to clustering errors in the original data. Other results of the paper include accelerated algorithms for optimizing the standard MDS objective functions, and two methods for finding the most appropriate dimension in which to embed a given set of data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,APR,2004,5,,,,,,399,420J,,,,,,,,,,,,,,,,WOS:000236327400004,0
J,"Petridis, V; Kaburlasos, VG",,,,"Petridis, V; Kaburlasos, VG",,,FINkNN: A fuzzy interval number k-nearest neighbor classifier for prediction of sugar production from populations of samples,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This work introduces FINkNN, a k-nearest-neighbor classifier operating over the metric lattice of conventional interval-supported convex fuzzy sets. We show that for problems involving populations of measurements, data can be represented by fuzzy interval numbers (FINs) and we present an algorithm for constructing FINs from such populations. We then present a lattice-theoretic metric distance between FINs with arbitrary-shaped membership functions, which forms the basis for FINkNN's similarity measurements. We apply FINkNN to the task of predicting annual sugar production based on populations of measurements supplied by Hellenic Sugar Industry. We show that FINkNN improves prediction accuracy on this task, and discuss the broader scope and potential utility of these techniques.",,,,,"Kaburlasos, Vassilis/HIR-3051-2022","Kaburlasos, Vassilis/0000-0002-1639-0627",,,,,,,,,,,,,1532-4435,,,,,Jan-01,2004,4,1,,,,,17,37,,10.1162/153244304322765621,0,,,,,,,,,,,,,WOS:000221043500002,0
J,"Scheffer, T; Wrobel, S",,,,"Scheffer, T; Wrobel, S",,,Finding the most interesting patterns in a database quickly by using sequential sampling,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,18th International Conference on Machine Learning,"JUN 28-JUL 01, 2001","WILLIAMSTOWN, MA",,,,,"Many discovery problems, e.g., subgroup or association rule discovery, can naturally be cast as n-best hypotheses problems where the goal is to find the n hypotheses from a given hypothesis space that score best according to a certain utility function. We present a sampling algorithm that solves this problem by issuing a small number of database queries while guaranteeing precise bounds on the confidence and quality of solutions. Known sampling approaches have treated single hypothesis selection problems, assuming that the utility is the average (over the examples) of some function - which is not the case for many frequently used utility functions. We show that our algorithm works for all utilities that can be estimated with bounded error. We provide these error bounds and resulting worst-case sample bounds for some of the most frequently used utilities, and prove that there is no sampling algorithm for a popular class of utility functions that cannot be estimated with bounded error. The algorithm is sequential in the sense that it starts to return (or discard) hypotheses that already seem to be particularly good (or bad) after a few examples. Thus, the algorithm is almost always faster than its worst-case bounds.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,May-15,2003,3,04-May,,,,,833,862,,10.1162/jmlr.2003.3.4-5.833,0,,,,,,,,,,,,,WOS:000184926200010,0
J,"Steinwart, I",,,,"Steinwart, I",,,On the influence of the kernel on the consistency of support vector machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article we study the generalization abilities of several classifiers of support vector machine (SVM) type using a certain class of kernels that we call universal. It is shown that the soft margin algorithms with universal kernels are consistent for a large class of classification problems including some kind of noisy tasks provided that the regularization parameter is chosen well. In particular we derive a simple sufficient condition for this parameter in the case of Gaussian RBF kernels. On the one hand our considerations are based on an investigation of an approximation property-the so-called universality-of the used kernels that ensures that all continuous functions can be approximated by certain kernel expressions. This approximation property also gives a new insight into the role of kernels in these and other algorithms. On the other hand the results are achieved by a precise study of the underlying optimization problems of the classifiers. Furthermore, we show consistency for the maximal margin classifier as well as for the soft margin SVMs in the presence of large margins. In this case it turns out that also constant regularization parameters ensure consistency for the soft margin SVM's. Finally we prove that even for simple, noise free classification problems SVM's with polynomial kernels can behave arbitrarily badly.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,DEC,2001,2,1,,,,,67,93,,,,,,,,,,,,,,,,WOS:000173838200004,0
J,"Herbich, R; Graepel, T; Campbell, C",,,,"Herbich, R; Graepel, T; Campbell, C",,,Bayes point machines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Kernel-classifiers comprise a powerful class of non-linear decision functions for binary classification. The support vector machine is an example of a learning algorithm for kernel classifiers that singles out the consistent classifier with the largest margin, i.e. minimal real-valued output on the training sample, within the set of consistent hypotheses, the so-called version space. We suggest the Bayes point machine as a well-founded improvement which approximates the Bayes-optimal decision by the centre of mass of version space. We present two algorithms to stochastically approximate the centre of mass of version space: a billiard sampling algorithm and a sampling algorithm based on the well known perceptron algorithm. It is shown how both algorithms can be extended to allow for soft-boundaries in order to admit training errors. Experimentally, we find that - for the zero training error case - Bayes point machines consistently outperform support vector machines on both surrogate data and real-world benchmark data sets. In the soft-boundary/soft-maxgin case, the improvement over support vector machines is shown to be reduced. Finally, we demonstrate that the real-valued output of single Bayes points on novel test points is a valid confidence measure and leads to a steady decrease in generalisation error when used as a rejection criterion.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,AUG,2001,1,4,,,,,245,279,,10.1162/153244301753683717,0,,,,,,,,,,,,,WOS:000173337000001,0
J,"Benielli, D; Capponi, C; Koco, S; Kadri, H; Huusari, R; Bauvin, B; Laviolette, F",,,,"Benielli, Dominique; Capponi, Cecile; Koco, Sokol; Kadri, Hachem; Huusari, Riikka; Bauvin, Baptiste; Laviolette, Francois",,,Toolbox for Multimodal Learn (scikit multimodallearn),JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"scikit-multimodallearn is a Python library for multimodal supervised learning, licensed under Free BSD, and compatible with the well-known scikit-learn toolbox (Fabian Pedregosa, 2011). This paper details the content of the library, including a specific multimodal data formatting and classification and regression algorithms. Use cases and examples are also provided.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,7,,,,,,,,,,,,,,,,WOS:000752354800001,0
J,"Roy, S; Sarkar, S; Dutta, S; Ghosh, AK",,,,"Roy, Sarbojit; Sarkar, Soham; Dutta, Subhajit; Ghosh, Anil K.",,,On Generalizations of Some Distance Based Classifiers for HDLSS Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In high dimension, low sample size (HDLSS) settings, classifiers based on Euclidean distances like the nearest neighbor classifier and the average distance classifier perform quite poorly if differences between locations of the underlying populations get masked by scale differences. To rectify this problem, several modifications of these classifiers have been proposed in the literature. However, existing methods are confined to location and scale differences only, and they often fail to discriminate among populations differing outside of the first two moments. In this article, we propose some simple transformations of these classifiers resulting in improved performance even when the underlying populations have the same location and scale. We further propose a generalization of these classifiers based on the idea of grouping of variables. High-dimensional behavior of the proposed classifiers is studied theoretically. Numerical experiments with a variety of simulated examples as well as an extensive analysis of benchmark data sets from three different databases exhibit advantages of the proposed methods.",,,,,,"Sarkar, Soham/0000-0003-0412-6826",,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,41,,,,,,,,,,,,,,,,WOS:000752296500001,0
J,"Sheriff, MR; Chatterjee, D",,,,"Sheriff, Mohammed Rayyan; Chatterjee, Debasish",,,Novel Min-Max Reformulations of Linear Inverse Problems,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this article, we dwell into the class of so-called ill-posed Linear Inverse Problems (LIP) which simply refer to the task of recovering the entire signal from its relatively few random linear measurements. Such problems arise in a variety of settings with applications ranging from medical image processing, recommender systems, etc. We propose a slightly generalized version of the error constrained linear inverse problem and obtain a novel and equivalent convex-concave min-max reformulation by providing an exposition to its convex geometry. Saddle points of the min-max problem are completely characterized in terms of a solution to the LIP, and vice versa. Applying simple saddle point seeking ascend-descent type algorithms to solve the min-max problems provides novel and simple algorithms to find a solution to the LIP. Moreover, the reformulation of an LIP as the min-max problem provided in this article is crucial in developing methods to solve the dictionary learning problem with almost sure recovery constraints.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2022,23,,,,,,1,46,,,,,,,,,,,,,,,,WOS:000766881900001,0
J,"Atarashi, K; Oyama, S; Kurihara, M",,,,"Atarashi, Kyohei; Oyama, Satoshi; Kurihara, Masahito",,,Factorization Machines with Regularization for Sparse Feature Interactions,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Factorization machines (FMs) are machine learning predictive models based on second-order fea-ture interactions and FMs with sparse regularization are called sparse FMs. Such regularizations enable feature selection, which selects the most relevant features for accurate prediction, and there-fore they can contribute to the improvement of the model accuracy and interpretability. However, because FMs use second-order feature interactions, the selection of features often causes the loss of many relevant feature interactions in the resultant models. In such cases, FMs with regularization specially designed for feature interaction selection trying to achieve interaction-level sparsity may be preferred instead of those just for feature selection trying to achieve feature-level sparsity. In this paper, we present a new regularization scheme for feature interaction selection in FMs. For feature interaction selection, our proposed regularizer makes the feature interaction matrix sparse without a restriction on sparsity patterns imposed by the existing methods. We also describe efficient proximal algorithms for the proposed FMs and how our ideas can be applied or extended to feature selection and other related models such as higher-order FMs and the all-subsets model. The analysis and experimental results on synthetic and real-world datasets show the effectiveness of the proposed methods.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687041400001,0
J,"Dong, J; Tong, XT",,,,"Dong, Jing; Tong, Xin T.",,,Replica Exchange for Non-Convex Optimization,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Gradient descent (GD) is known to converge quickly for convex objective functions, but it can be trapped at local minima. On the other hand, Langevin dynamics (LD) can explore the state space and find global minima, but in order to give accurate estimates, LD needs to run with a small discretization step size and weak stochastic force, which in general slow down its convergence. This paper shows that these two algorithms can collaborate through a simple exchange mechanism, in which they swap their current positions if LD yields a lower objective function. This idea can be seen as the singular limit of the replica-exchange technique from the sampling literature. We show that this new algorithm converges to the global minimum linearly with high probability, assuming the objective function is strongly convex in a neighborhood of the unique global minimum. By replacing gradients with stochastic gradients, and adding a proper threshold to the exchange mechanism, our algorithm can also be used in online settings. We also study non-swapping variants of the algorithm, which achieve similar performance. We further verify our theoretical results through some numerical experiments, and observe superior performance of the proposed algorithm over running GD or LD alone.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687057700001,0
J,"Gu, B; Wei, XY; Gao, SQ; Xiong, ZR; Deng, C; Huang, H",,,,"Gu, Bin; Wei, Xiyuan; Gao, Shangqian; Xiong, Ziran; Deng, Cheng; Huang, Heng",,,Black-Box Reductions for Zeroth-Order Gradient Algorithms to Achieve Lower Query Complexity,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Zeroth-order (ZO) optimization has been the key technique for various machine learning applications especially for black-box adversarial attack, where models need to be learned in a gradient-free manner. Although many ZO algorithms have been proposed, the high function query complexities hinder their applications seriously. To address this challenging problem, we propose two stagewise black-box reduction frameworks for ZO algorithms under convex and non-convex settings respectively, which lower down the function query complexities of ZO algorithms. Moreover, our frameworks can directly derive the convergence results of ZO algorithms under convex and non-convex settings without extra analyses, as long as convergence results under strongly convex setting are given. To illustrate the advantages, we further study ZO-SVRG, ZO-SAGA and ZO-Varag under strongly-convex setting and use our frameworks to directly derive the convergence results under convex and non-convex settings. The function query complexities of these algorithms derived by our frameworks are lower than that of their vanilla counterparts without frameworks, or even lower than that of state-of-the-art algorithms. Finally we conduct numerical experiments to illustrate the superiority of our frameworks.",,,,,"Gao, Shangqian/AAE-5993-2022",,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000700312900001,0
J,"Hue, NTK; Chiogna, M",,,,"Hue, Nguyen Thi Kim; Chiogna, Monica",,,Structure Learning of Undirected Graphical Models for Count Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Mainly motivated by the problem of modelling biological processes underlying the basic functions of a cell -that typically involve complex interactions between genes- we present a new algorithm, called PC-LPGM, for learning the structure of undirected graphical models over discrete variables. We prove theoretical consistency of PC-LPGM in the limit of infinite observations and discuss its robustness to model misspecification. To evaluate the performance of PC-LPGM in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available, extensive simulation studies are conducted, that also allow to compare our proposal with its main competitors. A biological validation of the algorithm is presented through the analysis of two real data sets.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000656348600001,0
J,"Humbert, P; Le Bars, B; Oudre, L; Kalogeratos, A; Vayatis, N",,,,"Humbert, Pierre; Le Bars, Batiste; Oudre, Laurent; Kalogeratos, Argyris; Vayatis, Nicolas",,,Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we consider the problem of learning a graph structure from multivariate signals, known as graph signals. Such signals are multivariate observations carrying measurements corresponding to the nodes of an unknown graph, which we desire to infer. They are assumed to enjoy a sparse representation in the graph spectral domain, a feature which is known to carry information related to the cluster structure of a graph. The signals are also assumed to behave smoothly with respect to the underlying graph structure. For the graph learning problem, we propose a new optimization program to learn the Laplacian of this graph and provide two algorithms to solve it, called IGL-3SR and FGL-3SR. Based on a 3-step alternating procedure, both algorithms rely on standard minimization methods -such as manifold gradient descent or linear programming- and have lower complexity compared to state-of-the-art algorithms. While IGL-3SR ensures convergence, FGL-3SR acts as a relaxation and is significantly faster since its alternating process relies on multiple closed-form solutions. Both algorithms are evaluated on synthetic and real data. They are shown to perform as good or better than their competitors in terms of both numerical performance and scalability. Finally, we present a probabilistic interpretation of the proposed optimization program as a Factor Analysis Model.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000706884600001,0
J,"Matsuda, T; Uehara, M; Hyvarinen, A",,,,"Matsuda, Takeru; Uehara, Masatoshi; Hyvarinen, Aapo",,,Information criteria for non-normalized models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many statistical models are given in the form of non-normalized densities with an intractable normalization constant. Since maximum likelihood estimation is computationally intensive for these models, several estimation methods have been developed which do not require explicit computation of the normalization constant, such as noise contrastive estimation (NCE) and score matching. However, model selection methods for general non normalized models have not been proposed so far. In this study, we develop information criteria for non-normalized models estimated by NCE or score matching. They are approximately unbiased estimators of discrepancy measures for non-normalized models. Simulation results and applications to real data demonstrate that the proposed criteria enable selection of the appropriate non-normalized model in a data-driven manner.",,,,,,"Hyvarinen, Aapo/0000-0002-5806-4432",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000687225400001,0
J,"Metelli, AM; Pirotta, M; Calandriello, D; Restelli, M",,,,"Metelli, Alberto Maria; Pirotta, Matteo; Calandriello, Daniele; Restelli, Marcello",,,Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy-iteration algorithms. When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur. To address this issue, we consider safe policy improvements, i.e., at each iteration, we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy, until no improving policy can be found. We propose three safe policy-iteration schemas that differ in the way the next policy is chosen w.r.t. the estimated greedy policy. Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared on some chain-walk domains, the prison domain, and on the Blackjack card game.",,,,,"Metelli, Alberto Maria/AAY-5206-2020","Metelli, Alberto Maria/0000-0002-3424-5212; Restelli, Marcello/0000-0002-6322-1076",,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,97,,,,,,,,,,,,,,,WOS:000663149500001,0
J,"Parhi, R; Nowak, RD",,,,"Parhi, Rahul; Nowak, Robert D.",,,Banach Space Representer Theorems for Neural Networks and Ridge Splines,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We develop a variational framework to understand the properties of the functions learned by neural networks fit to data. We propose and study a family of continuous-domain linear inverse problems with total variation-like regularization in the Radon domain subject to data fitting constraints. We derive a representer theorem showing that finite-width, single-hidden layer neural networks are solutions to these inverse problems. We draw on many techniques from variational spline theory and so we propose the notion of polynomial ridge splines, which correspond to single-hidden layer neural networks with truncated power functions as the activation function. The representer theorem is reminiscent of the classical reproducing kernel Hilbert space representer theorem, but we show that the neural network problem is posed over a non-Hilbertian Banach space. While the learning problems are posed in the continuous-domain, similar to kernel methods, the problems can be recast as finite-dimensional neural network training problems. These neural network training problems have regularizers which are related to the well-known weight decay and path-norm regularizers. Thus, our result gives insight into functional characteristics of trained neural networks and also into the design neural network regularizers. We also show that these regularizers promote neural network solutions with desirable generalization properties.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500043,0
J,"Ye, HS; Luo, L; Zhang, ZH",,,,"Ye, Haishan; Luo, Luo; Zhang, Zhihua",,,Approximate Newton Methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Many machine learning models involve solving optimization problems. Thus, it is important to address a large-scale optimization problem in big data applications. Recently, subsampled Newton methods have emerged to attract much attention due to their efficiency at each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each iteration while commanding a high convergence rate. Other efficient stochastic second order methods have been also proposed. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and the empirical performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze both local and global convergence properties of second order methods. Accordingly, we present our theoretical results which match the empirical performance in real applications well.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,1,,,,,,,,,,,,,,,WOS:000656366000001,0
J,"Zhao, P; Wang, GH; Zhang, LJ; Zhou, ZH",,,,"Zhao, Peng; Wang, Guanghui; Zhang, Lijun; Zhou, Zhi-Hua",,,Bandit Convex Optimization in Non-stationary Environments,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Bandit Convex Optimization (BCO) is a fundamental framework for modeling sequential decision-making with partial information, where the only feedback available to the player is the one-point or two-point function values. In this paper, we investigate BCO in non-stationary environments and choose the dynamic regret as the performance measure, which is defined as the difference between the cumulative loss incurred by the algorithm and that of any feasible comparator sequence. Let T be the time horizon and P-T be the path-length of the comparator sequence that reflects the non-stationarity of environments. We propose a novel algorithm that achieves O(T-3/4(1 + P-T)(1/2)) and O(T-1/2(1 + P-T)(1/2)) dynamic regret respectively for the one-point and two-point feedback models. The latter result is optimal, matching the Omega(T-1/2(1 + P-T)(1/2)) lower bound established in this paper. Notably, our algorithm is adaptive to the non-stationary environments since it does not require prior knowledge of the path-length P-T ahead of time, which is generally unknown. We further extend the algorithm to an anytime version that does not require to know the time horizon T in advance. Moreover, we study the adaptive regret, another widely used performance measure for online learning in non-stationary environments, and design an algorithm that provably enjoys the adaptive regret guarantees for BCO problems. Finally, we present empirical studies to validate the effectiveness of the proposed approach.(1)",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,125,,,,,,,,,,,,,,,WOS:000663176700001,0
J,"Zrnic, T; Ramdas, A; Jordan, MI",,,,"Zrnic, Tijana; Ramdas, Aaditya; Jordan, Michael, I",,,Asynchronous Online Testing of Multiple Hypotheses,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of asynchronous online testing, aimed at providing control of the false discovery rate (FDR) during a continual stream of data collection and testing, where each test may be a sequential test that can start and stop at arbitrary times. This setting increasingly characterizes real-world applications in science and industry, where teams of researchers across large organizations may conduct tests of hypotheses in a decentralized manner. The overlap in time and space also tends to induce dependencies among test statistics, a challenge for classical methodology, which either assumes (overly optimistically) independence or (overly pessimistically) arbitrary dependence between test statistics. We present a general framework that addresses both of these issues via a unified computational abstraction that we refer to as conflict sets. We show how this framework yields algorithms with formal FDR guarantees under a more intermediate, local notion of dependence. We illustrate our algorithms in simulations by comparing to existing algorithms for online FDR control.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2021,22,,,,,,,,,,,,,,,,,,,,,,,WOS:000628529500033,0
J,"Ashraphijuo, M; Wang, XD",,,,"Ashraphijuo, Morteza; Wang, Xiaodong",,,Union of Low-Rank Tensor Spaces: Clustering and Completion,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We consider the problem of clustering and completing a set of tensors with missing data that are drawn from a union of low-rank tensor spaces. In the clustering problem, given a partially sampled tensor data that is composed of a number of subtensors, each chosen from one of a certain number of unknown tensor spaces, we need to group the subtensors that belong to the same tensor space. We provide a geometrical analysis on the sampling pattern and subsequently derive the sampling rate that guarantees the correct clustering under some assumptions with high probability. Moreover, we investigate the fundamental conditions for finite/unique completability for the union of tensor spaces completion problem. Both deterministic and probabilistic conditions on the sampling pattern to ensure finite/unique completability are obtained. For both the clustering and completion problems, our tensor analysis provides significantly better bound than the bound given by the matrix analysis applied to any unfolding of the tensor data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000529405000021,0
J,"Christiansen, R; Peters, J",,,,"Christiansen, Rune; Peters, Jonas",,,Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Given a response Y and a vector X = (X-1, ...,X-d) of d predictors, we investigate the problem of inferring direct causes of Y among the vector X. Models for Y that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of Y are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners.",,,,,,"Christiansen, Rune/0000-0002-8202-1176; Peters, Jonas/0000-0002-1487-7511",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000016,0
J,"Duan, LL",,,,"Duan, Leo L.",,,Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"High dimensional data often contain multiple facets, and several clustering patterns can co-exist under different variable subspaces, also known as the views. While multi-view clustering algorithms were proposed, the uncertainty quantification remains difficult - a particular challenge is in the high complexity of estimating the cluster assignment probability under each view, and sharing information among views. In this article, we propose an approximate Bayes approach - treating the similarity matrices generated over the views as rough first-stage estimates for the co-assignment probabilities; in its Kullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by the pairwise product of simplex coordinates. Interestingly, each simplex coordinate directly encodes the cluster assignment uncertainty. For multi-view clustering, we let each view draw a parameterization from a few candidates, leading to dimension reduction. With high model flexibility, the estimation can be efficiently carried out as a continuous optimization problem, hence enjoys gradient-based computation. The theory establishes the connection of this model to a random partition distribution under multiple views. Compared to single-view clustering approaches, substantially more interpretable results are obtained when clustering brains from a human traumatic brain injury study, using high-dimensional gene expression data.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000520962000013,0
J,"Henderson, P; Hu, JR; Romoff, J; Brunskill, E; Jurafsky, D; Pineau, J",,,,"Henderson, Peter; Hu, Jieru; Romoff, Joshua; Brunskill, Emma; Jurafsky, Dan; Pineau, Joelle",,,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,248,,,,,,,,,,,,,,,WOS:000608918500001,0
J,"Lin, JH; Michailidis, G",,,,"Lin, Jiahe; Michailidis, George",,,Regularized Estimation of High-dimensional Factor-Augmented Vector Autoregressive (FAVAR) Models,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR equation that captures lead-lag correlations amongst a set of observed variables X and latent factors F, and a calibration equation that relates another set of observed variables Y with F and X. The latter equation is used to estimate the factors that are subsequently used in estimating the parameters of the VAR system. The FAVAR model has become popular in applied economic research, since it can summarize a large number of variables of interest as a few factors through the calibration equation and subsequently examine their influence on core variables of primary interest through the VAR equation. However, there is increasing need for examining lead-lag relationships between a large number of time series, while incorporating information from another high-dimensional set of variables. Hence, in this paper we investigate the FAVAR model under high-dimensional scaling. We introduce an appropriate identification constraint for the model parameters, which when incorporated into the formulated optimization problem yields estimates with good statistical properties. Further, we address a number of technical challenges introduced by the fact that estimates of the VAR system model parameters are based on estimated rather than directly observed quantities. The performance of the proposed estimators is evaluated on synthetic data. Further, the model is applied to commodity prices and reveals interesting and interpretable relationships between the prices and the factors extracted from a set of global macroeconomic indicators.",,,,,,"Lin, Jiahe/0000-0001-9523-0981",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,117,,,,,,,,,,,,,,,WOS:000546631500001,0
J,"Tajbakhsh, SD; Aybat, NS; Del Castillo, E",,,,"Tajbakhsh, Sam Davanloo; Aybat, Necdet Serhat; Del Castillo, Enrique",,,On the Theoretical Guarantees for Parameter Estimation of Gaussian Random Field Models: A Sparse Precision Matrix Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Iterative methods for fitting a Gaussian Random Field (GRF) model via maximum likelihood (ML) estimation requires solving a nonconvex optimization problem. The problem is aggravated for anisotropic GRFs where the number of covariance function parameters increases with the dimension. Even evaluation of the likelihood function requires O(n(3)) floating point operations, where n denotes the number of data locations. In this paper(1), we propose a new two-stage procedure to estimate the parameters of second-order stationary GRFs. First, a convex likelihood problem regularized with a weighted l(1)-norm, utilizing the available distance information between observation locations, is solved to fit a sparse precision (inverse covariance) matrix to the observed data. Second, the parameters of the covariance function are estimated by solving a least squares problem. Theoretical error bounds for the solutions of stage I and II problems are provided, and their tightness are investigated.",,,,,"del Castillo, Enrique/A-1830-2010; Davanloo Tajbakhsh, Sam/AAB-5481-2022","del Castillo, Enrique/0000-0002-6148-5692; Davanloo Tajbakhsh, Sam/0000-0002-4776-0440",,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,217,,,,,,,,,,,,,,,WOS:000605737800001,0
J,"Tang, CY; Fang, EX; Dong, YX",,,,"Tang, Cheng Yong; Fang, Ethan X.; Dong, Yuexiao",,,High-Dimensional Interactions Detection with Sparse Principal Hessian Matrix,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In statistical learning framework with regressions, interactions are the contributions to the response variable from the products of the explanatory variables. In high-dimensional problems, detecting interactions is challenging due to combinatorial complexity and limited data information. We consider detecting interactions by exploring their connections with the principal Hessian matrix. Specifically, we propose a one-step synthetic approach for estimating the principal Hessian matrix by a penalized M-estimator. An alternating direction method of multipliers (ADMM) is proposed to efficiently solve the encountered regularized optimization problem. Based on the sparse estimator, we detect the interactions by identifying its nonzero components. Our method directly targets at the interactions, and it requires no structural assumption on the hierarchy of the interactions effects. We show that our estimator is theoretically valid, computationally efficient, and practically useful for detecting the interactions in a broad spectrum of scenarios.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2020,21,,,,,,,,,,,,,,,,,,,,,,,WOS:000513691300019,0
J,"Chazelle, B; Wang, C",,,,"Chazelle, Bernard; Wang, Chu",,,Iterated Learning in Dynamic Social Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"A classic finding by (Kalish et al., 2007) shows that no language can be learned iteratively by rational agents in a self-sustained manner. In other words, if A teaches a foreign language to B, who then teaches what she learned to C, and so on, the language will quickly get lost and agents will wind up teaching their own common native language. If so, how can linguistic novelty ever be sustained? We address this apparent paradox by considering the case of iterated learning in a social network: we show that by varying the lengths of the learning sessions over time or by keeping the networks dynamic, it is possible for iterated learning to endure forever with arbitrarily small loss.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,29,,,,,,,,,,,,,,,WOS:000458670400001,0
J,"Nguyen, LM; Nguyen, PH; Richtarik, P; Scheinberg, K; Takac, M; van Dijk, M",,,,"Nguyen, Lam M.; Phuong Ha Nguyen; Richtarik, Peter; Scheinberg, Katya; Takac, Martin; van Dijk, Marten",,,New Convergence Aspects of Stochastic Gradient Algorithms,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is violated for cases where the objective function is strongly convex. In Bottou et al. (2018), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. We show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime. We then move on to the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime in the case of diminished learning rate. It is well-known that SGD converges if a sequence of learning rates {70 satisfies 7 17t 00 and Et o 7/t2 < 00  We show the convergence of SGD for strongly convex objective function without using bounded gradient assumption when {70 is a diminishing sequence and oo. Inz_,V't 0 other words, we extend the current state-of-the-art class of learning rates satisfying the convergence of SGD.",,,,,"Richtarik, Peter/O-5797-2018; van Dijk, Marten/ABC-2807-2020; Takac, Martin/AAA-8564-2022","Takac, Martin/0000-0001-7455-2025; Nguyen, Lam/0000-0001-6083-606X; van Dijk, Marten/0000-0001-9388-8050; , Katya/0000-0003-3547-1841",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,176,,,,,,,,,,,,,,,WOS:000506403100016,0
J,"Pavlichin, DS; Jiao, JT; Weissman, T",,,,"Pavlichin, Dmitri S.; Jiao, Jiantao; Weissman, Tsachy",,,Approximate Profile Maximum Likelihood,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We propose an efficient algorithm for approximate computation of the profile maximum likelihood (PML), a variant of maximum likelihood maximizing the probability of observing a sufficient statistic rather than the empirical sample. The PML has appealing theoretical properties, but is difficult to compute exactly. Inspired by observations gleaned from exactly solvable cases, we look for an approximate PML solution, which, intuitively, clumps comparably frequent symbols into one symbol. This amounts to lower-bounding a certain matrix permanent by summing over a subgroup of the symmetric group rather than the whole group during the computation. We extensively experiment with the approximate solution, and the empirical performance of our approach is competitive and sometimes significantly better than state-of-the-art performances for various estimation problems.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,122,,,,,,,,,,,,,,,WOS:000487068900006,0
J,"Sanchez-Monedero, J; Gutierrez, PA; Perez-Ortiz, M",,,,"Sanchez-Monedero, Javier; Gutierrez, Pedro A.; Perez-Ortiz, Maria",,,ORCA: A Matlab/Octave Toolbox for Ordinal Regression,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Ordinal regression, also named ordinal classification, studies classification problems where there exist a natural order between class labels. This structured order of the labels is crucial in all steps of the learning process in order to take full advantage of the data. ORCA (Ordinal Regression and Classification Algorithms) is a Matlab/Octave framework that implements and integrates different ordinal classification algorithms and specifically designed performance metrics. The framework simplifies the task of experimental comparison to a great extent, allowing the user to: (i) describe experiments by simple configuration files; (ii) automatically run different data partitions; (iii) parallelize the executions; (iv) generate a variety of performance reports and (v) include new algorithms by using its intuitive interface.",,,,,"Sanchez-Monedero, Javier/GNH-3136-2022; Guti√©rrez, Pedro Antonio/K-6051-2014; Sanchez-Monedero, Javier/K-8621-2014; Perez-Ortiz, Maria/F-3910-2016","Sanchez-Monedero, Javier/0000-0001-8649-1709; Guti√©rrez, Pedro Antonio/0000-0002-2657-776X; Sanchez-Monedero, Javier/0000-0001-8649-1709; Perez-Ortiz, Maria/0000-0003-1302-6093",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,125,,,,,,,,,,,,,,,WOS:000487068900009,0
J,"Szabo, B; van Zanten, H",,,,"Szabo, Botond; van Zanten, Harry",,,An asymptotic analysis of distributed nonparametric methods,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We investigate and compare the fundamental performance of several distributed learning methods that have been proposed recently. We do this in the context of a distributed version of the classical signal-in-Gaussian-white-noise model, which serves as a benchmark model for studying performance in this setting. The results show how the design and tuning of a distributed method can have great impact on convergence rates and validity of uncertainty quantification. Moreover, we highlight the difficulty of designing nonparametric distributed procedures that automatically adapt to smoothness.",,,,,"Szabo, Botond/AAC-7236-2021; Szabo, Botond/AAB-1493-2020","Szabo, Botond/0000-0002-5526-8747; ",,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,87,,,,,,,,,,,,,,,WOS:000470907200001,0
J,"Zhao, YQ; Laber, EB; Ning, Y; Saha, S; Sands, BE",,,,"Zhao, Ying-Qi; Laber, Eric B.; Ning, Yang; Saha, Sumona; Sands, Bruce E.",,,Efficient augmentation and relaxation learning for individualized treatment rules using observational data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Individualized treatment rules aim to identify if, when, which, and to whom treatment should be applied. A globally aging population, rising healthcare costs, and increased access to patient-level data have created an urgent need for high-quality estimators of individualized treatment rules that can be applied to observational data. A recent and promising line of research for estimating individualized treatment rules recasts the problem of estimating an optimal treatment rule as a weighted classification problem. We consider a class of estimators for optimal treatment rules that are analogous to convex large-margin classifiers. The proposed class applies to observational data and is doubly-robust in the sense that correct specification of either a propensity or outcome model leads to consistent estimation of the optimal individualized treatment rule. Using techniques from semiparametric efficiency theory, we derive rates of convergence for the proposed estimators and use these rates to characterize the bias-variance trade-off for estimating individualized treatment rules with classification-based methods. Simulation experiments informed by these results demonstrate that it is possible to construct new estimators within the proposed framework that significantly outperform existing ones. We illustrate the proposed methods using data from a labor training program and a study of inflammatory bowel syndrome.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2019,20,,,,,,,,48,,,,,,,,,,31440118,,,,,WOS:000463321800001,0
J,"Alaa, AM; van der Schaar, M",,,,"Alaa, Ahmed M.; van der Schaar, Mihaela",,,A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike existing models, the HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning the HASMM parameters from the EHR data is achieved via a novel forward-filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward-filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. In particular, we show that using HASMMs, a patient's clinical deterioration can be predicted 8-9 hours prior to intensive care unit admission, with a 22% AUC gain compared to the Rothman index, which is the state-of-the-art critical care risk scoring technology.",,,,,,"van der schaar, Mihaela/0000-0003-3933-6049",,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,1,62,4,,,,,,,,,,,,,,,WOS:000443222000001,0
J,"Burr, M; Gao, SH; Knoll, F",,,,"Burr, Michael; Gao, Shuhong; Knoll, Fiona",,,Optimal Bounds for Johnson-Lindenstrauss Transformations,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In 1984, Johnson and Lindenstrauss proved that any finite set of data in a high-dimensional space can be projected to a lower-dimensional space while preserving the pairwise Euclidean distances between points up to a bounded relative error. If the desired dimension of the image is too small, however, Kane, Meka, and Nelson (2011) and Jayram and Woodruff (2013) proved that such a projection does not exist. In this paper, we provide a precise asymptotic threshold for the dimension of the image, above which, there exists a projection preserving the Euclidean distance, but, below which, there does not exist such a projection.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000452056600001,0
J,"Fan, JQ; Wang, WC; Zhong, YQ",,,,"Fan, Jianqing; Wang, Weichen; Zhong, Yiqiao",,,An l(infinity) Eigenvector Perturbation Bound and Its Application to Robust Covariance Estimation,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis-Kahan sin theta theorem is often used to bound the difference between the eigenvectors of a matrix A and those of a perturbed matrix (A) over tilde = A+E, in terms of l(2) norm. In this paper, we prove that when A is a low-rank and incoherent matrix, the l(infinity) norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of root d(1) or root d(2) for left and right vectors, where d(1) and d(2) are the matrix dimensions. The power of this new erturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments.",,,,,"Fan, Jianqing/B-2115-2008",,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,207,,,,,,,,,,31749664,,,,,WOS:000435627900001,0
J,"Shamir, O",,,,"Shamir, Ohad",,,Distribution-Specific Hardness of Learning Neural Networks,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Although neural networks are routinely and successfully trained in practice using simple gradient-based methods, most existing theoretical results are negative, showing that learning such networks is difficult, in a worst-case sense over all data distributions. In this paper, we take a more nuanced view, and consider whether specific assumptions on the niceness of the input distribution, or niceness of the target function (e.g. in terms of smoothness, non-degeneracy, incoherence, random choice of parameters etc.), are sufficient to guarantee learnability using gradient-based methods. We provide evidence that neither class of assumptions alone is sufficient: On the one hand, for any member of a class of nice target functions, there are difficult input distributions. On the other hand, we identify a family of simple target functions, which are difficult to learn even if the input distribution is nice. To prove our results, we develop some tools which may be of independent interest, such as extending Fourier-based hardness techniques developed in the context of statistical queries (Blum et al., 1994), from the Boolean cube to Euclidean space and to more general classes of functions.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,19,,,,,,,,,,,,,,,,,,,,,,,WOS:000444402800001,0
J,"van Rooyen, B; Williamson, RC",,,,"van Rooyen, Brendan; Williamson, Robert C.",,,A Theory of Learning with Corrupted Labels,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"It is usual in machine learning theory to assume that the training and testing sets comprise of draws from the same distribution. This is rarely, if ever, true and one must admit the presence of corruption. There are many different types of corruption that can arise and as of yet there is no general means to compare the relative ease of learning in these settings. Such results are necessary if we are to make informed economic decisions regarding the acquisition of data. Here we begin to develop an abstract framework for tackling these problems. We present a generic method for learning from a fixed, known, reconstructible corruption, along with an analyses of its statistical properties. We demonstrate the utility of our framework via concrete novel results in solving supervised learning problems wherein the labels are corrupted, such as learning with noisy labels, semi-supervised learning and learning with partial labels.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2018,18,,,,,,,,228,,,,,,,,,,,,,,,WOS:000440885500001,0
J,"Bertsimas, D; Copenhaver, MS; Mazumder, R",,,,"Bertsimas, Dimitris; Copenhaver, Martin S.; Mazumder, Rahul",,,Certifiably Optimal Low Rank Factor Analysis,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix (Sigma) by the sum of a Positive Semidefinite (PSD) low-rank component (Theta) and a diagonal matrix (Phi) (with nonnegative entries) subject to Sigma - Phi being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,29,,,,,,,,,,,,,,,WOS:000399842600001,0
J,"De Castro, Y; Espinasse, T; Rochet, P",,,,"De Castro, Yohann; Espinasse, Thibault; Rochet, Paul",,,Reconstructing Undirected Graphs from Eigenspaces,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We aim at recovering the weighted adjacency matrix W of an undirected graph from a perturbed version of its eigenspaces. This situation arises for instance when working with stationary signals on graphs or Markov chains observed at random times. Our approach relies on minimizing a cost function based on the Frobenius norm of the commutator AB BA between symmetric matrices A and B. We describe a particular framework in which we have access to an estimation of the eigenspaces and provide support selection procedures from theoretical and practical points of view. In the Erdos-Renyi model on N vertices with no self-loops, we show that identifiability (i.e., the ability to reconstruct W from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function N log N / 2. Simulated and real life numerical experiments assert our methodology.",,,,,,"De Castro, Yohann/0000-0002-9008-7474",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,1,24,,,,,,,,,,,,,,,,WOS:000405980200001,0
J,"Guo, ZC; Shi, L; Wu, Q",,,,"Guo, Zheng-Chu; Shi, Lei; Wu, Qiang",,,Learning Theory of Distributed Regression with Bias Corrected Regularization Kernel Network,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Distributed learning is an effective way to analyze big data. In distributed regression, a typical approach is to divide the big data into multiple blocks, apply a base regression algorithm on each of them, and then simply average the output functions learnt from these blocks. Since the average process will decrease the variance, not the bias, bias correction is expected to improve the learning performance if the base regression algorithm is a biased one. Regularization kernel network is an effective and widely used method for nonlinear regression analysis. In this paper we will investigate a bias corrected version of regularization kernel network. We derive the error bounds when it is applied to a single data set and when it is applied as a base algorithm in distributed regression. We show that, under certain appropriate conditions, the optimal learning rates can be reached in both situations.",,,,,"Shi, Lei/P-1989-2018; Wu, Qiang/B-1620-2008","Shi, Lei/0000-0002-9512-5273; Wu, Qiang/0000-0002-4698-6966",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,118,,,,,,,,,,,,,,,WOS:000424540500001,0
J,"Lee, JD; Lin, QH; Ma, TY; Yang, TB",,,,"Lee, Jason D.; Lin, Qihang; Ma, Tengyu; Yang, Tianbao",,,Distributed Stochastic Variance Reduced Gradient Methods by Sampling Extra Data with Replacement,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"We study the round complexity of minimizing the average of convex functions under a new setting of distributed optimization where each machine can receive two subsets of functions. The first subset is from a random partition and the second subset is randomly sampled with replacement. Under this setting, we define a broad class of distributed algorithms whose local computation can utilize both subsets and design a distributed stochastic variance reduced gradient method belonging to in this class. When the condition number of the problem is small, our method achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. When the condition number is relatively large, a lower bound is provided for the number of rounds of communication needed by any algorithm in this class. Then, we present an accelerated version of our method whose the rounds of communication matches the lower bound up to logarithmic terms, which establishes that this accelerated algorithm has the lowest round complexity among all algorithms in our class under this new setting.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,122,,,,,,,,,,,,,,,WOS:000424541500001,0
J,"Ma, YT; Zheng, T",,,,"Ma, Yuting; Zheng, Tian",,,Stabilized Sparse Online Learning for Sparse Data,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Langford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high-dimensional sparse data, however, this method suffers from slow convergence and high variance due to heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft-thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original truncated gradient SGD in terms of prediction accuracy, achieving both better sparsity and stability.",,,,,,,,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000424546100001,0
J,"Tolstikhin, I; Sriperumbudur, BK; Muandet, K",,,,"Tolstikhin, Ilya; Sriperumbudur, Bharath K.; Muandet, Krikamol",,,Minimax Estimation of Kernel Mean Embeddings,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"In this paper, we study the minimax estimation of the Bochner integral mu(k)(P) := integral x(k)(.,x) dP(x) also called as the kernel mean embedding, based on random samples drawn i.i.d. from P, where k : X x X -> R is a positive definite kernel. Various estimators ( including theempirical estimator), theta(n) of mu k (P) are studied in the literature wherein all of them satisfy parallel to(theta) over cap (n) -mu(k) (P) parallel to H-k= O-P (n-(1/2))with H-k being the reproducing kernel Hilbert space inducedby k. The main contribution of the paper is in showing that the above mentioned rate of n(1-2) is minimax in parallel to .parallel to H-k and parallel to . parallel to L-2 (R-d)-norms over the class of discrete measures andthe class of measures that has an infinitely differentiable density, with k being a continuous translation-invariant kernel on R-d. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of P ( if it exists).",,,,,,"Muandet, Krikamol/0000-0002-4182-5282",,,,,,,,,,,,,1532-4435,,,,,,2017,18,,,,,,,,,,,,,,,,,,,,,,,WOS:000412069700001,0
J,"Adam, SP; Magoulas, GD; Karras, DA; Vrahatis, MN",,,,"Adam, Stavros P.; Magoulas, George D.; Karras, Dimitrios A.; Vrahatis, Michael N.",,,Bounding the Search Space for Global Optimization of Neural Networks Learning Error: An Interval Analysis Approach,JOURNAL OF MACHINE LEARNING RESEARCH,,,,,,,,,,,,"Training a multilayer perceptron (MLP) with algorithms employing global search strategies has been an important research direction in the field of neural networks. Despite a number of significant results, an important matter concerning the bounds of the search region-typically defined as a box-where a global optimization method has to search for a potential global minimizer seems to be unresolved. The approach presented in this paper builds on interval analysis and attempts to de fine guaranteed bounds in the search space prior to applying a global search algorithm for training an MLP. These bounds depend on the machine precision and the term guaranteed denotes that the region defined surely encloses weight sets that are global minimizers of the neural network's error function. Although the solution set to the bounding problem for an MLP is in general non-convex, the paper presents the theoretical results that help deriving a box which is a convex set. This box is an outer approximation of the algebraic solutions to the interval equations resulting from the function implemented by the network nodes. An experimental study using well known benchmarks is presented in accordance with the theoretical results.",,,,,"Magoulas, George/D-5597-2014; Karras, Dimitrios Alexios/AAD-7229-2019; Vrahatis, Michael N./G-2187-2014","Magoulas, George/0000-0003-1884-0772; Karras, Dimitrios Alexios/0000-0002-2759-8482; ",,,,,,,,,,,,,1532-4435,,,,,,2016,17,,,,,,,,169,,,,,,,,,,,,,,,WOS:000391674400001,0
